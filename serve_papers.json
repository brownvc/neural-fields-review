[{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"This paper considers the problem of temporal video interpolation, where the goal is to synthesize a new video frame given its two neighbors. We propose Cross-Video Neural Representation (CURE) as the first video interpolation method based on neural fields (NF). NF refers to the recent class of methods for the neural representation of complex 3D scenes that has seen widespread success and application across computer vision. CURE represents the video as a continuous function parameterized by a coordinate-based neural network, whose inputs are the spatiotemporal coordinates and outputs are the corresponding RGB values. CURE introduces a new architecture that conditions the neural network on the input frames for imposing space-time consistency in the synthesized video. This not only improves the final interpolation quality, but also enables CURE to learn a prior across multiple videos. Experimental evaluations show that CURE achieves the state-of-the-art performance on video interpolation on several benchmark datasets.","Authors (format: First Last, First Middle Last, ...)":"Wentao Shangguan, Yu Sun, Weijie Gan, Ulugbek S. Kamilov","Bibtex (e.g. @inproceedings...)":"@article{shangguan2022cure,\n    url = {http://arxiv.org/abs/2203.00137v1},\n    month = {Feb},\n    year = {2022},\n    title = {Learning Cross-Video Neural Representations for High-Quality Frame Interpolation},\n    author = {Wentao Shangguan and Yu Sun and Weijie Gan and Ulugbek S. Kamilov}\n}","Bibtex Name":"shangguan2022cure","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/wustl-cig/CURE","Coordinates all at once":"","Data Release (link)":"https://github.com/wustl-cig/CURE","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"02/28/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"sunyu@caltech.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Dynamic/Temporal, Local Conditioning, Video Interpolation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"CURE","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2203.00137.pdf","Project webpage link":"https://www.cse.wustl.edu/~kamilov/projects/2022/cure/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/26/2022 15:20:55","Title":"Learning Cross-Video Neural Representations for High-Quality Frame Interpolation","Training time (hr)":"","UID":"395","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce a new implicit shape representation called Primary Ray-based Implicit Function (PRIF). In contrast to most existing approaches based on the signed distance function (SDF) which handles spatial locations, our representation operates on oriented rays. Specifically, PRIF is formulated to directly produce the surface hit point of a given input ray, without the expensive sphere-tracing operations, hence enabling efficient shape extraction and differentiable rendering. We demonstrate that neural networks trained to encode PRIF achieve successes in various tasks including single shape representation, category-wise shape generation, shape completion from sparse or noisy observations, inverse rendering for camera pose estimation, and neural rendering with color.","Authors (format: First Last, First Middle Last, ...)":"Brandon Yushan Feng, Yinda Zhang, Danhang Tang, Ruofei Du, Amitabh Varshney","Bibtex (e.g. @inproceedings...)":"@article{feng2022prif,\n    url = {http://arxiv.org/abs/2208.06143v1},\n    month = {Aug},\n    year = {2022},\n    title = {PRIF: Primary Ray-based Implicit Function},\n    author = {Brandon Yushan Feng and Yinda Zhang and Danhang Tang and Ruofei Du and Amitabh Varshney}\n}","Bibtex Name":"feng2022prif","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"08/12/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yfeng97@umd.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Speed & Computational Efficiency, Geometry Only, Robotics, Graphics","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"PRIF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2208.06143.pdf","Project webpage link":"https://augmentariumlab.github.io/PRIF/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/31/2022 19:27:22","Title":"PRIF: Primary Ray-based Implicit Function","Training time (hr)":"","UID":"396","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a novel neural representation for light field content that enables compact storage and easy local reconstruction with high fidelity. We use a fully-connected neural network to learn the mapping function between each light field pixel's coordinates and its corresponding color values. Since neural networks that simply take in raw coordinates are unable to accurately learn data containing fine details, we present an input transformation strategy based on the Gegenbauer polynomials, which previously showed theoretical advantages over the Fourier basis. We conduct experiments that show our Gegenbauer-based design combined with sinusoidal activation functions leads to a better light field reconstruction quality than a variety of network designs, including those with Fourier-inspired techniques introduced by prior works. Moreover, our SInusoidal Gegenbauer NETwork, or SIGNET, can represent light field scenes more compactly than the state-of-the-art compression methods while maintaining a comparable reconstruction quality. SIGNET also innately allows random access to encoded light field pixels due to its functional design. We further demonstrate that SIGNET's super-resolution capability without any additional training.","Authors (format: First Last, First Middle Last, ...)":"Brandon Yushan Feng, Amitabh Varshney","Bibtex (e.g. @inproceedings...)":"@inproceedings{feng2021signet,\n    year = {2021},\n    pages = {14224--14233},\n    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},\n    author = {Brandon Yushan Feng and Amitabh Varshney},\n    title = {{SIGNET: Efficient Neural Representation for Light Fields}}\n}","Bibtex Name":"feng2021signet","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/AugmentariumLab/SIGNET","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/21/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yfeng97@umd.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"No","Keywords":"Graphics, 2D Image Neural Fields, Compression, Image-Based Rendering, Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SIGNET","PDF link (arXiv perferred)":"https://brandonyfeng.github.io/papers/SIGNET.pdf","Project webpage link":"https://augmentariumlab.github.io/SIGNET/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/31/2022 19:29:59","Title":"SIGNET: Efficient Neural Representations for Light Fields","Training time (hr)":"","UID":"397","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Stochastic Variational Inference is a powerful framework for learning large-scale probabilistic latent variable models. However, typical assumptions on the factorization or independence  of the latent variables can substantially restrict its capacity for inference and generative modeling. A major line of active research aims at building more expressive variational models by designing deep hierarchies of interdependent latent variables. Although these models exhibit superior performance and enable richer latent representations, we show that they incur diminishing returns: adding more stochastic layers to an already very deep model yields small predictive improvement while substantially increasing the inference and training time. Moreover, the architecture for this class of models favors local interactions among the latent variables between neighboring layers when designing the conditioning factors of the involved distributions. This is the first work that proposes attention mechanisms to build more expressive variational distributions in deep probabilistic models by explicitly modeling both local and global interactions in the latent space. Specifically, we propose deep attentive variational autoencoder and test it on a variety of established datasets. We show it achieves state-of-the-art log-likelihoods while using fewer latent layers and requiring less  training time than existing models. The proposed non-local inference reduces computational footprint by alleviating the need for deep hierarchies. ","Authors (format: First Last, First Middle Last, ...)":"Ifigeneia Apostolopoulou, Ian Char, Elan Rosenfeld, Artur Dubrawski","Bibtex (e.g. @inproceedings...)":"@inproceedings{apostolopoulou2022deep,\n    author = {Ifigeneia Apostolopoulou and Ian Char and Elan Rosenfeld and Artur Dubrawski},\n    booktitle = {International Conference on Learning Representations},\n    pub_year = {2021},\n    title = {Deep Attentive Variational Inference}\n}","Bibtex Name":"apostolopoulou2022deep","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/ifiaposto/Deep_Attentive_VI","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/1/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"iapostol@andrew.cmu.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"No","Keywords":"Generative Models","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://openreview.net/pdf?id=T4-65DNlDij","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"11/1/2022 6:05:26","Title":"Deep Attentive Variational Inference ","Training time (hr)":"","UID":"398","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICLR 2022","Venue no Year":"ICLR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural radiance fields (NeRFs) have recently emerged as a promising approach for 3D reconstruction and novel view synthesis. However, NeRF-based methods encode shape, reflectance, and illumination implicitly and this makes it challenging for users to manipulate these properties in the rendered images explicitly. Existing approaches only enable limited editing of the scene and deformation of the geometry. Furthermore, no existing work enables accurate scene illumination after object deformation. In this work, we introduce SPIDR, a new hybrid neural SDF representation. SPIDR combines point cloud and neural implicit representations to enable the reconstruction of higher quality object surfaces for geometry deformation and lighting estimation. meshes and surfaces for object deformation and lighting estimation. To more accurately capture environment illumination for scene relighting, we propose a novel neural implicit model to learn environment light. To enable more accurate illumination updates after deformation, we use the shadow mapping technique to approximate the light visibility updates caused by geometry editing. We demonstrate the effectiveness of SPIDR in enabling high quality geometry editing with more accurate updates to the illumination of the scene.","Authors (format: First Last, First Middle Last, ...)":"Ruofan Liang, Jiahao Zhang, Haoda Li, Chen Yang, Yushi Guan, Nandita Vijaykumar","Bibtex (e.g. @inproceedings...)":"@article{liang2022spidr,\n    author = {Ruofan Liang and Jiahao Zhang and Haoda Li and Chen Yang and Yushi Guan and Nandita Vijaykumar},\n    title = {SPIDR: SDF-based Neural Point Fields for Illumination and Deformation},\n    year = {2022},\n    month = {Oct},\n    url = {http://arxiv.org/abs/2210.08398v2}\n}","Bibtex Name":"liang2022spidr","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/15/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"ruofan@cs.toronto.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Graphics, Material/Lighting Estimation, Editable","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SPIDR","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2210.08398.pdf","Project webpage link":"https://nexuslrf.github.io/SPIDR_webpage/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"11/1/2022 13:22:08","Title":"SPIDR: SDF-based Neural Point Fields for Illumination and Deformation","Training time (hr)":"","UID":"399","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Joint representation of geometry, colour and semantics using a 3D neural field enables accurate dense labelling from ultra-sparse interactions as a user reconstructs a scene in real-time using a handheld RGB-D sensor. Our iLabel system requires no training data, yet can densely label scenes more accurately than standard methods trained on large, expensively labelled image datasets. Furthermore, it works in an 'open set' manner, with semantic classes defined on the fly by the user.  ILabel's underlying model is a multilayer perceptron (MLP) trained from scratch in real-time to learn a joint neural scene representation. The scene model is updated and visualised in real-time, allowing the user to focus interactions to achieve efficient labelling. A room or similar scene can be accurately labelled into 10+ semantic categories with only a few tens of clicks. Quantitative labelling accuracy scales powerfully with the number of clicks, and rapidly surpasses standard pre-trained semantic segmentation methods. We also demonstrate a hierarchical labelling variant.","Authors (format: First Last, First Middle Last, ...)":"Shuaifeng Zhi, Edgar Sucar, Andre Mouton, Iain Haughton, Tristan Laidlow, Andrew J. Davison","Bibtex (e.g. @inproceedings...)":"@article{zhi2021ilabel,\n    author = {Shuaifeng Zhi and Edgar Sucar and Andre Mouton and Iain Haughton and Tristan Laidlow and Andrew J. Davison},\n    title = {ILabel: Interactive Neural Scene Labelling},\n    year = {2021},\n    month = {Nov},\n    url = {http://arxiv.org/abs/2111.14637v2}\n}","Bibtex Name":"zhi2021ilabel","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/29/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"n.tsagkas@ed.ac.uk","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Image-Based Rendering, Semantic Segmentation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"iLabel","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2111.14637.pdf","Project webpage link":"https://edgarsucar.github.io/ilabel/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=bL7RZaMhRbk&feature=youtu.be","Timestamp":"11/2/2022 5:31:08","Title":"ILabel: Interactive Neural Scene Labelling","Training time (hr)":"","UID":"400","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"General scene understanding for robotics requires flexible semantic representation, so that novel objects and structures which may not have been known at training time can be identified, segmented and grouped. We present an algorithm which fuses general learned features from a standard pre-trained network into a highly efficient 3D geometric neural field representation during real-time SLAM. The fused 3D feature maps inherit the coherence of the neural field's geometry representation. This means that tiny amounts of human labelling interacting at runtime enable objects or even parts of objects to be robustly and accurately segmented in an open set manner.","Authors (format: First Last, First Middle Last, ...)":"Kirill Mazur, Edgar Sucar, Andrew J. Davison","Bibtex (e.g. @inproceedings...)":"@article{mazur2022featurerealistic,\n    author = {Kirill Mazur and Edgar Sucar and Andrew J. Davison},\n    title = {Feature-Realistic Neural Fusion for Real-Time, Open Set Scene Understanding},\n    year = {2022},\n    month = {Oct},\n    url = {http://arxiv.org/abs/2210.03043v1}\n}","Bibtex Name":"mazur2022featurerealistic","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/06/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"n.tsagkas@ed.ac.uk","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Image-Based Rendering, Semantic Segmentation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2210.03043.pdf","Project webpage link":"https://makezur.github.io/FeatureRealisticFusion/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=ysaohKI_Pf0","Timestamp":"11/2/2022 5:36:10","Title":"Feature-Realistic Neural Fusion for Real-Time, Open Set Scene Understanding","Training time (hr)":"","UID":"401","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present Loc-NeRF, a real-time vision-based robot localization approach that combines Monte Carlo localization and Neural Radiance Fields (NeRF). Our system uses a pre-trained NeRF model as the map of an environment and can localize itself in real-time using an RGB camera as the only exteroceptive sensor onboard the robot. While neural radiance fields have seen significant applications for visual rendering in computer vision and graphics, they have found limited use in robotics. Existing approaches for NeRF-based localization require both a good initial pose guess and significant computation, making them impractical for real-time robotics applications. By using Monte Carlo localization as a workhorse to estimate poses using a NeRF map model, Loc-NeRF is able to perform localization faster than the state of the art and without relying on an initial pose estimate. In addition to testing on synthetic data, we also run our system using real data collected by a Clearpath Jackal UGV and demonstrate for the first time the ability to perform real-time global localization with neural radiance fields. We make our code publicly available at https://github.com/MIT-SPARK/Loc-NeRF.","Authors (format: First Last, First Middle Last, ...)":"Dominic Maggio, Marcus Abate, Jingnan Shi, Courtney Mario, Luca Carlone","Bibtex (e.g. @inproceedings...)":"@article{maggio2022locnerf,\n    author = {Dominic Maggio and Marcus Abate and Jingnan Shi and Courtney Mario and Luca Carlone},\n    title = {Loc-NeRF: Monte Carlo Localization using Neural Radiance Fields},\n    year = {2022},\n    month = {Sep},\n    url = {http://arxiv.org/abs/2209.09050v1}\n}","Bibtex Name":"maggio2022locnerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/MIT-SPARK/Loc-NeRF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"09/19/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"n.tsagkas@ed.ac.uk","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Robotics, Camera Parameter Estimation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Loc-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2209.09050.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=yIgl3kbpbXY","Timestamp":"11/2/2022 6:54:45","Title":"Loc-NeRF: Monte Carlo Localization using Neural Radiance Fields","Training time (hr)":"","UID":"402","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"","Authors (format: First Last, First Middle Last, ...)":"","Bibtex (e.g. @inproceedings...)":"","Bibtex Name":"","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://osrt-paper.github.io/#code","Coordinates all at once":"","Data Release (link)":"https://osrt-paper.github.io/#dataset","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"msajjadi@google.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Editable, Generalization, Global Conditioning, Local Conditioning, Object-Centric","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"OSRT","PDF link (arXiv perferred)":"https://arxiv.org/abs/2206.06922v1","Project webpage link":"https://osrt-paper.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"11/4/2022 8:51:00","Title":"","Training time (hr)":"","UID":"403","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2022","Venue no Year":"","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":""},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Videos typically record the streaming and continuous visual data as discrete consecutive frames. Since the storage cost is expensive for videos of high fidelity, most of them are stored in a relatively low resolution and frame rate. Recent works of Space-Time Video Super-Resolution (STVSR) are developed to incorporate temporal interpolation and spatial super-resolution in a unified framework. However, most of them only support a fixed up-sampling scale, which limits their flexibility and applications. In this work, instead of following the discrete representations, we propose Video Implicit Neural Representation (VideoINR), and we show its applications for STVSR. The learned implicit neural representation can be decoded to videos of arbitrary spatial resolution and frame rate. We show that VideoINR achieves competitive performances with state-of-the-art STVSR methods on common up-sampling scales and significantly outperforms prior works on continuous and out-of-training-distribution scales. Our project page is at http://zeyuan-chen.com/VideoINR/ .","Authors (format: First Last, First Middle Last, ...)":"Zeyuan Chen, Yinbo Chen, Jingwen Liu, Xingqian Xu, Vidit Goel, Zhangyang Wang, Humphrey Shi, Xiaolong Wang","Bibtex (e.g. @inproceedings...)":"@article{chen2022videoinr,\n    author = {Zeyuan Chen and Yinbo Chen and Jingwen Liu and Xingqian Xu and Vidit Goel and Zhangyang Wang and Humphrey Shi and Xiaolong Wang},\n    title = {VideoINR: Learning Video Implicit Neural Representation for Continuous Space-Time Super-Resolution},\n    year = {2022},\n    month = {Jun},\n    url = {http://arxiv.org/abs/2206.04647v1}\n}","Bibtex Name":"chen2022videoinr","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/Picsart-AI-Research/VideoINR-Continuous-Space-Time-Super-Resolution","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"06/09/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"cheng-you_lu@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Dynamic/Temporal, 2D Image Neural Fields, Compression, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"VideoINR","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2206.04647.pdf","Project webpage link":"http://zeyuan-chen.com/VideoINR/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=n0J5H-F_s0k","Timestamp":"11/4/2022 23:57:51","Title":"VideoINR: Learning Video Implicit Neural Representation for Continuous Space-Time Super-Resolution","Training time (hr)":"","UID":"404","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CP decomposition -- that factorizes tensors into rank-one components with compact vectors -- in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction (<30 min) with better rendering quality and even a smaller model size (<4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time (<10 min) and retaining a compact model size (<75 MB).","Authors (format: First Last, First Middle Last, ...)":"Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su","Bibtex (e.g. @inproceedings...)":"@article{chen2022tensorf,\n    author = {Anpei Chen and Zexiang Xu and Andreas Geiger and Jingyi Yu and Hao Su},\n    title = {TensoRF: Tensorial Radiance Fields},\n    year = {2022},\n    month = {Mar},\n    url = {http://arxiv.org/abs/2203.09517v1}\n}","Bibtex Name":"chen2022tensorf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/apchenstu/TensoRF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"03/17/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"cheng-you_lu@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Speed & Computational Efficiency, Compression, Global Conditioning, Coarse-to-Fine","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"TensoRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2203.09517.pdf","Project webpage link":"https://apchenstu.github.io/TensoRF/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=ujOMgaKV3lA&t=1s","Timestamp":"11/5/2022 0:07:16","Title":"TensoRF: Tensorial Radiance Fields","Training time (hr)":"","UID":"405","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Implicit Neural Representations (INRs) have emerged and shown their benefits over discrete representations in recent years. However, fitting an INR to the given observations usually requires optimization with gradient descent from scratch, which is inefficient and does not generalize well with sparse observations. To address this problem, most of the prior works train a hypernetwork that generates a single vector to modulate the INR weights, where the single vector becomes an information bottleneck that limits the reconstruction precision of the output INR. Recent work shows that the whole set of weights in INR can be precisely inferred without the single-vector bottleneck by gradient-based meta-learning. Motivated by a generalized formulation of gradient-based meta-learning, we propose a formulation that uses Transformers as hypernetworks for INRs, where it can directly build the whole set of INR weights with Transformers specialized as set-to-set mapping. We demonstrate the effectiveness of our method for building INRs in different tasks and domains, including 2D image regression and view synthesis for 3D objects. Our work draws connections between the Transformer hypernetworks and gradient-based meta-learning algorithms and we provide further analysis for understanding the generated INRs.","Authors (format: First Last, First Middle Last, ...)":"Yinbo Chen, Xiaolong Wang","Bibtex (e.g. @inproceedings...)":"@article{chen2022transformermetalearner,\n    author = {Yinbo Chen and Xiaolong Wang},\n    title = {Transformers as Meta-Learners for Implicit Neural Representations},\n    year = {2022},\n    month = {Aug},\n    url = {http://arxiv.org/abs/2208.02801v2}\n}","Bibtex Name":"chen2022transformermetalearner","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/yinboc/trans-inr","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"08/04/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"cheng-you_lu@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Sampling, Local Conditioning, Hypernetwork/Meta-learning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Transformer meta-learner","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2208.02801.pdf","Project webpage link":"https://yinboc.github.io/trans-inr/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"11/5/2022 0:19:26","Title":"Transformers as Meta-Learners for Implicit Neural Representations","Training time (hr)":"","UID":"406","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recent advances in radiance fields enable photorealistic rendering of static or dynamic 3D scenes, but still do not support explicit deformation that is used for scene manipulation or animation. In this paper, we propose a method that enables a new type of deformation of the radiance field: free-form radiance field deformation. We use a triangular mesh that encloses the foreground object called cage as an interface, and by manipulating the cage vertices, our approach enables the free-form deformation of the radiance field. The core of our approach is cage-based deformation which is commonly used in mesh deformation. We propose a novel formulation to extend it to the radiance field, which maps the position and the view direction of the sampling points from the deformed space to the canonical space, thus enabling the rendering of the deformed scene. The deformation results of the synthetic datasets and the real-world datasets demonstrate the effectiveness of our approach.","Authors (format: First Last, First Middle Last, ...)":"Tianhan Xu, Tatsuya Harada","Bibtex (e.g. @inproceedings...)":"@article{xu2022deforming,\n    author = {Tianhan Xu and Tatsuya Harada},\n    title = {Deforming Radiance Fields with Cages},\n    year = {2022},\n    month = {Jul},\n    url = {http://arxiv.org/abs/2207.12298v1}\n}","Bibtex Name":"xu2022deforming","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/xth430/deforming-nerf","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"07/25/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"cheng-you_lu@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Speed & Computational Efficiency, Dynamic/Temporal, Graphics","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2207.12298.pdf","Project webpage link":"https://xth430.github.io/deforming-nerf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=apE1q-_iQmQ","Timestamp":"11/5/2022 0:36:08","Title":"Deforming Radiance Fields with Cages","Training time (hr)":"","UID":"407","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural Radiance Fields (NeRFs) have demonstrated amazing ability to synthesize images of 3D scenes from novel views. However, they rely upon specialized volumetric rendering algorithms based on ray marching that are mismatched to the capabilities of widely deployed graphics hardware. This paper introduces a new NeRF representation based on textured polygons that can synthesize novel images efficiently with standard rendering pipelines. The NeRF is represented as a set of polygons with textures representing binary opacities and feature vectors. Traditional rendering of the polygons with a z-buffer yields an image with features at every pixel, which are interpreted by a small, view-dependent MLP running in a fragment shader to produce a final pixel color. This approach enables NeRFs to be rendered with the traditional polygon rasterization pipeline, which provides massive pixel-level parallelism, achieving interactive frame rates on a wide range of compute platforms, including mobile phones.","Authors (format: First Last, First Middle Last, ...)":"Zhiqin Chen, Thomas Funkhouser, Peter Hedman, Andrea Tagliasacchi","Bibtex (e.g. @inproceedings...)":"@article{chen2022mobilenerf,\n    author = {Zhiqin Chen and Thomas Funkhouser and Peter Hedman and Andrea Tagliasacchi},\n    title = {MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures},\n    year = {2022},\n    month = {Jul},\n    url = {http://arxiv.org/abs/2208.00277v3}\n}","Bibtex Name":"chen2022mobilenerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/google-research/jax3d/tree/main/jax3d/projects/mobilenerf","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"07/30/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"cheng-you_lu@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Speed & Computational Efficiency, Graphics, Compression","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"MobileNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2208.00277.pdf","Project webpage link":"https://mobile-nerf.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"11/5/2022 0:46:28","Title":"MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures","Training time (hr)":"","UID":"408","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present an explicit-grid based method for efficiently reconstructing streaming radiance fields for novel view synthesis of real world dynamic scenes. Instead of training a single model that combines all the frames, we formulate the dynamic modeling problem with an incremental learning paradigm in which per-frame model difference is trained to complement the adaption of a base model on the current frame. By exploiting the simple yet effective tuning strategy with narrow bands, the proposed method realizes a feasible framework for handling video sequences on-the-fly with high training efficiency. The storage overhead induced by using explicit grid representations can be significantly reduced through the use of model difference based compression. We also introduce an efficient strategy to further accelerate model optimization for each frame. Experiments on challenging video sequences demonstrate that our approach is capable of achieving a training speed of 15 seconds per-frame with competitive rendering quality, which attains $1000 \\times$ speedup over the state-of-the-art implicit methods. Code is available at https://github.com/AlgoHunt/StreamRF.","Authors (format: First Last, First Middle Last, ...)":"Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, Ping Tan","Bibtex (e.g. @inproceedings...)":"@article{li2022streamrf,\n    author = {Lingzhi Li and Zhen Shen and Zhongshu Wang and Li Shen and Ping Tan},\n    title = {Streaming Radiance Fields for 3D Video Synthesis},\n    year = {2022},\n    month = {Oct},\n    url = {http://arxiv.org/abs/2210.14831v1}\n}","Bibtex Name":"li2022streamrf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/AlgoHunt/StreamRF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/26/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"cheng-you_lu@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Speed & Computational Efficiency, Dynamic/Temporal, Compression, Local Conditioning, Voxel Grid","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"StreamRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2210.14831.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"11/7/2022 21:04:31","Title":"Streaming Radiance Fields for 3D Video Synthesis","Training time (hr)":"","UID":"409","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2022","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"For computer vision systems to operate in dynamic situations, they need to be able to represent and reason about object permanence. We introduce a framework for learning to estimate 4D visual representations from monocular RGB-D, which is able to persist objects, even once they become obstructed by occlusions. Unlike traditional video representations, we encode point clouds into a continuous representation, which permits the model to attend across the spatiotemporal context to resolve occlusions. On two large video datasets that we release along with this paper, our experiments show that the representation is able to successfully reveal occlusions for several tasks, without any architectural changes. Visualizations show that the attention mechanism automatically learns to follow occluded objects. Since our approach can be trained end-to-end and is easily adaptable, we believe it will be useful for handling occlusions in many video understanding tasks. Data, code, and models are available at https://occlusions.cs.columbia.edu/.","Authors (format: First Last, First Middle Last, ...)":"Basile Van Hoorick, Purva Tendulkar, Didac Suris, Dennis Park, Simon Stent, Carl Vondrick","Bibtex (e.g. @inproceedings...)":"@article{hoorick2022revealing,\n    url = {http://arxiv.org/abs/2204.10916v1},\n    month = {Apr},\n    year = {2022},\n    title = {Revealing Occlusions with 4D Neural Fields},\n    author = {Basile Van Hoorick and Purva Tendulkar and Didac Suris and Dennis Park and Simon Stent and Carl Vondrick}\n}","Bibtex Name":"hoorick2022revealing","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/basilevh/occlusions-4d","Coordinates all at once":"","Data Release (link)":"https://docs.google.com/forms/d/e/1FAIpQLSchfYW_ABEjqp1a1MZtxO_dXO95hkqYDI2lNluToRpy7sL2zQ/viewform","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"04/22/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"asztrajman@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Dynamic/Temporal","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2204.10916.pdf","Project webpage link":"https://occlusions.cs.columbia.edu/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=Q2S-ogGjcHo","Timestamp":"10/3/2022 13:58:21","Title":"Revealing Occlusions with 4D Neural Fields","Training time (hr)":"","UID":"378","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural Motion Planners (NMPs) have emerged as a promising tool for solving robot navigation tasks in complex environments. However, these methods often require expert data for learning, which limits their application to scenarios where data generation is time-consuming. Recent developments have also led to physics-informed deep neural models capable of representing complex dynamical Partial Differential Equations (PDEs). Inspired by these developments, we propose Neural Time Fields (NTFields) for robot motion planning in cluttered scenarios. Our framework represents a wave propagation model generating continuous arrival time to find path solutions informed by a nonlinear first-order PDE called Eikonal Equation. We evaluate our method in various cluttered 3D environments, including the Gibson dataset, and demonstrate its ability to solve motion planning problems for 4-DOF and 6-DOF robot manipulators where the traditional grid-based Eikonal planners often face the curse of dimensionality. Furthermore, the results show that our method exhibits high success rates and significantly lower computational times than the state-of-the-art methods, including NMPs that require training data from classical planners.","Authors (format: First Last, First Middle Last, ...)":"Ruiqi Ni, Ahmed H. Qureshi","Bibtex (e.g. @inproceedings...)":"@article{ni2022ntfields,\n    author = {Ruiqi Ni and Ahmed H. Qureshi},\n    title = {NTFields: Neural Time Fields for Physics-Informed Robot Motion Planning},\n    year = {2022},\n    month = {Sep},\n    url = {http://arxiv.org/abs/2210.00120v1}\n}","Bibtex Name":"ni2022ntfields","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"09/30/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"ni117@purdue.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Robotics, Supervision by Gradient (PDE)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NTFields","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2210.00120.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/3/2022 22:04:30","Title":"NTFields: Neural Time Fields for Physics-Informed Robot Motion Planning","Training time (hr)":"","UID":"379","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Personalised 3D vascular models are valuable for diagnosis, prognosis and treatment planning in patients with cardiovascular disease. Traditionally, such models have been constructed with explicit representations such as meshes and voxel masks, or implicit representations such as radial basis functions or atomic (tubular) shapes. Here, we propose to represent surfaces by the zero level set of their signed distance function (SDF) in a differentiable implicit neural representation (INR). This allows us to model complex vascular structures with a representation that is implicit, continuous, light-weight, and easy to integrate with deep learning algorithms. We here demonstrate the potential of this approach with three practical examples. First, we obtain an accurate and watertight surface for an abdominal aortic aneurysm (AAA) from CT images and show robust fitting from as little as 200 points on the surface. Second, we simultaneously fit nested vessel walls in a single INR without intersections. Third, we show how 3D models of individual arteries can be smoothly blended into a single watertight surface. Our results show that INRs are a flexible representation with potential for minimally interactive annotation and manipulation of complex vascular structures.","Authors (format: First Last, First Middle Last, ...)":"Dieuwertje Alblas, Christoph Brune, Kak Khee Yeung, Jelmer M. Wolterink","Bibtex (e.g. @inproceedings...)":"@article{alblas2022goingoffgrid,\n    author = {Dieuwertje Alblas and Christoph Brune and Kak Khee Yeung and Jelmer M. Wolterink},\n    title = {Going Off-Grid: Continuous Implicit Neural Representations for 3D Vascular Modeling},\n    year = {2022},\n    month = {Jul},\n    url = {http://arxiv.org/abs/2207.14663v2}\n}","Bibtex Name":"alblas2022goingoffgrid","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"07/29/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"d.alblas@utwente.nl","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Geometry Only, Human (Body), Regularization, Supervision by Gradient (PDE), Object-Centric","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Going off-grid","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2207.14663.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/10/2022 5:47:18","Title":"Going Off-Grid: Continuous Implicit Neural Representations for 3D Vascular Modeling","Training time (hr)":"","UID":"380","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"MICCAI STACOM 2022","Venue no Year":"MICCAI STACOM","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural rendering has received tremendous attention since the advent of Neural Radiance Fields (NeRF), and has pushed the state-of-the-art on novel-view synthesis considerably. The recent focus has been on models that overfit to a single scene, and the few attempts to learn models that can synthesize novel views of unseen scenes mostly consist of combining deep convolutional features with a NeRF-like model. We propose a different paradigm, where no deep features and no NeRF-like volume rendering are needed. Our method is capable of predicting the color of a target ray in a novel scene directly, just from a collection of patches sampled from the scene. We first leverage epipolar geometry to extract patches along the epipolar lines of each reference view. Each patch is linearly projected into a 1D feature vector and a sequence of transformers process the collection. For positional encoding, we parameterize rays as in a light field representation, with the crucial difference that the coordinates are canonicalized with respect to the target ray, which makes our method independent of the reference frame and improves generalization. We show that our approach outperforms the state-of-the-art on novel view synthesis of unseen scenes even when being trained with considerably less data than prior work.","Authors (format: First Last, First Middle Last, ...)":"Mohammed Suhail, Carlos Esteves, Leonid Sigal, Ameesh Makadia","Bibtex (e.g. @inproceedings...)":"@article{suhail2022generalizable,\n    author = {Mohammed Suhail and Carlos Esteves and Leonid Sigal and Ameesh Makadia},\n    title = {Generalizable Patch-Based Neural Rendering},\n    year = {2022},\n    month = {Jul},\n    url = {http://arxiv.org/abs/2207.10662v2}\n}","Bibtex Name":"suhail2022generalizable","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/google-research/google-research/tree/master/gen_patch_neural_rendering","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"07/21/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"suhail33@cs.ubc.ca","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Generalization, Image-Based Rendering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2207.10662.pdf","Project webpage link":"https://mohammedsuhail.net/gen_patch_neural_rendering/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/10/2022 16:57:36","Title":"Generalizable Patch-Based Neural Rendering","Training time (hr)":"","UID":"381","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Methods allowing the synthesis of realistic cell shapes could help generate training data sets to improve cell tracking and segmentation in biomedical images. Deep generative models for cell shape synthesis require a light-weight and flexible representation of the cell shape. However, commonly used voxel-based representations are unsuitable for high-resolution shape synthesis, and polygon meshes have limitations when modeling topology changes such as cell growth or mitosis. In this work, we propose to use level sets of signed distance functions (SDFs) to represent cell shapes. We optimize a neural network as an implicit neural representation of the SDF value at any point in a 3D+time domain. The model is conditioned on a latent code, thus allowing the synthesis of new and unseen shape sequences. We validate our approach quantitatively and qualitatively on C. elegans cells that grow and divide, and lung cancer cells with growing complex filopodial protrusions. Our results show that shape descriptors of synthetic cells resemble those of real cells, and that our model is able to generate topologically plausible sequences of complex cell shapes in 3D+time.","Authors (format: First Last, First Middle Last, ...)":"David Wiesner, Julian Suk, Sven Dummer, David Svoboda, Jelmer M. Wolterink","Bibtex (e.g. @inproceedings...)":"@article{wiesner2022implicitneuralrepresentations,\n    author = {David Wiesner and Julian Suk and Sven Dummer and David Svoboda and Jelmer M. Wolterink},\n    title = {Implicit Neural Representations for Generative Modeling of Living Cell Shapes},\n    doi = {10.1007/978-3-031-16440-8_6},\n    year = {2022},\n    month = {Jul},\n    url = {http://arxiv.org/abs/2207.06283v2}\n}","Bibtex Name":"wiesner2022implicitneuralrepresentations","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Source code and data available on the official webpage","Coordinates all at once":"","Data Release (link)":"https://cbia.fi.muni.cz/research/simulations/implicit_shapes.html#produced-datasets","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"07/13/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"wiesner@fi.muni.cz","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Speed & Computational Efficiency, Dynamic/Temporal, Compression, Sampling, Generative Models, Regularization","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Implicit Neural Representations","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2207.06283.pdf","Project webpage link":"https://cbia.fi.muni.cz/research/simulations/implicit_shapes.html","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://cbia.fi.muni.cz/files/simulations/implicit_shapes/MICCAI_2022_Oral_5_Implicit_Cell_Shapes.mp4","Timestamp":"10/11/2022 9:09:28","Title":"Implicit Neural Representations for Generative Modeling of Living Cell Shapes","Training time (hr)":"","UID":"382","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"MICCAI 2022","Venue no Year":"MICCAI","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recent work in Deep Learning has re-imagined the representation of data as functions mapping from a coordinate space to an underlying continuous signal. When such functions are approximated by neural networks this introduces a compelling alternative to the more common multi-dimensional array representation. Recent work on such Implicit Neural Representations (INRs) has shown that - following careful architecture search - INRs can outperform established compression methods such as JPEG (e.g. Dupont et al., 2021). In this paper, we propose crucial steps towards making such ideas scalable: Firstly, we employ state-of-the-art network sparsification techniques to drastically improve compression. Secondly, introduce the first method allowing for sparsification to be employed in the inner-loop of commonly used Meta-Learning algorithms, drastically improving both compression and the computational cost of learning INRs. The generality of this formalism allows us to present results on diverse data modalities such as images, manifolds, signed distance functions, 3D shapes and scenes, several of which establish new state-of-the-art results.","Authors (format: First Last, First Middle Last, ...)":"Jonathan Richard Schwarz, Yee Whye Teh","Bibtex (e.g. @inproceedings...)":"@article{schwarz2022mscn,\n    author = {Jonathan Richard Schwarz and Yee Whye Teh},\n    title = {Meta-Learning Sparse Compression Networks},\n    year = {2022},\n    month = {May},\n    url = {http://arxiv.org/abs/2205.08957v2}\n}","Bibtex Name":"schwarz2022mscn","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"05/18/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"schwarzjn@google.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Sparse Reconstruction, Compression, Hypernetwork/Meta-learning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"MSCN","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2205.08957.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/11/2022 12:35:15","Title":"Meta-Learning Sparse Compression Networks","Training time (hr)":"","UID":"383","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"TMLR 2022","Venue no Year":"TMLR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"For computer vision systems to operate in dynamic situations, they need to be able to represent and reason about object permanence. We introduce a framework for learning to estimate 4D visual representations from monocular RGB-D, which is able to persist objects, even once they become obstructed by occlusions. Unlike traditional video representations, we encode point clouds into a continuous representation, which permits the model to attend across the spatiotemporal context to resolve occlusions. On two large video datasets that we release along with this paper, our experiments show that the representation is able to successfully reveal occlusions for several tasks, without any architectural changes. Visualizations show that the attention mechanism automatically learns to follow occluded objects. Since our approach can be trained end-to-end and is easily adaptable, we believe it will be useful for handling occlusions in many video understanding tasks. Data, code, and models are available at https://occlusions.cs.columbia.edu/.","Authors (format: First Last, First Middle Last, ...)":"Basile Van Hoorick, Purva Tendulkar, Didac Suris, Dennis Park, Simon Stent, Carl Vondrick","Bibtex (e.g. @inproceedings...)":"@article{hoorick2022revealing,\n    author = {Basile Van Hoorick and Purva Tendulkar and Didac Suris and Dennis Park and Simon Stent and Carl Vondrick},\n    title = {Revealing Occlusions with 4D Neural Fields},\n    year = {2022},\n    month = {Apr},\n    url = {http://arxiv.org/abs/2204.10916v1}\n}","Bibtex Name":"hoorick2022revealing","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/basilevh/occlusions-4d","Coordinates all at once":"","Data Release (link)":"https://docs.google.com/forms/d/e/1FAIpQLSchfYW_ABEjqp1a1MZtxO_dXO95hkqYDI2lNluToRpy7sL2zQ/viewform","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"04/22/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"basile@cs.columbia.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Dynamic/Temporal, Generalization, Local Conditioning, Object Pemanence or Occlusions; Featurized Point Cloud (instead of Voxel Grid for conditioning); Scene Priors; Segmentation; Tracking","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2204.10916.pdf","Project webpage link":"https://occlusions.cs.columbia.edu/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=Q2S-ogGjcHo","Timestamp":"10/13/2022 14:33:57","Title":"Revealing Occlusions with 4D Neural Fields","Training time (hr)":"","UID":"384","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Understanding and mapping a new environment are core abilities of any autonomously navigating agent. While classical robotics usually estimates maps in a stand-alone manner with SLAM variants, which maintain a topological or metric representation, end-to-end learning of navigation keeps some form of memory in a neural network. Networks are typically imbued with inductive biases, which can range from vectorial representations to birds-eye metric tensors or topological structures. In this work, we propose to structure neural networks with two neural implicit representations, which are learned dynamically during each episode and map the content of the scene: (i) the Semantic Finder predicts the position of a previously seen queried object; (ii) the Occupancy and Exploration Implicit Representation encapsulates information about explored area and obstacles, and is queried with a novel global read mechanism which directly maps from function space to a usable embedding space. Both representations are leveraged by an agent trained with Reinforcement Learning (RL) and learned online during each episode. We evaluate the agent on Multi-Object Navigation and show the high impact of using neural implicit representations as a memory source.","Authors (format: First Last, First Middle Last, ...)":"Pierre Marza, Laetitia Matignon, Olivier Simonin, Christian Wolf","Bibtex (e.g. @inproceedings...)":"@article{marza2022multiobject,\n    author = {Pierre Marza and Laetitia Matignon and Olivier Simonin and Christian Wolf},\n    title = {Multi-Object Navigation with dynamically learned neural implicit representations},\n    year = {2022},\n    month = {Oct},\n    url = {http://arxiv.org/abs/2210.05129v1}\n}","Bibtex Name":"marza2022multiobject","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/11/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"pierre.marza@insa-lyon.fr","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Speed & Computational Efficiency, Robotics, Large-Scale Scenes","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2210.05129.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/14/2022 22:53:01","Title":"Multi-Object Navigation with dynamically learned neural implicit representations","Training time (hr)":"","UID":"385","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"arXiv 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce a neural implicit framework that exploits the differentiable properties of neural networks and the discrete geometry of point-sampled surfaces to approximate them as the level sets of neural implicit functions.  To train a neural implicit function, we propose a loss functional that approximates a signed distance function, and allows terms with high-order derivatives, such as the alignment between the principal directions of curvature, to learn more geometric details. During training, we consider a non-uniform sampling strategy based on the curvatures of the point-sampled surface to prioritize points with more geometric details. This sampling implies faster learning while preserving geometric accuracy when compared with previous approaches.  We also use the analytical derivatives of a neural implicit function to estimate the differential measures of the underlying point-sampled surface.","Authors (format: First Last, First Middle Last, ...)":"Tiago Novello, Guilherme Schardong, Luiz Schirmer, Vinicius da Silva, Helio Lopes, Luiz Velho","Bibtex (e.g. @inproceedings...)":"@article{novello2022i3d,\n    author = {Tiago Novello and Guilherme Schardong and Luiz Schirmer and Vinicius da Silva and Helio Lopes and Luiz Velho},\n    title = {Exploring Differential Geometry in Neural Implicits},\n    year = {2022},\n    month = {Jan},\n    url = {http://arxiv.org/abs/2201.09263v4}\n}","Bibtex Name":"novello2022i3d","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/dsilvavinicius/differential_geometry_in_neural_implicits/","Coordinates all at once":"","Data Release (link)":"2022","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"01/23/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"tiago.novello90@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Graphics, Sampling, Regularization, Supervision by Gradient (PDE)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"i3D","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2201.09263.pdf","Project webpage link":"https://dsilvavinicius.github.io/differential_geometry_in_neural_implicits/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/15/2022 22:58:32","Title":"Exploring Differential Geometry in Neural Implicits","Training time (hr)":"","UID":"386","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"Computer&Graphics 2022","Venue no Year":"Computer&Graphics","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"This work investigates the use of smooth neural networks for modeling dynamic variations of implicit surfaces under partial differential equations (PDE). For this purpose, it extends the representation of neural implicit surfaces to the space-time $\\mathbb{R}^3\\times \\mathbb{R}$, which opens up mechanisms for \\textbf{continuous} geometric transformations. Examples include evolving an initial condition surface towards general vector fields, smoothing and sharpening using the mean curvature equation, and interpolations of initial conditions regularized by specific differential equations.  The network training considers two constraints. A data term is responsible for fitting the PDE's initial condition to the corresponding time instant, usually $\\mathbb{R}^3 \\times \\{0\\}$. Then, a PDE term forces the network to approximate a solution of the underlying equation, \\textbf{without any supervision}. The network can also be initialized based on previously trained initial conditions resulting in faster convergence when compared with the standard approach.","Authors (format: First Last, First Middle Last, ...)":"Tiago Novello, Vinicius da Silva, Guilherme Schardong, Luiz Schirmer, Helio Lopes, Luiz Velho","Bibtex (e.g. @inproceedings...)":"@article{novello2022i4d,\n    author = {Tiago Novello and Vinicius da Silva and Guilherme Schardong and Luiz Schirmer and Helio Lopes and Luiz Velho},\n    title = {Neural Implicit Surface Evolution using Differential Equations},\n    year = {2022},\n    month = {Jan},\n    url = {http://arxiv.org/abs/2201.09636v3}\n}","Bibtex Name":"novello2022i4d","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"2022","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"01/24/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"tiago.novello90@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Geometry Only, Graphics, Regularization, Supervision by Gradient (PDE)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"i4D","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2201.09636.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/16/2022 9:36:39","Title":"Neural Implicit Surface Evolution using Differential Equations","Training time (hr)":"","UID":"387","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Numerically solving partial differential equations (PDEs) often entails spatial and temporal discretizations. Traditional methods (e.g., finite difference, finite element, smoothed-particle hydrodynamics) frequently adopt explicit spatial discretizations, such as grids, meshes, and point clouds, where each degree-of-freedom corresponds to a location in space. While these explicit spatial correspondences are intuitive to model and understand, these representations are not necessarily optimal for accuracy, memory-usage, or adaptivity. In this work, we explore implicit neural representation as an alternative spatial discretization, where spatial information is implicitly stored in the neural network weights. With implicit neural spatial representation, PDE-constrained time-stepping translates into updating neural network weights, which naturally integrates with commonly adopted optimization time integrators. We validate our approach on a variety of classic PDEs with examples involving large elastic deformations, turbulent fluids, and multiscale phenomena. While slower to compute than traditional representations, our approach exhibits higher accuracy, lower memory consumption, and dynamically adaptive allocation of degrees of freedom without complex remeshing.","Authors (format: First Last, First Middle Last, ...)":"Honglin Chen, Rundi Wu, Eitan Grinspun, Changxi Zheng, Peter Yichen Chen","Bibtex (e.g. @inproceedings...)":"@article{chen2022implicit,\n    author = {Honglin Chen and Rundi Wu and Eitan Grinspun and Changxi Zheng and Peter Yichen Chen},\n    title = {Implicit Neural Spatial Representations for Time-dependent PDEs},\n    year = {2022},\n    month = {Sep},\n    url = {http://arxiv.org/abs/2210.00124v1}\n}","Bibtex Name":"chen2022implicit","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"09/30/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"honglin.chen@columbia.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Dynamic/Temporal, Graphics, Science & Engineering, Simulation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2210.00124.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/16/2022 20:33:36","Title":"Implicit Neural Spatial Representations for Time-dependent PDEs","Training time (hr)":"","UID":"388","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In this work we develop a generalizable and efficient Neural Radiance Field (NeRF) pipeline for high-fidelity free-viewpoint human body synthesis under settings with sparse camera views. Though existing NeRF-based methods can synthesize rather realistic details for human body, they tend to produce poor results when the input has self-occlusion, especially for unseen humans under sparse views. Moreover, these methods often require a large number of sampling points for rendering, which leads to low efficiency and limits their real-world applicability. To address these challenges, we propose a Geometry-guided Progressive NeRF (GP-NeRF). In particular, to better tackle self-occlusion, we devise a geometry-guided multi-view feature integration approach that utilizes the estimated geometry prior to integrate the incomplete information from input views and construct a complete geometry volume for the target human body. Meanwhile, for achieving higher rendering efficiency, we introduce a progressive rendering pipeline through geometry guidance, which leverages the geometric feature volume and the predicted density values to progressively reduce the number of sampling points and speed up the rendering process. Experiments on the ZJU-MoCap and THUman datasets show that our method outperforms the state-of-the-arts significantly across multiple generalization settings, while the time cost is reduced > 70% via applying our efficient progressive rendering pipeline.","Authors (format: First Last, First Middle Last, ...)":"Mingfei Chen, Jianfeng Zhang, Xiangyu Xu, Lijuan Liu, Yujun Cai, Jiashi Feng, Shuicheng Yan","Bibtex (e.g. @inproceedings...)":"@article{chen2022gpnerf,\n    author = {Mingfei Chen and Jianfeng Zhang and Xiangyu Xu and Lijuan Liu and Yujun Cai and Jiashi Feng and Shuicheng Yan},\n    title = {Geometry-Guided Progressive NeRF for Generalizable and Efficient Neural Human Rendering},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.04312v3}\n}","Bibtex Name":"chen2022gpnerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/sail-sg/GP-Nerf","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/08/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"xuxiangyu2014@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Speed & Computational Efficiency, Human (Body), Generalization, Data-Driven Method, Image-Based Rendering, Coarse-to-Fine","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"GP-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.04312.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/18/2022 1:53:17","Title":"Geometry-Guided Progressive NeRF for Generalizable and Efficient Neural Human Rendering","Training time (hr)":"","UID":"389","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Generative models have emerged as an essential building block for many image synthesis and editing tasks. Recent advances in this field have also enabled high-quality 3D or video content to be generated that exhibits either multi-view or temporal consistency. With our work, we explore 4D generative adversarial networks (GANs) that learn unconditional generation of 3D-aware videos. By combining neural implicit representations with time-aware discriminator, we develop a GAN framework that synthesizes 3D video supervised only with monocular videos. We show that our method learns a rich embedding of decomposable 3D structures and motions that enables new visual effects of spatio-temporal renderings while producing imagery with quality comparable to that of existing 3D or video GANs.","Authors (format: First Last, First Middle Last, ...)":"Sherwin Bahmani, Jeong Joon Park, Despoina Paschalidou, Hao Tang, Gordon Wetzstein, Leonidas Guibas, Luc Van Gool, Radu Timofte","Bibtex (e.g. @inproceedings...)":"@article{bahmani20223daware,\n    author = {Sherwin Bahmani and Jeong Joon Park and Despoina Paschalidou and Hao Tang and Gordon Wetzstein and Leonidas Guibas and Luc Van Gool and Radu Timofte},\n    title = {3D-Aware Video Generation},\n    year = {2022},\n    month = {Jun},\n    url = {http://arxiv.org/abs/2206.14797v3}\n}","Bibtex Name":"bahmani20223daware","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/sherwinbahmani/3dvideogeneration/","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"06/29/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"sherwinbahmani@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Dynamic/Temporal, Human (Head), Generative Models","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2206.14797.pdf","Project webpage link":"https://sherwinbahmani.github.io/3dvidgen/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/19/2022 11:20:12","Title":"3D-Aware Video Generation","Training time (hr)":"","UID":"390","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"arXiv 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present PanoHDR-NeRF, a neural representation of the full HDR radiance field of an indoor scene, and a pipeline to capture it casually, without elaborate setups or complex capture protocols. First, a user captures a low dynamic range (LDR) omnidirectional video of the scene by freely waving an off-the-shelf camera around the scene. Then, an LDR2HDR network uplifts the captured LDR frames to HDR, which are used to train a tailored NeRF++ model. The resulting PanoHDR-NeRF can render full HDR images from any location of the scene. Through experiments on a novel test dataset of real scenes with the ground truth HDR radiance captured at locations not seen during training, we show that PanoHDR-NeRF predicts plausible HDR radiance from any scene point. We also show that the predicted radiance can synthesize correct lighting effects, enabling the augmentation of indoor scenes with synthetic objects that are lit correctly. Datasets and code are available at https://lvsn.github.io/PanoHDR-NeRF/.","Authors (format: First Last, First Middle Last, ...)":"Pulkit Gera, Mohammad Reza Karimi Dastjerdi, Charles Renaud, P. J. Narayanan, Jean-Fran\u00e7ois Lalonde","Bibtex (e.g. @inproceedings...)":"@article{gera2022panohdrnerf,\n    author = {Pulkit Gera and Mohammad Reza Karimi Dastjerdi and Charles Renaud and P. J. Narayanan and Jean-Francois Lalonde},\n    title = {Casual Indoor HDR Radiance Capture from Omnidirectional Images},\n    year = {2022},\n    month = {Aug},\n    url = {http://arxiv.org/abs/2208.07903v2}\n}","Bibtex Name":"gera2022panohdrnerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"08/16/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"pgera@mpi-inf.mpg.de","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Material/Lighting Estimation, Large-Scale Scenes","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"PanoHDR-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2208.07903.pdf","Project webpage link":"https://lvsn.github.io/PanoHDR-NeRF/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/19/2022 12:34:29","Title":"Casual Indoor HDR Radiance Capture from Omnidirectional Images","Training time (hr)":"","UID":"391","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"BMVC 2022","Venue no Year":"BMVC","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recently Implicit Neural Representations (INRs) gained attention as a novel and effective representation for various data types. Thus far, prior work mostly focused on optimizing their reconstruction performance. This work investigates INRs from a novel perspective, i.e., as a tool for image compression. To this end, we propose the first comprehensive compression pipeline based on INRs including quantization, quantization-aware retraining and entropy coding. Encoding with INRs, i.e. overfitting to a data sample, is typically orders of magnitude slower. To mitigate this drawback, we leverage meta-learned initializations based on MAML to reach the encoding in fewer gradient updates which also generally improves rate-distortion performance of INRs. We find that our approach to source compression with INRs vastly outperforms similar prior work, is competitive with common compression algorithms designed specifically for images and closes the gap to state-of-the-art learned approaches based on Rate-Distortion Autoencoders. Moreover, we provide an extensive ablation study on the importance of individual components of our method which we hope facilitates future research on this novel approach to image compression.","Authors (format: First Last, First Middle Last, ...)":"Yannick Str\u00fcmpler, Janis Postels, Ren Yang, Luc van Gool, Federico Tombari","Bibtex (e.g. @inproceedings...)":"@article{strumpler2022implicit,\n    author = {Yannick Strumpler and Janis Postels and Ren Yang and Luc van Gool and Federico Tombari},\n    title = {Implicit Neural Representations for Image Compression},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.04267v2}\n}","Bibtex Name":"strumpler2022implicit","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/YannickStruempler/inr_based_compression","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/08/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"jpostels@vision.ee.ethz.ch","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Speed & Computational Efficiency, Compression, Hypernetwork/Meta-learning, Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.04267.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/20/2022 8:16:42","Title":"Implicit Neural Representations for Image Compression","Training time (hr)":"","UID":"392","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Emerging neural radiance fields (NeRF) are a promising scene representation for computer graphics, enabling high-quality 3D reconstruction and novel view synthesis from image observations. However, editing a scene represented by a NeRF is challenging, as the underlying connectionist representations such as MLPs or voxel grids are not object-centric or compositional. In particular, it has been difficult to selectively edit specific regions or objects. In this work, we tackle the problem of semantic scene decomposition of NeRFs to enable query-based local editing of the represented 3D scenes. We propose to distill the knowledge of off-the-shelf, self-supervised 2D image feature extractors such as CLIP-LSeg or DINO into a 3D feature field optimized in parallel to the radiance field. Given a user-specified query of various modalities such as text, an image patch, or a point-and-click selection, 3D feature fields semantically decompose 3D space without the need for re-training and enable us to semantically select and edit regions in the radiance field. Our experiments validate that the distilled feature fields (DFFs) can transfer recent progress in 2D vision and language foundation models to 3D scene representations, enabling convincing 3D segmentation and selective editing of emerging neural graphics representations.","Authors (format: First Last, First Middle Last, ...)":"Sosuke Kobayashi, Eiichi Matsumoto, Vincent Sitzmann","Bibtex (e.g. @inproceedings...)":"@article{kobayashi2022dff,\n    author = {Sosuke Kobayashi and Eiichi Matsumoto and Vincent Sitzmann},\n    title = {Decomposing NeRF for Editing via Feature Field Distillation},\n    year = {2022},\n    month = {May},\n    url = {http://arxiv.org/abs/2205.15585v2}\n}","Bibtex Name":"kobayashi2022dff","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"05/31/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"in2400@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Editable, Fundamentals, Object-Centric, Semantics, Foundation Models, Distillation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DFF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2205.15585.pdf","Project webpage link":"https://pfnet-research.github.io/distilled-feature-fields/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/23/2022 18:21:16","Title":"Decomposing NeRF for Editing via Feature Field Distillation","Training time (hr)":"","UID":"393","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2022","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose a spatial calibration method for wide Field-of-View (FoV) Near-Eye Displays (NEDs) with complex image distortions. Image distortions in NEDs can destroy the reality of the virtual object and cause sickness. To achieve distortion-free images in NEDs, it is necessary to establish a pixel-by-pixel correspondence between the viewpoint and the displayed image. Designing compact and wide-FoV NEDs requires complex optical designs. In such designs, the displayed images are subject to gaze-contingent, non-linear geometric distortions, which explicit geometric models can be difficult to represent or computationally intensive to optimize.  To solve these problems, we propose Neural Distortion Field (NDF), a fully-connected deep neural network that implicitly represents display surfaces complexly distorted in spaces. NDF takes spatial position and gaze direction as input and outputs the display pixel coordinate and its intensity as perceived in the input gaze direction. We synthesize the distortion map from a novel viewpoint by querying points on the ray from the viewpoint and computing a weighted sum to project output display coordinates into an image. Experiments showed that NDF calibrates an augmented reality NED with 90$^{\\circ}$ FoV with about 3.23 pixel (5.8 arcmin) median error using only 8 training viewpoints. Additionally, we confirmed that NDF calibrates more accurately than the non-linear polynomial fitting, especially around the center of the FoV.","Authors (format: First Last, First Middle Last, ...)":"Yuichi Hiroi, Kiyosato Someya, Yuta Itoh","Bibtex (e.g. @inproceedings...)":"@article{hiroi2022neural,\n    author = {Yuichi Hiroi and Kiyosato Someya and Yuta Itoh},\n    title = {Neural Distortion Fields for Spatial Calibration of Wide Field-of-View Near-Eye Displays},\n    doi = {10.1364/OE.472288},\n    year = {2022},\n    month = {Oct},\n    url = {http://arxiv.org/abs/2210.12389v1}\n}","Bibtex Name":"hiroi2022neural","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/22/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yhiroi@g.ecc.u-tokyo.ac.jp","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Graphics, Science & Engineering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2210.12389.pdf","Project webpage link":"https://www.ar.c.titech.ac.jp/projects/neural-distortion-fields-2022","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=Q6XjDvBP4kg","Timestamp":"10/24/2022 23:03:11","Title":"Neural Distortion Fields for Spatial Calibration of Wide Field-of-View Near-Eye Displays","Training time (hr)":"","UID":"394","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"Opt. Exp. 2022","Venue no Year":"Opt. Exp.","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We design a physics-aware auto-encoder to specifically reduce the dimensionality of solutions arising from convection-dominated nonlinear physical systems. Although existing nonlinear manifold learning methods seem to be compelling tools to reduce the dimensionality of data characterized by a large Kolmogorov n-width, they typically lack a straightforward mapping from the latent space to the high-dimensional physical space. Moreover, the realized latent variables are often hard to interpret. Therefore, many of these methods are often dismissed in the reduced order modeling of dynamical systems governed by the partial differential equations (PDEs). Accordingly, we propose an auto-encoder type nonlinear dimensionality reduction algorithm. The unsupervised learning problem trains a diffeomorphic spatio-temporal grid, that registers the output sequence of the PDEs on a non-uniform parameter/time-varying grid, such that the Kolmogorov n-width of the mapped data on the learned grid is minimized. We demonstrate the efficacy and interpretability of our approach to separate convection/advection from diffusion/scaling on various manufactured and physical systems.","Authors (format: First Last, First Middle Last, ...)":"Rambod Mojgani, Maciej Balajewicz","Bibtex (e.g. @inproceedings...)":"@article{mojgani2021physicsaware,\n    url = {http://arxiv.org/abs/2006.15655v1},\n    month = {Jun},\n    year = {2020},\n    doi = {10.1609/aaai.v35i1.16116},\n    title = {Physics-aware registration based auto-encoder for convection dominated PDEs},\n    author = {Rambod Mojgani and Maciej Balajewicz}\n}","Bibtex Name":"mojgani2021physicsaware","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/rmojgani/PhysicsAwareAE/","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"06/28/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"rm99@rice.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Compression, Science & Engineering, Data-Driven Method, Manifold learning, Autoencoder","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2006.15655.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=fDYPAj9WAbk","Timestamp":"9/14/2022 13:23:18","Title":"Physics-aware registration based auto-encoder for convection dominated PDEs","Training time (hr)":"","UID":"376","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"AAAI 2021","Venue no Year":"AAAI","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present Neural Feature Fusion Fields (N3F), a method that improves dense 2D image feature extractors when the latter are applied to the analysis of multiple images reconstructible as a 3D scene. Given an image feature extractor, for example pre-trained using self-supervision, N3F uses it as a teacher to learn a student network defined in 3D space. The 3D student network is similar to a neural radiance field that distills said features and can be trained with the usual differentiable rendering machinery. As a consequence, N3F is readily applicable to most neural rendering formulations, including vanilla NeRF and its extensions to complex dynamic scenes. We show that our method not only enables semantic understanding in the context of scene-specific neural fields without the use of manual labels, but also consistently improves over the self-supervised 2D baselines. This is demonstrated by considering various tasks, such as 2D object retrieval, 3D segmentation, and scene editing, in diverse sequences, including long egocentric videos in the EPIC-KITCHENS benchmark.","Authors (format: First Last, First Middle Last, ...)":"Vadim Tschernezki, Iro Laina, Diane Larlus, Andrea Vedaldi","Bibtex (e.g. @inproceedings...)":"@article{tschernezki2022n3f,\n    author = {Vadim Tschernezki and Iro Laina and Diane Larlus and Andrea Vedaldi},\n    title = {Neural Feature Fusion Fields: 3D Distillation of Self-Supervised 2D Image Representations},\n    year = {2022},\n    month = {Sep},\n    url = {http://arxiv.org/abs/2209.03494v1}\n}","Bibtex Name":"tschernezki2022n3f","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/dichotomies/N3F","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"09/07/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"iro.laina@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Dynamic/Temporal, Editable, Object-Centric, Semantic","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"N3F","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2209.03494.pdf","Project webpage link":"https://www.robots.ox.ac.uk/~vadim/n3f/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=qAIpStmMHjY&feature=youtu.be","Timestamp":"9/16/2022 10:12:47","Title":"Neural Feature Fusion Fields: 3D Distillation of Self-Supervised 2D Image Representations","Training time (hr)":"","UID":"377","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"3DV 2022","Venue no Year":"DV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural radiance fields (NeRF) have shown great success in modeling 3D scenes and synthesizing novel-view images. However, most previous NeRF methods take much time to optimize one single scene. Explicit data structures, e.g. voxel features, show great potential to accelerate the training process. However, voxel features face two big challenges to be applied to dynamic scenes, i.e. modeling temporal information and capturing different scales of point motions. We propose a radiance field framework by representing scenes with time-aware voxel features, named as TiNeuVox. A tiny coordinate deformation network is introduced to model coarse motion trajectories and temporal information is further enhanced in the radiance network. A multi-distance interpolation method is proposed and applied on voxel features to model both small and large motions. Our framework significantly accelerates the optimization of dynamic radiance fields while maintaining high rendering quality. Empirical evaluation is performed on both synthetic and real scenes. Our TiNeuVox completes training with only 8 minutes and 8-MB storage cost while showing similar or even better rendering performance than previous dynamic NeRF methods.","Authors (format: First Last, First Middle Last, ...)":"Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Matthias Nie\u00dfner, Qi Tian","Bibtex (e.g. @inproceedings...)":"@article{fang2022tineuvox,\n    url = {http://arxiv.org/abs/2205.15285v2},\n    month = {May},\n    year = {2022},\n    doi = {10.1145/3550469.3555383},\n    title = {Fast Dynamic Radiance Fields with Time-Aware Neural Voxels},\n    author = {Jiemin Fang and Taoran Yi and Xinggang Wang and Lingxi Xie and Xiaopeng Zhang and Wenyu Liu and Matthias Niessner and Qi Tian}\n}","Bibtex Name":"fang2022tineuvox","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/hustvl/TiNeuVox","Coordinates all at once":"","Data Release (link)":"https://github.com/hustvl/TiNeuVox","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"05/30/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"xgwang@hust.edu.cn","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Sparse Reconstruction, Dynamic/Temporal","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"TiNeuVox","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2205.15285.pdf","Project webpage link":"https://jaminfong.cn/tineuvox/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://youtu.be/sROLfK_VkCk","Timestamp":"8/3/2022 2:28:00","Title":"Fast Dynamic Radiance Fields with Time-Aware Neural Voxels","Training time (hr)":"","UID":"363","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"arXiv 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Explicit neural surface representations allow for exact and efficient extraction of the encoded surface at arbitrary precision, as well as analytic derivation of differential geometric properties such as surface normal and curvature. Such desirable properties, which are absent in its implicit counterpart, makes it ideal for various applications in computer vision, graphics and robotics. However, SOTA works are limited in terms of the topology it can effectively describe, distortion it introduces to reconstruct complex surfaces and model efficiency. In this work, we present Minimal Neural Atlas, a novel atlas-based explicit neural surface representation. At its core is a fully learnable parametric domain, given by an implicit probabilistic occupancy field defined on an open square of the parametric space. In contrast, prior works generally predefine the parametric domain. The added flexibility enables charts to admit arbitrary topology and boundary. Thus, our representation can learn a minimal atlas of 3 charts with distortion-minimal parameterization for surfaces of arbitrary topology, including closed and open surfaces with arbitrary connected components. Our experiments support the hypotheses and show that our reconstructions are more accurate in terms of the overall geometry, due to the separation of concerns on topology and geometry.","Authors (format: First Last, First Middle Last, ...)":"Weng Fei Low, Gim Hee Lee","Bibtex (e.g. @inproceedings...)":"@article{low2022minimalneuralatlas,\n    author = {Weng Fei Low and Gim Hee Lee},\n    title = {Minimal Neural Atlas: Parameterizing Complex Surfaces with Minimal Charts and Distortion},\n    year = {2022},\n    month = {Jul},\n    url = {http://arxiv.org/abs/2207.14782v1}\n}","Bibtex Name":"low2022minimalneuralatlas","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/low5545/minimal-neural-atlas","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"07/29/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"wengfei@u.nus.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Geometry Only, Graphics, Fundamentals, Regularization, Supervision by Gradient (PDE), Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Minimal Neural Atlas","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2207.14782.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/3/2022 3:52:08","Title":"Minimal Neural Atlas: Parameterizing Complex Surfaces with Minimal Charts and Distortion","Training time (hr)":"","UID":"364","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a novel neural implicit representation for articulated human bodies. Compared to explicit template meshes, neural implicit body representations provide an efficient mechanism for modeling interactions with the environment, which is essential for human motion reconstruction and synthesis in 3D scenes. However, existing neural implicit bodies suffer from either poor generalization on highly articulated poses or slow inference time. In this work, we observe that prior knowledge about the human body's shape and kinematic structure can be leveraged to improve generalization and efficiency. We decompose the full-body geometry into local body parts and employ a part-aware encoder-decoder architecture to learn neural articulated occupancy that models complex deformations locally. Our local shape encoder represents the body deformation of not only the corresponding body part but also the neighboring body parts. The decoder incorporates the geometric constraints of local body shape which significantly improves pose generalization. We demonstrate that our model is suitable for resolving self-intersections and collisions with 3D environments. Quantitative and qualitative experiments show that our method largely outperforms existing solutions in terms of both efficiency and accuracy. The code and models are available at https://neuralbodies.github.io/COAP/index.html","Authors (format: First Last, First Middle Last, ...)":"Marko Mihajlovic, Shunsuke Saito, Aayush Bansal, Michael Zollhoefer, Siyu Tang","Bibtex (e.g. @inproceedings...)":"@article{mihajlovic2022coap,\n    author = {Marko Mihajlovic and Shunsuke Saito and Aayush Bansal and Michael Zollhoefer and Siyu Tang},\n    title = {COAP: Compositional Articulated Occupancy of People},\n    year = {2022},\n    month = {Apr},\n    url = {http://arxiv.org/abs/2204.06184v1}\n}","Bibtex Name":"mihajlovic2022coap","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/markomih/COAP","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"04/13/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"markomih@ethz.ch","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Human (Body)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"COAP","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2204.06184.pdf","Project webpage link":"https://neuralbodies.github.io/COAP","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=qU0q5h6IldU","Timestamp":"8/11/2022 6:11:57","Title":"COAP: Compositional Articulated Occupancy of People","Training time (hr)":"","UID":"365","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Reconstructing an object's geometry and appearance from multiple images, also known as inverse rendering, is a fundamental problem in computer graphics and vision. Inverse rendering is inherently ill-posed because the captured image is an intricate function of unknown lighting conditions, material properties and scene geometry. Recent progress in representing scene properties as coordinate-based neural networks have facilitated neural inverse rendering resulting in impressive geometry reconstruction and novel-view synthesis. Our key insight is that polarization is a useful cue for neural inverse rendering as polarization strongly depends on surface normals and is distinct for diffuse and specular reflectance. With the advent of commodity, on-chip, polarization sensors, capturing polarization has become practical. Thus, we propose PANDORA, a polarimetric inverse rendering approach based on implicit neural representations. From multi-view polarization images of an object, PANDORA jointly extracts the object's 3D geometry, separates the outgoing radiance into diffuse and specular and estimates the illumination incident on the object. We show that PANDORA outperforms state-of-the-art radiance decomposition techniques. PANDORA outputs clean surface reconstructions free from texture artefacts, models strong specularities accurately and estimates illumination under practical unstructured scenarios.","Authors (format: First Last, First Middle Last, ...)":"Akshat Dave, Yongyi Zhao, Ashok Veeraraghavan","Bibtex (e.g. @inproceedings...)":"@article{dave2022pandora,\n    author = {Akshat Dave and Yongyi Zhao and Ashok Veeraraghavan},\n    title = {PANDORA: Polarization-Aided Neural Decomposition Of Radiance},\n    year = {2022},\n    month = {Mar},\n    url = {http://arxiv.org/abs/2203.13458v1}\n}","Bibtex Name":"dave2022pandora","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/akshatdave/pandora","Coordinates all at once":"","Data Release (link)":"https://drive.google.com/file/d/1FvOi_2wfSUnASHulQdBhHQcQCxOuJ8zz/view?usp=sharing","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"03/25/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"ad74@rice.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Graphics, Material/Lighting Estimation, Editable, Science & Engineering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"PANDORA","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2203.13458.pdf","Project webpage link":"https://akshatdave.github.io/pandora/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://akshatdave.github.io/pandora/video/teaser_animation.mp4","Timestamp":"8/20/2022 19:15:03","Title":"PANDORA: Polarization-Aided Neural Decomposition Of Radiance","Training time (hr)":"","UID":"366","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Photorealistic rendering and reposing of humans is important for enabling augmented reality experiences. We propose a novel framework to reconstruct the human and the scene that can be rendered with novel human poses and views from just a single in-the-wild video. Given a video captured by a moving camera, we train two NeRF models: a human NeRF model and a scene NeRF model. To train these models, we rely on existing methods to estimate the rough geometry of the human and the scene. Those rough geometry estimates allow us to create a warping field from the observation space to the canonical pose-independent space, where we train the human model in. Our method is able to learn subject specific details, including cloth wrinkles and accessories, from just a 10 seconds video clip, and to provide high quality renderings of the human under novel poses, from novel views, together with the background.","Authors (format: First Last, First Middle Last, ...)":"Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel, Anurag Ranjan","Bibtex (e.g. @inproceedings...)":"@article{jiang2022neuman,\n    author = {Wei Jiang and Kwang Moo Yi and Golnoosh Samei and Oncel Tuzel and Anurag Ranjan},\n    title = {NeuMan: Neural Human Radiance Field from a Single Video},\n    year = {2022},\n    month = {Mar},\n    url = {http://arxiv.org/abs/2203.12575v2}\n}","Bibtex Name":"jiang2022neuman","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/apple/ml-neuman","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"03/23/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"akshay.krishnan.30@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Dynamic/Temporal, Human (Body), Graphics, Editable, Generative Models, Object-Centric","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeuMan","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2203.12575.pdf","Project webpage link":"https://machinelearning.apple.com/research/neural-human-radiance-field","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/22/2022 19:06:37","Title":"NeuMan: Neural Human Radiance Field from a Single Video","Training time (hr)":"","UID":"367","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"This work investigates the use of Neural implicit representations, specifically Neural Radiance Fields (NeRF), for geometrical queries and motion planning. We show that by adding the capacity to infer occupancy in a radius to a pre-trained NeRF, we are effectively learning an approximation to a Euclidean Signed Distance Field (ESDF). Using backward differentiation of the augmented network, we obtain an obstacle gradient that is integrated into an obstacle avoidance policy based on the Riemannian Motion Policies (RMP) framework. Thus, our findings allow for very fast sampling-free obstacle avoidance planning in the implicit representation.","Authors (format: First Last, First Middle Last, ...)":"Michael Pantic, Cesar Cadena, Roland Siegwart, Lionel Ott","Bibtex (e.g. @inproceedings...)":"@article{pantic2022samplingfree,\n    author = {Michael Pantic and Cesar Cadena and Roland Siegwart and Lionel Ott},\n    title = {Sampling-free obstacle gradients and reactive planning in Neural Radiance Fields (NeRF)},\n    year = {2022},\n    month = {May},\n    url = {http://arxiv.org/abs/2205.01389v1}\n}","Bibtex Name":"pantic2022samplingfree","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming Soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"05/03/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"akshay.krishnan.30@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Geometry Only, Robotics","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2205.01389.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/23/2022 11:13:01","Title":"Sampling-free obstacle gradients and reactive planning in Neural Radiance Fields (NeRF)","Training time (hr)":"","UID":"368","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"Workshop on Implicit Neural Geometry, ICRA 2022","Venue no Year":"Workshop on Implicit Neural Geometry, ICRA","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In this paper, we present a novel double diffusion based neural radiance field, dubbed DD-NeRF, to reconstruct human body geometry and render the human body appearance in novel views from a sparse set of images. We first propose a double diffusion mechanism to achieve expressive representations of input images by fully exploiting human body priors and image appearance details at two levels. At the coarse level, we first model the coarse human body poses and shapes via an unclothed 3D deformable vertex model as guidance. At the fine level, we present a multi-view sampling network to capture subtle geometric deformations and image detailed appearances, such as clothing and hair, from multiple input views. Considering the sparsity of the two level features, we diffuse them into feature volumes in the canonical space to construct neural radiance fields. Then, we present a signed distance function (SDF) regression network to construct body surfaces from the diffused features. Thanks to our double diffused representations, our method can even synthesize novel views of unseen subjects. Experiments on various datasets demonstrate that our approach outperforms the state-of-the-art in both geometric reconstruction and novel view synthesis.","Authors (format: First Last, First Middle Last, ...)":"Guangming Yao, Hongzhi Wu, Yi Yuan, Lincheng Li, Kun Zhou, Xin Yu","Bibtex (e.g. @inproceedings...)":"@article{yao2021ddnerf,\n    author = {Guangming Yao and Hongzhi Wu and Yi Yuan and Lincheng Li and Kun Zhou and Xin Yu},\n    title = {Learning Implicit Body Representations from Double Diffusion Based Neural Radiance Fields},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.12390v2}\n}","Bibtex Name":"yao2021ddnerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/23/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"abhishek16005@iiitd.ac.in","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DD-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.12390.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/23/2022 13:34:55","Title":"Learning Implicit Body Representations from Double Diffusion Based Neural Radiance Fields","Training time (hr)":"","UID":"369","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"Arxiv 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Off-road image semantic segmentation is challenging due to the presence of uneven terrains, unstructured class boundaries, irregular features and strong textures. These aspects affect the perception of the vehicle from which the information is used for path planning. Current off-road datasets exhibit difficulties like class imbalance and understanding of varying environmental topography. To overcome these issues we propose a framework for off-road semantic segmentation called as OFFSEG that involves (i) a pooled class semantic segmentation with four classes (sky, traversable region, non-traversable region and obstacle) using state-of-the-art deep learning architectures (ii) a colour segmentation methodology to segment out specific sub-classes (grass, puddle, dirt, gravel, etc.) from the traversable region for better scene understanding. The evaluation of the framework is carried out on two off-road driving datasets, namely, RELLIS-3D and RUGD. We have also tested proposed framework in IISERB campus frames. The results show that OFFSEG achieves good performance and also provides detailed information on the traversable region.","Authors (format: First Last, First Middle Last, ...)":"Kasi Viswanath, Kartikeya Singh, Peng Jiang, Sujit P. B., Srikanth Saripalli","Bibtex (e.g. @inproceedings...)":"@article{viswanath2021offseg,\n    author = {Kasi Viswanath and Kartikeya Singh and Peng Jiang and Sujit P. B. and Srikanth Saripalli},\n    title = {OFFSEG: A Semantic Segmentation Framework For Off-Road Driving},\n    year = {2021},\n    month = {Mar},\n    url = {http://arxiv.org/abs/2103.12417v1}\n}","Bibtex Name":"viswanath2021offseg","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/kasiv008/OFFSEG","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"03/23/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"kasi18@iiserb.ac.in","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Robotics, Science & Engineering, Data-Driven Method, Semantic Segmentation, Offroad, Unstructured Terrains","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"OFFSeg","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.12417.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=h6FcPIYO9tk","Timestamp":"8/24/2022 4:18:29","Title":"OFFSEG: A Semantic Segmentation Framework For Off-Road Driving","Training time (hr)":"","UID":"370","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"IEEE CASE 2021","Venue no Year":"IEEE CASE","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Modeling the human body in a canonical space is a common practice for capturing and animation. But when involving the neural radiance field (NeRF), learning a static NeRF in the canonical space is not enough because the lighting of the body changes when the person moves even though the scene lighting is constant. Previous methods alleviate the inconsistency of lighting by learning a per-frame embedding, but this operation does not generalize to unseen poses. Given that the lighting condition is static in the world space while the human body is consistent in the canonical space, we propose a dual-space NeRF that models the scene lighting and the human body with two MLPs in two separate spaces. To bridge these two spaces, previous methods mostly rely on the linear blend skinning (LBS) algorithm. However, the blending weights for LBS of a dynamic neural field are intractable and thus are usually memorized with another MLP, which does not generalize to novel poses. Although it is possible to borrow the blending weights of a parametric mesh such as SMPL, the interpolation operation introduces more artifacts. In this paper, we propose to use the barycentric mapping, which can directly generalize to unseen poses and surprisingly achieves superior results than LBS with neural blending weights. Quantitative and qualitative results on the Human3.6M and the ZJU-MoCap datasets show the effectiveness of our method.","Authors (format: First Last, First Middle Last, ...)":"Yihao Zhi, Shenhan Qian, Xinhao Yan, Shenghua Gao","Bibtex (e.g. @inproceedings...)":"@article{zhi2022dualspacenerf,\n    author = {Yihao Zhi and Shenhan Qian and Xinhao Yan and Shenghua Gao},\n    title = {Dual-Space NeRF: Learning Animatable Avatars and Scene Lighting in Separate Spaces},\n    year = {2022},\n    month = {Aug},\n    url = {http://arxiv.org/abs/2208.14851v1}\n}","Bibtex Name":"zhi2022dualspacenerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/zyhbili/Dual-Space-NeRF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"08/31/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"akshay.krishnan.30@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Dynamic/Temporal, Human (Body), Graphics, Material/Lighting Estimation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Dual-Space NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2208.14851.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=2qk4WOO8YMw","Timestamp":"9/1/2022 10:30:35","Title":"Dual-Space NeRF: Learning Animatable Avatars and Scene Lighting in Separate Spaces","Training time (hr)":"","UID":"371","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"3DV 2022","Venue no Year":"DV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Active soft bodies can affect their shape through an internal actuation mechanism that induces a deformation. Similar to recent work, this paper utilizes a differentiable, quasi-static, and physics-based simulation layer to optimize for actuation signals parameterized by neural networks. Our key contribution is a general and implicit formulation to control active soft bodies by defining a function that enables a continuous mapping from a spatial point in the material space to the actuation value. This property allows us to capture the signal's dominant frequencies, making the method discretization agnostic and widely applicable. We extend our implicit model to mandible kinematics for the particular case of facial animation and show that we can reliably reproduce facial expressions captured with high-quality capture systems. We apply the method to volumetric soft bodies, human poses, and facial expressions, demonstrating artist-friendly properties, such as simple control over the latent space and resolution invariance at test time.","Authors (format: First Last, First Middle Last, ...)":"Yang, Lingchencc Kim, Byungsoo Zoss, Gaspard G\u00f6zc\u00fc, Baran Gross, Markus Solenthaler, Barbara","Bibtex (e.g. @inproceedings...)":"@article{yang2022implicit,\n    publisher = {ACM New York, NY, USA},\n    year = {2022},\n    pages = {1--10},\n    number = {4},\n    volume = {41},\n    journal = {ACM Transactions on Graphics (TOG)},\n    author = {Lingchen Yang and Byungsoo Kim and Gaspard Zoss and Baran G{\\\"o}zc{\\\"u} and Markus Gross and Barbara Solenthaler},\n    title = {Implicit neural representation for physics-driven actuated soft bodies}\n}","Bibtex Name":"yang2022implicit","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"none","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/4/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"2324358757@qq.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"No","Keywords":"Human (Body), Human (Head), Graphics, soft body control","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"none","PDF link (arXiv perferred)":"https://dl.acm.org/doi/abs/10.1145/3528223.3530156","Project webpage link":"https://www.research-collection.ethz.ch/handle/20.500.11850/562054","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=9EERe_CTazk","Timestamp":"9/4/2022 4:55:51","Title":"Implicit neural representation for physics-driven actuated soft bodies","Training time (hr)":"","UID":"372","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ACM Transactions on Graphics 2022","Venue no Year":"ACM Transactions on Graphics","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We explore a new idea for learning based shape reconstruction from a point cloud, based on the recently popularized implicit neural shape representations. We cast the problem as a few-shot learning of implicit neural signed distance functions in feature space, that we approach using gradient based meta-learning. We use a convolutional encoder to build a feature space given the input point cloud. An implicit decoder learns to predict signed distance values given points represented in this feature space. Setting the input point cloud, i.e. samples from the target shape function's zero level set, as the support (i.e. context) in few-shot learning terms, we train the decoder such that it can adapt its weights to the underlying shape of this context with a few (5) tuning steps. We thus combine two types of implicit neural network conditioning mechanisms simultaneously for the first time, namely feature encoding and meta-learning. Our numerical and qualitative evaluation shows that in the context of implicit reconstruction from a sparse point cloud, our proposed strategy, i.e. meta-learning in feature space, outperforms existing alternatives, namely standard supervised learning in feature space, and meta-learning in euclidean space, while still providing fast inference.","Authors (format: First Last, First Middle Last, ...)":"Amine Ouasfi, Adnane Boukhayma","Bibtex (e.g. @inproceedings...)":"@article{ouasfi2022fssdf,\n    author = {Amine Ouasfi and Adnane Boukhayma},\n    title = {Few 'Zero Level Set'-Shot Learning of Shape Signed Distance Functions in Feature Space},\n    year = {2022},\n    month = {Jul},\n    url = {http://arxiv.org/abs/2207.04161v1}\n}","Bibtex Name":"ouasfi2022fssdf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/Ouasfi/FS-SDF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"07/09/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"adnane.boukhayma@inria.fr","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Sparse Reconstruction, Geometry Only, Graphics, Generalization, Local Conditioning, Data-Driven Method, Hypernetwork/Meta-learning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"FS-SDF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2207.04161.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/11/2022 6:55:44","Title":"Few 'Zero Level Set'-Shot Learning of Shape Signed Distance Functions in Feature Space","Training time (hr)":"","UID":"373","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CP decomposition -- that factorizes tensors into rank-one components with compact vectors -- in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction (<30 min) with better rendering quality and even a smaller model size (<4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time (<10 min) and retaining a compact model size (<75 MB).","Authors (format: First Last, First Middle Last, ...)":"Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su","Bibtex (e.g. @inproceedings...)":"@article{chen2022tensorf,\n    author = {Anpei Chen and Zexiang Xu and Andreas Geiger and Jingyi Yu and Hao Su},\n    title = {TensoRF: Tensorial Radiance Fields},\n    year = {2022},\n    month = {Mar},\n    url = {http://arxiv.org/abs/2203.09517v1}\n}","Bibtex Name":"chen2022tensorf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/apchenstu/TensoRF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"03/17/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"lhwzy99@163.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Speed & Computational Efficiency","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"TensoRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2203.09517.pdf","Project webpage link":"https://apchenstu.github.io/TensoRF/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=ujOMgaKV3lA","Timestamp":"9/12/2022 11:45:18","Title":"TensoRF: Tensorial Radiance Fields","Training time (hr)":"","UID":"374","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a method that learns neural shadow fields which are neural scene representations that are only learnt from the shadows present in the scene. While traditional shape-from-shadow (SfS) algorithms reconstruct geometry from shadows, they assume a fixed scanning setup and fail to generalize to complex scenes. Neural rendering algorithms, on the other hand, rely on photometric consistency between RGB images, but largely ignore physical cues such as shadows, which have been shown to provide valuable information about the scene. We observe that shadows are a powerful cue that can constrain neural scene representations to learn SfS, and even outperform NeRF to reconstruct otherwise hidden geometry. We propose a graphics-inspired differentiable approach to render accurate shadows with volumetric rendering, predicting a shadow map that can be compared to the ground truth shadow. Even with just binary shadow maps, we show that neural rendering can localize the object and estimate coarse geometry. Our approach reveals that sparse cues in images can be used to estimate geometry using differentiable volumetric rendering. Moreover, our framework is highly generalizable and can work alongside existing 3D reconstruction techniques that otherwise only use photometric consistency.","Authors (format: First Last, First Middle Last, ...)":"Kushagra Tiwary, Tzofi Klinghoffer, Ramesh Raskar","Bibtex (e.g. @inproceedings...)":"@article{tiwary2022neuralshadowfields,\n    author = {Kushagra Tiwary and Tzofi Klinghoffer and Ramesh Raskar},\n    title = {Towards Learning Neural Representations from Shadows},\n    year = {2022},\n    month = {Mar},\n    url = {http://arxiv.org/abs/2203.15946v2}\n}","Bibtex Name":"tiwary2022neuralshadowfields","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming Soon","Coordinates all at once":"","Data Release (link)":"n/a","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"03/29/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"ktiwary@mit.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Geometry Only, Graphics, Science & Engineering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Neural Shadow Fields","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2203.15946.pdf","Project webpage link":"https://github.com/ktiwary2/neural_shadow_fields","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/12/2022 21:34:36","Title":"Towards Learning Neural Representations from Shadows","Training time (hr)":"","UID":"375","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"With the increases in computational power and advances in machine learning, data-driven learning-based methods have gained significant attention in solving PDEs. Physics-informed neural networks (PINNs) have recently emerged and succeeded in various forward and inverse PDEs problems thanks to their excellent properties, such as flexibility, mesh-free solutions, and unsupervised training. However, their slower convergence speed and relatively inaccurate solutions often limit their broader applicability in many science and engineering domains. This paper proposes a new kind of data-driven PDEs solver, physics-informed cell representations (PIXEL), elegantly combining classical numerical methods and learning-based approaches. We adopt a grid structure from the numerical methods to improve accuracy and convergence speed and overcome the spectral bias presented in PINNs. Moreover, the proposed method enjoys the same benefits in PINNs, e.g., using the same optimization frameworks to solve both forward and inverse PDE problems and readily enforcing PDE constraints with modern automatic differentiation techniques. We provide experimental results on various challenging PDEs that the original PINNs have struggled with and show that PIXEL achieves fast convergence speed and high accuracy.","Authors (format: First Last, First Middle Last, ...)":"Namgyu Kang, Byeonghyeon Lee, Youngjoon Hong, Seok-Bae Yun, Eunbyung Park","Bibtex (e.g. @inproceedings...)":"@article{kang2022pixel,\n    author = {Namgyu Kang and Byeonghyeon Lee and Youngjoon Hong and Seok-Bae Yun and Eunbyung Park},\n    title = {PIXEL: Physics-Informed Cell Representations for Fast and Accurate PDE Solvers},\n    year = {2022},\n    month = {Jul},\n    url = {http://arxiv.org/abs/2207.12800v1}\n}","Bibtex Name":"kang2022pixel","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"07/26/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"epark@skku.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Science & Engineering, Data-Driven Method, Supervision by Gradient (PDE), Voxel Grid","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Pixel","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2207.12800.pdf","Project webpage link":"https://namgyukang.github.io/PIXEL/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/28/2022 12:49:42","Title":"PIXEL: Physics-Informed Cell Representations for Fast and Accurate PDE Solvers","Training time (hr)":"","UID":"325","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"Arxiv 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Our method studies the complex task of object-centric 3D understanding from a single RGB-D observation. As it is an ill-posed problem, existing methods suffer from low performance for both 3D shape and 6D pose and size estimation in complex multi-object scenarios with occlusions. We present ShAPO, a method for joint multi-object detection, 3D textured reconstruction, 6D object pose and size estimation. Key to ShAPO is a single-shot pipeline to regress shape, appearance and pose latent codes along with the masks of each object instance, which is then further refined in a sparse-to-dense fashion. A novel disentangled shape and appearance database of priors is first learned to embed objects in their respective shape and appearance space. We also propose a novel, octree-based differentiable optimization step, allowing us to further improve object shape, pose and appearance simultaneously under the learned latent space, in an analysis-by-synthesis fashion. Our novel joint implicit textured object representation allows us to accurately identify and reconstruct novel unseen objects without having access to their 3D meshes. Through extensive experiments, we show that our method, trained on simulated indoor scenes, accurately regresses the shape, appearance and pose of novel objects in the real-world with minimal fine-tuning. Our method significantly out-performs all baselines on the NOCS dataset with an 8% absolute improvement in mAP for 6D pose estimation. Project page: https://zubair-irshad.github.io/projects/ShAPO.html","Authors (format: First Last, First Middle Last, ...)":"Muhammad Zubair Irshad, Sergey Zakharov, Rares Ambrus, Thomas Kollar, Zsolt Kira, Adrien Gaidon","Bibtex (e.g. @inproceedings...)":"@article{irshad2022shapo,\n    author = {Muhammad Zubair Irshad and Sergey Zakharov and Rares Ambrus and Thomas Kollar and Zsolt Kira and Adrien Gaidon},\n    title = {ShAPO: Implicit Representations for Multi-Object Shape, Appearance, and Pose Optimization},\n    year = {2022},\n    month = {Jul},\n    url = {http://arxiv.org/abs/2207.13691v1}\n}","Bibtex Name":"irshad2022shapo","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"07/27/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"muhammadzubairirshad@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Robotics, Camera Parameter Estimation, Editable, Generalization, Global Conditioning, Hybrid Geometry Representation, Object-Centric, 6D object pose estimation, Octree-based differentiable rendering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"ShAPO","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2207.13691.pdf","Project webpage link":"https://zubair-irshad.github.io/projects/ShAPO.html","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://youtu.be/LMg7NDcLDcA","Timestamp":"7/28/2022 1:29:50","Title":"ShAPO: Implicit Representations for Multi-Object Shape, Appearance, and Pose Optimization","Training time (hr)":"","UID":"326","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose CLIP-Actor, a text-driven motion recommendation and neural mesh stylization system for human mesh animation. CLIP-Actor animates a 3D human mesh to conform to a text prompt by recommending a motion sequence and optimizing mesh style attributes. We build a text-driven human motion recommendation system by leveraging a large-scale human motion dataset with language labels. Given a natural language prompt, CLIP-Actor suggests a text-conforming human motion in a coarse-to-fine manner. Then, our novel zero-shot neural style optimization detailizes and texturizes the recommended mesh sequence to conform to the prompt in a temporally-consistent and pose-agnostic manner. This is distinctive in that prior work fails to generate plausible results when the pose of an artist-designed mesh does not conform to the text from the beginning. We further propose the spatio-temporal view augmentation and mask-weighted embedding attention, which stabilize the optimization process by leveraging multi-frame human motion and rejecting poorly rendered views. We demonstrate that CLIP-Actor produces plausible and human-recognizable style 3D human mesh in motion with detailed geometry and texture solely from a natural language prompt.","Authors (format: First Last, First Middle Last, ...)":"Kim Youwang, Kim Ji-Yeon, Tae-Hyun Oh","Bibtex (e.g. @inproceedings...)":"@article{youwang2022clipactor,\n    author = {Kim Youwang and Kim Ji-Yeon and Tae-Hyun Oh},\n    title = {CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes},\n    year = {2022},\n    month = {Jun},\n    url = {http://arxiv.org/abs/2206.04382v2}\n}","Bibtex Name":"youwang2022clipactor","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/postech-ami/CLIP-Actor","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"06/09/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"youwang.kim@postech.ac.kr","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Dynamic/Temporal, Human (Body), Material/Lighting Estimation, Editable, Science & Engineering, Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"CLIP-Actor","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2206.04382.pdf","Project webpage link":"https://clip-actor.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/27/2022 2:04:02","Title":"CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes","Training time (hr)":"","UID":"327","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022 ","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recently, Implicit Neural Representations (INRs) parameterized by neural networks have emerged as a powerful and promising tool to represent different kinds of signals due to its continuous, differentiable properties, showing superiorities to classical discretized representations. However, the training of neural networks for INRs only utilizes input-output pairs, and the derivatives of the target output with respect to the input, which can be accessed in some cases, are usually ignored. In this paper, we propose a training paradigm for INRs whose target output is image pixels, to encode image derivatives in addition to image values in the neural network. Specifically, we use finite differences to approximate image derivatives. We show how the training paradigm can be leveraged to solve typical INRs problems, i.e., image regression and inverse rendering, and demonstrate this training paradigm can improve the data-efficiency and generalization capabilities of INRs. The code of our method is available at \\url{https://github.com/megvii-research/Sobolev_INRs}.","Authors (format: First Last, First Middle Last, ...)":"Wentao Yuan, Qingtian Zhu, Xiangyue Liu, Yikang Ding, Haotian Zhang, Chi Zhang","Bibtex (e.g. @inproceedings...)":"@article{yuan2022sobolev_inrs,\n    author = {Wentao Yuan and Qingtian Zhu and Xiangyue Liu and Yikang Ding and Haotian Zhang and Chi Zhang},\n    title = {Sobolev Training for Implicit Neural Representations with Approximated Image Derivatives},\n    year = {2022},\n    month = {Jul},\n    url = {http://arxiv.org/abs/2207.10395v1}\n}","Bibtex Name":"yuan2022sobolev_inrs","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/megvii-research/Sobolev_INRs","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"07/21/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"wtyuan@pku.edu.cn","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"2D Image Neural Fields, Regularization, Supervision by Gradient (PDE), Image-Based Rendering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Sobolev_INRs","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2207.10395.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/24/2022 4:32:14","Title":"Sobolev Training for Implicit Neural Representations with Approximated Image Derivatives","Training time (hr)":"","UID":"328","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose CLIP-Actor, a text-driven motion recommendation and neural mesh stylization system for human mesh animation. CLIP-Actor animates a 3D human mesh to conform to a text prompt by recommending a motion sequence and optimizing mesh style attributes. We build a text-driven human motion recommendation system by leveraging a large-scale human motion dataset with language labels. Given a natural language prompt, CLIP-Actor suggests a text-conforming human motion in a coarse-to-fine manner. Then, our novel zero-shot neural style optimization detailizes and texturizes the recommended mesh sequence to conform to the prompt in a temporally-consistent and pose-agnostic manner. This is distinctive in that prior work fails to generate plausible results when the pose of an artist-designed mesh does not conform to the text from the beginning. We further propose the spatio-temporal view augmentation and mask-weighted embedding attention, which stabilize the optimization process by leveraging multi-frame human motion and rejecting poorly rendered views. We demonstrate that CLIP-Actor produces plausible and human-recognizable style 3D human mesh in motion with detailed geometry and texture solely from a natural language prompt.","Authors (format: First Last, First Middle Last, ...)":"Kim Youwang, Kim Ji-Yeon, Tae-Hyun Oh","Bibtex (e.g. @inproceedings...)":"@article{youwang2022clipactor,\n    author = {Kim Youwang and Kim Ji-Yeon and Tae-Hyun Oh},\n    title = {CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes},\n    year = {2022},\n    month = {Jun},\n    url = {http://arxiv.org/abs/2206.04382v2}\n}","Bibtex Name":"youwang2022clipactor","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/postech-ami/CLIP-Actor","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"06/09/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"youwang.kim@postech.ac.kr","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Human (Body), Regularization, Coarse-to-Fine, Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"CLIP-Actor","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2206.04382.pdf","Project webpage link":"https://clip-actor.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/22/2022 11:05:10","Title":"CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes","Training time (hr)":"","UID":"329","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural fields have emerged as a new data representation paradigm and have shown remarkable success in various signal representations. Since they preserve signals in their network parameters, the data transfer by sending and receiving the entire model parameters prevents this emerging technology from being used in many practical scenarios. We propose streamable neural fields, a single model that consists of executable sub-networks of various widths. The proposed architectural and training techniques enable a single network to be streamable over time and reconstruct different qualities and parts of signals. For example, a smaller sub-network produces smooth and low-frequency signals, while a larger sub-network can represent fine details. Experimental results have shown the effectiveness of our method in various domains, such as 2D images, videos, and 3D signed distance functions. Finally, we demonstrate that our proposed method improves training stability, by exploiting parameter sharing.","Authors (format: First Last, First Middle Last, ...)":"Junwoo Cho, Seungtae Nam, Daniel Rho, Jong Hwan Ko, Eunbyung Park","Bibtex (e.g. @inproceedings...)":"@article{cho2022snf,\n    author = {Junwoo Cho and Seungtae Nam and Daniel Rho and Jong Hwan Ko and Eunbyung Park},\n    title = {Streamable Neural Fields},\n    year = {2022},\n    month = {Jul},\n    url = {http://arxiv.org/abs/2207.09663v1}\n}","Bibtex Name":"cho2022snf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/jwcho5576/streamable_nf","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"07/20/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"jwcho000@skku.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"2D Image Neural Fields, network architecture","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SNF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2207.09663.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/22/2022 0:00:21","Title":"Streamable Neural Fields","Training time (hr)":"","UID":"330","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"This paper tackles the task of uncalibrated photometric stereo for 3D object reconstruction, where both the object shape, object reflectance, and lighting directions are unknown. This is an extremely difficult task, and the challenge is further compounded with the existence of the well-known generalized bas-relief (GBR) ambiguity in photometric stereo. Previous methods to resolve this ambiguity either rely on an overly simplified reflectance model, or assume special light distribution. We propose a new method that jointly optimizes object shape, light directions, and light intensities, all under general surfaces and lights assumptions. The specularities are used explicitly to solve uncalibrated photometric stereo via a neural inverse rendering process. We gradually fit specularities from shiny to rough using novel progressive specular bases. Our method leverages a physically based rendering equation by minimizing the reconstruction error on a per-object-basis. Our method demonstrates state-of-the-art accuracy in light estimation and shape recovery on real-world datasets.","Authors (format: First Last, First Middle Last, ...)":"Junxuan Li, Hongdong Li","Bibtex (e.g. @inproceedings...)":"@article{li2022scpsnir,\n    author = {Junxuan Li and Hongdong Li},\n    title = {Self-calibrating Photometric Stereo by Neural Inverse Rendering},\n    year = {2022},\n    month = {Jul},\n    url = {http://arxiv.org/abs/2207.07815v1}\n}","Bibtex Name":"li2022scpsnir","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/junxuan-li/SCPS-NIR","Coordinates all at once":"","Data Release (link)":"https://github.com/junxuan-li/SCPS-NIR","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"07/16/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"junxuan.li94@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Sparse Reconstruction, Geometry Only, 2D Image Neural Fields, Material/Lighting Estimation, Image-Based Rendering, photometric stereo","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SCPS-NIR","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2207.07815.pdf","Project webpage link":"https://github.com/junxuan-li/SCPS-NIR","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/20/2022 23:58:50","Title":"Self-calibrating Photometric Stereo by Neural Inverse Rendering","Training time (hr)":"","UID":"331","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"This paper aims at recovering the shape of a scene with unknown, non-Lambertian, and possibly spatially-varying surface materials. When the shape of the object is highly complex and that shadows cast on the surface, the task becomes very challenging. To overcome these challenges, we propose a coordinate-based deep MLP (multilayer perceptron) to parameterize both the unknown 3D shape and the unknown reflectance at every surface point. This network is able to leverage the observed photometric variance and shadows on the surface, and recover both surface shape and general non-Lambertian reflectance. We explicitly predict cast shadows, mitigating possible artifacts on these shadowing regions, leading to higher estimation accuracy. Our framework is entirely self-supervised, in the sense that it requires neither ground truth shape nor BRDF. Tests on real-world images demonstrate that our method outperform existing methods by a significant margin. Thanks to the small size of the MLP-net, our method is an order of magnitude faster than previous CNN-based methods.","Authors (format: First Last, First Middle Last, ...)":"Junxuan Li, Hongdong Li","Bibtex (e.g. @inproceedings...)":"@article{li2022neuralreflectanceps,\n    author = {Junxuan Li and Hongdong Li},\n    title = {Neural Reflectance for Shape Recovery with Shadow Handling},\n    year = {2022},\n    month = {Mar},\n    url = {http://arxiv.org/abs/2203.12909v1}\n}","Bibtex Name":"li2022neuralreflectanceps","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/junxuan-li/Neural-Reflectance-PS","Coordinates all at once":"","Data Release (link)":"https://github.com/junxuan-li/Neural-Reflectance-PS","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"03/24/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"junxuan.li94@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Sparse Reconstruction, Geometry Only, 2D Image Neural Fields, Material/Lighting Estimation, Image-Based Rendering, photometric stereo","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Neural-Reflectance-PS","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2203.12909.pdf","Project webpage link":"https://github.com/junxuan-li/Neural-Reflectance-PS","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/20/2022 23:57:31","Title":"Neural Reflectance for Shape Recovery with Shadow Handling","Training time (hr)":"","UID":"332","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose a pipeline to generate Neural Radiance Fields~(NeRF) of an object or a scene of a specific class, conditioned on a single input image. This is a challenging task, as training NeRF requires multiple views of the same scene, coupled with corresponding poses, which are hard to obtain. Our method is based on $\\pi$-GAN, a generative model for unconditional 3D-aware image synthesis, which maps random latent codes to radiance fields of a class of objects. We jointly optimize (1) the $\\pi$-GAN objective to utilize its high-fidelity 3D-aware generation and (2) a carefully designed reconstruction objective. The latter includes an encoder coupled with $\\pi$-GAN generator to form an auto-encoder. Unlike previous few-shot NeRF approaches, our pipeline is unsupervised, capable of being trained with independent images without 3D, multi-view, or pose supervision. Applications of our pipeline include 3d avatar generation, object-centric novel view synthesis with a single input image, and 3d-aware super-resolution, to name a few.","Authors (format: First Last, First Middle Last, ...)":"Shengqu Cai, Anton Obukhov, Dengxin Dai, Luc Van Gool","Bibtex (e.g. @inproceedings...)":"@article{cai2022pix2nerf,\n    author = {Shengqu Cai and Anton Obukhov and Dengxin Dai and Luc Van Gool},\n    title = {Pix2NeRF: Unsupervised Conditional $p$-GAN for Single Image to Neural Radiance Fields Translation},\n    year = {2022},\n    month = {Feb},\n    url = {http://arxiv.org/abs/2202.13162v1}\n}","Bibtex Name":"cai2022pix2nerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/primecai/Pix2NeRF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"02/26/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"caishengqu@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Graphics, Generative Models, Generalization, Global Conditioning, Image-Based Rendering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Pix2NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2202.13162.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=RoVu3hvvzGg&t=3s","Timestamp":"7/20/2022 2:58:52","Title":"Pix2NeRF: Unsupervised Conditional $\u03c0$-GAN for Single Image to Neural Radiance Fields Translation","Training time (hr)":"","UID":"333","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present NeRF-SR, a solution for high-resolution (HR) novel view synthesis with mostly low-resolution (LR) inputs. Our method is built upon Neural Radiance Fields (NeRF) that predicts per-point density and color with a multi-layer perceptron. While producing images at arbitrary scales, NeRF struggles with resolutions that go beyond observed images. Our key insight is that NeRF benefits from 3D consistency, which means an observed pixel absorbs information from nearby views. We first exploit it by a supersampling strategy that shoots multiple rays at each image pixel, which further enforces multi-view constraint at a sub-pixel level. Then, we show that NeRF-SR can further boost the performance of supersampling by a refinement network that leverages the estimated depth at hand to hallucinate details from related patches on only one HR reference image. Experiment results demonstrate that NeRF-SR generates high-quality results for novel view synthesis at HR on both synthetic and real-world datasets without any external information.","Authors (format: First Last, First Middle Last, ...)":"Chen Wang, Xian Wu, Yuan-Chen Guo, Song-Hai Zhang, Yu-Wing Tai, Shi-Min Hu","Bibtex (e.g. @inproceedings...)":"@article{wang2022nerfsr,\n    author = {Chen Wang and Xian Wu and Yuan-Chen Guo and Song-Hai Zhang and Yu-Wing Tai and Shi-Min Hu},\n    title = {NeRF-SR: High-Quality Neural Radiance Fields using Supersampling},\n    doi = {10.1145/3503161.3547808},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.01759v3}\n}","Bibtex Name":"wang2022nerfsr","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/cwchenwang/NeRF-SR","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/03/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"cw.chenwang@outlook.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Graphics, 2D Image Neural Fields, Sampling, Image-Based Rendering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeRF-SR","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.01759.pdf","Project webpage link":"https://cwchenwang.github.io/NeRF-SR/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://youtu.be/c3Yx2nGvi8o","Timestamp":"7/16/2022 12:20:10","Title":"NeRF-SR: High-Quality Neural Radiance Fields using Supersampling","Training time (hr)":"","UID":"334","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"Multimedia 2022","Venue no Year":"Multimedia","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural Radiance Fields (NeRF) have demonstrated very impressive performance in novel view synthesis via implicitly modelling 3D representations from multi-view 2D images. However, most existing studies train NeRF models with either reasonable camera pose initialization or manually-crafted camera pose distributions which are often unavailable or hard to acquire in various real-world data. We design VMRF, an innovative view matching NeRF that enables effective NeRF training without requiring prior knowledge in camera poses or camera pose distributions. VMRF introduces a view matching scheme, which exploits unbalanced optimal transport to produce a feature transport plan for mapping a rendered image with randomly initialized camera pose to the corresponding real image. With the feature transport plan as the guidance, a novel pose calibration technique is designed which rectifies the initially randomized camera poses by predicting relative pose transformations between the pair of rendered and real images. Extensive experiments over a number of synthetic and real datasets show that the proposed VMRF outperforms the state-of-the-art qualitatively and quantitatively by large margins.","Authors (format: First Last, First Middle Last, ...)":"Jiahui Zhang, Fangneng Zhan, Rongliang Wu, Yingchen Yu, Wenqing Zhang, Bai Song, Xiaoqin Zhang, Shijian Lu","Bibtex (e.g. @inproceedings...)":"@article{zhang2022vmrf,\n    author = {Jiahui Zhang and Fangneng Zhan and Rongliang Wu and Yingchen Yu and Wenqing Zhang and Bai Song and Xiaoqin Zhang and Shijian Lu},\n    title = {VMRF: View Matching Neural Radiance Fields},\n    year = {2022},\n    month = {Jul},\n    url = {http://arxiv.org/abs/2207.02621v1}\n}","Bibtex Name":"zhang2022vmrf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"07/06/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"srinath@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Graphics, Image-Based Rendering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"VMRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2207.02621.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/15/2022 17:36:17","Title":"VMRF: View Matching Neural Radiance Fields","Training time (hr)":"","UID":"335","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"arXiv 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce a neural implicit representation for grasps of objects from multiple robotic hands. Different grasps across multiple robotic hands are encoded into a shared latent space. Each latent vector is learned to decode to the 3D shape of an object and the 3D shape of a robotic hand in a grasping pose in terms of the signed distance functions of the two 3D shapes. In addition, the distance metric in the latent space is learned to preserve the similarity between grasps across different robotic hands, where the similarity of grasps is defined according to contact regions of the robotic hands. This property enables our method to transfer grasps between different grippers including a human hand, and grasp transfer has the potential to share grasping skills between robots and enable robots to learn grasping skills from humans. Furthermore, the encoded signed distance functions of objects and grasps in our implicit representation can be used for 6D object pose estimation with grasping contact optimization from partial point clouds, which enables robotic grasping in the real world.","Authors (format: First Last, First Middle Last, ...)":"Ninad Khargonkar, Neil Song, Zesheng Xu, Balakrishnan Prabhakaran, Yu Xiang","Bibtex (e.g. @inproceedings...)":"@article{khargonkar2022neuralgrasps,\n    author = {Ninad Khargonkar and Neil Song and Zesheng Xu and Balakrishnan Prabhakaran and Yu Xiang},\n    title = {NeuralGrasps: Learning Implicit Representations for Grasps of Multiple Robotic Hands},\n    year = {2022},\n    month = {Jul},\n    url = {http://arxiv.org/abs/2207.02959v1}\n}","Bibtex Name":"khargonkar2022neuralgrasps","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"07/06/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"srinath@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Robotics","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeuralGrasps","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2207.02959.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/15/2022 10:35:04","Title":"NeuralGrasps: Learning Implicit Representations for Grasps of Multiple Robotic Hands","Training time (hr)":"","UID":"336","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"arXiv 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Current methods for learning realistic and animatable 3D clothed avatars need either posed 3D scans or 2D images with carefully controlled user poses. In contrast, our goal is to learn an avatar from only 2D images of people in unconstrained poses. Given a set of images, our method estimates a detailed 3D surface from each image and then combines these into an animatable avatar. Implicit functions are well suited to the first task, as they can capture details like hair and clothes. Current methods, however, are not robust to varied human poses and often produce 3D surfaces with broken or disembodied limbs, missing details, or non-human shapes. The problem is that these methods use global feature encoders that are sensitive to global pose. To address this, we propose ICON (\"Implicit Clothed humans Obtained from Normals\"), which, instead, uses local features. ICON has two main modules, both of which exploit the SMPL(-X) body model. First, ICON infers detailed clothed-human normals (front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware implicit surface regressor produces an iso-surface of a human occupancy field. Importantly, at inference time, a feedback loop alternates between refining the SMPL(-X) mesh using the inferred clothed normals and then refining the normals. Given multiple reconstructed frames of a subject in varied poses, we use SCANimate to produce an animatable avatar from them. Evaluation on the AGORA and CAPE datasets shows that ICON outperforms the state of the art in reconstruction, even with heavily limited training data. Additionally, it is much more robust to out-of-distribution samples, e.g., in-the-wild poses/images and out-of-frame cropping. ICON takes a step towards robust 3D clothed human reconstruction from in-the-wild images. This enables creating avatars directly from video with personalized and natural pose-dependent cloth deformation.","Authors (format: First Last, First Middle Last, ...)":"Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, Michael J. Black","Bibtex (e.g. @inproceedings...)":"@article{xiu2022icon,\n    author = {Yuliang Xiu and Jinlong Yang and Dimitrios Tzionas and Michael J. Black},\n    title = {ICON: Implicit Clothed humans Obtained from Normals},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.09127v2}\n}","Bibtex Name":"xiu2022icon","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/YuliangXiu/ICON","Coordinates all at once":"","Data Release (link)":"https://github.com/YuliangXiu/ICON","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/16/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yuliang.xiu@tuebingen.mpg.de","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Geometry Only, Human (Body), Graphics, 2D Image Neural Fields, Generalization, Local Conditioning, Data-Driven Method, Coarse-to-Fine","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"ICON","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.09127.pdf","Project webpage link":"https://icon.is.tue.mpg.de/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://youtu.be/hZd6AYin2DE","Timestamp":"7/13/2022 21:10:16","Title":"ICON: Implicit Clothed humans Obtained from Normals","Training time (hr)":"","UID":"337","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Coordinate-based neural networks parameterizing implicit surfaces have emerged as efficient representations of geometry. They effectively act as parametric level sets with the zero-level set defining the surface of interest. We present a framework that allows applying deformation operations defined for triangle meshes onto such implicit surfaces. Several of these operations can be viewed as energy-minimization problems that induce an instantaneous flow field on the explicit surface. Our method uses the flow field to deform parametric implicit surfaces by extending the classical theory of level sets. We also derive a consolidated view for existing methods on differentiable surface extraction and rendering, by formalizing connections to the level-set theory. We show that these methods drift from the theory and that our approach exhibits improvements for applications like surface smoothing, mean-curvature flow, inverse rendering and user-defined editing on implicit geometry.","Authors (format: First Last, First Middle Last, ...)":"Ishit Mehta, Manmohan Chandraker, Ravi Ramamoorthi","Bibtex (e.g. @inproceedings...)":"@article{mehta2022neuralimplicitevolution,\n    author = {Ishit Mehta and Manmohan Chandraker and Ravi Ramamoorthi},\n    title = {A Level Set Theory for Neural Implicit Evolution under Explicit Flows},\n    year = {2022},\n    month = {Apr},\n    url = {http://arxiv.org/abs/2204.07159v2}\n}","Bibtex Name":"mehta2022neuralimplicitevolution","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://drive.google.com/file/d/1xcyzqpBNiWApVKEYg8QuG29nEloU2V6_/view","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"04/14/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"ibmehta@eng.ucsd.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Geometry Only, Graphics, Editable, Object-Centric, Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Neural Implicit Evolution","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2204.07159.pdf","Project webpage link":"https://ishit.github.io/nie/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/11/2022 18:29:44","Title":"A Level Set Theory for Neural Implicit Evolution under Explicit Flows","Training time (hr)":"","UID":"338","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2022","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Positional encodings have enabled recent works to train a single adversarial network that can generate images of different scales. However, these approaches are either limited to a set of discrete scales or struggle to maintain good perceptual quality at the scales for which the model is not trained explicitly. We propose the design of scale-consistent positional encodings invariant to our generator's layers transformations. This enables the generation of arbitrary-scale images even at scales unseen during training. Moreover, we incorporate novel inter-scale augmentations into our pipeline and partial generation training to facilitate the synthesis of consistent images at arbitrary scales. Lastly, we show competitive results for a continuum of scales on various commonly used datasets for image synthesis.","Authors (format: First Last, First Middle Last, ...)":"Evangelos Ntavelis, Mohamad Shahbazi, Iason Kastanis, Radu Timofte, Martin Danelljan, Luc Van Gool","Bibtex (e.g. @inproceedings...)":"@article{ntavelis2022scaleparty,\n    author = {Evangelos Ntavelis and Mohamad Shahbazi and Iason Kastanis and Radu Timofte and Martin Danelljan and Luc Van Gool},\n    title = {Arbitrary-Scale Image Synthesis},\n    year = {2022},\n    month = {Apr},\n    url = {http://arxiv.org/abs/2204.02273v1}\n}","Bibtex Name":"ntavelis2022scaleparty","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/vglsd/ScaleParty","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"04/05/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"entavelis@vision.ee.ethz.ch","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Human (Head), 2D Image Neural Fields, Editable, Generative Models, Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"ScaleParty","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2204.02273.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/7/2022 8:46:30","Title":"Arbitrary-Scale Image Synthesis","Training time (hr)":"","UID":"339","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"The task of shape space learning involves mapping a train set of shapes to and from a latent representation space with good generalization properties. Often, real-world collections of shapes have symmetries, which can be defined as transformations that do not change the essence of the shape. A natural way to incorporate symmetries in shape space learning is to ask that the mapping to the shape space (encoder) and mapping from the shape space (decoder) are equivariant to the relevant symmetries.  In this paper, we present a framework for incorporating equivariance in encoders and decoders by introducing two contributions: (i) adapting the recent Frame Averaging (FA) framework for building generic, efficient, and maximally expressive Equivariant autoencoders; and (ii) constructing autoencoders equivariant to piecewise Euclidean motions applied to different parts of the shape. To the best of our knowledge, this is the first fully piecewise Euclidean equivariant autoencoder construction. Training our framework is simple: it uses standard reconstruction losses and does not require the introduction of new losses. Our architectures are built of standard (backbone) architectures with the appropriate frame averaging to make them equivariant. Testing our framework on both rigid shapes dataset using implicit neural representations, and articulated shape datasets using mesh-based neural networks show state-of-the-art generalization to unseen test shapes, improving relevant baselines by a large margin. In particular, our method demonstrates significant improvement in generalizing to unseen articulated poses.","Authors (format: First Last, First Middle Last, ...)":"Matan Atzmon, Koki Nagano, Sanja Fidler, Sameh Khamis, Yaron Lipman","Bibtex (e.g. @inproceedings...)":"@article{atzmon2022frameaveraging,\n    author = {Matan Atzmon and Koki Nagano and Sanja Fidler and Sameh Khamis and Yaron Lipman},\n    title = {Frame Averaging for Equivariant Shape Space Learning},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.01741v1}\n}","Bibtex Name":"atzmon2022frameaveraging","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/03/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"matanatz@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Geometry Only, Generative Models, Generalization","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Frame Averaging","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.01741.pdf","Project webpage link":"https://nv-tlabs.github.io/equivariant/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=Lft6r5oVyXM","Timestamp":"7/2/2022 11:40:38","Title":"Frame Averaging for Equivariant Shape Space Learning","Training time (hr)":"","UID":"340","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Learning 3D geometry directly from raw data, such as point clouds, triangle soups, or unoriented meshes is still a challenging task that feeds many downstream computer vision and graphics applications.\nIn this paper, we introduce SALD: a method for learning implicit neural representations of shapes directly from raw data. We generalize sign agnostic learning (SAL) to include derivatives: given an unsigned distance function to the input raw data, we advocate a novel sign agnostic regression loss, incorporating both pointwise values and gradients of the unsigned distance function. Optimizing this loss leads to a signed implicit function solution, the zero level set of which is a high-quality and valid manifold approximation to the input 3D data. The motivation behind SALD is that incorporating derivatives in a regression loss leads to a lower sample complexity, and consequently better fitting. In addition, we prove that SAL enjoys a minimal length property in 2D, favoring minimal length solutions. More importantly, we are able to show that this property still holds for SALD, i.e., with derivatives included.\nWe demonstrate the efficacy of SALD for shape space learning on two challenging datasets: ShapeNet which contains inconsistent orientation and non-manifold meshes, and D-Faust which contains raw 3D scans (triangle soups). On both these datasets, we present state-of-the-art results.","Authors (format: First Last, First Middle Last, ...)":"Matan Atzmon, Yaron Lipman","Bibtex (e.g. @inproceedings...)":"@inproceedings{atzmon2021sald,\n    year = {2021},\n    booktitle = {9th International Conference on Learning Representations, {ICLR} 2021},\n    title = {{SALD:} Sign Agnostic Learning with Derivatives},\n    author = {Matan Atzmon and Yaron Lipman}\n}","Bibtex Name":"atzmon2021sald","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/matanatz/SALD","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/9/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"matanatz@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"No","Keywords":"Geometry Only, Generative Models","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SALD","PDF link (arXiv perferred)":"https://openreview.net/pdf?id=7EDgLu9reQD","Project webpage link":"https://github.com/matanatz/SALD","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=Q1QMcsukN4k","Timestamp":"7/2/2022 11:38:52","Title":"SALD: Sign Agnostic Learning with Derivatives","Training time (hr)":"","UID":"341","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICLR 2021","Venue no Year":"ICLR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural radiance fields (NeRFs) produce state-of-the-art view synthesis results. However, they are slow to render, requiring hundreds of network evaluations per pixel to approximate a volume rendering integral. Baking NeRFs into explicit data structures enables efficient rendering, but results in a large increase in memory footprint and, in many cases, a quality reduction. In this paper, we propose a novel neural light field representation that, in contrast, is compact and directly predicts integrated radiance along rays. Our method supports rendering with a single network evaluation per pixel for small baseline light field datasets and can also be applied to larger baselines with only a few evaluations per pixel. At the core of our approach is a ray-space embedding network that maps the 4D ray-space manifold into an intermediate, interpolable latent space. Our method achieves state-of-the-art quality on dense forward-facing datasets such as the Stanford Light Field dataset. In addition, for forward-facing scenes with sparser inputs we achieve results that are competitive with NeRF-based approaches in terms of quality while providing a better speed/quality/memory trade-off with far fewer network evaluations.","Authors (format: First Last, First Middle Last, ...)":"Benjamin Attal, Jia-Bin Huang, Michael Zollhoefer, Johannes Kopf, Changil Kim","Bibtex (e.g. @inproceedings...)":"@article{attal2022neurallightfields,\n    author = {Benjamin Attal and Jia-Bin Huang and Michael Zollhoefer and Johannes Kopf and Changil Kim},\n    title = {Learning Neural Light Fields with Ray-Space Embedding Networks},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.01523v3}\n}","Bibtex Name":"attal2022neurallightfields","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/facebookresearch/neural-light-fields","Coordinates all at once":"","Data Release (link)":"https://drive.google.com/drive/folders/1MnniS2uk5vIQ4XtzVSbD-CY7oEZQoL-2","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/02/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"battal@andrew.cmu.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Speed & Computational Efficiency, Graphics, Compression","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Neural Light Fields","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.01523.pdf","Project webpage link":"https://neural-light-fields.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=Emnd16FxVHc&feature=emb_title","Timestamp":"6/27/2022 16:20:12","Title":"Learning Neural Light Fields with Ray-Space Embedding Networks","Training time (hr)":"","UID":"342","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recent works with an implicit neural function shed light on representing images in arbitrary resolution. However, a standalone multi-layer perceptron shows limited performance in learning high-frequency components. In this paper, we propose a Local Texture Estimator (LTE), a dominant-frequency estimator for natural images, enabling an implicit function to capture fine details while reconstructing images in a continuous manner. When jointly trained with a deep super-resolution (SR) architecture, LTE is capable of characterizing image textures in 2D Fourier space. We show that an LTE-based neural function achieves favorable performance against existing deep SR methods within an arbitrary-scale factor. Furthermore, we demonstrate that our implementation takes the shortest running time compared to previous works.","Authors (format: First Last, First Middle Last, ...)":"Jaewon Lee, Kyong Hwan Jin","Bibtex (e.g. @inproceedings...)":"@article{lee2022lte,\n    author = {Jaewon Lee and Kyong Hwan Jin},\n    title = {Local Texture Estimator for Implicit Representation Function},\n    year = {2021},\n    month = {Nov},\n    url = {http://arxiv.org/abs/2111.08918v6}\n}","Bibtex Name":"lee2022lte","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/jaewon-lee-b/lte","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/17/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"jaewon.lee.b@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"2D Image Neural Fields, Sampling, Generalization, Local Conditioning, Data-Driven Method, Hypernetwork/Meta-learning, Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"LTE","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2111.08918.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/25/2022 22:14:55","Title":"Local Texture Estimator for Implicit Representation Function","Training time (hr)":"","UID":"343","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Inverse rendering is an ill-posed problem. Previous work has sought to resolve this by focussing on priors for object or scene shape or appearance. In this work, we instead focus on a prior for natural illuminations. Current methods rely on spherical harmonic lighting or other generic representations and, at best, a simplistic prior on the parameters. We propose a conditional neural field representation based on a variational auto-decoder with a SIREN network and, extending Vector Neurons, build equivariance directly into the network. Using this we develop a rotation-equivariant, high dynamic range (HDR) neural illumination model that is compact and able to express complex, high-frequency features of natural environment maps. Training our model on a curated dataset of 1.6K HDR environment maps of natural scenes, we compare it against traditional representations, demonstrate its applicability for an inverse rendering task and show environment map completion from partial observations. A PyTorch implementation, our dataset and trained models can be found at jadgardner.github.io/RENI.","Authors (format: First Last, First Middle Last, ...)":"James A. D. Gardner, Bernhard Egger, William A. P. Smith","Bibtex (e.g. @inproceedings...)":"@article{gardner2022reni,\n    author = {James A. D. Gardner and Bernhard Egger and William A. P. Smith},\n    title = {Rotation-Equivariant Conditional Spherical Neural Fields for Learning a Natural Illumination Prior},\n    year = {2022},\n    month = {Jun},\n    url = {http://arxiv.org/abs/2206.03858v2}\n}","Bibtex Name":"gardner2022reni","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/JADGardner/RENI","Coordinates all at once":"","Data Release (link)":"https://drive.google.com/drive/folders/1pMx2oolATFSRIZ2iRc9x2cNrqZQDB1En","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"06/07/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"james.gardner@york.ac.uk","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Generative Models, Illumination Prior","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"RENI","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2206.03858.pdf","Project webpage link":"https://jadgardner.github.io/RENI.html","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/24/2022 5:57:01","Title":"Rotation-Equivariant Conditional Spherical Neural Fields for Learning a Natural Illumination Prior","Training time (hr)":"","UID":"344","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose a novel optimization-based paradigm for 3D human model fitting on images and scans. In contrast to existing approaches that directly regress the parameters of a low-dimensional statistical body model (e.g. SMPL) from input images, we train an ensemble of per-vertex neural fields network. The network predicts, in a distributed manner, the vertex descent direction towards the ground truth, based on neural features extracted at the current vertex projection. At inference, we employ this network, dubbed LVD, within a gradient-descent optimization pipeline until its convergence, which typically occurs in a fraction of a second even when initializing all vertices into a single point. An exhaustive evaluation demonstrates that our approach is able to capture the underlying body of clothed people with very different body shapes, achieving a significant improvement compared to state-of-the-art. LVD is also applicable to 3D model fitting of humans and hands, for which we show a significant improvement to the SOTA with a much simpler and faster method.","Authors (format: First Last, First Middle Last, ...)":"Enric Corona, Gerard Pons-Moll, Guillem Aleny\u00e0, Francesc Moreno-Noguer","Bibtex (e.g. @inproceedings...)":"@article{corona2022learnedvertexdescent(lvd),\n    author = {Enric Corona and Gerard Pons-Moll and Guillem Alenya and Francesc Moreno-Noguer},\n    title = {Learned Vertex Descent: A New Direction for 3D Human Model Fitting},\n    year = {2022},\n    month = {May},\n    url = {http://arxiv.org/abs/2205.06254v2}\n}","Bibtex Name":"corona2022learnedvertexdescent(lvd)","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"05/12/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"enriccorona93@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Human (Body), Generalization, Global Conditioning, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Learned Vertex Descent (LVD)","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2205.06254.pdf","Project webpage link":"https://www.iri.upc.edu/people/ecorona/lvd/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=2M7uoYdauJw","Timestamp":"6/21/2022 18:21:14","Title":"Learned Vertex Descent: A New Direction for 3D Human Model Fitting","Training time (hr)":"","UID":"345","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"Arxiv 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"This paper proposes a do-it-all neural model of human hands, named LISA. The model can capture accurate hand shape and appearance, generalize to arbitrary hand subjects, provide dense surface correspondences, be reconstructed from images in the wild and easily animated. We train LISA by minimizing the shape and appearance losses on a large set of multi-view RGB image sequences annotated with coarse 3D poses of the hand skeleton. For a 3D point in the hand local coordinate, our model predicts the color and the signed distance with respect to each hand bone independently, and then combines the per-bone predictions using predicted skinning weights. The shape, color and pose representations are disentangled by design, allowing to estimate or animate only selected parameters. We experimentally demonstrate that LISA can accurately reconstruct a dynamic hand from monocular or multi-view sequences, achieving a noticeably higher quality of reconstructed hand shapes compared to baseline approaches. Project page: https://www.iri.upc.edu/people/ecorona/lisa/.","Authors (format: First Last, First Middle Last, ...)":"Enric Corona, Tomas Hodan, Minh Vo, Francesc Moreno-Noguer, Chris Sweeney, Richard Newcombe, Lingni Ma","Bibtex (e.g. @inproceedings...)":"@article{corona2022lisa,\n    author = {Enric Corona and Tomas Hodan and Minh Vo and Francesc Moreno-Noguer and Chris Sweeney and Richard Newcombe and Lingni Ma},\n    title = {LISA: Learning Implicit Shape and Appearance of Hands},\n    year = {2022},\n    month = {Apr},\n    url = {http://arxiv.org/abs/2204.01695v1}\n}","Bibtex Name":"corona2022lisa","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"04/04/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"enriccorona93@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Human (Body), Generative Models, Generalization, Human (Hands)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"LISA","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2204.01695.pdf","Project webpage link":"https://www.iri.upc.edu/people/ecorona/lisa/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=bra6gN81cjo","Timestamp":"6/21/2022 18:18:09","Title":"LISA: Learning Implicit Shape and Appearance of Hands","Training time (hr)":"","UID":"346","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose an analysis-by-synthesis method for fast multi-view 3D reconstruction of opaque objects with arbitrary materials and illumination.\n\nState-of-the-art methods use both neural surface representations and neural rendering. While flexible, neural surface representations are a significant bottleneck in optimization runtime. Instead, we represent surfaces as triangle meshes and build a differentiable rendering pipeline around triangle rasterization and neural shading. The renderer is used in a gradient descent optimization where both a triangle mesh and a neural shader are jointly optimized to reproduce the multi-view images.\n\nWe evaluate our method on a public 3D reconstruction dataset and show that it can match the reconstruction accuracy of traditional baselines and neural approaches while surpassing them in optimization runtime. Additionally, we investigate the shader and find that it learns an interpretable representation of appearance, enabling applications such as 3D material editing.","Authors (format: First Last, First Middle Last, ...)":"Markus Worchel, Rodrigo Diaz, Weiwen Hu, Oliver Schreer, Ingo Feldmann, Peter Eisert","Bibtex (e.g. @inproceedings...)":"@inproceedings{worchel:2022:nds,\n    year = {2022},\n    month = {June},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    title = {Multi-View Mesh Reconstruction with Neural Deferred Shading},\n    author = {Markus Worchel and Rodrigo Diaz and Weiwen Hu and Oliver Schreer and Ingo Feldmann and Peter Eisert}\n}","Bibtex Name":"worchel:2022:nds","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/fraunhoferhhi/neural-deferred-shading","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/2/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"m.worchel@tu-berlin.de","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"No","Keywords":"Speed & Computational Efficiency, Graphics, Coarse-to-Fine","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NDS","PDF link (arXiv perferred)":"https://mworchel.github.io/assets/papers/neural_deferred_shading_with_supp.pdf","Project webpage link":"https://fraunhoferhhi.github.io/neural-deferred-shading/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://youtu.be/nIqmuylmpFY","Timestamp":"6/21/2022 6:04:15","Title":"Multi-View Mesh Reconstruction with Neural Deferred Shading","Training time (hr)":"","UID":"347","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Modelling individual objects as Neural Radiance Fields (NeRFs) within a robotic context can benefit many downstream tasks such as scene understanding and object manipulation. However, real-world training data collected by a robot deviate from the ideal in several key aspects. (i) The trajectories are constrained and full visual coverage is not guaranteed - especially when obstructions are present. (ii) The poses associated with the images are noisy. (iii) The objects are not easily isolated from the background. This paper addresses the above three points and uses the outputs of an object-based SLAM system to bound objects in the scene with coarse primitives and - in concert with instance masks - identify obstructions in the training images. Objects are therefore automatically bounded, and non-relevant geometry is excluded from the NeRF representation. The method's performance is benchmarked under ideal conditions and tested against errors in the poses and instance masks. Our results show that object-based NeRFs are robust to pose variations but sensitive to the quality of the instance masks.","Authors (format: First Last, First Middle Last, ...)":"Jad Abou-Chakra, Feras Dayoub, Niko S\u00fcnderhauf","Bibtex (e.g. @inproceedings...)":"@article{abou-chakra2022implicit,\n    author = {Jad Abou-Chakra and Feras Dayoub and Niko Sunderhauf},\n    title = {Implicit Object Mapping With Noisy Data},\n    year = {2022},\n    month = {Apr},\n    url = {http://arxiv.org/abs/2204.10516v1}\n}","Bibtex Name":"abou-chakra2022implicit","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"04/22/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"jad.chakra@hdr.qut.edu.au","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Robotics, Object-Centric","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2204.10516.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/20/2022 21:13:14","Title":"Implicit Object Mapping With Noisy Data","Training time (hr)":"","UID":"348","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce the Satellite Neural Radiance Field (Sat-NeRF), a new end-to-end model for learning multi-view satellite photogrammetry in the wild. Sat-NeRF combines some of the latest trends in neural rendering with native satellite camera models, represented by rational polynomial coefficient (RPC) functions. The proposed method renders new views and infers surface models of similar quality to those obtained with traditional state-of-the-art stereo pipelines. Multi-date images exhibit significant changes in appearance, mainly due to varying shadows and transient objects (cars, vegetation). Robustness to these challenges is achieved by a shadow-aware irradiance model and uncertainty weighting to deal with transient phenomena that cannot be explained by the position of the sun. We evaluate Sat-NeRF using WorldView-3 images from different locations and stress the advantages of applying a bundle adjustment to the satellite camera models prior to training. This boosts the network performance and can optionally be used to extract additional cues for depth supervision.","Authors (format: First Last, First Middle Last, ...)":"Roger Mar\u00ed, Gabriele Facciolo, Thibaud Ehret","Bibtex (e.g. @inproceedings...)":"@article{mari2022satnerf,\n    author = {Roger Mari and Gabriele Facciolo and Thibaud Ehret},\n    title = {Sat-NeRF: Learning Multi-View Satellite Photogrammetry With Transient Objects and Shadow Modeling Using RPC Cameras},\n    year = {2022},\n    month = {Mar},\n    url = {http://arxiv.org/abs/2203.08896v2}\n}","Bibtex Name":"mari2022satnerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/centreborelli/satnerf","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"03/16/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"roger.mari@ens-paris-saclay.fr","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Camera Parameter Estimation, Material/Lighting Estimation, Large-Scale Scenes, Satellite","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Sat-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2203.08896.pdf","Project webpage link":"https://centreborelli.github.io/satnerf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/20/2022 15:53:59","Title":"Sat-NeRF: Learning Multi-View Satellite Photogrammetry With Transient Objects and Shadow Modeling Using RPC Cameras","Training time (hr)":"","UID":"349","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022 EarthVision Workshop","Venue no Year":"CVPR 2022 EarthVision Workshop","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce UNIST, the first deep neural implicit model for general-purpose, unpaired shape-to-shape translation, in both 2D and 3D domains. Our model is built on autoencoding implicit fields, rather than point clouds which represents the state of the art. Furthermore, our translation network is trained to perform the task over a latent grid representation which combines the merits of both latent-space processing and position awareness, to not only enable drastic shape transforms but also well preserve spatial features and fine local details for natural shape translations. With the same network architecture and only dictated by the input domain pairs, our model can learn both style-preserving content alteration and content-preserving style transfer. We demonstrate the generality and quality of the translation results, and compare them to well-known baselines. Code is available at https://qiminchen.github.io/unist/.","Authors (format: First Last, First Middle Last, ...)":"Qimin Chen, Johannes Merz, Aditya Sanghi, Hooman Shayani, Ali Mahdavi-Amiri, Hao Zhang","Bibtex (e.g. @inproceedings...)":"@article{chen2022unist,\n    author = {Qimin Chen and Johannes Merz and Aditya Sanghi and Hooman Shayani and Ali Mahdavi-Amiri and Hao Zhang},\n    title = {UNIST: Unpaired Neural Implicit Shape Translation Network},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.05381v2}\n}","Bibtex Name":"chen2022unist","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/qiminchen/UNIST","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/10/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"qca43@sfu.ca","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Positional Encoding, Shape Translation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"UNIST","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.05381.pdf","Project webpage link":"https://qiminchen.github.io/unist/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=FOfMNhDYA84","Timestamp":"6/20/2022 14:50:37","Title":"UNIST: Unpaired Neural Implicit Shape Translation Network","Training time (hr)":"","UID":"350","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Modelling interactions between humans and objects in natural environments is central to many applications including gaming, virtual and mixed reality, as well as human behavior analysis and human-robot collaboration. This challenging operation scenario requires generalization to vast number of objects, scenes, and human actions. Unfortunately, there exist no such dataset. Moreover, this data needs to be acquired in diverse natural environments, which rules out 4D scanners and marker based capture systems. We present BEHAVE dataset, the first full body human- object interaction dataset with multi-view RGBD frames and corresponding 3D SMPL and object fits along with the annotated contacts between them. We record around 15k frames at 5 locations with 8 subjects performing a wide range of interactions with 20 common objects. We use this data to learn a model that can jointly track humans and objects in natural environments with an easy-to-use portable multi-camera setup. Our key insight is to predict correspondences from the human and the object to a statistical body model to obtain human-object contacts during interactions. Our approach can record and track not just the humans and objects but also their interactions, modeled as surface contacts, in 3D. Our code and data can be found at: http://virtualhumans.mpi-inf.mpg.de/behave","Authors (format: First Last, First Middle Last, ...)":"Bharat Lal Bhatnagar, Xianghui Xie, Ilya A. Petrov, Cristian Sminchisescu, Christian Theobalt, Gerard Pons-Moll","Bibtex (e.g. @inproceedings...)":"@article{bhatnagar2022behave,\n    author = {Bharat Lal Bhatnagar and Xianghui Xie and Ilya A. Petrov and Cristian Sminchisescu and Christian Theobalt and Gerard Pons-Moll},\n    title = {BEHAVE: Dataset and Method for Tracking Human Object Interactions},\n    year = {2022},\n    month = {Apr},\n    url = {http://arxiv.org/abs/2204.06950v1}\n}","Bibtex Name":"bhatnagar2022behave","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"https://github.com/xiexh20/behave-dataset","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"04/14/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"bbhatnag@mpi-inf.mpg.de","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Dynamic/Temporal, Human (Body), Object-Centric","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"BEHAVE","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2204.06950.pdf","Project webpage link":"http://virtualhumans.mpi-inf.mpg.de/behave/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://youtu.be/8SbNo4ePMGc","Timestamp":"6/20/2022 11:53:56","Title":"BEHAVE: Dataset and Method for Tracking Human Object Interactions","Training time (hr)":"","UID":"351","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present Panoptic Neural Fields (PNF), an object-aware neural scene representation that decomposes a scene into a set of objects (things) and background (stuff). Each object is represented by an oriented 3D bounding box and a multi-layer perceptron (MLP) that takes position, direction, and time and outputs density and radiance. The background stuff is represented by a similar MLP that additionally outputs semantic labels. Each object MLPs are instance-specific and thus can be smaller and faster than previous object-aware approaches, while still leveraging category-specific priors incorporated via meta-learned initialization. Our model builds a panoptic radiance field representation of any scene from just color images. We use off-the-shelf algorithms to predict camera poses, object tracks, and 2D image semantic segmentations. Then we jointly optimize the MLP weights and bounding box parameters using analysis-by-synthesis with self-supervision from color images and pseudo-supervision from predicted semantic segmentations. During experiments with real-world dynamic scenes, we find that our model can be used effectively for several tasks like novel view synthesis, 2D panoptic segmentation, 3D scene editing, and multiview depth prediction.","Authors (format: First Last, First Middle Last, ...)":"Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Caroline Pantofaru, Leonidas Guibas, Andrea Tagliasacchi, Frank Dellaert, Thomas Funkhouser","Bibtex (e.g. @inproceedings...)":"@article{kundu2022pnf,\n    author = {Abhijit Kundu and Kyle Genova and Xiaoqi Yin and Alireza Fathi and Caroline Pantofaru and Leonidas Guibas and Andrea Tagliasacchi and Frank Dellaert and Thomas Funkhouser},\n    title = {Panoptic Neural Fields: A Semantic Object-Aware Neural Scene Representation},\n    year = {2022},\n    month = {May},\n    url = {http://arxiv.org/abs/2205.04334v1}\n}","Bibtex Name":"kundu2022pnf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"05/09/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"abhijitkundu@google.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Dynamic/Temporal, Hypernetwork/Meta-learning, Object-Centric, Semantics","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"PNF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2205.04334.pdf","Project webpage link":"https://abhijitkundu.info/projects/pnf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://youtu.be/3aXHxuQ-xBM","Timestamp":"6/20/2022 11:44:19","Title":"Panoptic Neural Fields: A Semantic Object-Aware Neural Scene Representation","Training time (hr)":"","UID":"352","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Traditional 3D morphable face models (3DMMs) provide fine-grained control over expression but cannot easily capture geometric and appearance details. Neural volumetric representations approach photorealism but are hard to animate and do not generalize well to unseen expressions. To tackle this problem, we propose IMavatar (Implicit Morphable avatar), a novel method for learning implicit head avatars from monocular videos. Inspired by the fine-grained control mechanisms afforded by conventional 3DMMs, we represent the expression- and pose- related deformations via learned blendshapes and skinning fields. These attributes are pose-independent and can be used to morph the canonical geometry and texture fields given novel expression and pose parameters. We employ ray marching and iterative root-finding to locate the canonical surface intersection for each pixel. A key contribution is our novel analytical gradient formulation that enables end-to-end training of IMavatars from videos. We show quantitatively and qualitatively that our method improves geometry and covers a more complete expression space compared to state-of-the-art methods.","Authors (format: First Last, First Middle Last, ...)":"Yufeng Zheng, Victoria Fern\u00e1ndez Abrevaya, Marcel C. B\u00fchler, Xu Chen, Michael J. Black, Otmar Hilliges","Bibtex (e.g. @inproceedings...)":"@article{zheng2022imavatar,\n    author = {Yufeng Zheng and Victoria Fernandez Abrevaya and Marcel C. Buhler and Xu Chen and Michael J. Black and Otmar Hilliges},\n    title = {I M Avatar: Implicit Morphable Head Avatars from Videos},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.07471v5}\n}","Bibtex Name":"zheng2022imavatar","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/zhengyuf/IMavatar","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/14/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yufeng.zheng@inf.ethz.ch","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Dynamic/Temporal, Human (Head), Graphics, Generalization, Image-Based Rendering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"IMavatar","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.07471.pdf","Project webpage link":"https://ait.ethz.ch/projects/2022/IMavatar/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://youtu.be/915baJNX-IU","Timestamp":"6/20/2022 11:05:00","Title":"I M Avatar: Implicit Morphable Head Avatars from Videos","Training time (hr)":"","UID":"353","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Soft bodies made from flexible and deformable materials are popular in many robotics applications, but their proprioceptive sensing has been a long-standing challenge. In other words, there has hardly been a method to measure and model the high-dimensional 3D shapes of soft bodies with internal sensors. We propose a framework to measure the high-resolution 3D shapes of soft bodies in real-time with embedded cameras. The cameras capture visual patterns inside a soft body, and a convolutional neural network (CNN) produces a latent code representing the deformation state, which can then be used to reconstruct the body's 3D shape using another neural network. We test the framework on various soft bodies, such as a Baymax-shaped toy, a latex balloon, and some soft robot fingers, and achieve real-time computation ($\\leq$2.5ms/frame) for robust shape estimation with high precision ($\\leq$1% relative error) and high resolution. We believe the method could be applied to soft robotics and human-robot interaction for proprioceptive shape sensing. Our code is available at https://ai4ce.github.io/Deep-Soft-Prorioception/","Authors (format: First Last, First Middle Last, ...)":"Ruoyu Wang, Shiheng Wang, Songyu Du, Erdong Xiao, Wenzhen Yuan, Chen Feng","Bibtex (e.g. @inproceedings...)":"@article{wang2020deepsoro,\n    author = {Ruoyu Wang and Shiheng Wang and Songyu Du and Erdong Xiao and Wenzhen Yuan and Chen Feng},\n    title = {Real-time Soft Body 3D Proprioception via Deep Vision-based Sensing},\n    year = {2019},\n    month = {Apr},\n    url = {http://arxiv.org/abs/1904.03820v3}\n}","Bibtex Name":"wang2020deepsoro","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/ai4ce/DeepSoRo","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"04/08/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"cfeng@nyu.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Robotics","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DeepSoRo","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1904.03820.pdf","Project webpage link":"https://ai4ce.github.io/DeepSoRo/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://youtu.be/kVirop7rf8o","Timestamp":"6/20/2022 11:01:27","Title":"Real-time Soft Body 3D Proprioception via Deep Vision-based Sensing","Training time (hr)":"","UID":"354","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"RA-L 2020","Venue no Year":"RA-L","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Non-rigid point cloud registration is a key component in many computer vision and computer graphics applications. The high complexity of the unknown non-rigid motion make this task a challenging problem. In this paper, we break down this problem via hierarchical motion decomposition. Our method called Neural Deformation Pyramid (NDP) represents non-rigid motion using a pyramid architecture. Each pyramid level, denoted by a Multi-Layer Perception (MLP), takes as input a sinusoidally encoded 3D point and outputs its motion increments from the previous level. The sinusoidal function starts with a low input frequency and gradually increases when the pyramid level goes down. This allows a multi-level rigid to nonrigid motion decomposition and also speeds up the solving by 50 times compared to the existing MLP-based approach. Our method achieves advanced partialto-partial non-rigid point cloud registration results on the 4DMatch/4DLoMatch benchmark under both no-learned and supervised settings.","Authors (format: First Last, First Middle Last, ...)":"Yang Li, Tatsuya Harada","Bibtex (e.g. @inproceedings...)":"@article{li2021deformationpyramid,\n    author = {Yang Li and Tatsuya Harada},\n    title = {Non-rigid Point Cloud Registration with Neural Deformation Pyramid},\n    year = {2022},\n    month = {May},\n    url = {http://arxiv.org/abs/2205.12796v2}\n}","Bibtex Name":"li2021deformationpyramid","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/rabbityl/DeformationPyramid","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"05/25/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"liyang@mi.t.u-tokyo.ac.jp","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Dynamic/Temporal, Coarse-to-Fine, Positional Encoding, Motion estimation, Shape registration, Deformation representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DeformationPyramid","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2205.12796.pdf","Project webpage link":"https://github.com/rabbityl/DeformationPyramid","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/20/2022 11:01:12","Title":"Non-rigid Point Cloud Registration with Neural Deformation Pyramid","Training time (hr)":"","UID":"355","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"Arxiv 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present GeoNeRF, a generalizable photorealistic novel view synthesis method based on neural radiance fields. Our approach consists of two main stages: a geometry reasoner and a renderer. To render a novel view, the geometry reasoner first constructs cascaded cost volumes for each nearby source view. Then, using a Transformer-based attention mechanism and the cascaded cost volumes, the renderer infers geometry and appearance, and renders detailed images via classical volume rendering techniques. This architecture, in particular, allows sophisticated occlusion reasoning, gathering information from consistent source views. Moreover, our method can easily be fine-tuned on a single scene, and renders competitive results with per-scene optimized neural rendering methods with a fraction of computational cost. Experiments show that GeoNeRF outperforms state-of-the-art generalizable neural rendering models on various synthetic and real datasets. Lastly, with a slight modification to the geometry reasoner, we also propose an alternative model that adapts to RGBD images. This model directly exploits the depth information often available thanks to depth sensors. The implementation code is available at https://www.idiap.ch/paper/geonerf.","Authors (format: First Last, First Middle Last, ...)":"Mohammad Mahdi Johari, Yann Lepoittevin, Fran\u00e7ois Fleuret","Bibtex (e.g. @inproceedings...)":"@article{johari2022geonerf,\n    author = {Mohammad Mahdi Johari and Yann Lepoittevin and Francois Fleuret},\n    title = {GeoNeRF: Generalizing NeRF with Geometry Priors},\n    year = {2021},\n    month = {Nov},\n    url = {http://arxiv.org/abs/2111.13539v2}\n}","Bibtex Name":"johari2022geonerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/idiap/GeoNeRF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/26/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"mohammad.johari@idiap.ch","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Generalization, Image-Based Rendering, Coarse-to-Fine","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"GeoNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2111.13539.pdf","Project webpage link":"https://www.idiap.ch/paper/geonerf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=-jNBsG3IP54","Timestamp":"6/20/2022 10:59:59","Title":"GeoNeRF: Generalizing NeRF with Geometry Priors","Training time (hr)":"","UID":"356","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose DeepMapping, a novel registration framework using deep neural networks (DNNs) as auxiliary functions to align multiple point clouds from scratch to a globally consistent frame. We use DNNs to model the highly non-convex mapping process that traditionally involves hand-crafted data association, sensor pose initialization, and global refinement. Our key novelty is that \"training\" these DNNs with properly defined unsupervised losses is equivalent to solving the underlying registration problem, but less sensitive to good initialization than ICP. Our framework contains two DNNs: a localization network that estimates the poses for input point clouds, and a map network that models the scene structure by estimating the occupancy status of global coordinates. This allows us to convert the registration problem to a binary occupancy classification, which can be solved efficiently using gradient-based optimization. We further show that DeepMapping can be readily extended to address the problem of Lidar SLAM by imposing geometric constraints between consecutive point clouds. Experiments are conducted on both simulated and real datasets. Qualitative and quantitative comparisons demonstrate that DeepMapping often enables more robust and accurate global registration of multiple point clouds than existing techniques. Our code is available at https://ai4ce.github.io/DeepMapping/.","Authors (format: First Last, First Middle Last, ...)":"Li Ding, Chen Feng","Bibtex (e.g. @inproceedings...)":"@article{ding2019deepmapping,\n    author = {Li Ding and Chen Feng},\n    title = {DeepMapping: Unsupervised Map Estimation From Multiple Point Clouds},\n    year = {2018},\n    month = {Nov},\n    url = {http://arxiv.org/abs/1811.11397v2}\n}","Bibtex Name":"ding2019deepmapping","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/ai4ce/DeepMapping","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/28/2018","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"cfeng@nyu.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Geometry Only, Robotics","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DeepMapping","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1811.11397.pdf","Project webpage link":"https://ai4ce.github.io/DeepMapping/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://youtu.be/ts4ogdJW4_8?t=4722","Timestamp":"6/20/2022 10:59:17","Title":"DeepMapping: Unsupervised Map Estimation From Multiple Point Clouds","Training time (hr)":"","UID":"357","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2019","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2019"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recent deep networks that directly handle points in a point set, e.g., PointNet, have been state-of-the-art for supervised learning tasks on point clouds such as classification and segmentation. In this work, a novel end-to-end deep auto-encoder is proposed to address unsupervised learning challenges on point clouds. On the encoder side, a graph-based enhancement is enforced to promote local structures on top of PointNet. Then, a novel folding-based decoder deforms a canonical 2D grid onto the underlying 3D object surface of a point cloud, achieving low reconstruction errors even for objects with delicate structures. The proposed decoder only uses about 7% parameters of a decoder with fully-connected neural networks, yet leads to a more discriminative representation that achieves higher linear SVM classification accuracy than the benchmark. In addition, the proposed decoder structure is shown, in theory, to be a generic architecture that is able to reconstruct an arbitrary point cloud from a 2D grid. Our code is available at http://www.merl.com/research/license#FoldingNet","Authors (format: First Last, First Middle Last, ...)":"Yaoqing Yang, Chen Feng, Yiru Shen, Dong Tian","Bibtex (e.g. @inproceedings...)":"@article{yang2018foldingnet,\n    author = {Yaoqing Yang and Chen Feng and Yiru Shen and Dong Tian},\n    title = {FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation},\n    year = {2017},\n    month = {Dec},\n    url = {http://arxiv.org/abs/1712.07262v2}\n}","Bibtex Name":"yang2018foldingnet","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/19/2017","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"cfeng@nyu.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Geometry Only, Robotics, Object-Centric","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"FoldingNet","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1712.07262.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://youtu.be/csC6SodV6vk","Timestamp":"6/20/2022 10:27:48","Title":"FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation","Training time (hr)":"","UID":"358","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2018","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2018"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Image-based volumetric humans using pixel-aligned features promise generalization to unseen poses and identities. Prior work leverages global spatial encodings and multi-view geometric consistency to reduce spatial ambiguity. However, global encodings often suffer from overfitting to the distribution of the training data, and it is difficult to learn multi-view consistent reconstruction from sparse views. In this work, we investigate common issues with existing spatial encodings and propose a simple yet highly effective approach to modeling high-fidelity volumetric humans from sparse views. One of the key ideas is to encode relative spatial 3D information via sparse 3D keypoints. This approach is robust to the sparsity of viewpoints and cross-dataset domain gap. Our approach outperforms state-of-the-art methods for head reconstruction. On human body reconstruction for unseen subjects, we also achieve performance comparable to prior work that uses a parametric human body model and temporal feature aggregation. Our experiments show that a majority of errors in prior work stem from an inappropriate choice of spatial encoding and thus we suggest a new direction for high-fidelity image-based human modeling. https://markomih.github.io/KeypointNeRF","Authors (format: First Last, First Middle Last, ...)":"Marko Mihajlovic, Aayush Bansal, Michael Zollhoefer, Siyu Tang, Shunsuke Saito","Bibtex (e.g. @inproceedings...)":"@article{mihajlovic2022keypointnerf,\n    author = {Marko Mihajlovic and Aayush Bansal and Michael Zollhoefer and Siyu Tang and Shunsuke Saito},\n    title = {KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints},\n    year = {2022},\n    month = {May},\n    url = {http://arxiv.org/abs/2205.04992v2}\n}","Bibtex Name":"mihajlovic2022keypointnerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"05/10/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"shunsuke.saito16@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Sparse Reconstruction, Human (Body), Human (Head), Generalization, Local Conditioning, Image-Based Rendering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"KeypointNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2205.04992.pdf","Project webpage link":"https://markomih.github.io/KeypointNeRF/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/18/2022 14:37:16","Title":"KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints","Training time (hr)":"","UID":"359","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Implicit neural representations (INRs) have become fast, lightweight tools for storing continuous data, but to date there is no general method for learning directly with INRs as a data representation. We introduce a principled deep learning framework for learning and inference directly with INRs of any type without reverting to grid-based features or operations. Our INR-Nets evaluate INRs on a low discrepancy sequence, enabling quasi-Monte Carlo (QMC) integration throughout the network. We prove INR-Nets are universal approximators on a large class of maps between $L^2$ functions. Additionally, INR-Nets have convergent gradients under the empirical measure, enabling backpropagation. We design INR-Nets as a continuous generalization of discrete networks, enabling them to be initialized with pre-trained models. We demonstrate learning of INR-Nets on classification (INR$\\to$label) and segmentation (INR$\\to$INR) tasks.","Authors (format: First Last, First Middle Last, ...)":"Clinton J. Wang, Polina Golland","Bibtex (e.g. @inproceedings...)":"@article{wang2022inrnet,\n    author = {Clinton J. Wang and Polina Golland},\n    title = {Deep Learning on Implicit Neural Datasets},\n    year = {2022},\n    month = {Jun},\n    url = {http://arxiv.org/abs/2206.01178v1}\n}","Bibtex Name":"wang2022inrnet","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"06/02/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"clintonw@csail.mit.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Fundamentals, Generalization","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"INR-Net","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2206.01178.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/17/2022 10:28:09","Title":"Deep Learning on Implicit Neural Datasets","Training time (hr)":"","UID":"360","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"arXiv 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Implicit neural representations (INRs) have recently emerged as a promising alternative to classical discretized representations of signals. Nevertheless, despite their practical success, we still do not understand how INRs represent signals. We propose a novel unified perspective to theoretically analyse INRs. Leveraging results from harmonic analysis and deep learning theory, we show that most INR families are analogous to structured signal dictionaries whose atoms are integer harmonics of the set of initial mapping frequencies. This structure allows INRs to express signals with an exponentially increasing frequency support using a number of parameters that only grows linearly with depth. We also explore the inductive bias of INRs exploiting recent results about the empirical neural tangent kernel (NTK). Specifically, we show that the eigenfunctions of the NTK can be seen as dictionary atoms whose inner product with the target signal determines the final performance of their reconstruction. In this regard, we reveal that meta-learning has a reshaping effect on the NTK analogous to dictionary learning, building dictionary atoms as a combination of the examples seen during meta-training. Our results permit to design and tune novel INR architectures, but can also be of interest for the wider deep learning theory community.","Authors (format: First Last, First Middle Last, ...)":"Gizem Y\u00fcce, Guillermo Ortiz-Jim\u00e9nez, Beril Besbinar, Pascal Frossard","Bibtex (e.g. @inproceedings...)":"@article{yuce2022a,\n    author = {Gizem Yuce and Guillermo Ortiz-Jimenez and Beril Besbinar and Pascal Frossard},\n    title = {A Structured Dictionary Perspective on Implicit Neural Representations},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.01917v2}\n}","Bibtex Name":"yuce2022a","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/gortizji/inr_dictionaries","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/03/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"guillermo.ortizjimenez@epfl.ch","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Fundamentals","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.01917.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/17/2022 9:52:29","Title":"A Structured Dictionary Perspective on Implicit Neural Representations","Training time (hr)":"","UID":"361","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Inverse rendering of an object under entirely unknown capture conditions is a fundamental challenge in computer vision and graphics. Neural approaches such as NeRF have achieved photorealistic results on novel view synthesis, but they require known camera poses. Solving this problem with unknown camera poses is highly challenging as it requires joint optimization over shape, radiance, and pose. This problem is exacerbated when the input images are captured in the wild with varying backgrounds and illuminations. Standard pose estimation techniques fail in such image collections in the wild due to very few estimated correspondences across images. Furthermore, NeRF cannot relight a scene under any illumination, as it operates on radiance (the product of reflectance and illumination). We propose a joint optimization framework to estimate the shape, BRDF, and per-image camera pose and illumination. Our method works on in-the-wild online image collections of an object and produces relightable 3D assets for several use-cases such as AR/VR. To our knowledge, our method is the first to tackle this severely unconstrained task with minimal user interaction. Project page: https://markboss.me/publication/2022-samurai/ Video: https://youtu.be/LlYuGDjXp-8","Authors (format: First Last, First Middle Last, ...)":"Mark Boss, Andreas Engelhardt, Abhishek Kar, Yuanzhen Li, Deqing Sun, Jonathan T. Barron, Hendrik P. A. Lensch, Varun Jampani","Bibtex (e.g. @inproceedings...)":"@article{boss2022samurai,\n    author = {Mark Boss and Andreas Engelhardt and Abhishek Kar and Yuanzhen Li and Deqing Sun and Jonathan T. Barron and Hendrik P. A. Lensch and Varun Jampani},\n    title = {SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections},\n    year = {2022},\n    month = {May},\n    url = {http://arxiv.org/abs/2205.15768v1}\n}","Bibtex Name":"boss2022samurai","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"Coming Soon","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"05/31/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"hello@markboss.me","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Graphics, Camera Parameter Estimation, Material/Lighting Estimation, Object-Centric, Coarse-to-Fine, Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SAMURAI","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2205.15768.pdf","Project webpage link":"https://markboss.me/publication/2022-samurai/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://youtu.be/LlYuGDjXp-8","Timestamp":"6/17/2022 9:41:20","Title":"SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections","Training time (hr)":"","UID":"362","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural volume rendering enables photo-realistic renderings of a human performer in free-view, a critical task in immersive VR/AR applications. But the practice is severely limited by high computational costs in the rendering process. To solve this problem, we propose the UV Volumes, a new approach that can render an editable free-view video of a human performer in realtime. It separates the high-frequency (i.e., non-smooth) human appearance from the 3D volume, and encodes them into 2D neural texture stacks (NTS). The smooth UV volumes allow much smaller and shallower neural networks to obtain densities and texture coordinates in 3D while capturing detailed appearance in 2D NTS. For editability, the mapping between the parameterized human model and the smooth texture coordinates allows us a better generalization on novel poses and shapes. Furthermore, the use of NTS enables interesting applications, e.g., retexturing. Extensive experiments on CMU Panoptic, ZJU Mocap, and H36M datasets show that our model can render 960 * 540 images in 30FPS on average with comparable photo-realism to state-of-the-art methods. The project and supplementary materials are available at https://github.com/fanegg/UV-Volumes.","Authors (format: First Last, First Middle Last, ...)":"Yue Chen, Xuan Wang, Xingyu Chen, Qi Zhang, Xiaoyu Li, Yu Guo, Jue Wang, Fei Wang","Bibtex (e.g. @inproceedings...)":"@article{chen2022uvvolumes,\n    url = {http://arxiv.org/abs/2203.14402v3},\n    month = {Mar},\n    year = {2022},\n    title = {UV Volumes for Real-time Rendering of Editable Free-view Human Performance},\n    author = {Yue Chen and Xuan Wang and Xingyu Chen and Qi Zhang and Xiaoyu Li and Yu Guo and Jue Wang and Fei Wang}\n}","Bibtex Name":"chen2022uvvolumes","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"03/27/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"faneggchen@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Speed & Computational Efficiency, Sparse Reconstruction, Dynamic/Temporal, Human (Body), Editable","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"UV Volumes","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2203.14402.pdf","Project webpage link":"https://fanegg.github.io/UV-Volumes/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/30/2022 1:54:28","Title":"UV Volumes for Real-time Rendering of Editable Free-view Human Performance","Training time (hr)":"","UID":"324","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural scene representations, such as neural radiance fields (NeRF), are based on training a multilayer perceptron (MLP) using a set of color images with known poses. An increasing number of devices now produce RGB-D information, which has been shown to be very important for a wide range of tasks. Therefore, the aim of this paper is to investigate what improvements can be made to these promising implicit representations by incorporating depth information with the color images. In particular, the recently proposed Mip-NeRF approach, which uses conical frustums instead of rays for volume rendering, allows one to account for the varying area of a pixel with distance from the camera center. The proposed method additionally models depth uncertainty. This allows to address major limitations of NeRF-based approaches including improving the accuracy of geometry, reduced artifacts, faster training time, and shortened prediction time. Experiments are performed on well-known benchmark scenes, and comparisons show improved accuracy in scene geometry and photometric reconstruction, while reducing the training time by 3 - 5 times.","Authors (format: First Last, First Middle Last, ...)":"Arnab Dey, Yassine Ahmine, Andrew I. Comport","Bibtex (e.g. @inproceedings...)":"@article{dey2022mipnerfrgbd,\n    author = {Arnab Dey and Yassine Ahmine and Andrew I. Comport},\n    title = {Mip-NeRF RGB-D: Depth Assisted Fast Neural Radiance Fields},\n    year = {2022},\n    month = {May},\n    url = {http://arxiv.org/abs/2205.09351v1}\n}","Bibtex Name":"dey2022mipnerfrgbd","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"05/19/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"adey@i3s.unice.fr","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Speed & Computational Efficiency, Sampling","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Mip-NeRF RGB-D","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2205.09351.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/20/2022 3:41","Title":"Mip-NeRF RGB-D: Depth Assisted Fast Neural Radiance Fields","Training time (hr)":"","UID":"323","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"WSCG 2022","Venue no Year":"WSCG","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Implicit neural networks have been successfully used for surface reconstruction from point clouds. However, many of them face scalability issues as they encode the isosurface function of a whole object or scene into a single latent vector. To overcome this limitation, a few approaches infer latent vectors on a coarse regular 3D grid or on 3D patches, and interpolate them to answer occupancy queries. In doing so, they loose the direct connection with the input points sampled on the surface of objects, and they attach information uniformly in space rather than where it matters the most, i.e., near the surface. Besides, relying on fixed patch sizes may require discretization tuning. To address these issues, we propose to use point cloud convolutions and compute latent vectors at each input point. We then perform a learning-based interpolation on nearest neighbors using inferred weights. Experiments on both object and scene datasets show that our approach significantly outperforms other methods on most classical metrics, producing finer details and better reconstructing thinner volumes. The code is available at https://github.com/valeoai/POCO.","Authors (format: First Last, First Middle Last, ...)":"Alexandre Boulch, Renaud Marlet","Bibtex (e.g. @inproceedings...)":"@article{boulch2022poco,\n    url = {http://arxiv.org/abs/2201.01831v2},\n    month = {Jan},\n    year = {2022},\n    title = {POCO: Point Convolution for Surface Reconstruction},\n    author = {Alexandre Boulch and Renaud Marlet}\n}","Bibtex Name":"boulch2022poco","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/valeoai/POCO","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"1/5/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"ryan_shue23@milton.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Geometry Only, Generalization, Local Conditioning, Large-Scale Scenes","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"POCO","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2201.01831.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/5/2022 12:29","Title":"POCO: Point Convolution for Surface Reconstruction","Training time (hr)":"","UID":"321","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose a novel 3d shape representation for 3d shape reconstruction from a single image. Rather than predicting a shape directly, we train a network to generate a training set which will be fed into another learning algorithm to define the shape. The nested optimization problem can be modeled by bi-level optimization. Specifically, the algorithms for bi-level optimization are also being used in meta learning approaches for few-shot learning. Our framework establishes a link between 3D shape analysis and few-shot learning. We combine training data generating networks with bi-level optimization algorithms to obtain a complete framework for which all components can be jointly trained. We improve upon recent work on standard benchmarks for 3d shape reconstruction.","Authors (format: First Last, First Middle Last, ...)":"Biao Zhang, Peter Wonka","Bibtex (e.g. @inproceedings...)":"@article{zhang2022training,\n    author = {Biao Zhang and Peter Wonka},\n    title = {Training Data Generating Networks: Shape Reconstruction via Bi-level Optimization},\n    year = {2020},\n    month = {Oct},\n    url = {http://arxiv.org/abs/2010.08276v2}\n}","Bibtex Name":"zhang2022training","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/16/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"biao.zhang@kaust.edu.sa","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Global Conditioning, Hypernetwork/Meta-learning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2010.08276.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/16/2022 20:09","Title":"Training Data Generating Networks: Shape Reconstruction via Bi-level Optimization","Training time (hr)":"","UID":"322","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICLR 2022","Venue no Year":"ICLR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"A classical problem in computer vision is to infer a 3D scene representation from few images that can be used to render novel views at interactive rates. Previous work focuses on reconstructing pre-defined 3D representations, e.g. textured meshes, or implicit representations, e.g. radiance fields, and often requires input images with precise camera poses and long processing times for each novel scene.  In this work, we propose the Scene Representation Transformer (SRT), a method which processes posed or unposed RGB images of a new area, infers a \"set-latent scene representation\", and synthesises novel views, all in a single feed-forward pass. To calculate the scene representation, we propose a generalization of the Vision Transformer to sets of images, enabling global information integration, and hence 3D reasoning. An efficient decoder transformer parameterizes the light field by attending into the scene representation to render novel views. Learning is supervised end-to-end by minimizing a novel-view reconstruction error.  We show that this method outperforms recent baselines in terms of PSNR and speed on synthetic datasets, including a new dataset created for the paper. Further, we demonstrate that SRT scales to support interactive visualization and semantic segmentation of real-world outdoor environments using Street View imagery.","Authors (format: First Last, First Middle Last, ...)":"Mehdi S. M. Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Lucic, Daniel Duckworth, Alexey Dosovitskiy, Jakob Uszkoreit, Thomas Funkhouser, Andrea Tagliasacchi","Bibtex (e.g. @inproceedings...)":"@article{sajjadi2021srt,\n    url = {http://arxiv.org/abs/2111.13152v3},\n    month = {Nov},\n    year = {2021},\n    title = {Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations},\n    author = {Mehdi S. M. Sajjadi and Henning Meyer and Etienne Pot and Urs Bergmann and Klaus Greff and Noha Radwan and Suhani Vora and Mario Lucic and Daniel Duckworth and Alexey Dosovitskiy and Jakob Uszkoreit and Thomas Funkhouser and Andrea Tagliasacchi}\n}","Bibtex Name":"sajjadi2021srt","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/25/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"ryan_shue23@milton.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Local Conditioning, Image-Based Rendering, Large-Scale Scenes","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SRT","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2111.13152.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"4/27/2022 22:04","Title":"Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations","Training time (hr)":"","UID":"314","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Existing methods for spectral reconstruction usually learn a discrete mapping from RGB images to a number of spectral bands. However, this modeling strategy ignores the continuous nature of spectral signature. In this paper, we propose Neural Spectral Reconstruction (NeSR) to lift this limitation, by introducing a novel continuous spectral representation. To this end, we embrace the concept of implicit function and implement a parameterized embodiment with a neural network. Specifically, we first adopt a backbone network to extract spatial features of RGB inputs. Based on it, we devise Spectral Profile Interpolation (SPI) module and Neural Attention Mapping (NAM) module to enrich deep features, where the spatial-spectral correlation is involved for a better representation. Then, we view the number of sampled spectral bands as the coordinate of continuous implicit function, so as to learn the projection from deep features to spectral intensities. Extensive experiments demonstrate the distinct advantage of NeSR in reconstruction accuracy over baseline methods. Moreover, NeSR extends the flexibility of spectral reconstruction by enabling an arbitrary number of spectral bands as the target output.","Authors (format: First Last, First Middle Last, ...)":"Ruikang Xu, Mingde Yao, Chang Chen, Lizhi Wang, Zhiwei Xiong","Bibtex (e.g. @inproceedings...)":"@article{xu2021nesr,\n    author = {Ruikang Xu and Mingde Yao and Chang Chen and Lizhi Wang and Zhiwei Xiong},\n    title = {Continuous Spectral Reconstruction from RGB Images via Implicit Neural Representation},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.13003v1}\n}","Bibtex Name":"xu2021nesr","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/24/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"ryan_shue23@milton.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeSR","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.13003.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"4/27/2022 22:11","Title":"Continuous Spectral Reconstruction from RGB Images via Implicit Neural Representation","Training time (hr)":"","UID":"315","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Deep neural networks have been used widely to learn the latent structure of datasets, across modalities such as images, shapes, and audio signals. However, existing models are generally modality-dependent, requiring custom architectures and objectives to process different classes of signals. We leverage neural fields to capture the underlying structure in image, shape, audio and cross-modal audiovisual domains in a modality-independent manner. We cast our task as one of learning a manifold, where we aim to infer a low-dimensional, locally linear subspace in which our data resides. By enforcing coverage of the manifold, local linearity, and local isometry, our model -- dubbed GEM -- learns to capture the underlying structure of datasets across modalities. We can then travel along linear regions of our manifold to obtain perceptually consistent interpolations between samples, and can further use GEM to recover points on our manifold and glean not only diverse completions of input images, but cross-modal hallucinations of audio or image signals. Finally, we show that by walking across the underlying manifold of GEM, we may generate new samples in our signal domains. Code and additional results are available at https://yilundu.github.io/gem/.","Authors (format: First Last, First Middle Last, ...)":"Yilun Du, Katherine M. Collins, Joshua B. Tenenbaum, Vincent Sitzmann","Bibtex (e.g. @inproceedings...)":"@article{du2021gem,\n    author = {Yilun Du and Katherine M. Collins and Joshua B. Tenenbaum and Vincent Sitzmann},\n    title = {Learning Signal-Agnostic Manifolds of Neural Fields},\n    year = {2021},\n    month = {Nov},\n    url = {http://arxiv.org/abs/2111.06387v1}\n}","Bibtex Name":"du2021gem","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/11/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"ryan_shue23@milton.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Generalization, Global Conditioning, Hypernetwork/Meta-learning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"GEM","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2111.06387.pdf","Project webpage link":"https://yilundu.github.io/gem/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"4/27/2022 22:19","Title":"Learning Signal-Agnostic Manifolds of Neural Fields","Training time (hr)":"","UID":"316","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2021","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present Neural Descriptor Fields (NDFs), an object representation that encodes both points and relative poses between an object and a target (such as a robot gripper or a rack used for hanging) via category-level descriptors. We employ this representation for object manipulation, where given a task demonstration, we want to repeat the same task on a new object instance from the same category. We propose to achieve this objective by searching (via optimization) for the pose whose descriptor matches that observed in the demonstration. NDFs are conveniently trained in a self-supervised fashion via a 3D auto-encoding task that does not rely on expert-labeled keypoints. Further, NDFs are SE(3)-equivariant, guaranteeing performance that generalizes across all possible 3D object translations and rotations. We demonstrate learning of manipulation tasks from few (5-10) demonstrations both in simulation and on a real robot. Our performance generalizes across both object instances and 6-DoF object poses, and significantly outperforms a recent baseline that relies on 2D descriptors. Project website: https://yilundu.github.io/ndf/.","Authors (format: First Last, First Middle Last, ...)":"Anthony Simeonov, Yilun Du, Andrea Tagliasacchi, Joshua B. Tenenbaum, Alberto Rodriguez, Pulkit Agrawal, Vincent Sitzmann","Bibtex (e.g. @inproceedings...)":"@article{simeonov2021ndf,\n    author = {Anthony Simeonov and Yilun Du and Andrea Tagliasacchi and Joshua B. Tenenbaum and Alberto Rodriguez and Pulkit Agrawal and Vincent Sitzmann},\n    title = {Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.05124v1}\n}","Bibtex Name":"simeonov2021ndf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/anthonysimeonov/ndf_robot","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/9/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"ryan_shue23@milton.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Robotics","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NDF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.05124.pdf","Project webpage link":"https://yilundu.github.io/ndf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"4/27/2022 22:28","Title":"Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation","Training time (hr)":"","UID":"317","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present PHORHUM, a novel, end-to-end trainable, deep neural network methodology for photorealistic 3D human reconstruction given just a monocular RGB image. Our pixel-aligned method estimates detailed 3D geometry and, for the first time, the unshaded surface color together with the scene illumination. Observing that 3D supervision alone is not sufficient for high fidelity color reconstruction, we introduce patch-based rendering losses that enable reliable color reconstruction on visible parts of the human, and detailed and plausible color estimation for the non-visible parts. Moreover, our method specifically addresses methodological and practical limitations of prior work in terms of representing geometry, albedo, and illumination effects, in an end-to-end model where factors can be effectively disentangled. In extensive experiments, we demonstrate the versatility and robustness of our approach. Our state-of-the-art results validate the method qualitatively and for different metrics, for both geometric and color reconstruction.","Authors (format: First Last, First Middle Last, ...)":"Thiemo Alldieck, Mihai Zanfir, Cristian Sminchisescu","Bibtex (e.g. @inproceedings...)":"@article{alldieck2022phorhum,\n    author = {Thiemo Alldieck and Mihai Zanfir and Cristian Sminchisescu},\n    title = {Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing},\n    year = {2022},\n    month = {Apr},\n    url = {http://arxiv.org/abs/2204.08906v1}\n}","Bibtex Name":"alldieck2022phorhum","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/19/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"alldieck@google.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Human (Body), Generalization, Supervision by Gradient (PDE), Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"PHORHUM","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2204.08906.pdf","Project webpage link":"https://phorhum.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"4/28/2022 1:52","Title":"Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing","Training time (hr)":"","UID":"318","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural implicit representations have recently shown encouraging results in various domains, including promising progress in simultaneous localization and mapping (SLAM). Nevertheless, existing methods produce over-smoothed scene reconstructions and have difficulty scaling up to large scenes. These limitations are mainly due to their simple fully-connected network architecture that does not incorporate local information in the observations. In this paper, we present NICE-SLAM, a dense SLAM system that incorporates multi-level local information by introducing a hierarchical scene representation. Optimizing this representation with pre-trained geometric priors enables detailed reconstruction on large indoor scenes. Compared to recent neural implicit SLAM systems, our approach is more scalable, efficient, and robust. Experiments on five challenging datasets demonstrate competitive results of NICE-SLAM in both mapping and tracking quality. Project page: https://pengsongyou.github.io/nice-slam","Authors (format: First Last, First Middle Last, ...)":"Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hujun Bao, Zhaopeng Cui, Martin R. Oswald, Marc Pollefeys","Bibtex (e.g. @inproceedings...)":"@article{zhu2022niceslam,\n    author = {Zihan Zhu and Songyou Peng and Viktor Larsson and Weiwei Xu and Hujun Bao and Zhaopeng Cui and Martin R. Oswald and Marc Pollefeys},\n    title = {NICE-SLAM: Neural Implicit Scalable Encoding for SLAM},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.12130v2}\n}","Bibtex Name":"zhu2022niceslam","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/cvg/nice-slam","Coordinates all at once":"","Data Release (link)":"https://github.com/cvg/nice-slam","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/22/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"songyou.pp@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Camera Parameter Estimation, Local Conditioning, Hybrid Geometry Representation, Coarse-to-Fine, Large-Scale Scenes, SLAM","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NICE-SLAM","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.12130.pdf","Project webpage link":"https://pengsongyou.github.io/nice-slam","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://youtu.be/V5hYTz5os0M","Timestamp":"4/28/2022 11:25","Title":"NICE-SLAM: Neural Implicit Scalable Encoding for SLAM","Training time (hr)":"","UID":"319","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Tomographic reconstruction is concerned with computing the cross-sections of an object from a finite number of projections. Many conventional methods represent the cross-sections as images on a regular grid. In this paper, we study a recent coordinate-based\nneural network for tomographic reconstruction, where the network inputs a spatial coordinate and outputs the attenuation coefficient on the coordinate. This coordinate-based network allows the continuous representation of an object. Based on this network,\nwe propose a spatial regularization term, to obtain a high-quality reconstruction. Experimental results on synthetic data show that the regularization term improves the reconstruction quality significantly, compared to the baseline. We also provide an ablation\nstudy for different architecture configurations and hyper-parameters.","Authors (format: First Last, First Middle Last, ...)":"Jakeoung Koo, Elise Otterlei Brenne, Anders Bjorholm Dahl, Vedrana Andersen Dahl","Bibtex (e.g. @inproceedings...)":"@inproceedings{koo2021tomographic,\n    year = {2021},\n    volume = {2},\n    booktitle = {Proceedings of the Northern Lights Deep Learning Workshop},\n    author = {Jakeoung Koo and Elise Otterlei Brenne and Anders Bjorholm Dahl and Vedrana Andersen Dahl},\n    title = {A Tomographic Reconstruction Method using Coordinate-based Neural Network with Spatial Regularization}\n}","Bibtex Name":"koo2021tomographic","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/19/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"vand@dtu.dk","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"No","Keywords":"Tomographic Reconstruction","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"TomoSR","PDF link (arXiv perferred)":"https://septentrio.uit.no/index.php/nldl/article/download/5676/5573","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=2n6BU4D0aUo","Timestamp":"4/29/2022 2:48","Title":"A Tomographic Reconstruction Method using Coordinate-based Neural Network with Spatial Regularization","Training time (hr)":"","UID":"320","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"Northern Lights Deep Learning Workshop 2021 Northern Lights Deep Learning Workshop 2021","Venue no Year":"Northern Lights Deep Learning Workshop 2021 Northern Lights Deep Learning Workshop","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"A classical problem in computer vision is to infer a 3D scene representation from few images that can be used to render novel views at interactive rates. Previous work focuses on reconstructing pre-defined 3D representations, e.g. textured meshes, or implicit representations, e.g. radiance fields, and often requires input images with precise camera poses and long processing times for each novel scene.  In this work, we propose the Scene Representation Transformer (SRT), a method which processes posed or unposed RGB images of a new area, infers a \"set-latent scene representation\", and synthesises novel views, all in a single feed-forward pass. To calculate the scene representation, we propose a generalization of the Vision Transformer to sets of images, enabling global information integration, and hence 3D reasoning. An efficient decoder transformer parameterizes the light field by attending into the scene representation to render novel views. Learning is supervised end-to-end by minimizing a novel-view reconstruction error.  We show that this method outperforms recent baselines in terms of PSNR and speed on synthetic datasets, including a new dataset created for the paper. Further, we demonstrate that SRT scales to support interactive visualization and semantic segmentation of real-world outdoor environments using Street View imagery.","Authors (format: First Last, First Middle Last, ...)":"Mehdi S. M. Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Lucic, Daniel Duckworth, Alexey Dosovitskiy, Jakob Uszkoreit, Thomas Funkhouser, Andrea Tagliasacchi","Bibtex (e.g. @inproceedings...)":"@article{sajjadi2021srt,\n    url = {http://arxiv.org/abs/2111.13152v3},\n    month = {Nov},\n    year = {2021},\n    title = {Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations},\n    author = {Mehdi S. M. Sajjadi and Henning Meyer and Etienne Pot and Urs Bergmann and Klaus Greff and Noha Radwan and Suhani Vora and Mario Lucic and Daniel Duckworth and Alexey Dosovitskiy and Jakob Uszkoreit and Thomas Funkhouser and Andrea Tagliasacchi}\n}","Bibtex Name":"sajjadi2021srt","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/25/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"ryan_shue23@milton.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Local Conditioning, Image-Based Rendering, Large-Scale Scenes","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SRT","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2111.13152.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"4/27/2022 22:04","Title":"Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations","Training time (hr)":"","UID":"307","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Existing methods for spectral reconstruction usually learn a discrete mapping from RGB images to a number of spectral bands. However, this modeling strategy ignores the continuous nature of spectral signature. In this paper, we propose Neural Spectral Reconstruction (NeSR) to lift this limitation, by introducing a novel continuous spectral representation. To this end, we embrace the concept of implicit function and implement a parameterized embodiment with a neural network. Specifically, we first adopt a backbone network to extract spatial features of RGB inputs. Based on it, we devise Spectral Profile Interpolation (SPI) module and Neural Attention Mapping (NAM) module to enrich deep features, where the spatial-spectral correlation is involved for a better representation. Then, we view the number of sampled spectral bands as the coordinate of continuous implicit function, so as to learn the projection from deep features to spectral intensities. Extensive experiments demonstrate the distinct advantage of NeSR in reconstruction accuracy over baseline methods. Moreover, NeSR extends the flexibility of spectral reconstruction by enabling an arbitrary number of spectral bands as the target output.","Authors (format: First Last, First Middle Last, ...)":"Ruikang Xu, Mingde Yao, Chang Chen, Lizhi Wang, Zhiwei Xiong","Bibtex (e.g. @inproceedings...)":"@article{xu2021nesr,\n    author = {Ruikang Xu and Mingde Yao and Chang Chen and Lizhi Wang and Zhiwei Xiong},\n    title = {Continuous Spectral Reconstruction from RGB Images via Implicit Neural Representation},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.13003v1}\n}","Bibtex Name":"xu2021nesr","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/24/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"ryan_shue23@milton.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeSR","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.13003.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"4/27/2022 22:11","Title":"Continuous Spectral Reconstruction from RGB Images via Implicit Neural Representation","Training time (hr)":"","UID":"308","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Deep neural networks have been used widely to learn the latent structure of datasets, across modalities such as images, shapes, and audio signals. However, existing models are generally modality-dependent, requiring custom architectures and objectives to process different classes of signals. We leverage neural fields to capture the underlying structure in image, shape, audio and cross-modal audiovisual domains in a modality-independent manner. We cast our task as one of learning a manifold, where we aim to infer a low-dimensional, locally linear subspace in which our data resides. By enforcing coverage of the manifold, local linearity, and local isometry, our model -- dubbed GEM -- learns to capture the underlying structure of datasets across modalities. We can then travel along linear regions of our manifold to obtain perceptually consistent interpolations between samples, and can further use GEM to recover points on our manifold and glean not only diverse completions of input images, but cross-modal hallucinations of audio or image signals. Finally, we show that by walking across the underlying manifold of GEM, we may generate new samples in our signal domains. Code and additional results are available at https://yilundu.github.io/gem/.","Authors (format: First Last, First Middle Last, ...)":"Yilun Du, Katherine M. Collins, Joshua B. Tenenbaum, Vincent Sitzmann","Bibtex (e.g. @inproceedings...)":"@article{du2021gem,\n    author = {Yilun Du and Katherine M. Collins and Joshua B. Tenenbaum and Vincent Sitzmann},\n    title = {Learning Signal-Agnostic Manifolds of Neural Fields},\n    year = {2021},\n    month = {Nov},\n    url = {http://arxiv.org/abs/2111.06387v1}\n}","Bibtex Name":"du2021gem","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/11/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"ryan_shue23@milton.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Generalization, Global Conditioning, Hypernetwork/Meta-learning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"GEM","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2111.06387.pdf","Project webpage link":"https://yilundu.github.io/gem/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"4/27/2022 22:19","Title":"Learning Signal-Agnostic Manifolds of Neural Fields","Training time (hr)":"","UID":"309","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2021","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present Neural Descriptor Fields (NDFs), an object representation that encodes both points and relative poses between an object and a target (such as a robot gripper or a rack used for hanging) via category-level descriptors. We employ this representation for object manipulation, where given a task demonstration, we want to repeat the same task on a new object instance from the same category. We propose to achieve this objective by searching (via optimization) for the pose whose descriptor matches that observed in the demonstration. NDFs are conveniently trained in a self-supervised fashion via a 3D auto-encoding task that does not rely on expert-labeled keypoints. Further, NDFs are SE(3)-equivariant, guaranteeing performance that generalizes across all possible 3D object translations and rotations. We demonstrate learning of manipulation tasks from few (5-10) demonstrations both in simulation and on a real robot. Our performance generalizes across both object instances and 6-DoF object poses, and significantly outperforms a recent baseline that relies on 2D descriptors. Project website: https://yilundu.github.io/ndf/.","Authors (format: First Last, First Middle Last, ...)":"Anthony Simeonov, Yilun Du, Andrea Tagliasacchi, Joshua B. Tenenbaum, Alberto Rodriguez, Pulkit Agrawal, Vincent Sitzmann","Bibtex (e.g. @inproceedings...)":"@article{simeonov2021ndf,\n    author = {Anthony Simeonov and Yilun Du and Andrea Tagliasacchi and Joshua B. Tenenbaum and Alberto Rodriguez and Pulkit Agrawal and Vincent Sitzmann},\n    title = {Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.05124v1}\n}","Bibtex Name":"simeonov2021ndf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/anthonysimeonov/ndf_robot","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/9/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"ryan_shue23@milton.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Robotics","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NDF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.05124.pdf","Project webpage link":"https://yilundu.github.io/ndf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"4/27/2022 22:28","Title":"Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation","Training time (hr)":"","UID":"310","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present PHORHUM, a novel, end-to-end trainable, deep neural network methodology for photorealistic 3D human reconstruction given just a monocular RGB image. Our pixel-aligned method estimates detailed 3D geometry and, for the first time, the unshaded surface color together with the scene illumination. Observing that 3D supervision alone is not sufficient for high fidelity color reconstruction, we introduce patch-based rendering losses that enable reliable color reconstruction on visible parts of the human, and detailed and plausible color estimation for the non-visible parts. Moreover, our method specifically addresses methodological and practical limitations of prior work in terms of representing geometry, albedo, and illumination effects, in an end-to-end model where factors can be effectively disentangled. In extensive experiments, we demonstrate the versatility and robustness of our approach. Our state-of-the-art results validate the method qualitatively and for different metrics, for both geometric and color reconstruction.","Authors (format: First Last, First Middle Last, ...)":"Thiemo Alldieck, Mihai Zanfir, Cristian Sminchisescu","Bibtex (e.g. @inproceedings...)":"@article{alldieck2022phorhum,\n    author = {Thiemo Alldieck and Mihai Zanfir and Cristian Sminchisescu},\n    title = {Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing},\n    year = {2022},\n    month = {Apr},\n    url = {http://arxiv.org/abs/2204.08906v1}\n}","Bibtex Name":"alldieck2022phorhum","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/19/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"alldieck@google.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Human (Body), Generalization, Supervision by Gradient (PDE), Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"PHORHUM","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2204.08906.pdf","Project webpage link":"https://phorhum.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"4/28/2022 1:52","Title":"Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing","Training time (hr)":"","UID":"311","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural implicit representations have recently shown encouraging results in various domains, including promising progress in simultaneous localization and mapping (SLAM). Nevertheless, existing methods produce over-smoothed scene reconstructions and have difficulty scaling up to large scenes. These limitations are mainly due to their simple fully-connected network architecture that does not incorporate local information in the observations. In this paper, we present NICE-SLAM, a dense SLAM system that incorporates multi-level local information by introducing a hierarchical scene representation. Optimizing this representation with pre-trained geometric priors enables detailed reconstruction on large indoor scenes. Compared to recent neural implicit SLAM systems, our approach is more scalable, efficient, and robust. Experiments on five challenging datasets demonstrate competitive results of NICE-SLAM in both mapping and tracking quality. Project page: https://pengsongyou.github.io/nice-slam","Authors (format: First Last, First Middle Last, ...)":"Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hujun Bao, Zhaopeng Cui, Martin R. Oswald, Marc Pollefeys","Bibtex (e.g. @inproceedings...)":"@article{zhu2022niceslam,\n    author = {Zihan Zhu and Songyou Peng and Viktor Larsson and Weiwei Xu and Hujun Bao and Zhaopeng Cui and Martin R. Oswald and Marc Pollefeys},\n    title = {NICE-SLAM: Neural Implicit Scalable Encoding for SLAM},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.12130v2}\n}","Bibtex Name":"zhu2022niceslam","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/cvg/nice-slam","Coordinates all at once":"","Data Release (link)":"https://github.com/cvg/nice-slam","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/22/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"songyou.pp@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Camera Parameter Estimation, Local Conditioning, Hybrid Geometry Representation, Coarse-to-Fine, Large-Scale Scenes, SLAM","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NICE-SLAM","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.12130.pdf","Project webpage link":"https://pengsongyou.github.io/nice-slam","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://youtu.be/V5hYTz5os0M","Timestamp":"4/28/2022 11:25","Title":"NICE-SLAM: Neural Implicit Scalable Encoding for SLAM","Training time (hr)":"","UID":"312","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Tomographic reconstruction is concerned with computing the cross-sections of an object from a finite number of projections. Many conventional methods represent the cross-sections as images on a regular grid. In this paper, we study a recent coordinate-based\nneural network for tomographic reconstruction, where the network inputs a spatial coordinate and outputs the attenuation coefficient on the coordinate. This coordinate-based network allows the continuous representation of an object. Based on this network,\nwe propose a spatial regularization term, to obtain a high-quality reconstruction. Experimental results on synthetic data show that the regularization term improves the reconstruction quality significantly, compared to the baseline. We also provide an ablation\nstudy for different architecture configurations and hyper-parameters.","Authors (format: First Last, First Middle Last, ...)":"Jakeoung Koo, Elise Otterlei Brenne, Anders Bjorholm Dahl, Vedrana Andersen Dahl","Bibtex (e.g. @inproceedings...)":"@inproceedings{koo2021tomographic,\n    year = {2021},\n    volume = {2},\n    booktitle = {Proceedings of the Northern Lights Deep Learning Workshop},\n    author = {Jakeoung Koo and Elise Otterlei Brenne and Anders Bjorholm Dahl and Vedrana Andersen Dahl},\n    title = {A Tomographic Reconstruction Method using Coordinate-based Neural Network with Spatial Regularization}\n}","Bibtex Name":"koo2021tomographic","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/19/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"vand@dtu.dk","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"No","Keywords":"Tomographic Reconstruction","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"TomoSR","PDF link (arXiv perferred)":"https://septentrio.uit.no/index.php/nldl/article/download/5676/5573","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=2n6BU4D0aUo","Timestamp":"4/29/2022 2:48","Title":"A Tomographic Reconstruction Method using Coordinate-based Neural Network with Spatial Regularization","Training time (hr)":"","UID":"313","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"Northern Lights Deep Learning Workshop 2021 Northern Lights Deep Learning Workshop 2021","Venue no Year":"Northern Lights Deep Learning Workshop 2021 Northern Lights Deep Learning Workshop","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Precise representations of 3D faces are beneficial to various computer vision and graphics applications. Due to the data discretization and model linearity, however, it remains challenging to capture accurate identity and expression clues in current studies. This paper presents a novel 3D morphable face model, namely ImFace, to learn a nonlinear and continuous space with implicit neural representations. It builds two explicitly disentangled deformation fields to model complex shapes associated with identities and expressions, respectively, and designs an improved learning strategy to extend embeddings of expressions to allow more diverse changes. We further introduce a Neural Blend-Field to learn sophisticated details by adaptively blending a series of local fields. In addition to ImFace, an effective preprocessing pipeline is proposed to address the issue of watertight input requirement in implicit representations, enabling them to work with common facial surfaces for the first time. Extensive experiments are performed to demonstrate the superiority of ImFace.","Authors (format: First Last, First Middle Last, ...)":"Mingwu Zheng, Hongyu Yang, Di Huang, Liming Chen","Bibtex (e.g. @inproceedings...)":"@article{zheng2022imface,\n    url = {http://arxiv.org/abs/2203.14510v2},\n    month = {Mar},\n    year = {2022},\n    title = {ImFace: A Nonlinear 3D Morphable Face Model with Implicit Neural Representations},\n    author = {Mingwu Zheng and Hongyu Yang and Di Huang and Liming Chen}\n}","Bibtex Name":"zheng2022imface","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/MingwuZheng/ImFace","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/28/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"zhengmingwu@buaa.edu.cn","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Geometry Only, Dynamic/Temporal, Human (Head)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"ImFace","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2203.14510.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"4/24/2022 2:47","Title":"ImFace: A Nonlinear 3D Morphable Face Model with Implicit Neural Representations","Training time (hr)":"","UID":"305","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural fields such as implicit surfaces have recently enabled avatar modeling from raw scans without explicit temporal correspondences. In this work, we exploit autoregressive modeling to further extend this notion to capture dynamic effects, such as soft-tissue deformations. Although autoregressive models are naturally capable of handling dynamics, it is non-trivial to apply them to implicit representations, as explicit state decoding is infeasible due to prohibitive memory requirements. In this work, for the first time, we enable autoregressive modeling of implicit avatars. To reduce the memory bottleneck and efficiently model dynamic implicit surfaces, we introduce the notion of articulated observer points, which relate implicit states to the explicit surface of a parametric human body model. We demonstrate that encoding implicit surfaces as a set of height fields defined on articulated observer points leads to significantly better generalization compared to a latent representation. The experiments show that our approach outperforms the state of the art, achieving plausible dynamic deformations even for unseen motions. https://zqbai-jeremy.github.io/autoavatar","Authors (format: First Last, First Middle Last, ...)":"Ziqian Bai, Timur Bagautdinov, Javier Romero, Michael Zollh\u00f6fer, Ping Tan, Shunsuke Saito","Bibtex (e.g. @inproceedings...)":"@article{bai2022autoavatar,\n    author = {Ziqian Bai and Timur Bagautdinov and Javier Romero and Michael Zollhofer and Ping Tan and Shunsuke Saito},\n    title = {AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling},\n    year = {2022},\n    month = {Mar},\n    url = {http://arxiv.org/abs/2203.13817v1}\n}","Bibtex Name":"bai2022autoavatar","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/25/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"shunsukesaito@fb.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Geometry Only, Dynamic/Temporal, Human (Body)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"AutoAvatar","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2203.13817.pdf","Project webpage link":"https://zqbai-jeremy.github.io/autoavatar/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://zqbai-jeremy.github.io/autoavatar/static/images/video_arxiv.mp4","Timestamp":"4/16/2022 20:50","Title":"AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling","Training time (hr)":"","UID":"306","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recent advances in machine learning have created increasing interest in solving visual computing problems using a class of coordinate-based neural networks that parametrize physical properties of scenes or objects across space and time. These methods, which we call neural fields, have seen successful application in the synthesis of 3D shapes and image, animation of human bodies, 3D reconstruction, and pose estimation. However, due to rapid progress in a short time, many papers exist but a comprehensive review and formulation of the problem has not yet emerged. In this report, we address this limitation by providing context, mathematical grounding, and an extensive review of literature on neural fields. This report covers research along two dimensions. In Part I, we focus on techniques in neural fields by identifying common components of neural field methods, including different representations, architectures, forward mapping, and generalization methods. In Part II, we focus on applications of neural fields to different problems in visual computing, and beyond (e.g., robotics, audio). Our review shows the breadth of topics already covered in visual computing, both historically and in current incarnations, demonstrating the improved quality, flexibility, and capability brought by neural fields methods. Finally, we present a companion website that contributes a living version of this review that can be continually updated by the community.","Authors (format: First Last, First Middle Last, ...)":"Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, Srinath Sridhar","Bibtex (e.g. @inproceedings...)":"@article{10.1111:cgf.14505,\n    journal = {Computer Graphics Forum},\n    title = {Neural Fields in Visual Computing and Beyond},\n    author = {Xie, Yiheng and Takikawa, Towaki and Saito, Shunsuke and Litany, Or and Yan, Shiqin and Khan, Numair and Tombari, Federico and Tompkin, James and Sitzmann, Vincent and Sridhar, Srinath},\n    year = {2022},\n    publisher = {The Eurographics Association and John Wiley & Sons Ltd.},\n    ISSN = {1467-8659},\n    DOI = {10.1111/cgf.14505}\n}","Bibtex Name":"xie2022neuralfieldsreview","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/brownvc/neural-fields-review","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/22/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Neural Fields Review","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2111.11426.pdf","Project webpage link":"https://neuralfields.cs.brown.edu/index.html","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"4/20/2022 22:10","Title":"Neural Fields in Visual Computing and Beyond","Training time (hr)":"","UID":"304","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"EUROGRAPHICS 2022","Venue no Year":"EUROGRAPHICS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"It is common practice in deep learning to represent a measurement of the world on a discrete grid, e.g. a 2D grid of pixels. However, the underlying signal represented by these measurements is often continuous, e.g. the scene depicted in an image. A powerful continuous alternative is then to represent these measurements using an implicit neural representation, a neural function trained to output the appropriate measurement value for any input spatial location. In this paper, we take this idea to its next level: what would it take to perform deep learning on these functions instead, treating them as data? In this context we refer to the data as functa, and propose a framework for deep learning on functa. This view presents a number of challenges around efficient conversion from data to functa, compact representation of functa, and effectively solving downstream tasks on functa. We outline a recipe to overcome these challenges and apply it to a wide range of data modalities including images, 3D shapes, neural radiance fields (NeRF) and data on manifolds. We demonstrate that this approach has various compelling properties across data modalities, in particular on the canonical tasks of generative modeling, data imputation, novel view synthesis and classification.","Authors (format: First Last, First Middle Last, ...)":"Emilien Dupont, Hyunjik Kim, S. M. Ali Eslami, Danilo Rezende, Dan Rosenbaum","Bibtex (e.g. @inproceedings...)":"@article{dupont2022functa,\n    url = {http://arxiv.org/abs/2201.12204v1},\n    month = {Jan},\n    year = {2022},\n    title = {From data to functa: Your data point is a function and you should treat it like one},\n    author = {Emilien Dupont and Hyunjik Kim and S. M. Ali Eslami and Danilo Rezende and Dan Rosenbaum}\n}","Bibtex Name":"dupont2022functa","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"1/28/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"ryan_shue23@milton.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Speed & Computational Efficiency, Geometry Only, 2D Image Neural Fields, Compression, Fundamentals, Generative Models, Global Conditioning, Hypernetwork/Meta-learning, Classification","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Functa","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2201.12204.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"4/9/2022 22:31","Title":"From data to functa: Your data point is a function and you should treat it like one","Training time (hr)":"","UID":"299","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural radiance fields (NeRF) methods have demonstrated impressive novel view synthesis performance. The core approach is to render individual rays by querying a neural network at points sampled along the ray to obtain the density and colour of the sampled points, and integrating this information using the rendering equation. Since dense sampling is computationally prohibitive, a common solution is to perform coarse-to-fine sampling.  In this work we address a clear limitation of the vanilla coarse-to-fine approach -- that it is based on a heuristic and not trained end-to-end for the task at hand. We introduce a differentiable module that learns to propose samples and their importance for the fine network, and consider and compare multiple alternatives for its neural architecture. Training the proposal module from scratch can be unstable due to lack of supervision, so an effective pre-training strategy is also put forward. The approach, named `NeRF in detail' (NeRF-ID), achieves superior view synthesis quality over NeRF and the state-of-the-art on the synthetic Blender benchmark and on par or better performance on the real LLFF-NeRF scenes. Furthermore, by leveraging the predicted sample importance, a 25% saving in computation can be achieved without significantly sacrificing the rendering quality.","Authors (format: First Last, First Middle Last, ...)":"Relja Arandjelovi\u00c4\u2021, Andrew Zisserman","Bibtex (e.g. @inproceedings...)":"@article{arandjelovic2021nerfid,\n    author = {Relja Arandjelovic and Andrew Zisserman},\n    title = {NeRF in detail: Learning to sample for view synthesis},\n    year = {2021},\n    month = {Jun},\n    url = {http://arxiv.org/abs/2106.05264v1}\n}","Bibtex Name":"arandjelovic2021nerfid","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/9/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"qian_zhang@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Speed & Computational Efficiency, Sampling, Coarse-to-Fine","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeRF-ID","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.05264.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"4/10/2022 21:27","Title":"NeRF in detail: Learning to sample for view synthesis","Training time (hr)":"","UID":"300","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Current trends in the computer graphics community propose leveraging the massive parallel computational power of GPUs to accelerate physically based simulations. Collision detection and solving is a fundamental part of this process. It is also the most significant bottleneck on physically based simulations and it easily becomes intractable as the number of vertices in the scene increases. Brute force approaches carry a quadratic growth in both computational time and memory footprint. While their parallelization is trivial in GPUs, their complexity discourages from using such approaches. Acceleration structures -- such as BVH -- are often applied to increase performance, achieving logarithmic computational times for individual point queries. Nonetheless, their memory footprint also grows rapidly and their parallelization in a GPU is problematic due to their branching nature. We propose using implicit surface representations learnt through deep learning for collision handling in physically based simulations. Our proposed architecture has a complexity of O(n) -- or O(1) for a single point query -- and has no parallelization issues. We will show how this permits accurate and efficient collision handling in physically based simulations, more specifically, for cloth. In our experiments, we query up to 1M points in 300 milliseconds.","Authors (format: First Last, First Middle Last, ...)":"Hugo Bertiche, Meysam Madadi, Sergio Escalera","Bibtex (e.g. @inproceedings...)":"@article{bertiche2021neural,\n    author = {Hugo Bertiche and Meysam Madadi and Sergio Escalera},\n    title = {Neural Implicit Surfaces for Efficient and Accurate Collisions in Physically Based Simulations},\n    year = {2021},\n    month = {Oct},\n    url = {http://arxiv.org/abs/2110.01614v1}\n}","Bibtex Name":"bertiche2021neural","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/hbertiche/NeuralColliders","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/3/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"qian_zhang@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Geometry Only, Graphics, Compression","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2110.01614.pdf","Project webpage link":"https://www.catalyzex.com/paper/arxiv:2110.01614","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"4/10/2022 21:54","Title":"Neural Implicit Surfaces for Efficient and Accurate Collisions in Physically Based Simulations","Training time (hr)":"","UID":"301","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a modern solution to the multi-view photometric stereo problem (MVPS). Our work suitably exploits the image formation model in a MVPS experimental setup to recover the dense 3D reconstruction of an object from images. We procure the surface orientation using a photometric stereo (PS) image formation model and blend it with a multi-view neural radiance field representation to recover the object's surface geometry. Contrary to the previous multi-staged framework to MVPS, where the position, iso-depth contours, or orientation measurements are estimated independently and then fused later, our method is simple to implement and realize. Our method performs neural rendering of multi-view images while utilizing surface normals estimated by a deep photometric stereo network. We render the MVPS images by considering the object's surface normals for each 3D sample point along the viewing direction rather than explicitly using the density gradient in the volume space via 3D occupancy information. We optimize the proposed neural radiance field representation for the MVPS setup efficiently using a fully connected deep network to recover the 3D geometry of an object. Extensive evaluation on the DiLiGenT-MV benchmark dataset shows that our method performs better than the approaches that perform only PS or only multi-view stereo (MVS) and provides comparable results against the state-of-the-art multi-stage fusion methods.","Authors (format: First Last, First Middle Last, ...)":"Berk Kaya, Suryansh Kumar, Francesco Sarno, Vittorio Ferrari, Luc Van Gool","Bibtex (e.g. @inproceedings...)":"@article{kaya2022neural,\n    author = {Berk Kaya and Suryansh Kumar and Francesco Sarno and Vittorio Ferrari and Luc Van Gool},\n    title = {Neural Radiance Fields Approach to Deep Multi-View Photometric Stereo},\n    year = {2021},\n    month = {Oct},\n    url = {http://arxiv.org/abs/2110.05594v1}\n}","Bibtex Name":"kaya2022neural","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/11/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"qian_zhang@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2110.05594.pdf","Project webpage link":"https://berk95kaya.github.io/Neural_MVPS_WACV2022/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://berk95kaya.github.io/Neural_MVPS_WACV2022/","Timestamp":"4/10/2022 22:53","Title":"Neural Radiance Fields Approach to Deep Multi-View Photometric Stereo","Training time (hr)":"","UID":"302","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"WACV 2022","Venue no Year":"WACV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recent history has seen a tremendous growth of work exploring implicit representations of geometry and radiance, popularized through Neural Radiance Fields (NeRF). Such works are fundamentally based on a (implicit) volumetric representation of occupancy, allowing them to model diverse scene structure including translucent objects and atmospheric obscurants. But because the vast majority of real-world scenes are composed of well-defined surfaces, we introduce a surface analog of such implicit models called Neural Reflectance Surfaces (NeRS). NeRS learns a neural shape representation of a closed surface that is diffeomorphic to a sphere, guaranteeing water-tight reconstructions. Even more importantly, surface parameterizations allow NeRS to learn (neural) bidirectional surface reflectance functions (BRDFs) that factorize view-dependent appearance into environmental illumination, diffuse color (albedo), and specular \"shininess.\" Finally, rather than illustrating our results on synthetic scenes or controlled in-the-lab capture, we assemble a novel dataset of multi-view images from online marketplaces for selling goods. Such \"in-the-wild\" multi-view image sets pose a number of challenges, including a small number of views with unknown/rough camera estimates. We demonstrate that surface-based neural reconstructions enable learning from such data, outperforming volumetric neural rendering-based reconstructions. We hope that NeRS serves as a first step toward building scalable, high-quality libraries of real-world shape, materials, and illumination. The project page with code and video visualizations can be found at https://jasonyzhang.com/ners.","Authors (format: First Last, First Middle Last, ...)":"Jason Y. Zhang, Gengshan Yang, Shubham Tulsiani, Deva Ramanan","Bibtex (e.g. @inproceedings...)":"@article{zhang2021ners,\n    author = {Jason Y. Zhang and Gengshan Yang and Shubham Tulsiani and Deva Ramanan},\n    title = {NeRS: Neural Reflectance Surfaces for Sparse-view 3D Reconstruction in the Wild},\n    year = {2021},\n    month = {Oct},\n    url = {http://arxiv.org/abs/2110.07604v3}\n}","Bibtex Name":"zhang2021ners","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/jasonyzhang/ners","Coordinates all at once":"","Data Release (link)":"https://jasonyzhang.com/ners/meshes","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/14/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"qian_zhang@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Sparse Reconstruction, Graphics, Camera Parameter Estimation, Material/Lighting Estimation, Generalization","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeRS","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2110.07604.pdf","Project webpage link":"https://jasonyzhang.com/ners/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=zVyaw_sn1xM","Timestamp":"4/10/2022 23:33","Title":"NeRS: Neural Reflectance Surfaces for Sparse-view 3D Reconstruction in the Wild","Training time (hr)":"","UID":"303","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"Advances in Neural Information Processing Systems 2021","Venue no Year":"Advances in Neural Information Processing Systems","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"While neural representations for static 3D shapes are widely studied, representations for deformable surfaces are limited to be template-dependent or lack efficiency. We introduce Canonical Deformation Coordinate Space (CaDeX), a unified representation of both shape and nonrigid motion. Our key insight is the factorization of the deformation between frames by continuous bijective canonical maps (homeomorphisms) and their inverses that go through a learned canonical shape. Our novel deformation representation and its implementation are simple, efficient, and guarantee cycle consistency, topology preservation, and, if needed, volume conservation. Our modelling of the learned canonical shapes provides a flexible and stable space for shape prior learning. We demonstrate state-of-the-art performance in modelling a wide range of deformable geometries: human bodies, animal bodies, and articulated objects.","Authors (format: First Last, First Middle Last, ...)":"Jiahui Lei, Kostas Daniilidis","Bibtex (e.g. @inproceedings...)":"@article{lei2022cadex,\n    author = {Jiahui Lei and Kostas Daniilidis},\n    title = {CaDeX: Learning Canonical Deformation Coordinate Space for Dynamic Surface Representation via Neural Homeomorphism},\n    year = {2022},\n    month = {Mar},\n    url = {http://arxiv.org/abs/2203.16529v1}\n}","Bibtex Name":"lei2022cadex","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/JiahuiLei/CaDeX","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/30/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"leijh@seas.upenn.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Dynamic/Temporal, Regularization","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"CaDeX","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2203.16529.pdf","Project webpage link":"https://www.cis.upenn.edu/~leijh/projects/cadex/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=MPocopCgUj8","Timestamp":"3/31/2022 8:00","Title":"CaDeX: Learning Canonical Deformation Coordinate Space for Dynamic Surface Representation via Neural Homeomorphism","Training time (hr)":"","UID":"298","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural fields have gained significant attention in the computer vision community due to their excellent performance in novel view synthesis, geometry reconstruction, and generative modeling. Some of their advantages are a sound theoretic foundation and an easy implementation in current deep learning frameworks. While neural fields have been applied to signals on manifolds, e.g., for texture reconstruction, their representation has been limited to extrinsically embedding the shape into Euclidean space. The extrinsic embedding ignores known intrinsic manifold properties and is inflexible wrt. transfer of the learned function. To overcome these limitations, this work introduces intrinsic neural fields, a novel and versatile representation for neural fields on manifolds. Intrinsic neural fields combine the advantages of neural fields with the spectral properties of the Laplace-Beltrami operator. We show theoretically that intrinsic neural fields inherit many desirable properties of the extrinsic neural field framework but exhibit additional intrinsic qualities, like isometry invariance. In experiments, we show intrinsic neural fields can reconstruct high-fidelity textures from images with state-of-the-art quality and are robust to the discretization of the underlying manifold. We demonstrate the versatility of intrinsic neural fields by tackling various applications: texture transfer between deformed shapes & different shapes, texture reconstruction from real-world images with view dependence, and discretization-agnostic learning on meshes and point clouds.","Authors (format: First Last, First Middle Last, ...)":"Lukas Koestler, Daniel Grittner, Michael Moeller, Daniel Cremers, Zorah L\u00e4hner","Bibtex (e.g. @inproceedings...)":"@article{koestler2022intrinsicneuralfields,\n    author = {Lukas Koestler and Daniel Grittner and Michael Moeller and Daniel Cremers and Zorah Lahner},\n    title = {Intrinsic Neural Fields: Learning Functions on Manifolds},\n    year = {2022},\n    month = {Mar},\n    url = {http://arxiv.org/abs/2203.07967v3}\n}","Bibtex Name":"koestler2022intrinsicneuralfields","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/15/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Fundamentals, Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Intrinsic Neural Fields","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2203.07967.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"3/21/2022 15:50","Title":"Intrinsic Neural Fields: Learning Functions on Manifolds","Training time (hr)":"","UID":"297","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural implicit fields have recently emerged as a useful representation for 3D shapes. These fields are commonly represented as neural networks which map latent descriptors and 3D coordinates to implicit function values. The latent descriptor of a neural field acts as a deformation handle for the 3D shape it represents. Thus, smoothness with respect to this descriptor is paramount for performing shape-editing operations. In this work, we introduce a novel regularization designed to encourage smooth latent spaces in neural fields by penalizing the upper bound on the field's Lipschitz constant. Compared with prior Lipschitz regularized networks, ours is computationally fast, can be implemented in four lines of code, and requires minimal hyperparameter tuning for geometric applications. We demonstrate the effectiveness of our approach on shape interpolation and extrapolation as well as partial shape reconstruction from 3D point clouds, showing both qualitative and quantitative improvements over existing state-of-the-art and non-regularized baselines.","Authors (format: First Last, First Middle Last, ...)":"Hsueh-Ti Derek Liu, Francis Williams, Alec Jacobson, Sanja Fidler, Or Litany","Bibtex (e.g. @inproceedings...)":"@article{liu2022learning,\n    author = {Hsueh-Ti Derek Liu and Francis Williams and Alec Jacobson and Sanja Fidler and Or Litany},\n    title = {Learning Smooth Neural Functions via Lipschitz Regularization},\n    year = {2022},\n    month = {Feb},\n    url = {http://arxiv.org/abs/2202.08345v1}\n}","Bibtex Name":"liu2022learning","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"2/16/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Geometry Only, Fundamentals, Generalization, Global Conditioning, Supervision by Gradient (PDE), Regularization","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2202.08345.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"3/21/2022 11:02","Title":"Learning Smooth Neural Functions via Lipschitz Regularization","Training time (hr)":"","UID":"296","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present Block-NeRF, a variant of Neural Radiance Fields that can represent large-scale environments. Specifically, we demonstrate that when scaling NeRF to render city-scale scenes spanning multiple blocks, it is vital to decompose the scene into individually trained NeRFs. This decomposition decouples rendering time from scene size, enables rendering to scale to arbitrarily large environments, and allows per-block updates of the environment. We adopt several architectural changes to make NeRF robust to data captured over months under different environmental conditions. We add appearance embeddings, learned pose refinement, and controllable exposure to each individual NeRF, and introduce a procedure for aligning appearance between adjacent NeRFs so that they can be seamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to create the largest neural scene representation to date, capable of rendering an entire neighborhood of San Francisco.","Authors (format: First Last, First Middle Last, ...)":"Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P. Srinivasan, Jonathan T. Barron, Henrik Kretzschmar","Bibtex (e.g. @inproceedings...)":"@article{tancik2022blocknerf,\n    author = {Matthew Tancik and Vincent Casser and Xinchen Yan and Sabeek Pradhan and Ben Mildenhall and Pratul P. Srinivasan and Jonathan T. Barron and Henrik Kretzschmar},\n    title = {Block-NeRF: Scalable Large Scene Neural View Synthesis},\n    year = {2022},\n    month = {Feb},\n    url = {http://arxiv.org/abs/2202.05263v1}\n}","Bibtex Name":"tancik2022blocknerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"2/10/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Voxel Grid, Large-Scale Scenes","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Block-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2202.05263.pdf","Project webpage link":"https://waymo.com/research/block-nerf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=6lGMCAzBzOQ","Timestamp":"3/21/2022 11:05","Title":"Block-NeRF: Scalable Large Scene Neural View Synthesis","Training time (hr)":"","UID":"295","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural implicit representations, which encode a surface as the level set of a neural network applied to spatial coordinates, have proven to be remarkably effective for optimizing, compressing, and generating 3D geometry. Although these representations are easy to fit, it is not clear how to best evaluate geometric queries on the shape, such as intersecting against a ray or finding a closest point. The predominant approach is to encourage the network to have a signed distance property. However, this property typically holds only approximately, leading to robustness issues, and holds only at the conclusion of training, inhibiting the use of queries in loss functions. Instead, this work presents a new approach to perform queries directly on general neural implicit functions for a wide range of existing architectures. Our key tool is the application of range analysis to neural networks, using automatic arithmetic rules to bound the output of a network over a region; we conduct a study of range analysis on neural networks, and identify variants of affine arithmetic which are highly effective. We use the resulting bounds to develop geometric queries including ray casting, intersection testing, constructing spatial hierarchies, fast mesh extraction, closest-point evaluation, evaluating bulk properties, and more. Our queries can be efficiently evaluated on GPUs, and offer concrete accuracy guarantees even on randomly-initialized networks, enabling their use in training objectives and beyond. We also show a preliminary application to inverse rendering.","Authors (format: First Last, First Middle Last, ...)":"Nicholas Sharp, Alec Jacobson","Bibtex (e.g. @inproceedings...)":"@article{sharp2022spelunkingthedeep,\n    author = {Nicholas Sharp and Alec Jacobson},\n    title = {Spelunking the Deep: Guaranteed Queries for General Neural Implicit Surfaces},\n    year = {2022},\n    month = {Feb},\n    url = {http://arxiv.org/abs/2202.02444v1}\n}","Bibtex Name":"sharp2022spelunkingthedeep","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"2/5/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Speed & Computational Efficiency, Fundamentals, Sampling, Graphics","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Spelunking the Deep","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2202.02444.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"3/19/2022 12:29","Title":"Spelunking the Deep: Guaranteed Queries for General Neural Implicit Surfaces","Training time (hr)":"","UID":"294","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present ShapeFormer, a transformer-based network that produces a distribution of object completions, conditioned on incomplete, and possibly noisy, point clouds. The resultant distribution can then be sampled to generate likely completions, each exhibiting plausible shape details while being faithful to the input. To facilitate the use of transformers for 3D, we introduce a compact 3D representation, vector quantized deep implicit function, that utilizes spatial sparsity to represent a close approximation of a 3D shape by a short sequence of discrete variables. Experiments demonstrate that ShapeFormer outperforms prior art for shape completion from ambiguous partial inputs in terms of both completion quality and diversity. We also show that our approach effectively handles a variety of shape types, incomplete patterns, and real-world scans.","Authors (format: First Last, First Middle Last, ...)":"Xingguang Yan, Liqiang Lin, Niloy J. Mitra, Dani Lischinski, Danny Cohen-Or, Hui Huang","Bibtex (e.g. @inproceedings...)":"@misc{yan2022shapeformer,\n    year = {2022},\n    author = {Xingguang Yan and Liqiang Lin and Niloy J. Mitra and Dani Lischinski and Danny Cohen-Or and Hui Huang},\n    title = {ShapeFormer: Transformer-based Shape Completion via Sparse Representation},\n    entrytype = {misc},\n    id = {yan2022shapeformer}\n}","Bibtex Name":"yan2022shapeformer","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/QhelDIV/ShapeFormer","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"1/25/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"qheldiv@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Geometry Only, Compression, Generative Models, Generalization, Local Conditioning, Data-Driven Method, Voxel Grid","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"ShapeFormer","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2201.10326.pdf","Project webpage link":"https://shapeformer.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"2/8/2022 6:04","Title":"ShapeFormer: Transformer-based Shape Completion via Sparse Representation","Training time (hr)":"","UID":"293","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce MIP-plicits, a novel approach for rendering 3D and 4D Neural Implicits that divide the problem into macro and meso components. We rely on the iterative nature of the sphere tracing algorithm, the spatial continuity of the Neural Implicit representation, and the association of the network architecture complexity with the details it can represent. This approach does not rely on spatial data structures, and can be used to mix Neural Implicits trained previously and separately as detail levels.  We also introduce Neural Implicit Normal Mapping, which is a core component of the problem factorization. This concept is very close and analogous to the classic normal mapping on meshes, broadly used in Computer Graphics.  Finally, we derive an analytic equation and an algorithm to simplify the normal calculation of Neural Implicits, adapted to be evaluated by the General Matrix Multiply algorithm (GEMM). Current approaches rely on finite differences, which impose additional inferences on auxiliary points and discretization error.","Authors (format: First Last, First Middle Last, ...)":"Vin\u00edcius da Silva, Tiago Novello, Guilherme Schardong, Luiz Schirmer, H\u00e9lio Lopes, Luiz Velho","Bibtex (e.g. @inproceedings...)":"@article{silva2022mipplicits,\n    author = {Vinicius da Silva and Tiago Novello and Guilherme Schardong and Luiz Schirmer and Helio Lopes and Luiz Velho},\n    title = {MIP-plicits: Level of Detail Factorization of Neural Implicits Sphere Tracing},\n    year = {2022},\n    month = {Jan},\n    url = {http://arxiv.org/abs/2201.09147v1}\n}","Bibtex Name":"silva2022mipplicits","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"1/22/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Geometry Only, Graphics, Coarse-to-Fine","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"MIP-plicits","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2201.09147.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"3/27/2022 16:18","Title":"MIP-plicits: Level of Detail Factorization of Neural Implicits Sphere Tracing","Training time (hr)":"","UID":"292","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of ${1920\\!\\times\\!1080}$.","Authors (format: First Last, First Middle Last, ...)":"Thomas M\u00fcller, Alex Evans, Christoph Schied, Alexander Keller","Bibtex (e.g. @inproceedings...)":"@article{muller2022instantngp,\n    author = {Thomas Muller and Alex Evans and Christoph Schied and Alexander Keller},\n    title = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},\n    year = {2022},\n    month = {Jan},\n    url = {http://arxiv.org/abs/2201.05989v1}\n}","Bibtex Name":"muller2022instantngp","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/NVlabs/instant-ngp","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"1/16/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Speed & Computational Efficiency, Fundamentals, Generalization, Hybrid Geometry Representation, Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Instant-NGP","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2201.05989.pdf","Project webpage link":"https://nvlabs.github.io/instant-ngp/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.mp4","Timestamp":"3/21/2022 10:54","Title":"Instant Neural Graphics Primitives with a Multiresolution Hash Encoding","Training time (hr)":"","UID":"291","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce a free-viewpoint rendering method -- HumanNeRF -- that works on a given monocular video of a human performing complex body motions, e.g. a video from YouTube. Our method enables pausing the video at any frame and rendering the subject from arbitrary new camera viewpoints or even a full 360-degree camera path for that particular frame and body pose. This task is particularly challenging, as it requires synthesizing photorealistic details of the body, as seen from various camera angles that may not exist in the input video, as well as synthesizing fine details such as cloth folds and facial appearance. Our method optimizes for a volumetric representation of the person in a canonical T-pose, in concert with a motion field that maps the estimated canonical representation to every frame of the video via backward warps. The motion field is decomposed into skeletal rigid and non-rigid motions, produced by deep networks. We show significant performance improvements over prior work, and compelling examples of free-viewpoint renderings from monocular video of moving humans in challenging uncontrolled capture scenarios.","Authors (format: First Last, First Middle Last, ...)":"Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron, Ira Kemelmacher-Shlizerman","Bibtex (e.g. @inproceedings...)":"@article{weng2022humannerf,\n    author = {Chung-Yi Weng and Brian Curless and Pratul P. Srinivasan and Jonathan T. Barron and Ira Kemelmacher-Shlizerman},\n    title = {HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video},\n    year = {2022},\n    month = {Jan},\n    url = {http://arxiv.org/abs/2201.04127v1}\n}","Bibtex Name":"weng2022humannerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"1/11/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Dynamic/Temporal, Human (Body), Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"HumanNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2201.04127.pdf","Project webpage link":"https://grail.cs.washington.edu/projects/humannerf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=GM-RoZEymmw","Timestamp":"3/27/2022 15:40","Title":"HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video","Training time (hr)":"","UID":"290","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Videos show continuous events, yet most - if not all - video synthesis frameworks treat them discretely in time. In this work, we think of videos of what they should be - time-continuous signals, and extend the paradigm of neural representations to build a continuous-time video generator. For this, we first design continuous motion representations through the lens of positional embeddings. Then, we explore the question of training on very sparse videos and demonstrate that a good generator can be learned by using as few as 2 frames per clip. After that, we rethink the traditional image and video discriminators pair and propose to use a single hypernetwork-based one. This decreases the training cost and provides richer learning signal to the generator, making it possible to train directly on 1024$^2$ videos for the first time. We build our model on top of StyleGAN2 and it is just 5% more expensive to train at the same resolution while achieving almost the same image quality. Moreover, our latent space features similar properties, enabling spatial manipulations that our method can propagate in time. We can generate arbitrarily long videos at arbitrary high frame rate, while prior work struggles to generate even 64 frames at a fixed rate. Our model achieves state-of-the-art results on four modern 256$^2$ video synthesis benchmarks and one 1024$^2$ resolution one. Videos and the source code are available at the project website: https://universome.github.io/stylegan-v.","Authors (format: First Last, First Middle Last, ...)":"Ivan Skorokhodov, Sergey Tulyakov, Mohamed Elhoseiny","Bibtex (e.g. @inproceedings...)":"@article{skorokhodov2022styleganv,\n    author = {Ivan Skorokhodov and Sergey Tulyakov and Mohamed Elhoseiny},\n    title = {StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.14683v1}\n}","Bibtex Name":"skorokhodov2022styleganv","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/29/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Dynamic/Temporal, 2D Image Neural Fields, Generative Models, Generalization, Global Conditioning, Hypernetwork/Meta-learning, Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"StyleGAN-V","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.14683.pdf","Project webpage link":"https://universome.github.io/stylegan-v","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://kaust-cair.s3.amazonaws.com/stylegan-v/stylegan-v.mp4","Timestamp":"3/19/2022 14:24","Title":"StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2","Training time (hr)":"","UID":"289","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose a novel method to enhance the performance of coordinate-MLPs by learning instance-specific positional embeddings. End-to-end optimization of positional embedding parameters along with network weights leads to poor generalization performance. Instead, we develop a generic framework to learn the positional embedding based on the classic graph-Laplacian regularization, which can implicitly balance the trade-off between memorization and generalization. This framework is then used to propose a novel positional embedding scheme, where the hyperparameters are learned per coordinate (i.e, instance) to deliver optimal performance. We show that the proposed embedding achieves better performance with higher stability compared to the well-established random Fourier features (RFF). Further, we demonstrate that the proposed embedding scheme yields stable gradients, enabling seamless integration into deep architectures as intermediate layers.","Authors (format: First Last, First Middle Last, ...)":"Sameera Ramasinghe, Simon Lucey","Bibtex (e.g. @inproceedings...)":"@article{ramasinghe2021learning,\n    author = {Sameera Ramasinghe and Simon Lucey},\n    title = {Learning Positional Embeddings for Coordinate-MLPs},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.11577v2}\n}","Bibtex Name":"ramasinghe2021learning","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/21/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"2D Image Neural Fields, Fundamentals, Generalization, Global Conditioning, Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.11577.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"3/19/2022 14:19","Title":"Learning Positional Embeddings for Coordinate-MLPs","Training time (hr)":"","UID":"288","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce a high resolution, 3D-consistent image and shape generation technique which we call StyleSDF. Our method is trained on single-view RGB data only, and stands on the shoulders of StyleGAN2 for image generation, while solving two main challenges in 3D-aware GANs: 1) high-resolution, view-consistent generation of the RGB images, and 2) detailed 3D shape. We achieve this by merging a SDF-based 3D representation with a style-based 2D generator. Our 3D implicit network renders low-resolution feature maps, from which the style-based network generates view-consistent, 1024x1024 images. Notably, our SDF-based 3D modeling defines detailed 3D surfaces, leading to consistent volume rendering. Our method shows higher quality results compared to state of the art in terms of visual and geometric quality.","Authors (format: First Last, First Middle Last, ...)":"Roy Or-El, Xuan Luo, Mengyi Shan, Eli Shechtman, Jeong Joon Park, Ira Kemelmacher-Shlizerman","Bibtex (e.g. @inproceedings...)":"@article{or-el2021stylesdf,\n    url = {http://arxiv.org/abs/2112.11427v1},\n    month = {Dec},\n    year = {2021},\n    title = {StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation},\n    author = {Roy Or-El and Xuan Luo and Mengyi Shan and Eli Shechtman and Jeong Joon Park and Ira Kemelmacher-Shlizerman},\n    entrytype = {article},\n    id = {or-el2021stylesdf}\n}","Bibtex Name":"or-el2021stylesdf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/royorel/StyleSDF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/21/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"royorel@cs.washington.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Sinusoidal Activation (SIREN)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Generative Models","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"StyleSDF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.11427.pdf","Project webpage link":"https://stylesdf.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"3/1/2022 19:09","Title":"StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation","Training time (hr)":"","UID":"287","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We explore how to leverage neural radiance fields (NeRFs) to build interactive 3D environments from large-scale visual captures spanning buildings or even multiple city blocks collected primarily from drone data. In contrast to the single object scenes against which NeRFs have been traditionally evaluated, this setting poses multiple challenges including (1) the need to incorporate thousands of images with varying lighting conditions, all of which capture only a small subset of the scene, (2) prohibitively high model capacity and ray sampling requirements beyond what can be naively trained on a single GPU, and (3) an arbitrarily large number of possible viewpoints that make it unfeasible to precompute all relevant information beforehand (as real-time NeRF renderers typically do). To address these challenges, we begin by analyzing visibility statistics for large-scale scenes, motivating a sparse network structure where parameters are specialized to different regions of the scene. We introduce a simple geometric clustering algorithm that partitions training images (or rather pixels) into different NeRF submodules that can be trained in parallel. We evaluate our approach across scenes taken from the Quad 6k and UrbanScene3D datasets as well as against our own drone footage and show a 3x training speedup while improving PSNR by over 11% on average. We subsequently perform an empirical evaluation of recent NeRF fast renderers on top of Mega-NeRF and introduce a novel method that exploits temporal coherence. Our technique achieves a 40x speedup over conventional NeRF rendering while remaining within 0.5 db in PSNR quality, exceeding the fidelity of existing fast renderers.","Authors (format: First Last, First Middle Last, ...)":"Haithem Turki, Deva Ramanan, Mahadev Satyanarayanan","Bibtex (e.g. @inproceedings...)":"@article{turki2022meganerf,\n    author = {Haithem Turki and Deva Ramanan and Mahadev Satyanarayanan},\n    title = {Mega-NeRF: Scalable Construction of Large-Scale NeRFs for Virtual Fly-Throughs},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.10703v1}\n}","Bibtex Name":"turki2022meganerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/cmusatyalab/mega-nerf","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/20/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Speed & Computational Efficiency, Sampling, Large-Scale Scenes","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Mega-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.10703.pdf","Project webpage link":"https://meganerf.cmusatyalab.org/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"3/21/2022 13:06","Title":"Mega-NeRF: Scalable Construction of Large-Scale NeRFs for Virtual Fly-Throughs","Training time (hr)":"","UID":"286","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Classical light field rendering for novel view synthesis can accurately reproduce view-dependent effects such as reflection, refraction, and translucency, but requires a dense view sampling of the scene. Methods based on geometric reconstruction need only sparse views, but cannot accurately model non-Lambertian effects. We introduce a model that combines the strengths and mitigates the limitations of these two directions. By operating on a four-dimensional representation of the light field, our model learns to represent view-dependent effects accurately. By enforcing geometric constraints during training and inference, the scene geometry is implicitly learned from a sparse set of views. Concretely, we introduce a two-stage transformer-based model that first aggregates features along epipolar lines, then aggregates features along reference views to produce the color of a target ray. Our model outperforms the state-of-the-art on multiple forward-facing and 360{\\deg} datasets, with larger margins on scenes with severe view-dependent variations.","Authors (format: First Last, First Middle Last, ...)":"Mohammed Suhail, Carlos Esteves, Leonid Sigal, Ameesh Makadia","Bibtex (e.g. @inproceedings...)":"@article{suhail2021light,\n    author = {Mohammed Suhail and Carlos Esteves and Leonid Sigal and Ameesh Makadia},\n    title = {Light Field Neural Rendering},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.09687v1}\n}","Bibtex Name":"suhail2021light","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/google-research/google-research/tree/master/light_field_neural_rendering","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/17/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Speed & Computational Efficiency, Graphics, Sampling","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.09687.pdf","Project webpage link":"https://light-field-neural-rendering.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"3/27/2022 15:29","Title":"Light Field Neural Rendering","Training time (hr)":"","UID":"285","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Deformable medical image registration has in past years been revolutionized by deep learning with convolutional neural networks. These methods surpass conventional image registration techniques in speed but not in accuracy. Here, we present an alternative approach to leveraging neural networks for image registration. Instead of using a neural network to predict the transformation between images, we optimize a neural network to represent this continuous transformation. Using recent insights from differentiable rendering, we show how such an implicit deformable image registration (IDIR) model can be naturally combined with regularization terms based on standard automatic differentiation techniques. We demonstrate the effectiveness of this model on 4D chest CT registration in the DIR-LAB data set and find that a single three-layer multi-layer perceptron with periodic activation functions outperforms all published deep learning-based methods, without any folding and without the need for training data. The model is flexible enough to be extended to include different losses, regularizers, and optimization schemes and is implemented using standard deep learning libraries.","Authors (format: First Last, First Middle Last, ...)":"Jelmer M. Wolterink, Jesse Zwienenberg, Christoph Brune","Bibtex (e.g. @inproceedings...)":"@inproceedings{wolterink2021implicit,\n    booktitle = {Medical Imaging with Deep Learning},\n    year = {2022},\n    author = {Jelmer M Wolterink and Jesse C Zwienenberg and Christoph Brune},\n    title = {Implicit Neural Representations for Deformable Image Registration}\n}","Bibtex Name":"wolterink2021implicit","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/17/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"j.m.wolterink@utwente.nl","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Sinusoidal Activation (SIREN)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Deformation vector field","Inputs":"","Is the PDF linked to arXiv?":"No (Please fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"2D Image Neural Fields, Data-Driven Method","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"IDIR","PDF link (arXiv perferred)":"https://openreview.net/pdf?id=BP29eKzQBu3","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"N/A","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"3/1/2022 16:59","Title":"Implicit Neural Representations for Deformable Image Registration","Training time (hr)":"","UID":"284","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"MIDL 2022","Venue no Year":"MIDL","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"3D-aware image generative modeling aims to generate 3D-consistent images with explicitly controllable camera poses. Recent works have shown promising results by training neural radiance field (NeRF) generators on unstructured 2D images, but still can not generate highly-realistic images with fine details. A critical reason is that the high memory and computation cost of volumetric representation learning greatly restricts the number of point samples for radiance integration during training. Deficient sampling not only limits the expressive power of the generator to handle fine details but also impedes effective GAN training due to the noise caused by unstable Monte Carlo sampling. We propose a novel approach that regulates point sampling and radiance field learning on 2D manifolds, embodied as a set of learned implicit surfaces in the 3D volume. For each viewing ray, we calculate ray-surface intersections and accumulate their radiance generated by the network. By training and rendering such radiance manifolds, our generator can produce high quality images with realistic fine details and strong visual 3D consistency.","Authors (format: First Last, First Middle Last, ...)":"Yu Deng, Jiaolong Yang, Jianfeng Xiang, Xin Tong","Bibtex (e.g. @inproceedings...)":"@article{deng2022gram,\n    author = {Yu Deng and Jiaolong Yang and Jianfeng Xiang and Xin Tong},\n    title = {GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.08867v2}\n}","Bibtex Name":"deng2022gram","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/16/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Sampling, Generative Models, Data-Driven Method, Regularization","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"GRAM","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.08867.pdf","Project webpage link":"https://yudeng.github.io/GRAM/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=hBJWZwl_JCI","Timestamp":"3/27/2022 15:26","Title":"GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation","Training time (hr)":"","UID":"283","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute-intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efficiency and image quality of 3D GANs without overly relying on these approximations. For this purpose, we introduce an expressive hybrid explicit-implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments.","Authors (format: First Last, First Middle Last, ...)":"Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, Gordon Wetzstein","Bibtex (e.g. @inproceedings...)":"@article{chan2021eg3d,\n    author = {Eric R. Chan and Connor Z. Lin and Matthew A. Chan and Koki Nagano and Boxiao Pan and Shalini De Mello and Orazio Gallo and Leonidas Guibas and Jonathan Tremblay and Sameh Khamis and Tero Karras and Gordon Wetzstein},\n    title = {Efficient Geometry-aware 3D Generative Adversarial Networks},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.07945v1}\n}","Bibtex Name":"chan2021eg3d","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/NVlabs/eg3d","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/15/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Generative Models, Generalization, Local Conditioning, Data-Driven Method, Hybrid Geometry Representation, Voxel Grid","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"EG3D","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.07945.pdf","Project webpage link":"https://matthew-a-chan.github.io/EG3D/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=cXxEwI7QbKg","Timestamp":"3/27/2022 15:22","Title":"Efficient Geometry-aware 3D Generative Adversarial Networks","Training time (hr)":"","UID":"282","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Capturing and rendering life-like hair is particularly challenging due to its fine geometric structure, the complex physical interaction and its non-trivial visual appearance.Yet, hair is a critical component for believable avatars. In this paper, we address the aforementioned problems: 1) we use a novel, volumetric hair representation that is com-posed of thousands of primitives. Each primitive can be rendered efficiently, yet realistically, by building on the latest advances in neural rendering. 2) To have a reliable control signal, we present a novel way of tracking hair on the strand level. To keep the computational effort manageable, we use guide hairs and classic techniques to expand those into a dense hood of hair. 3) To better enforce temporal consistency and generalization ability of our model, we further optimize the 3D scene flow of our representation with multi-view optical flow, using volumetric ray marching. Our method can not only create realistic renders of recorded multi-view sequences, but also create renderings for new hair configurations by providing new control signals. We compare our method with existing work on viewpoint synthesis and drivable animation and achieve state-of-the-art results. Please check out our project website at https://ziyanw1.github.io/hvh/.","Authors (format: First Last, First Middle Last, ...)":"Ziyan Wang, Giljoo Nam, Tuur Stuyck, Stephen Lombardi, Michael Zollhoefer, Jessica Hodgins, Christoph Lassner","Bibtex (e.g. @inproceedings...)":"@article{wang2021hvh,\n    author = {Ziyan Wang and Giljoo Nam and Tuur Stuyck and Stephen Lombardi and Michael Zollhoefer and Jessica Hodgins and Christoph Lassner},\n    title = {HVH: Learning a Hybrid Neural Volumetric Representation for Dynamic Hair Performance Capture},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.06904v3}\n}","Bibtex Name":"wang2021hvh","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/13/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Human (Head), Graphics, Material/Lighting Estimation, Editable, Local Conditioning, Data-Driven Method, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"HVH","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.06904.pdf","Project webpage link":"https://ziyanw1.github.io/hvh/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"3/27/2022 15:17","Title":"HVH: Learning a Hybrid Neural Volumetric Representation for Dynamic Hair Performance Capture","Training time (hr)":"","UID":"281","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural Radiance Field (NeRF) has achieved outstanding performance in modeling 3D objects and controlled scenes, usually under a single scale. In this work, we make the first attempt to bring NeRF to city-scale, with views ranging from satellite-level that captures the overview of a city, to ground-level imagery showing complex details of an architecture. The wide span of camera distance to the scene yields multi-scale data with different levels of detail and spatial coverage, which casts great challenges to vanilla NeRF and biases it towards compromised results. To address these issues, we introduce CityNeRF, a progressive learning paradigm that grows the NeRF model and training set synchronously. Starting from fitting distant views with a shallow base block, as training progresses, new blocks are appended to accommodate the emerging details in the increasingly closer views. The strategy effectively activates high-frequency channels in the positional encoding and unfolds more complex details as the training proceeds. We demonstrate the superiority of CityNeRF in modeling diverse city-scale scenes with drastically varying views, and its support for rendering views in different levels of detail.","Authors (format: First Last, First Middle Last, ...)":"Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai, Dahua Lin","Bibtex (e.g. @inproceedings...)":"@article{xiangli2021citynerf,\n    author = {Yuanbo Xiangli and Linning Xu and Xingang Pan and Nanxuan Zhao and Anyi Rao and Christian Theobalt and Bo Dai and Dahua Lin},\n    title = {CityNeRF: Building NeRF at City Scale},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.05504v2}\n}","Bibtex Name":"xiangli2021citynerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/city-super/citynerf","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/10/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Coarse-to-Fine, Large-Scale Scenes, Network Architecture","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"CityNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.05504.pdf","Project webpage link":"https://city-super.github.io/citynerf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://city-super.github.io/citynerf/img/citynerf_beautifuldemo_540p.mp4","Timestamp":"3/27/2022 15:45","Title":"CityNeRF: Building NeRF at City Scale","Training time (hr)":"","UID":"280","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In this work, we develop intuitive controls for editing the style of 3D objects. Our framework, Text2Mesh, stylizes a 3D mesh by predicting color and local geometric details which conform to a target text prompt. We consider a disentangled representation of a 3D object using a fixed mesh input (content) coupled with a learned neural network, which we term neural style field network. In order to modify style, we obtain a similarity score between a text prompt (describing style) and a stylized mesh by harnessing the representational power of CLIP. Text2Mesh requires neither a pre-trained generative model nor a specialized 3D mesh dataset. It can handle low-quality meshes (non-manifold, boundaries, etc.) with arbitrary genus, and does not require UV parameterization. We demonstrate the ability of our technique to synthesize a myriad of styles over a wide variety of 3D meshes.","Authors (format: First Last, First Middle Last, ...)":"Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, Rana Hanocka","Bibtex (e.g. @inproceedings...)":"@article{michel2022text2mesh,\n    author = {Oscar Michel and Roi Bar-On and Richard Liu and Sagie Benaim and Rana Hanocka},\n    title = {Text2Mesh: Text-Driven Neural Stylization for Meshes},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.03221v1},\n    entrytype = {article},\n    id = {michel2022text2mesh}\n}","Bibtex Name":"michel2022text2mesh","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/threedle/text2mesh","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/6/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"ranahanocka@uchicago.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Mesh","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Editable, Generative Models, Neural Style Field","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"Text2Mesh","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.03221.pdf","Project webpage link":"https://threedle.github.io/text2mesh/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"https://arxiv.org/pdf/2112.03221.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"1/7/2022 6:22","Title":"Text2Mesh: Text-Driven Neural Stylization for Meshes","Training time (hr)":"","UID":"279","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We extend neural 3D representations to allow for intuitive and interpretable user control beyond novel view rendering (i.e. camera control). We allow the user to annotate which part of the scene one wishes to control with just a small number of mask annotations in the training images. Our key idea is to treat the attributes as latent variables that are regressed by the neural network given the scene encoding. This leads to a few-shot learning framework, where attributes are discovered automatically by the framework, when annotations are not provided. We apply our method to various scenes with different types of controllable attributes (e.g. expression control on human faces, or state control in movement of inanimate objects). Overall, we demonstrate, to the best of our knowledge, for the first time novel view and novel attribute re-rendering of scenes from a single video.","Authors (format: First Last, First Middle Last, ...)":"Kacper Kania, Kwang Moo Yi, Marek Kowalski, Tomasz Trzci\u0144ski, Andrea Tagliasacchi","Bibtex (e.g. @inproceedings...)":"@article{kania2022conerf,\n    author = {Kacper Kania and Kwang Moo Yi and Marek Kowalski and Tomasz Trzcinski and Andrea Tagliasacchi},\n    title = {CoNeRF: Controllable Neural Radiance Fields},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.01983v2}\n}","Bibtex Name":"kania2022conerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/3/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Dynamic/Temporal, Human (Head), Editable, Generalization","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"CoNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.01983.pdf","Project webpage link":"https://conerf.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://conerf.github.io/showcase.mp4","Timestamp":"3/22/2022 10:17","Title":"CoNeRF: Controllable Neural Radiance Fields","Training time (hr)":"","UID":"278","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We combine neural rendering with multi-modal image and text representations to synthesize diverse 3D objects solely from natural language descriptions. Our method, Dream Fields, can generate the geometry and color of a wide range of objects without 3D supervision. Due to the scarcity of diverse, captioned 3D data, prior methods only generate objects from a handful of categories, such as ShapeNet. Instead, we guide generation with image-text models pre-trained on large datasets of captioned images from the web. Our method optimizes a Neural Radiance Field from many camera views so that rendered images score highly with a target caption according to a pre-trained CLIP model. To improve fidelity and visual quality, we introduce simple geometric priors, including sparsity-inducing transmittance regularization, scene bounds, and new MLP architectures. In experiments, Dream Fields produce realistic, multi-view consistent object geometry and color from a variety of natural language captions.","Authors (format: First Last, First Middle Last, ...)":"Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, Ben Poole","Bibtex (e.g. @inproceedings...)":"@article{jain2021dreamfield,\n    author = {Ajay Jain and Ben Mildenhall and Jonathan T. Barron and Pieter Abbeel and Ben Poole},\n    title = {Zero-Shot Text-Guided Object Generation with Dream Fields},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.01455v1}\n}","Bibtex Name":"jain2021dreamfield","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/google-research/google-research/tree/master/dreamfields","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/2/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Generative Models, Multi-Modal Signals, Natural Languages","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Dream Field","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.01455.pdf","Project webpage link":"https://ajayj.com/dreamfields","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=1Fke6w46tv4&t=2s","Timestamp":"3/22/2022 8:45","Title":"Zero-Shot Text-Guided Object Generation with Dream Fields","Training time (hr)":"","UID":"277","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural Radiance Fields (NeRF) have emerged as a powerful representation for the task of novel view synthesis due to their simplicity and state-of-the-art performance. Though NeRF can produce photorealistic renderings of unseen viewpoints when many input views are available, its performance drops significantly when this number is reduced. We observe that the majority of artifacts in sparse input scenarios are caused by errors in the estimated scene geometry, and by divergent behavior at the start of training. We address this by regularizing the geometry and appearance of patches rendered from unobserved viewpoints, and annealing the ray sampling space during training. We additionally use a normalizing flow model to regularize the color of unobserved viewpoints. Our model outperforms not only other methods that optimize over a single scene, but in many cases also conditional models that are extensively pre-trained on large multi-view datasets.","Authors (format: First Last, First Middle Last, ...)":"Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi, Andreas Geiger, Noha Radwan","Bibtex (e.g. @inproceedings...)":"@article{niemeyer2022regnerf,\n    author = {Michael Niemeyer and Jonathan T. Barron and Ben Mildenhall and Mehdi S. M. Sajjadi and Andreas Geiger and Noha Radwan},\n    title = {RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs},\n    year = {2021},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2112.00724v1}\n}","Bibtex Name":"niemeyer2022regnerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/google-research/google-research/tree/master/regnerf","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/1/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Sampling, Regularization","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"RegNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2112.00724.pdf","Project webpage link":"https://m-niemeyer.github.io/regnerf/index.html","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=QyyyvA4-Kwc","Timestamp":"3/21/2022 15:58","Title":"RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs","Training time (hr)":"","UID":"276","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2022","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural Radiance Fields (NeRF) has recently gained popularity for its impressive novel view synthesis ability. This paper studies the problem of hallucinated NeRF: i.e. recovering a realistic NeRF at a different time of day from a group of tourism images. Existing solutions adopt NeRF with a controllable appearance embedding to render novel views under various conditions, but cannot render view-consistent images with an unseen appearance. To solve this problem, we present an end-to-end framework for constructing a hallucinated NeRF, dubbed as Ha-NeRF. Specifically, we propose an appearance hallucination module to handle time-varying appearances and transfer them to novel views. Considering the complex occlusions of tourism images, an anti-occlusion module is introduced to decompose the static subjects for visibility accurately. Experimental results on synthetic data and real tourism photo collections demonstrate that our method can not only hallucinate the desired appearances, but also render occlusion-free images from different views. The project and supplementary materials are available at https://rover-xingyu.github.io/Ha-NeRF/.","Authors (format: First Last, First Middle Last, ...)":"Xingyu Chen, Qi Zhang, Xiaoyu Li, Yue Chen, Ying Feng, Xuan Wang, Jue Wang","Bibtex (e.g. @inproceedings...)":"@article{chen2021hanerf,\n    url = {http://arxiv.org/abs/2111.15246v2},\n    month = {Nov},\n    year = {2021},\n    title = {Hallucinated Neural Radiance Fields in the Wild},\n    author = {Xingyu Chen and Qi Zhang and Xiaoyu Li and Yue Chen and Ying Feng and Xuan Wang and Jue Wang},\n    entrytype = {article},\n    id = {chen2021hanerf}\n}","Bibtex Name":"chen2021hanerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/rover-xingyu/Ha-NeRF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/30/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"xingyu@stu.xjtu.edu.cn","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Dynamic/Temporal, Editable, Global Conditioning","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"Ha-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2111.15246.pdf","Project webpage link":"https://rover-xingyu.github.io/Ha-NeRF/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"12/2/2021 3:31","Title":"Hallucinated Neural Radiance Fields in the Wild","Training time (hr)":"","UID":"275","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present Neural Kernel Fields: a novel method for reconstructing implicit 3D shapes based on a learned kernel ridge regression. Our technique achieves state-of-the-art results when reconstructing 3D objects and large scenes from sparse oriented points, and can reconstruct shape categories outside the training set with almost no drop in accuracy. The core insight of our approach is that kernel methods are extremely effective for reconstructing shapes when the chosen kernel has an appropriate inductive bias. We thus factor the problem of shape reconstruction into two parts: (1) a backbone neural network which learns kernel parameters from data, and (2) a kernel ridge regression that fits the input points on-the-fly by solving a simple positive definite linear system using the learned kernel. As a result of this factorization, our reconstruction gains the benefits of data-driven methods under sparse point density while maintaining interpolatory behavior, which converges to the ground truth shape as input sampling density increases. Our experiments demonstrate a strong generalization capability to objects outside the train-set category and scanned scenes. Source code and pretrained models are available at https://nv-tlabs.github.io/nkf.","Authors (format: First Last, First Middle Last, ...)":"Francis Williams, Zan Gojcic, Sameh Khamis, Denis Zorin, Joan Bruna, Sanja Fidler, Or Litany","Bibtex (e.g. @inproceedings...)":"@article{williams2021neuralkernelfields(nkf),\n    url = {http://arxiv.org/abs/2111.13674v1},\n    month = {Nov},\n    year = {2021},\n    title = {Neural Fields as Learnable Kernels for 3D Reconstruction},\n    author = {Francis Williams and Zan Gojcic and Sameh Khamis and Denis Zorin and Joan Bruna and Sanja Fidler and Or Litany},\n    entrytype = {article},\n    id = {williams2021neuralkernelfields(nkf)}\n}","Bibtex Name":"williams2021neuralkernelfields(nkf)","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/26/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"or.litany@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Sparse Reconstruction, Geometry Only, Generalization, Global Conditioning, Local Conditioning, Data-Driven Method, Hypernetwork/Meta-learning, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"Neural Kernel Fields (NKF)","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2111.13674.pdf","Project webpage link":"https://nv-tlabs.github.io/nkf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"11/29/2021 11:40","Title":"Neural Fields as Learnable Kernels for 3D Reconstruction","Training time (hr)":"","UID":"274","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural Radiance Field (NeRF) is a popular method in data-driven 3D reconstruction. Given its simplicity and high quality rendering, many NeRF applications are being developed. However, NeRF's big limitation is its slow speed. Many attempts are made to speeding up NeRF training and inference, including intricate code-level optimization and caching, use of sophisticated data structures, and amortization through multi-task and meta learning. In this work, we revisit the basic building blocks of NeRF through the lens of classic techniques before NeRF. We propose Voxel-Accelearated NeRF (VaxNeRF), integrating NeRF with visual hull, a classic 3D reconstruction technique only requiring binary foreground-background pixel labels per image. Visual hull, which can be optimized in about 10 seconds, can provide coarse in-out field separation to omit substantial amounts of network evaluations in NeRF. We provide a clean fully-pythonic, JAX-based implementation on the popular JaxNeRF codebase, consisting of only about 30 lines of code changes and a modular visual hull subroutine, and achieve about 2-8x faster learning on top of the highly-performative JaxNeRF baseline with zero degradation in rendering quality. With sufficient compute, this effectively brings down full NeRF training from hours to 30 minutes. We hope VaxNeRF -- a careful combination of a classic technique with a deep method (that arguably replaced it) -- can empower and accelerate new NeRF extensions and applications, with its simplicity, portability, and reliable performance gains. Codes are available at https://github.com/naruya/VaxNeRF .","Authors (format: First Last, First Middle Last, ...)":"Naruya Kondo, Yuya Ikeda, Andrea Tagliasacchi, Yutaka Matsuo, Yoichi Ochiai, Shixiang Shane Gu","Bibtex (e.g. @inproceedings...)":"@article{kondo2021vaxnerf,\n    author = {Naruya Kondo and Yuya Ikeda and Andrea Tagliasacchi and Yutaka Matsuo and Yoichi Ochiai and Shixiang Shane Gu},\n    title = {VaxNeRF: Revisiting the Classic for Voxel-Accelerated Neural Radiance Field},\n    year = {2021},\n    month = {Nov},\n    url = {http://arxiv.org/abs/2111.13112v1},\n    entrytype = {article},\n    id = {kondo2021vaxnerf}\n}","Bibtex Name":"kondo2021vaxnerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/naruya/VaxNeRF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/25/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"n-kondo@digitalnature.slis.tsukuba.ac.jp","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Speed & Computational Efficiency, Generative Models, Voxel Grid, Object-Centric, Coarse-to-Fine","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"VaxNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2111.13112.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"12/17/2021 13:16","Title":"VaxNeRF: Revisiting the Classic for Voxel-Accelerated Neural Radiance Field","Training time (hr)":"","UID":"273","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Internal computational models of physical bodies are fundamental to the ability of robots and animals alike to plan and control their actions. These \"self-models\" allow robots to consider outcomes of multiple possible future actions, without trying them out in physical reality. Recent progress in fully data-driven self-modeling has enabled machines to learn their own forward kinematics directly from task-agnostic interaction data. However, forward-kinema\\-tics models can only predict limited aspects of the morphology, such as the position of end effectors or velocity of joints and masses. A key challenge is to model the entire morphology and kinematics, without prior knowledge of what aspects of the morphology will be relevant to future tasks. Here, we propose that instead of directly modeling forward-kinematics, a more useful form of self-modeling is one that could answer space occupancy queries, conditioned on the robot's state. Such query-driven self models are continuous in the spatial domain, memory efficient, fully differentiable and kinematic aware. In physical experiments, we demonstrate how a visual self-model is accurate to about one percent of the workspace, enabling the robot to perform various motion planning and control tasks. Visual self-modeling can also allow the robot to detect, localize and recover from real-world damage, leading to improved machine resiliency. Our project website is at: https://robot-morphology.cs.columbia.edu/","Authors (format: First Last, First Middle Last, ...)":"Boyuan Chen, Robert Kwiatkowski, Carl Vondrick, Hod Lipson","Bibtex (e.g. @inproceedings...)":"@article{chen2021visualselfmodelingrobot,\n    url = {http://arxiv.org/abs/2111.06389v2},\n    month = {Nov},\n    year = {2021},\n    title = {Full-Body Visual Self-Modeling of Robot Morphologies},\n    author = {Boyuan Chen and Robert Kwiatkowski and Carl Vondrick and Hod Lipson},\n    entrytype = {article},\n    id = {chen2021visualselfmodelingrobot}\n}","Bibtex Name":"chen2021visualselfmodelingrobot","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/BoyuanChen/visual-selfmodeling","Coordinates all at once":"","Data Release (link)":"https://github.com/BoyuanChen/visual-selfmodeling","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/11/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"bchen@cs.columbia.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Sinusoidal Activation (SIREN)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Robotics, Science & Engineering, Data-Driven Method","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"Visual Self-Modeling Robot","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2111.06389.pdf","Project webpage link":"https://robot-morphology.cs.columbia.edu/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"https://youtu.be/aoCAplokoWE","Talk/Video (link, Youtube preferred)":"https://youtu.be/aoCAplokoWE","Timestamp":"11/23/2021 19:00","Title":"Full-Body Visual Self-Modeling of Robot Morphologies","Training time (hr)":"","UID":"272","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline. This state-of-the-art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non-rigidly deforming objects...","Authors (format: First Last, First Middle Last, ...)":"Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, Yifan Wang, Christoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, Tomas Simon, Christian Theobalt, Matthias Niessner, Jonathan T. Barron, Gordon Wetzstein, Michael Zollhoefer, Vladislav Golyanik","Bibtex (e.g. @inproceedings...)":"@article{tewari2021advances,\n    author = {Ayush Tewari and Justus Thies and Ben Mildenhall and Pratul Srinivasan and Edgar Tretschk and Yifan Wang and Christoph Lassner and Vincent Sitzmann and Ricardo Martin-Brualla and Stephen Lombardi and Tomas Simon and Christian Theobalt and Matthias Niessner and Jonathan T. Barron and Gordon Wetzstein and Michael Zollhoefer and Vladislav Golyanik},\n    title = {Advances in Neural Rendering},\n    year = {2021},\n    month = {Nov},\n    url = {http://arxiv.org/abs/2111.05849v1},\n    entrytype = {article},\n    id = {tewari2021advances}\n}","Bibtex Name":"tewari2021advances","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/10/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"james_tompkin@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"2D Image Neural Fields, Image-Based Rendering","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2111.05849.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"11/23/2021 10:32","Title":"Advances in Neural Rendering","Training time (hr)":"","UID":"271","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Most existing geometry processing algorithms use meshes as the default shape representation. Manipulating meshes, however, requires one to maintain high quality in the surface discretization. For example, changing the topology of a mesh usually requires additional procedures such as remeshing. This paper instead proposes the use of neural fields for geometry processing. Neural fields can compactly store complicated shapes without spatial discretization. Moreover, neural fields are infinitely differentiable, which allows them to be optimized for objectives that involve higher-order derivatives. This raises the question: can geometry processing be done entirely using neural fields? We introduce loss functions and architectures to show that some of the most challenging geometry processing tasks, such as deformation and filtering, can be done with neural fields. Experimental results show that our methods are on par with the well-established mesh-based methods without committing to a particular surface discretization. Code is available at https://github.com/stevenygd/NFGP.","Authors (format: First Last, First Middle Last, ...)":"Guandao Yang, Serge Belongie, Bharath Hariharan, Vladlen Koltun","Bibtex (e.g. @inproceedings...)":"@article{yang2021nfgp,\n    volume = {34},\n    title = {Geometry Processing with Neural Fields},\n    year = {2021},\n    journal = {Advances in Neural Information Processing Systems},\n    author = {Guandao Yang and Serge Belongie and Bharath Hariharan and Vladlen Koltun},\n    entrytype = {article},\n    id = {yang2021nfgp}\n}","Bibtex Name":"yang2021nfgp","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"https://github.com/stevenygd/NFGP","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/10/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"gy46@cornell.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Sinusoidal Activation (SIREN)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"No (Please fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Geometry Processing","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"NFGP","PDF link (arXiv perferred)":"https://openreview.net/pdf?id=JG-SlCAx5_K","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"11/23/2021 0:07","Title":"Geometry Processing with Neural Fields","Training time (hr)":"","UID":"270","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2021","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"The advent of generative radiance fields has significantly promoted the development of 3D-aware image synthesis. The cumulative rendering process in radiance fields makes training these generative models much easier since gradients are distributed over the entire volume, but leads to diffused object surfaces. In the meantime, compared to radiance fields occupancy representations could inherently ensure deterministic surfaces. However, if we directly apply occupancy representations to generative models, during training they will only receive sparse gradients located on object surfaces and eventually suffer from the convergence problem. In this paper, we propose Generative Occupancy Fields (GOF), a novel model based on generative radiance fields that can learn compact object surfaces without impeding its training convergence. The key insight of GOF is a dedicated transition from the cumulative rendering in radiance fields to rendering with only the surface points as the learned surface gets more and more accurate. In this way, GOF combines the merits of two representations in a unified framework. In practice, the training-time transition of start from radiance fields and march to occupancy representations is achieved in GOF by gradually shrinking the sampling region in its rendering process from the entire volume to a minimal neighboring region around the surface. Through comprehensive experiments on multiple datasets, we demonstrate that GOF can synthesize high-quality images with 3D consistency and simultaneously learn compact and smooth object surfaces. Code, models, and demo videos are available at https://sheldontsui.github.io/projects/GOF","Authors (format: First Last, First Middle Last, ...)":"Xudong Xu, Xingang Pan, Dahua Lin, Bo Dai","Bibtex (e.g. @inproceedings...)":"@inproceedings{xu2021generative,\n    title = {Generative Occupancy Fields for 3D Surface-Aware Image Synthesis},\n    author = {Xudong Xu and Xingang Pan and Dahua Lin and Bo Dai},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    year = {2021},\n    entrytype = {inproceedings},\n    id = {xu2021generative}\n}","Bibtex Name":"xu2021generative","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/SheldonTsui/GOF_NeurIPS2021","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/1/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"xpan@mpi-inf.mpg.de","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Sinusoidal Activation (SIREN)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Geometry Only, Generative Models, Generalization","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"GOF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2111.00969.pdf","Project webpage link":"https://sheldontsui.github.io/projects/GOF","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"11/23/2021 4:37","Title":"Generative Occupancy Fields for 3D Surface-Aware Image Synthesis","Training time (hr)":"","UID":"269","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2021","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"The advancement of generative radiance fields has pushed the boundary of 3D-aware image synthesis. Motivated by the observation that a 3D object should look realistic from multiple viewpoints, these methods introduce a multi-view constraint as regularization to learn valid 3D radiance fields from 2D images. Despite the progress, they often fall short of capturing accurate 3D shapes due to the shape-color ambiguity, limiting their applicability in downstream tasks. In this work, we address this ambiguity by proposing a novel shading-guided generative implicit model that is able to learn a starkly improved shape representation. Our key insight is that an accurate 3D shape should also yield a realistic rendering under different lighting conditions. This multi-lighting constraint is realized by modeling illumination explicitly and performing shading with various lighting conditions. Gradients are derived by feeding the synthesized images to a discriminator. To compensate for the additional computational burden of calculating surface normals, we further devise an efficient volume rendering strategy via surface tracking, reducing the training and inference time by 24% and 48%, respectively. Our experiments on multiple datasets show that the proposed approach achieves photorealistic 3D-aware image synthesis while capturing accurate underlying 3D shapes. We demonstrate improved performance of our approach on 3D shape reconstruction against existing methods, and show its applicability on image relighting. Our code will be released at https://github.com/XingangPan/ShadeGAN.","Authors (format: First Last, First Middle Last, ...)":"Xingang Pan, Xudong Xu, Chen Change Loy, Christian Theobalt, Bo Dai","Bibtex (e.g. @inproceedings...)":"@inproceedings{pan2021shadegan,\n    title = {A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis},\n    author = {Xingang Pan and Xudong Xu and Chen Change Loy and Christian Theobalt and Bo Dai},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    year = {2021},\n    entrytype = {inproceedings},\n    id = {pan2021shadegan}\n}","Bibtex Name":"pan2021shadegan","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/XingangPan/ShadeGAN","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/29/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"xpan@mpi-inf.mpg.de","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Sinusoidal Activation (SIREN)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Material/Lighting Estimation, Generative Models, Generalization","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"ShadeGAN","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2110.15678.pdf","Project webpage link":"https://xingangpan.github.io/projects/ShadeGAN.html","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"11/23/2021 4:33","Title":"A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis","Training time (hr)":"","UID":"268","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2021","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Implicit neural representations are a promising new avenue of representing general signals by learning a continuous function that, parameterized as a neural network, maps the domain of a signal to its codomain; the mapping from spatial coordinates of an image to its pixel values, for example. Being capable of conveying fine details in a high dimensional signal, unboundedly of its domain, implicit neural representations ensure many advantages over conventional discrete representations. However, the current approach is difficult to scale for a large number of signals or a data set, since learning a neural representation -- which is parameter heavy by itself -- for each signal individually requires a lot of memory and computations. To address this issue, we propose to leverage a meta-learning approach in combination with network compression under a sparsity constraint, such that it renders a well-initialized sparse parameterization that evolves quickly to represent a set of unseen signals in the subsequent training. We empirically demonstrate that meta-learned sparse neural representations achieve a much smaller loss than dense meta-learned models with the same number of parameters, when trained to fit each signal using the same number of optimization steps.","Authors (format: First Last, First Middle Last, ...)":"Jaeho Lee, Jihoon Tack, Namhoon Lee, Jinwoo Shin","Bibtex (e.g. @inproceedings...)":"@article{lee2021metasparseinr,\n    author = {Jaeho Lee and Jihoon Tack and Namhoon Lee and Jinwoo Shin},\n    title = {Meta-Learning Sparse Implicit Neural Representations},\n    year = {2021},\n    month = {Oct},\n    url = {http://arxiv.org/abs/2110.14678v2},\n    entrytype = {article},\n    id = {lee2021metasparseinr}\n}","Bibtex Name":"lee2021metasparseinr","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/jaeho-lee/MetaSparseINR","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/27/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"jaeho-lee@kaist.ac.kr","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Sinusoidal Activation (SIREN)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"None, N/A","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Speed & Computational Efficiency, 2D Image Neural Fields, Compression, Hypernetwork/Meta-learning","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"Meta-SparseINR","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2110.14678.pdf","Project webpage link":"https://github.com/jaeho-lee/MetaSparseINR","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"N/A","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"1/14/2022 2:35","Title":"Meta-leaning Sparse Implicit Neural Representations","Training time (hr)":"","UID":"267","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2021","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"The ability to grasp and manipulate transparent objects is a major challenge for robots. Existing depth cameras have difficulty detecting, localizing, and inferring the geometry of such objects. We propose using neural radiance fields (NeRF) to detect, localize, and infer the geometry of transparent objects with sufficient accuracy to find and grasp them securely. We leverage NeRF's view-independent learned density, place lights to increase specular reflections, and perform a transparency-aware depth-rendering that we feed into the Dex-Net grasp planner. We show how additional lights create specular reflections that improve the quality of the depth map, and test a setup for a robot workcell equipped with an array of cameras to perform transparent object manipulation. We also create synthetic and real datasets of transparent objects in real-world settings, including singulated objects, cluttered tables, and the top rack of a dishwasher. In each setting we show that NeRF and Dex-Net are able to reliably compute robust grasps on transparent objects, achieving 90% and 100% grasp success rates in physical experiments on an ABB YuMi, on objects where baseline methods fail.","Authors (format: First Last, First Middle Last, ...)":"Jeffrey Ichnowski, Yahav Avigal, Justin Kerr, Ken Goldberg","Bibtex (e.g. @inproceedings...)":"@article{ichnowski2021dexnerf,\n    author = {Jeffrey Ichnowski and Yahav Avigal and Justin Kerr and Ken Goldberg},\n    title = {Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects},\n    year = {2021},\n    month = {Oct},\n    url = {http://arxiv.org/abs/2110.14217v1},\n    entrytype = {article},\n    id = {ichnowski2021dexnerf}\n}","Bibtex Name":"ichnowski2021dexnerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/27/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"srinath@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Robotics","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"Dex-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2110.14217.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=F9R6Nf1d7P4","Timestamp":"12/21/2021 15:47","Title":"Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects","Training time (hr)":"","UID":"266","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CoRL 2021","Venue no Year":"CoRL","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Decomposing a scene into its shape, reflectance and illumination is a fundamental problem in computer vision and graphics. Neural approaches such as NeRF have achieved remarkable success in view synthesis, but do not explicitly perform decomposition and instead operate exclusively on radiance (the product of reflectance and illumination). Extensions to NeRF, such as NeRD, can perform decomposition but struggle to accurately recover detailed illumination, thereby significantly limiting realism. We propose a novel reflectance decomposition network that can estimate shape, BRDF, and per-image illumination given a set of object images captured under varying illumination. Our key technique is a novel illumination integration network called Neural-PIL that replaces a costly illumination integral operation in the rendering with a simple network query. In addition, we also learn deep low-dimensional priors on BRDF and illumination representations using novel smooth manifold auto-encoders. Our decompositions can result in considerably better BRDF and light estimates enabling more accurate novel view-synthesis and relighting compared to prior art. Project page: https://markboss.me/publication/2021-neural-pil/","Authors (format: First Last, First Middle Last, ...)":"Mark Boss, Varun Jampani, Raphael Braun, Ce Liu, Jonathan T. Barron, Hendrik P. A. Lensch","Bibtex (e.g. @inproceedings...)":"@article{boss2021neuralpil,\n    author = {Mark Boss and Varun Jampani and Raphael Braun and Ce Liu and Jonathan T. Barron and Hendrik P. A. Lensch},\n    title = {Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition},\n    year = {2021},\n    month = {Oct},\n    url = {http://arxiv.org/abs/2110.14373v1},\n    entrytype = {article},\n    id = {boss2021neuralpil}\n}","Bibtex Name":"boss2021neuralpil","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/27/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"yiheng_xie@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Material/Lighting Estimation, Generative Models, Data-Driven Method, Coarse-to-Fine","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"Neural-PIL","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2110.14373.pdf","Project webpage link":"https://markboss.me/publication/2021-neural-pil/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=p5cKaNwVp4M","Timestamp":"11/21/2021 16:12","Title":"Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition","Training time (hr)":"","UID":"265","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2021","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present neural radiance fields for rendering and temporal (4D) reconstruction of humans in motion (H-NeRF), as captured by a sparse set of cameras or even from a monocular video. Our approach combines ideas from neural scene representation, novel-view synthesis, and implicit statistical geometric human representations, coupled using novel loss functions. Instead of learning a radiance field with a uniform occupancy prior, we constrain it by a structured implicit human body model, represented using signed distance functions. This allows us to robustly fuse information from sparse views and generalize well beyond the poses or views observed in training. Moreover, we apply geometric constraints to co-learn the structure of the observed subject -- including both body and clothing -- and to regularize the radiance field to geometrically plausible solutions. Extensive experiments on multiple datasets demonstrate the robustness and the accuracy of our approach, its generalization capabilities significantly outside a small training set of poses and views, and statistical extrapolation beyond the observed shape.","Authors (format: First Last, First Middle Last, ...)":"Hongyi Xu, Thiemo Alldieck, Cristian Sminchisescu","Bibtex (e.g. @inproceedings...)":"@article{xu2021hnerf,\n    url = {http://arxiv.org/abs/2110.13746v2},\n    month = {Oct},\n    year = {2021},\n    title = {H-NeRF: Neural Radiance Fields for Rendering and Temporal Reconstruction of Humans in Motion},\n    author = {Hongyi Xu and Thiemo Alldieck and Cristian Sminchisescu},\n    entrytype = {article},\n    id = {xu2021hnerf}\n}","Bibtex Name":"xu2021hnerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/26/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"alldieck@google.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density, SDF","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Dynamic/Temporal, Human (Body), Editable","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"H-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2110.13746.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"11/25/2021 3:55","Title":"H-NeRF: Neural Radiance Fields for Rendering and Temporal Reconstruction of Humans in Motion","Training time (hr)":"","UID":"264","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2021","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"This paper introduces Attentive Implicit Representation Networks (AIR-Nets), a simple, but highly effective architecture for 3D reconstruction from point clouds. Since representing 3D shapes in a local and modular fashion increases generalization and reconstruction quality, AIR-Nets encode an input point cloud into a set of local latent vectors anchored in 3D space, which locally describe the object's geometry, as well as a global latent description, enforcing global consistency. Our model is the first grid-free, encoder-based approach that locally describes an implicit function. The vector attention mechanism from [Zhao et al. 2020] serves as main point cloud processing module, and allows for permutation invariance and translation equivariance. When queried with a 3D coordinate, our decoder gathers information from the global and nearby local latent vectors in order to predict an occupancy value. Experiments on the ShapeNet dataset show that AIR-Nets significantly outperform previous state-of-the-art encoder-based, implicit shape learning methods and especially dominate in the sparse setting. Furthermore, our model generalizes well to the FAUST dataset in a zero-shot setting. Finally, since AIR-Nets use a sparse latent representation and follow a simple operating scheme, the model offers several exiting avenues for future work. Our code is available at https://github.com/SimonGiebenhain/AIR-Nets.","Authors (format: First Last, First Middle Last, ...)":"Simon Giebenhain, Bastian Goldl\u00fccke","Bibtex (e.g. @inproceedings...)":"@inproceedings{giebenhain2021airnets,\n    organization = {IEEE},\n    year = {2021},\n    booktitle = {2021 International Conference on 3D Vision (3DV)},\n    author = {Simon Giebenhain and Bastian Goldluecke},\n    title = {AIR-Nets: An Attention-Based Framework for Locally Conditioned Implicit Representations},\n    entrytype = {inproceedings},\n    id = {giebenhain2021airnets}\n}","Bibtex Name":"giebenhain2021airnets","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/SimonGiebenhain/AIR-Nets","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/22/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"simon.giebenhain@uni-konstanz.de","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"MLP on relative coordinates","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Geometry Only, Local Conditioning, Hybrid Geometry Representation, Purely Point-Based, Positional Encoding","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"AIR-Nets","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2110.11860.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"https://github.com/SimonGiebenhain/AIR-Nets/blob/main/AIR-Nets_supp.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"11/26/2021 16:38","Title":"AIR-Nets: An Attention-Based Framework for Locally Conditioned Implicit Representations","Training time (hr)":"","UID":"263","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"3DV 2021","Venue no Year":"3DV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose StyleNeRF, a 3D-aware generative model for photo-realistic highresolution image synthesis with high multi-view consistency, which can be trained on unstructured 2D images. Existing approaches either cannot synthesize highresolution images with fine details or yield clearly noticeable 3D-inconsistent artifacts. In addition, many of them lack control on style attributes and explicit 3D camera poses. To address these issues, StyleNeRF integrates the neural radiance field (NeRF) into a style-based generator to tackle the aforementioned challenges, i.e., improving rendering efficiency and 3D consistency for high-resolution image generation. To address the first issue, we perform volume rendering only to produce a low-resolution feature map, and progressively apply upsampling in 2D. To mitigate the inconsistencies caused by 2D upsampling, we propose multiple designs including a better upsampler choice and a new regularization loss to enforce 3D consistency. With these designs, StyleNeRF is able to synthesize high-resolution images at interactive rates while preserving 3D consistency at high quality. StyleNeRF also enables control of camera poses and different levels of styles, which can generalize to unseen views. It also supports challenging tasks such as style mixing, inversion and simple semantic edits.","Authors (format: First Last, First Middle Last, ...)":"Anonymous","Bibtex (e.g. @inproceedings...)":"@inproceedings{anonymous2022stylenerf,\n    title = {StyleNe{RF}: A Style-based 3D Aware Generator for High-resolution Image Synthesis},\n    author = {Anonymous},\n    booktitle = {Submitted to The Tenth International Conference on Learning Representations },\n    year = {2022},\n    url = {https://openreview.net/forum?id=iUuzzTMUw9K},\n    entrytype = {inproceedings},\n    id = {anonymous2022stylenerf}\n}","Bibtex Name":"anonymous2022stylenerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/5/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"StyleNeRF","PDF link (arXiv perferred)":"https://openreview.net/pdf?id=iUuzzTMUw9K","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/8/2021 16:50","Title":"StyleNeRF: A Style-based 3D Aware Generator for High-resolution Image Synthesis","Training time (hr)":"","UID":"262","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"OpenReview 2021","Venue no Year":"OpenReview","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural Radiance Fields (NeRFs) have recently emerged as a powerful paradigm for the representation of natural, complex 3D scenes. NeRFs represent continuous volumetric density and RGB values in a neural network, and generate photo-realistic images from unseen camera viewpoints through ray tracing. We propose an algorithm for navigating a robot through a 3D environment represented as a NeRF using only an on-board RGB camera for localization. We assume the NeRF for the scene has been pre-trained offline, and the robot's objective is to navigate through unoccupied space in the NeRF to reach a goal pose. We introduce a trajectory optimization algorithm that avoids collisions with high-density regions in the NeRF based on a discrete time version of differential flatness that is amenable to constraining the robot's full pose and control inputs. We also introduce an optimization based filtering method to estimate 6DoF pose and velocities for the robot in the NeRF given only an onboard RGB camera. We combine the trajectory planner with the pose filter in an online replanning loop to give a vision-based robot navigation pipeline. We present simulation results with a quadrotor robot navigating through a jungle gym environment, the inside of a church, and Stonehenge using only an RGB camera. We also demonstrate an omnidirectional ground robot navigating through the church, requiring it to reorient to fit through the narrow gap. Videos of this work can be found at https://mikh3x4.github.io/nerf-navigation/ .","Authors (format: First Last, First Middle Last, ...)":"Michal Adamkiewicz, Timothy Chen, Adam Caccavale, Rachel Gardner, Preston Culbertson, Jeannette Bohg, Mac Schwager","Bibtex (e.g. @inproceedings...)":"@article{adamkiewicz2022visiononly,\n    url = {http://arxiv.org/abs/2110.00168v1},\n    month = {Oct},\n    year = {2021},\n    title = {Vision-Only Robot Navigation in a Neural Radiance World},\n    author = {Michal Adamkiewicz and Timothy Chen and Adam Caccavale and Rachel Gardner and Preston Culbertson and Jeannette Bohg and Mac Schwager},\n    entrytype = {article},\n    id = {adamkiewicz2022visiononly}\n}","Bibtex Name":"adamkiewicz2022visiononly","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/mikh3x4/nerf-navigation","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/1/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"pculbertson@stanford.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"None, N/A","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Robotics, Camera Parameter Estimation, Data-Driven Method","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2110.00168.pdf","Project webpage link":"https://mikh3x4.github.io/nerf-navigation","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"N/A","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://youtu.be/5JjWpv9BaaE","Timestamp":"12/7/2021 16:59","Title":"Vision-Only Robot Navigation in a Neural Radiance World","Training time (hr)":"","UID":"261","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ArXiV (RA-L submission) 2022","Venue no Year":"ArXiV (RA-L submission)","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural networks can represent and accurately reconstruct radiance fields for static 3D scenes (e.g., NeRF). Several works extend these to dynamic scenes captured with monocular video, with promising performance. However, the monocular setting is known to be an under-constrained problem, and so methods rely on data-driven priors for reconstructing dynamic content. We replace these priors with measurements from a time-of-flight (ToF) camera, and introduce a neural representation based on an image formation model for continuous-wave ToF cameras. Instead of working with processed depth maps, we model the raw ToF sensor measurements to improve reconstruction quality and avoid issues with low reflectance regions, multi-path interference, and a sensor's limited unambiguous depth range. We show that this approach improves robustness of dynamic scene reconstruction to erroneous calibration and large motions, and discuss the benefits and limitations of integrating RGB+ToF sensors that are now available on modern smartphones.","Authors (format: First Last, First Middle Last, ...)":"Benjamin Attal, Eliot Laidlaw, Aaron Gokaslan, Changil Kim, Christian Richardt, James Tompkin, Matthew O'Toole","Bibtex (e.g. @inproceedings...)":"@inproceedings{attal2021torf,\n    publisher = {Curran Associates, Inc.},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    author = {Benjamin Attal and Eliot Laidlaw and Aaron Gokaslan and Changil Kim and Christian Richardt and James Tompkin and Matthew O'Toole},\n    title = {ToRF: Time-of-Flight Radiance Fields for Dynamic Scene View Synthesis},\n    year = {2021},\n    url = {http://arxiv.org/abs/2109.15271v1},\n    entrytype = {inproceedings},\n    id = {attal2021torf}\n}","Bibtex Name":"attal2021torf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"No","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/30/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"No","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Science & Engineering, Global Conditioning, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"T\u00f6RF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2109.15271.pdf","Project webpage link":"https://imaging.cs.cmu.edu/torf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/1/2021 13:30","Title":"T\u00f6RF: Time-of-Flight Radiance Fields for Dynamic Scene View Synthesis","Training time (hr)":"","UID":"260","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2021","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Coordinate-based Multilayer Perceptron (MLP) networks, despite being capable of learning neural implicit representations, are not performant for internal image synthesis applications. Convolutional Neural Networks (CNNs) are typically used instead for a variety of internal generative tasks, at the cost of a larger model. We propose Neural Knitwork, an architecture for neural implicit representation learning of natural images that achieves image synthesis by optimizing the distribution of image patches in an adversarial manner and by enforcing consistency between the patch predictions. To the best of our knowledge, this is the first implementation of a coordinate-based MLP tailored for synthesis tasks such as image inpainting, super-resolution, and denoising. We demonstrate the utility of the proposed technique by training on these three tasks. The results show that modeling natural images using patches, rather than pixels, produces results of higher fidelity. The resulting model requires 80% fewer parameters than alternative CNN-based solutions while achieving comparable performance and training time.","Authors (format: First Last, First Middle Last, ...)":"Mikolaj Czerkawski, Javier Cardona, Robert Atkinson, Craig Michie, Ivan Andonovic, Carmine Clemente, Christos Tachtatzis","Bibtex (e.g. @inproceedings...)":"@article{czerkawski2021neuralknitworks,\n    url = {http://arxiv.org/abs/2109.14406v1},\n    year = {2021},\n    title = {Neural Knitworks: Patched Neural Implicit Representation Networks},\n    author = {Mikolaj Czerkawski and Javier Cardona and Robert Atkinson and Craig Michie and Ivan Andonovic and Carmine Clemente and Christos Tachtatzis},\n    booktitle = {ArXiv Pre-print},\n    journal = {arXiv preprint arXiv:2109.14406},\n    entrytype = {article},\n    id = {czerkawski2021neuralknitworks}\n}","Bibtex Name":"czerkawski2021neuralknitworks","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"No","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/29/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Indirect","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"Yes","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"2D Image Neural Fields, Generative Models","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Neural Knitworks","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2109.14406.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/4/2021 22:11","Title":"Neural Knitworks: Patched Neural Implicit Representation Networks","Training time (hr)":"","UID":"259","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"This work proposes a model-reduction approach for the material point method on nonlinear manifolds. The technique approximates the $\\textit{kinematics}$ by approximating the deformation map in a manner that restricts deformation trajectories to reside on a low-dimensional manifold expressed from the extrinsic view via a parameterization function. By explicitly approximating the deformation map and its spatial-temporal gradients, the deformation gradient and the velocity can be computed simply by differentiating the associated parameterization function. Unlike classical model reduction techniques that build a subspace for a finite number of degrees of freedom, the proposed method approximates the entire deformation map with infinite degrees of freedom. Therefore, the technique supports resolution changes in the reduced simulation, attaining the challenging task of zero-shot super-resolution by generating material points unseen in the training data. The ability to generate material points also allows for adaptive quadrature rules for stress update. A family of projection methods is devised to generate $\\textit{dynamics}$, i.e., at every time step, the methods perform three steps: (1) generate quadratures in the full space from the reduced space, (2) compute position and velocity updates in the full space, and (3) perform a least-squares projection of the updated position and velocity onto the low-dimensional manifold and its tangent space. Computational speedup is achieved via hyper-reduction, i.e., only a subset of the original material points are needed for dynamics update. Large-scale numerical examples with millions of material points illustrate the method's ability to gain an order-of-magnitude computational-cost saving -- indeed $\\textit{real-time simulations}$ in some cases -- with negligible errors.","Authors (format: First Last, First Middle Last, ...)":"Peter Yichen Chen, Maurizio Chiaramonte, Eitan Grinspun, Kevin Carlberg","Bibtex (e.g. @inproceedings...)":"@article{chen2021neuraldefmap,\n    author = {Peter Yichen Chen and Maurizio Chiaramonte and Eitan Grinspun and Kevin Carlberg},\n    title = {Model reduction for the material point method via learning the deformation map and its spatial-temporal gradients},\n    year = {2021},\n    month = {Sep},\n    url = {http://arxiv.org/abs/2109.12390v1},\n    entrytype = {article},\n    id = {chen2021neuraldefmap}\n}","Bibtex Name":"chen2021neuraldefmap","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/peterchencyc/libNeuralDefMap","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/25/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"cyc@cs.columbia.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"deformation map","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Dynamic/Temporal, Science & Engineering, Data-Driven Method, Supervision by Gradient (PDE)","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"NeuralDefMap","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2109.12390.pdf","Project webpage link":"https://peterchencyc.com/projects/rom4mpm/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://peterchencyc.com/files/rom4mpm/supplemental_video.mp4","Timestamp":"12/2/2021 16:41","Title":"Model Reduction for the Material Point Method via Learning the Deformation map and its Spatial-temporal Gradients","Training time (hr)":"","UID":"258","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"The objective of this work is to achieve sensorless reconstruction of a 3D volume from a set of 2D freehand ultrasound images with deep implicit representation. In contrast to the conventional way that represents a 3D volume as a discrete voxel grid, we do so by parameterizing it as the zero level-set of a continuous function, i.e. implicitly representing the 3D volume as a mapping from the spatial coordinates to the corresponding intensity values. Our proposed model, termed as ImplicitVol, takes a set of 2D scans and their estimated locations in 3D as input, jointly re?fing the estimated 3D locations and learning a full reconstruction of the 3D volume. When testing on real 2D ultrasound images, novel cross-sectional views that are sampled from ImplicitVol show significantly better visual quality than those sampled from existing reconstruction approaches, outperforming them by over 30% (NCC and SSIM), between the output and ground-truth on the 3D volume testing data. The code will be made publicly available.","Authors (format: First Last, First Middle Last, ...)":"Pak-Hei Yeung, Linde Hesse, Moska Aliasi, Monique Haak, the INTERGROWTH-21st Consortium, Weidi Xie, Ana I. L. Namburete","Bibtex (e.g. @inproceedings...)":"@article{yeung2021implicitvol,\n    journal = {arXiv preprint arXiv:2109.12108},\n    booktitle = {ArXiv Pre-print},\n    author = {Pak-Hei Yeung and Linde Hesse and Moska Aliasi and Monique Haak and the INTERGROWTH-21st Consortium and Weidi Xie and Ana I. L. Namburete},\n    title = {ImplicitVol: Sensorless 3D Ultrasound Reconstruction with Deep Implicit Representation},\n    year = {2021},\n    url = {http://arxiv.org/abs/2109.12108v1},\n    entrytype = {article},\n    id = {yeung2021implicitvol}\n}","Bibtex Name":"yeung2021implicitvol","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/pakheiyeung/ImplicitVol","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/24/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"ImplicitVol","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2109.12108.pdf","Project webpage link":"https://pakheiyeung.github.io/ImplicitVol_wp/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=D4ZCo14mqxs","Timestamp":"10/2/2021 8:15","Title":"ImplicitVol: Sensorless 3D Ultrasound Reconstruction with Deep Implicit Representation","Training time (hr)":"","UID":"257","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present Hand ArticuLated Occupancy (HALO), a novel representation of articulated hands that bridges the advantages of 3D keypoints and neural implicit surfaces and can be used in end-to-end trainable architectures. Unlike existing statistical parametric hand models (e.g.~MANO), HALO directly leverages 3D joint skeleton as input and produces a neural occupancy volume representing the posed hand surface. The key benefits of HALO are (1) it is driven by 3D key points, which have benefits in terms of accuracy and are easier to learn for neural networks than the latent hand-model parameters; (2) it provides a differentiable volumetric occupancy representation of the posed hand; (3) it can be trained end-to-end, allowing the formulation of losses on the hand surface that benefit the learning of 3D keypoints. We demonstrate the applicability of HALO to the task of conditional generation of hands that grasp 3D objects. The differentiable nature of HALO is shown to improve the quality of the synthesized hands both in terms of physical plausibility and user preference.","Authors (format: First Last, First Middle Last, ...)":"Korrawe Karunratanakul, Adrian Spurr, Zicong Fan, Otmar Hilliges, Siyu Tang","Bibtex (e.g. @inproceedings...)":"@article{karunratanakul2021halo,\n    journal = {arXiv preprint arXiv:2109.11399},\n    booktitle = {ArXiv Pre-print},\n    author = {Korrawe Karunratanakul and Adrian Spurr and Zicong Fan and Otmar Hilliges and Siyu Tang},\n    title = {A Skeleton-Driven Neural Occupancy Representation for Articulated Hands},\n    year = {2021},\n    url = {http://arxiv.org/abs/2109.11399v1},\n    entrytype = {article},\n    id = {karunratanakul2021halo}\n}","Bibtex Name":"karunratanakul2021halo","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/23/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human hand","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"HALO","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2109.11399.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/27/2021 21:57","Title":"A Skeleton-Driven Neural Occupancy Representation for Articulated Hands","Training time (hr)":"","UID":"256","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a method that decomposes, or \"unwraps\", an input video into a set of layered 2D atlases, each providing a unified representation of the appearance of an object (or background) over the video. For each pixel in the video, our method estimates its corresponding 2D coordinate in each of the atlases, giving us a consistent parameterization of the video, along with an associated alpha (opacity) value. Importantly, we design our atlases to be interpretable and semantic, which facilitates easy and intuitive editing in the atlas domain, with minimal manual work required. Edits applied to a single 2D atlas (or input video frame) are automatically and consistently mapped back to the original video frames, while preserving occlusions, deformation, and other complex scene effects such as shadows and reflections. Our method employs a coordinate-based Multilayer Perceptron (MLP) representation for mappings, atlases, and alphas, which are jointly optimized on a per-video basis, using a combination of video reconstruction and regularization losses. By operating purely in 2D, our method does not require any prior 3D knowledge about scene geometry or camera poses, and can handle complex dynamic real world videos. We demonstrate various video editing applications, including texture mapping, video style transfer, image-to-video texture transfer, and segmentation/labeling propagation, all automatically produced by editing a single 2D atlas image.","Authors (format: First Last, First Middle Last, ...)":"Yoni Kasten, Dolev Ofri, Oliver Wang, Tali Dekel","Bibtex (e.g. @inproceedings...)":"@article{kasten2021layered,\n    publisher = {Association for Computing Machinery},\n    journal = {ACM Transactions on Graphics (TOG)},\n    author = {Yoni Kasten and Dolev Ofri and Oliver Wang and Tali Dekel},\n    title = {Layered Neural Atlases for Consistent Video Editing},\n    year = {2021},\n    url = {http://arxiv.org/abs/2109.11418v1},\n    entrytype = {article},\n    id = {kasten2021layered}\n}","Bibtex Name":"kasten2021layered","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/23/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, 2D Image Neural Fields, Editable","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2109.11418.pdf","Project webpage link":"https://layered-neural-atlases.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"https://layered-neural-atlases.github.io/supplementary/index.html","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=aQhakPFC4oQ","Timestamp":"9/27/2021 22:03","Title":"Layered Neural Atlases for Consistent Video Editing","Training time (hr)":"","UID":"255","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"SIGGRAPH 2021","Venue no Year":"SIGGRAPH","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"The task of 3D shape classification is closely related to finding a good representation of the shapes. In this study, we focus on surface representations of complex anatomies and on how such representations can be utilized for super-and unsupervised classification. We present a novel Implicit Neural Distance Representation based on unsigned distance fields (UDFs). The UDFs can be embedded into a low-dimensional latent space, which is optimized using only the shape itself. We demonstrate that this self-optimized latent space","Authors (format: First Last, First Middle Last, ...)":"Kristine Aavild Juhl, Xabier Morales, Oscar Camara, Ole de Backer and Rasmus Reinhold Paulsen","Bibtex (e.g. @inproceedings...)":"@inproceedings{juhl2021implicit,\n    author = {Kristine Aavild Juhl and Xabier Morales and Ole de Backer and Oscar Camara and Rasmus Reinhold Paulsen},\n    booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},\n    organization = {Springer},\n    pages = {405--415},\n    year = {2021},\n    title = {Implicit Neural Distance Representation for Unsupervised and Supervised Classification of Complex Anatomies},\n    entrytype = {inproceedings},\n    id = {juhl2021implicit}\n}","Bibtex Name":"juhl2021implicit","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/kristineaajuhl/Implicit-Neural-Distance-Representation-of-Complex-Anatomie","Coordinates all at once":"No","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/21/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"UDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Human (Head), Science & Engineering, Classification, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://link.springer.com/content/pdf/10.1007%2F978-3-030-87196-3.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/27/2021 21:52","Title":"Implicit Neural Distance Representation for Unsupervised and Supervised Classification of Complex Anatomies","Training time (hr)":"","UID":"254","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"MICCAI 2021","Venue no Year":"MICCAI","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Physics-informed neural networks (PINNs) have become a popular choice for solving high-dimensional partial differential equations (PDEs) due to their excellent approximation power and generalization ability. Recently, Extended PINNs (XPINNs) based on domain decomposition methods have attracted considerable attention due to their effectiveness in modeling multiscale and multiphysics problems and their parallelization. However, theoretical understanding on their convergence and generalization properties remains unexplored. In this study, we take an initial step towards understanding how and when XPINNs outperform PINNs. Specifically, for general multi-layer PINNs and XPINNs, we first provide a prior generalization bound via the complexity of the target functions in the PDE problem, and a posterior generalization bound via the posterior matrix norms of the networks after optimization. Moreover, based on our bounds, we analyze the conditions under which XPINNs improve generalization. Concretely, our theory shows that the key building block of XPINN, namely the domain decomposition, introduces a tradeoff for generalization. On the one hand, XPINNs decompose the complex PDE solution into several simple parts, which decreases the complexity needed to learn each part and boosts generalization. On the other hand, decomposition leads to less training data being available in each subdomain, and hence such model is typically prone to overfitting and may become less generalizable. Empirically, we choose five PDEs to show when XPINNs perform better than, similar to, or worse than PINNs, hence demonstrating and justifying our new theory.","Authors (format: First Last, First Middle Last, ...)":"Zheyuan Hu, Ameya D. Jagtap, George Em Karniadakis, Kenji Kawaguchi","Bibtex (e.g. @inproceedings...)":"@article{hu2021when,\n    url = {http://arxiv.org/abs/2109.09444v2},\n    year = {2021},\n    title = {When Do Extended Physics-Informed Neural Networks (XPINNs) Improve Generalization?},\n    author = {Zheyuan Hu and Ameya D. Jagtap and George Em Karniadakis and Kenji Kawaguchi},\n    booktitle = {ArXiv Pre-print},\n    journal = {arXiv preprint arXiv:2109.09444},\n    entrytype = {article},\n    id = {hu2021when}\n}","Bibtex Name":"hu2021when","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/20/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2109.09444.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/8/2021 16:39","Title":"When Do Extended Physics-Informed Neural Networks (XPINNs) Improve Generalization?","Training time (hr)":"","UID":"253","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Coherent rendering is important for generating plausible Mixed Reality presentations of virtual objects within a user's real-world environment. Besides photo-realistic rendering and correct lighting, visual coherence requires simulating the imaging system that is used to capture the real environment. While existing approaches either focus on a specific camera or a specific component of the imaging system, we introduce Neural Cameras, the first approach that jointly simulates all major components of an arbitrary modern camera using","Authors (format: First Last, First Middle Last, ...)":"David Mandl, Peter Mohr, Tobias Langlotz, Christoph Ebner, Shohei Mori, Stefanie Zollmann, Peter M Roth, Denis Kalkofen","Bibtex (e.g. @inproceedings...)":"@article{mandl2021neuralcameras,\n    author = {David Mandl and Peter Mohr and Tobias Langlotz and Christoph Ebner and Shohei Mori and Stefanie Zollmann and Peter M Roth and Denis Kalkofen},\n    year = {2021},\n    title = {Neural Cameras: Learning Camera Characteristics for Coherent Mixed Reality Rendering},\n    booktitle = {IEEE Symp. on Mixed and Augmented Reality (ISMAR)},\n    month = {Oct},\n    entrytype = {article},\n    id = {mandl2021neuralcameras}\n}","Bibtex Name":"mandl2021neuralcameras","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/18/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Camera Parameter Estimation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Neural Cameras","PDF link (arXiv perferred)":"https://www.hci.otago.ac.nz/papers/MandlIEEEISMAR2021.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/18/2021 8:47","Title":"Neural Cameras: Learning Camera Characteristics for Coherent Mixed Reality Rendering","Training time (hr)":"","UID":"252","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ISMAR 2021","Venue no Year":"ISMAR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Multisensory object-centric perception, reasoning, and interaction have been a key research topic in recent years. However, the progress in these directions is limited by the small set of objects available -- synthetic objects are not realistic enough and are mostly centered around geometry, while real object datasets such as YCB are often practically challenging and unstable to acquire due to international shipping, inventory, and financial cost. We present ObjectFolder, a dataset of 100 virtualized objects that addresses both challenges with two key innovations. First, ObjectFolder encodes the visual, auditory, and tactile sensory data for all objects, enabling a number of multisensory object recognition tasks, beyond existing datasets that focus purely on object geometry. Second, ObjectFolder employs a uniform, object-centric, and implicit representation for each object's visual textures, acoustic simulations, and tactile readings, making the dataset flexible to use and easy to share. We demonstrate the usefulness of our dataset as a testbed for multisensory perception and control by evaluating it on a variety of benchmark tasks, including instance recognition, cross-sensory retrieval, 3D reconstruction, and robotic grasping.","Authors (format: First Last, First Middle Last, ...)":"Ruohan Gao, Yen-Yu Chang, Shivani Mall, Li Fei-Fei, Jiajun Wu","Bibtex (e.g. @inproceedings...)":"@inproceedings{gao2021objectfolder,\n    booktitle = {Proceedings of the Conference on Robot Learning (CoRL)},\n    author = {Ruohan Gao and Yen-Yu Chang and Shivani Mall and Li Fei-Fei and Jiajun Wu},\n    title = {ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations},\n    year = {2021},\n    url = {http://arxiv.org/abs/2109.07991v2},\n    entrytype = {inproceedings},\n    id = {gao2021objectfolder}\n}","Bibtex Name":"gao2021objectfolder","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"No","Data Release (link)":"Coming soon","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/16/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Compression, Science & Engineering, Robotics, Audio","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"ObjectFolder","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2109.07991.pdf","Project webpage link":"https://ai.stanford.edu/~rhgao/objectfolder/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"https://ai.stanford.edu/~rhgao/objectfolder/ObjectFolder_Supp.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=wQ4o8XeS-X0","Timestamp":"9/26/2021 16:32","Title":"ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations","Training time (hr)":"","UID":"251","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CoRL 2021","Venue no Year":"CoRL","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In this paper, we aim at synthesizing a free-viewpoint video of an arbitrary human performance using sparse multi-view cameras. Recently, several works have addressed this problem by learning person-specific neural radiance fields (NeRF) to capture the appearance of a particular human. In parallel, some work proposed to use pixel-aligned features to generalize radiance fields to arbitrary new scenes and objects. Adopting such generalization approaches to humans, however, is highly challenging due to the heavy occlusions and dynamic articulations of body parts. To tackle this, we propose Neural Human Performer, a novel approach that learns generalizable neural radiance fields based on a parametric human body model for robust performance capture. Specifically, we first introduce a temporal transformer that aggregates tracked visual features based on the skeletal body motion over time. Moreover, a multi-view transformer is proposed to perform cross-attention between the temporally-fused features and the pixel-aligned features at each time step to integrate observations on the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets show that our method significantly outperforms recent generalizable NeRF methods on unseen identities and poses. The video results and code are available at https://youngjoongunc.github.io/nhp.","Authors (format: First Last, First Middle Last, ...)":"Youngjoong Kwon, Dahun Kim, Duygu Ceylan, Henry Fuchs","Bibtex (e.g. @inproceedings...)":"@inproceedings{kwon2021neuralhumanperformer,\n    url = {http://arxiv.org/abs/2109.07448v1},\n    year = {2021},\n    title = {Neural Human Performer: Learning Generalizable Radiance Fields for Human Performance Rendering},\n    author = {Youngjoong Kwon and Dahun Kim and Duygu Ceylan and Henry Fuchs},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    publisher = {Curran Associates, Inc.},\n    entrytype = {inproceedings},\n    id = {kwon2021neuralhumanperformer}\n}","Bibtex Name":"kwon2021neuralhumanperformer","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/YoungJoongUNC/Neural_Human_Performer","Coordinates all at once":"","Data Release (link)":"https://github.com/YoungJoongUNC/Neural_Human_Performer","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/15/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"Yes","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Human (Body), Generalization, Transformer, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Neural Human Performer","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2109.07448.pdf","Project webpage link":"https://youngjoongunc.github.io/nhp/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=4b5SPwPOKVo","Timestamp":"9/26/2021 16:35","Title":"Neural Human Performer: Learning Generalizable Radiance Fields for Human Performance Rendering","Training time (hr)":"","UID":"250","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2021","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce Multiresolution Deep Implicit Functions (MDIF), a hierarchical representation that can recover fine geometry detail, while being able to perform global operations such as shape completion. Our model represents a complex 3D shape with a hierarchy of latent grids, which can be decoded into different levels of detail and also achieve better accuracy. For shape completion, we propose latent grid dropout to simulate partial data in the latent space and therefore defer the completing functionality to the decoder side. This along with our multires design significantly improves the shape completion quality under decoder-only latent optimization. To the best of our knowledge, MDIF is the first deep implicit function model that can at the same time (1) represent different levels of detail and allow progressive decoding; (2) support both encoder-decoder inference and decoder-only latent optimization, and fulfill multiple applications; (3) perform detailed decoder-only shape completion. Experiments demonstrate its superior performance against prior art in various 3D reconstruction tasks.","Authors (format: First Last, First Middle Last, ...)":"Zhang Chen, Yinda Zhang, Kyle Genova, Sean Fanello, Sofien Bouaziz, Christian Haene, Ruofei Du, Cem Keskin, Thomas Funkhouser, Danhang Tang","Bibtex (e.g. @inproceedings...)":"@inproceedings{chen2021mdif,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Zhang Chen and Yinda Zhang and Kyle Genova and Sean Fanello and Sofien Bouaziz and Christian Haene and Ruofei Du and Cem Keskin and Thomas Funkhouser and Danhang Tang},\n    title = {Multiresolution Deep Implicit Functions for 3D Shape Representation},\n    year = {2021},\n    url = {http://arxiv.org/abs/2109.05591v2},\n    entrytype = {inproceedings},\n    id = {chen2021mdif}\n}","Bibtex Name":"chen2021mdif","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/12/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"Yes","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Geometry Only, Local Conditioning, Coarse-to-Fine, Voxel Grid","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"MDIF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2109.05591.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/18/2021 8:37","Title":"Multiresolution Deep Implicit Functions for 3D Shape Representation","Training time (hr)":"","UID":"249","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Efficient reasoning about the semantic, spatial, and temporal structure of a scene is a crucial prerequisite for autonomous driving. We present NEural ATtention fields (NEAT), a novel representation that enables such reasoning for end-to-end imitation learning models. NEAT is a continuous function which maps locations in Bird's Eye View (BEV) scene coordinates to waypoints and semantics, using intermediate attention maps to iteratively compress high-dimensional 2D image features into a compact representation. This allows our model to selectively attend to relevant regions in the input while ignoring information irrelevant to the driving task, effectively associating the images with the BEV representation. In a new evaluation setting involving adverse environmental conditions and challenging scenarios, NEAT outperforms several strong baselines and achieves driving scores on par with the privileged CARLA expert used to generate its training data. Furthermore, visualizing the attention maps for models with NEAT intermediate representations provides improved interpretability.","Authors (format: First Last, First Middle Last, ...)":"Kashyap Chitta, Aditya Prakash, Andreas Geiger","Bibtex (e.g. @inproceedings...)":"@inproceedings{chitta2021neat,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Kashyap Chitta and Aditya Prakash and Andreas Geiger},\n    title = {NEAT: Neural Attention Fields for End-to-End Autonomous Driving},\n    year = {2021},\n    url = {http://arxiv.org/abs/2109.04456v1},\n    entrytype = {inproceedings},\n    id = {chitta2021neat}\n}","Bibtex Name":"chitta2021neat","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/autonomousvision/neat","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/9/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Science & Engineering, Data-Driven Method","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NEAT","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2109.04456.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"http://www.cvlibs.net/publications/Chitta2021ICCV_supplementary.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=gtO-ghjKkRs","Timestamp":"9/17/2021 18:10","Title":"NEAT: Neural Attention Fields for End-to-End Autonomous Driving","Training time (hr)":"","UID":"248","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural Radiance Fields (NeRF) has become a popular framework for learning implicit 3D representations and addressing different tasks such as novel-view synthesis or depth-map estimation. However, in downstream applications where decisions need to be made based on automatic predictions, it is critical to leverage the confidence associated with the model estimations. Whereas uncertainty quantification is a long-standing problem in Machine Learning, it has been largely overlooked in the recent NeRF literature. In this context, we propose Stochastic Neural Radiance Fields (S-NeRF), a generalization of standard NeRF that learns a probability distribution over all the possible radiance fields modeling the scene. This distribution allows to quantify the uncertainty associated with the scene information provided by the model. S-NeRF optimization is posed as a Bayesian learning problem which is efficiently addressed using the Variational Inference framework. Exhaustive experiments over benchmark datasets demonstrate that S-NeRF is able to provide more reliable predictions and confidence values than generic approaches previously proposed for uncertainty estimation in other domains.","Authors (format: First Last, First Middle Last, ...)":"Jianxiong Shen, Adria Ruiz, Antonio Agudo, Francesc Moreno","Bibtex (e.g. @inproceedings...)":"@article{shen2021snerf,\n    journal = {arXiv preprint arXiv:2109.02123},\n    booktitle = {ArXiv Pre-print},\n    author = {Jianxiong Shen and Adria Ruiz and Antonio Agudo and Francesc Moreno-Noguer},\n    title = {Stochastic Neural Radiance Fields: Quantifying Uncertainty in Implicit 3D Representations},\n    year = {2021},\n    url = {http://arxiv.org/abs/2109.02123v3},\n    entrytype = {article},\n    id = {shen2021snerf}\n}","Bibtex Name":"shen2021snerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/5/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"S-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2109.02123.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/17/2021 17:49","Title":"Stochastic Neural Radiance Fields: Quantifying Uncertainty in Implicit 3D Representations","Training time (hr)":"","UID":"247","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Implicit neural rendering techniques have shown promising results for novel view synthesis. However, existing methods usually encode the entire scene as a whole, which is generally not aware of the object identity and limits the ability to the high-level editing tasks such as moving or adding furniture. In this paper, we present a novel neural scene rendering system, which learns an object-compositional neural radiance field and produces realistic rendering with editing capability for a clustered and real-world scene. Specifically, we design a novel two-pathway architecture, in which the scene branch encodes the scene geometry and appearance, and the object branch encodes each standalone object conditioned on learnable object activation codes. To survive the training in heavily cluttered scenes, we propose a scene-guided training strategy to solve the 3D space ambiguity in the occluded regions and learn sharp boundaries for each object. Extensive experiments demonstrate that our system not only achieves competitive performance for static scene novel-view synthesis, but also produces realistic rendering for object-level editing.","Authors (format: First Last, First Middle Last, ...)":"Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, Zhaopeng Cui","Bibtex (e.g. @inproceedings...)":"@inproceedings{yang2021learning,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Bangbang Yang and Yinda Zhang and Yinghao Xu and Yijin Li and Han Zhou and Hujun Bao and Guofeng Zhang and Zhaopeng Cui},\n    title = {Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering},\n    year = {2021},\n    url = {http://arxiv.org/abs/2109.01847v1},\n    entrytype = {inproceedings},\n    id = {yang2021learning}\n}","Bibtex Name":"yang2021learning","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/zju3dv/object_nerf","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/4/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Editable, Local Conditioning, Voxel Grid, Object-Centric, Hybrid Geometry Representation, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2109.01847.pdf","Project webpage link":"https://zju3dv.github.io/object_nerf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"http://www.cad.zju.edu.cn/home/gfzhang/papers/object_nerf/object_nerf_supp.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=VTEROu-Yz04","Timestamp":"9/17/2021 21:31","Title":"Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering","Training time (hr)":"","UID":"246","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"CodeNeRF is an implicit 3D neural representation that learns the variation of object shapes and textures across a category and can be trained, from a set of posed images, to synthesize novel views of unseen objects. Unlike the original NeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture by learning separate embeddings. At test time, given a single unposed image of an unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and appearance codes via optimization. Unseen objects can be reconstructed from a single image, and then rendered from new viewpoints or their shape and texture edited by varying the latent codes. We conduct experiments on the SRN benchmark, which show that CodeNeRF generalises well to unseen objects and achieves on-par performance with methods that require known camera pose at test time. Our results on real-world images demonstrate that CodeNeRF can bridge the sim-to-real gap. Project page: \\url{https://github.com/wayne1123/code-nerf}","Authors (format: First Last, First Middle Last, ...)":"Wonbong Jang, Lourdes Agapito","Bibtex (e.g. @inproceedings...)":"@inproceedings{jang2021codenerf,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Wonbong Jang and Lourdes Agapito},\n    title = {CodeNeRF: Disentangled Neural Radiance Fields for Object Categories},\n    year = {2021},\n    url = {http://arxiv.org/abs/2109.01750v1},\n    entrytype = {inproceedings},\n    id = {jang2021codenerf}\n}","Bibtex Name":"jang2021codenerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/wayne1123/code-nerf","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/3/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Editable, Data-Driven Method, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"CodeNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2109.01750.pdf","Project webpage link":"https://sites.google.com/view/wbjang/home/codenerf","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://user-images.githubusercontent.com/32883157/130004248-0ff74d4e-993e-43f2-91ee-bd25776e65bc.mp4","Timestamp":"9/17/2021 22:57","Title":"CodeNeRF: Disentangled Neural Radiance Fields for Object Categories","Training time (hr)":"","UID":"245","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In this work, we present a new multi-view depth estimation method that utilizes both conventional SfM reconstruction and learning-based priors over the recently proposed neural radiance fields (NeRF). Unlike existing neural network based optimization method that relies on estimated correspondences, our method directly optimizes over implicit volumes, eliminating the challenging step of matching pixels in indoor scenes. The key to our approach is to utilize the learning-based priors to guide the optimization process of NeRF. Our system firstly adapts a monocular depth network over the target scene by finetuning on its sparse SfM reconstruction. Then, we show that the shape-radiance ambiguity of NeRF still exists in indoor environments and propose to address the issue by employing the adapted depth priors to monitor the sampling process of volume rendering. Finally, a per-pixel confidence map acquired by error computation on the rendered image can be used to further improve the depth quality. Experiments show that our proposed framework significantly outperforms state-of-the-art methods on indoor scenes, with surprising findings presented on the effectiveness of correspondence-based optimization and NeRF-based optimization over the adapted depth priors. In addition, we show that the guided optimization scheme does not sacrifice the original synthesis capability of neural radiance fields, improving the rendering quality on both seen and novel views. Code is available at https://github.com/weiyithu/NerfingMVS.","Authors (format: First Last, First Middle Last, ...)":"Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, Jie Zhou","Bibtex (e.g. @inproceedings...)":"@article{wei2021nerfingmvs,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Yi Wei and Shaohui Liu and Yongming Rao and Wang Zhao and Jiwen Lu and Jie Zhou},\n    title = {NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo},\n    year = {2021},\n    publisher = {IEEE},\n    month = {Sep},\n    url = {http://arxiv.org/abs/2109.01129v2},\n    entrytype = {article},\n    id = {wei2021nerfingmvs}\n}","Bibtex Name":"wei2021nerfingmvs","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/weiyithu/NerfingMVS","Coordinates all at once":"","Data Release (link)":"https://drive.google.com/drive/folders/1X_w57Q_MIFlI3lzhRt7Z8C5X9tNS8cg-","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/2/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Sampling, Data-Driven Method, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NerfingMVS","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2109.01129.pdf","Project webpage link":"https://weiyithu.github.io/NerfingMVS/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=i-b5lPnYipA","Timestamp":"9/18/2021 8:55","Title":"NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo","Training time (hr)":"","UID":"244","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose IntraTomo, a powerful framework that combines the benefits of learning-based and model-based approaches for solving highly ill-posed inverse problems, in the Computed Tomography (CT) context. IntraTomo is composed of two core modules: a novel sinogram prediction module and a geometry refinement module, which are applied iteratively. In the first module, the unknown density field is represented as a continuous and differentiable function, parameterized by a deep neural network. This network is learned, in","Authors (format: First Last, First Middle Last, ...)":"Guangming Zang, Ramzi Idoughi, Rui Li, Peter Wonka, Wolfgang Heidrich","Bibtex (e.g. @inproceedings...)":"@inproceedings{zang2021intratomo,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    year = {2021},\n    author = {Guangming Zang and Ramzi Idoughi and Rui Li and Peter Wonka and Wolfgang Heidrich},\n    publisher = {IEEE},\n    title = {IntraTomo: Self-supervised Learning-based Tomography via Sinogram Synthesis and Prediction},\n    entrytype = {inproceedings},\n    id = {zang2021intratomo}\n}","Bibtex Name":"zang2021intratomo","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/gmzang/IntraTomo","Coordinates all at once":"","Data Release (link)":"https://github.com/gmzang/IntraTomo","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/2/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"IntraTomo","PDF link (arXiv perferred)":"https://vccimaging.org/Publications/Zang2021IntraTomo/Zang2021IntraTomo.pdf","Project webpage link":"https://vccimaging.org/Publications/Zang2021IntraTomo/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"https://vccimaging.org/Publications/Zang2021IntraTomo/Zang2021IntraTomo-supp.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/18/2021 10:03","Title":"IntraTomo: Self-supervised Learning-based Tomography via Sinogram Synthesis and Prediction","Training time (hr)":"","UID":"243","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recently developed physics-informed neural network (PINN) for solving for the scattered wavefield in the Helmholtz equation showed large potential in seismic modeling because of its flexibility, low memory requirement, and no limitations on the shape of the solution space. However, the predicted solutions were somewhat smooth and the convergence of the training was slow. Thus, we propose a modified PINN using sinusoidal activation functions and positional encoding, aiming to accelerate the convergence and fit better. We transform","Authors (format: First Last, First Middle Last, ...)":"Xinquan Huang, Tariq Alkhalifah, Chao Song","Bibtex (e.g. @inproceedings...)":"@inbook{huang2021a,\n    author = {Xinquan Huang and Tariq Alkhalifah and Chao Song},\n    booktitle = {First International Meeting for Applied Geoscience \\& Energy},\n    organization = {Society of Exploration Geophysicists},\n    pages = {2480--2484},\n    title = {A modified physics-informed neural network with positional encoding},\n    year = {2021},\n    doi = {10.1190/segam2021-3584127.1},\n    url = {https://library.seg.org/doi/abs/10.1190/segam2021-3584127.1},\n    entrytype = {inbook},\n    id = {huang2021a}\n}","Bibtex Name":"huang2021a","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/1/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals, Science & Engineering, Supervision by Gradient (PDE)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://library.seg.org/doi/pdf/10.1190/segam2021-3584127.1","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/18/2021 9:42","Title":"A modified physics-informed neural network with positional encoding","Training time (hr)":"","UID":"242","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"IMAGE 2021","Venue no Year":"IMAGE","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Implicit Neural Representations (INR) use multilayer perceptrons to represent high-frequency functions in low-dimensional problem domains. Recently these representations achieved state-of-the-art results on tasks related to complex 3D objects and scenes. A core problem is the representation of highly detailed signals, which is tackled using networks with periodic activation functions (SIRENs) or applying Fourier mappings to the input. This work analyzes the connection between the two methods and shows that a Fourier mapped perceptron is structurally like one hidden layer SIREN. Furthermore, we identify the relationship between the previously proposed Fourier mapping and the general d-dimensional Fourier series, leading to an integer lattice mapping. Moreover, we modify a progressive training strategy to work on arbitrary Fourier mappings and show that it improves the generalization of the interpolation task. Lastly, we compare the different mappings on the image regression and novel view synthesis tasks. We confirm the previous finding that the main contributor to the mapping performance is the size of the embedding and standard deviation of its elements.","Authors (format: First Last, First Middle Last, ...)":"Nuri Benbarka, Timon H\u00f6fer, Hamd ul-moqeet Riaz, Andreas Zell","Bibtex (e.g. @inproceedings...)":"@article{benbarka2021seeing,\n    journal = {arXiv preprint arXiv:2109.00249},\n    booktitle = {ArXiv Pre-print},\n    author = {Nuri Benbarka and Timon Hofer and Hamd ul-moqeet Riaz and Andreas Zell},\n    title = {Seeing Implicit Neural Representations as Fourier Series},\n    year = {2021},\n    url = {http://arxiv.org/abs/2109.00249v1},\n    entrytype = {article},\n    id = {benbarka2021seeing}\n}","Bibtex Name":"benbarka2021seeing","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/1/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Other","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"2D Image Neural Fields, Fundamentals, Coarse-to-Fine, Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2109.00249.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/18/2021 10:00","Title":"Seeing Implicit Neural Representations as Fourier Series","Training time (hr)":"","UID":"241","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Traditional approaches for learning 3D object categories have been predominantly trained and evaluated on synthetic datasets due to the unavailability of real 3D-annotated category-centric data. Our main goal is to facilitate advances in this field by collecting real-world data in a magnitude similar to the existing synthetic counterparts. The principal contribution of this work is thus a large-scale dataset, called Common Objects in 3D, with real multi-view images of object categories annotated with camera poses and ground truth 3D point clouds. The dataset contains a total of 1.5 million frames from nearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such, it is significantly larger than alternatives both in terms of the number of categories and objects. We exploit this new dataset to conduct one of the first large-scale \"in-the-wild\" evaluations of several new-view-synthesis and category-centric 3D reconstruction methods. Finally, we contribute NerFormer - a novel neural rendering method that leverages the powerful Transformer to reconstruct an object given a small number of its views. The CO3D dataset is available at https://github.com/facebookresearch/co3d .","Authors (format: First Last, First Middle Last, ...)":"Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, David Novotny","Bibtex (e.g. @inproceedings...)":"@inproceedings{reizenstein2021co3d,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Jeremy Reizenstein and Roman Shapovalov and Philipp Henzler and Luca Sbordone and Patrick Labatut and David Novotny},\n    title = {Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction},\n    year = {2021},\n    url = {http://arxiv.org/abs/2109.00512v1},\n    entrytype = {inproceedings},\n    id = {reizenstein2021co3d}\n}","Bibtex Name":"reizenstein2021co3d","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/facebookresearch/co3d","Coordinates all at once":"","Data Release (link)":"https://ai.facebook.com/datasets/co3d-downloads/","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/1/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Sparse Reconstruction, Generalization, Transformer, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"CO3D","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2109.00512.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=hMx9nzG50xQ","Timestamp":"9/28/2021 9:31","Title":"Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction","Training time (hr)":"","UID":"240","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In this work, we propose a camera self-calibration algorithm for generic cameras with arbitrary non-linear distortions. We jointly learn the geometry of the scene and the accurate camera parameters without any calibration objects. Our camera model consists of a pinhole model, a fourth order radial distortion, and a generic noise model that can learn arbitrary non-linear camera distortions. While traditional self-calibration algorithms mostly rely on geometric constraints, we additionally incorporate photometric consistency. This requires","Authors (format: First Last, First Middle Last, ...)":"Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Animashree Anandkumar, Minsu Cho, Jaesik Park","Bibtex (e.g. @inproceedings...)":"@inproceedings{jeong2021selfcalibrating,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    year = {2021},\n    author = {Yoonwoo Jeong and Seokjun Ahn and Christopher Choy and Animashree Anandkumar and Minsu Cho and Jaesik Park},\n    journal = {arXiv preprint arXiv:2108.13826},\n    title = {Self-Calibrating Neural Radiance Fields},\n    entrytype = {inproceedings},\n    id = {jeong2021selfcalibrating}\n}","Bibtex Name":"jeong2021selfcalibrating","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/POSTECH-CVLab/SCNeRF","Coordinates all at once":"","Data Release (link)":"https://github.com/POSTECH-CVLab/SCNeRF","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/30/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Camera Parameter Estimation, Coarse-to-Fine","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"http://jaesik.info/publications/data/21_iccv1.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=wsjx6geduvk","Timestamp":"8/30/2021 18:01","Title":"Self-Calibrating Neural Radiance Fields","Training time (hr)":"","UID":"239","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Image reconstruction is an inverse problem that solves for a computational image based on sampled sensor measurement. Sparsely sampled image reconstruction poses addition challenges due to limited measurements. In this work, we propose an implicit Neural Representation learning methodology with Prior embedding (NeRP) to reconstruct a computational image from sparsely sampled measurements. The method differs fundamentally from previous deep learning-based image reconstruction approaches in that NeRP exploits the internal information in an image prior, and the physics of the sparsely sampled measurements to produce a representation of the unknown subject. No large-scale data is required to train the NeRP except for a prior image and sparsely sampled measurements. In addition, we demonstrate that NeRP is a general methodology that generalizes to different imaging modalities such as CT and MRI. We also show that NeRP can robustly capture the subtle yet significant image changes required for assessing tumor progression.","Authors (format: First Last, First Middle Last, ...)":"Liyue Shen, John Pauly, Lei Xing","Bibtex (e.g. @inproceedings...)":"@article{shen2021nerp,\n    journal = {arXiv preprint arXiv:ers/2108/2108.10991},\n    booktitle = {ArXiv Pre-print},\n    author = {Liyue Shen and John Pauly and Lei Xing},\n    title = {NeRP: Implicit Neural Representation Learning with Prior Embedding for Sparsely Sampled Image Reconstruction},\n    year = {2021},\n    url = {http://arxiv.org/abs/2108.10991v1},\n    entrytype = {article},\n    id = {shen2021nerp}\n}","Bibtex Name":"shen2021nerp","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/24/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Science & Engineering, Data-Driven Method","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeRP","PDF link (arXiv perferred)":"https://arxiv.org/ftp/arxiv/papers/2108/2108.10991.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/30/2021 17:53","Title":"NeRP: Implicit Neural Representation Learning with Prior Embedding for Sparsely Sampled Image Reconstruction","Training time (hr)":"","UID":"238","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present imGHUM, the first holistic generative model of 3D human shape and articulated pose, represented as a signed distance function. In contrast to prior work, we model the full human body implicitly as a function zero-level-set and without the use of an explicit template mesh. We propose a novel network architecture and a learning paradigm, which make it possible to learn a detailed implicit generative model of human pose, shape, and semantics, on par with state-of-the-art mesh-based models. Our model features desired detail for human models, such as articulated pose including hand motion and facial expressions, a broad spectrum of shape variations, and can be queried at arbitrary resolutions and spatial locations. Additionally, our model has attached spatial semantics making it straightforward to establish correspondences between different shape instances, thus enabling applications that are difficult to tackle using classical implicit representations. In extensive experiments, we demonstrate the model accuracy and its applicability to current research problems.","Authors (format: First Last, First Middle Last, ...)":"Thiemo Alldieck, Hongyi Xu, Cristian Sminchisescu","Bibtex (e.g. @inproceedings...)":"@inproceedings{alldieck2021imghum,\n    year = {2021},\n    pages = {5461--5470},\n    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},\n    author = {Alldieck, Thiemo and Xu, Hongyi and Sminchisescu, Cristian},\n    title = {{imGHUM}: Implicit Generative Models of 3D Human Shape and Articulated Pose},\n    entrytype = {inproceedings},\n    id = {alldieck2021imghum}\n}","Bibtex Name":"alldieck2021imghum","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/24/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"imGHUM","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2108.10842.pdf","Project webpage link":"https://research.google/pubs/pub50642/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/17/2021 11:34","Title":"imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose","Training time (hr)":"","UID":"237","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Synthetic aperture sonar (SAS) image resolution is constrained by waveform bandwidth and array geometry. Specifically, the waveform bandwidth determines a point spread function (PSF) that blurs the locations of point scatterers in the scene. In theory, deconvolving the reconstructed SAS image with the scene PSF restores the original distribution of scatterers and yields sharper reconstructions. However, deconvolution is an ill-posed operation that is highly sensitive to noise. In this work, we leverage implicit neural representations (INRs)","Authors (format: First Last, First Middle Last, ...)":"Albert Reed, Thomas Blanford, Daniel C Brown, Suren Jayasuriya","Bibtex (e.g. @inproceedings...)":"@article{reed2021implicit,\n    author = {Albert Reed and Thomas Blanford and Daniel C Brown and Suren Jayasuriya},\n    title = {Implicit Neural Representations for Deconvolving SAS Images},\n    entrytype = {article},\n    id = {reed2021implicit}\n}","Bibtex Name":"reed2021implicit","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/23/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://web.asu.edu/sites/default/files/imaging-lyceum/files/2021151090.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/29/2021 16:15","Title":"Implicit Neural Representations for Deconvolving SAS Images","Training time (hr)":"","UID":"236","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"OCEANS 2021","Venue no Year":"OCEANS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recent works on implicit neural representations have shown promising results for multi-view surface reconstruction. However, most approaches are limited to relatively simple geometries and usually require clean object masks for reconstructing complex and concave objects. In this work, we introduce a novel neural surface reconstruction framework that leverages the knowledge of stereo matching and feature consistency to optimize the implicit surface representation. More specifically, we apply a signed distance field (SDF) and a surface light field to represent the scene geometry and appearance respectively. The SDF is directly supervised by geometry from stereo matching, and is refined by optimizing the multi-view feature consistency and the fidelity of rendered images. Our method is able to improve the robustness of geometry estimation and support reconstruction of complex scene topologies. Extensive experiments have been conducted on DTU, EPFL and Tanks and Temples datasets. Compared to previous state-of-the-art methods, our method achieves better mesh reconstruction in wide open scenes without masks as input.","Authors (format: First Last, First Middle Last, ...)":"Jingyang Zhang, Yao Yao, Long Quan","Bibtex (e.g. @inproceedings...)":"@inproceedings{zhang2021learning,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Jingyang Zhang and Yao Yao and Long Quan},\n    title = {Learning Signed Distance Field for Multi-view Surface Reconstruction},\n    year = {2021},\n    url = {http://arxiv.org/abs/2108.09964v1},\n    entrytype = {inproceedings},\n    id = {zhang2021learning}\n}","Bibtex Name":"zhang2021learning","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/23/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Data-Driven Method","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2108.09964.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/29/2021 16:40","Title":"Learning Signed Distance Field for Multi-view Surface Reconstruction","Training time (hr)":"","UID":"235","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Different from popular neural networks using quasiconvex activations, non-monotonic networks activated by periodic nonlinearities have emerged as a more competitive paradigm, offering revolutionary benefits: 1) compactly characterizing highfrequency patterns; 2) precisely representing highorder derivatives. Nevertheless, they are also wellknown for being hard to train, due to easily overfitting dissonant noise and only allowing for tiny architectures (shallower than 5 layers). The fundamental bottleneck is that the","Authors (format: First Last, First Middle Last, ...)":"Zheng-Fan Wu, Hui Xue, Weimin Bai","Bibtex (e.g. @inproceedings...)":"@inproceedings{wu2021learning,\n    publisher = {International Joint Conferences on Artificial Intelligence Organization},\n    booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},\n    author = {Zheng-Fan Wu and Hui Xue and Weimin Bai},\n    year = {2021},\n    title = {Learning Deeper Non-Monotonic Networks by Softly Transferring Solution Space},\n    entrytype = {inproceedings},\n    id = {wu2021learning}\n}","Bibtex Name":"wu2021learning","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/20/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Other","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals, Multi-task/Continual/Transfer learning, Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://www.ijcai.org/proceedings/2021/0440.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/29/2021 16:33","Title":"Learning Deeper Non-Monotonic Networks by Softly Transferring Solution Space","Training time (hr)":"","UID":"234","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"IJCAI 2021","Venue no Year":"IJCAI","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present Neural Generalized Implicit Functions(Neural-GIF), to animate people in clothing as a function of the body pose. Given a sequence of scans of a subject in various poses, we learn to animate the character for new poses. Existing methods have relied on template-based representations of the human body (or clothing). However such models usually have fixed and limited resolutions, require difficult data pre-processing steps and cannot be used with complex clothing. We draw inspiration from template-based methods, which factorize motion into articulation and non-rigid deformation, but generalize this concept for implicit shape learning to obtain a more flexible model. We learn to map every point in the space to a canonical space, where a learned deformation field is applied to model non-rigid effects, before evaluating the signed distance field. Our formulation allows the learning of complex and non-rigid deformations of clothing and soft tissue, without computing a template registration as it is common with current approaches. Neural-GIF can be trained on raw 3D scans and reconstructs detailed complex surface geometry and deformations. Moreover, the model can generalize to new poses. We evaluate our method on a variety of characters from different public datasets in diverse clothing styles and show significant improvements over baseline methods, quantitatively and qualitatively. We also extend our model to multiple shape setting. To stimulate further research, we will make the model, code and data publicly available at: https://virtualhumans.mpi-inf.mpg.de/neuralgif/","Authors (format: First Last, First Middle Last, ...)":"Garvita Tiwari, Nikolaos Sarafianos, Tony Tung, Gerard Pons-Moll","Bibtex (e.g. @inproceedings...)":"@inproceedings{tiwari2021neuralgif,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Garvita Tiwari and Nikolaos Sarafianos and Tony Tung and Gerard Pons-Moll},\n    title = {Neural-GIF: Neural Generalized Implicit Functions for Animating People in Clothing},\n    year = {2021},\n    url = {http://arxiv.org/abs/2108.08807v2},\n    entrytype = {inproceedings},\n    id = {tiwari2021neuralgif}\n}","Bibtex Name":"tiwari2021neuralgif","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/19/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Neural-GIF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2108.08807.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/29/2021 16:11","Title":"Neural-GIF: Neural Generalized Implicit Functions for Animating People in Clothing","Training time (hr)":"","UID":"233","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Implicit neural representation is a recent approach to learn shape collections as zero level-sets of neural networks, where each shape is represented by a latent code. So far, the focus has been shape reconstruction, while shape generalization was mostly left to generic encoder-decoder or auto-decoder regularization. In this paper we advocate deformation-aware regularization for implicit neural representations, aiming at producing plausible deformations as latent code changes. The challenge is that implicit representations do not capture correspondences between different shapes, which makes it difficult to represent and regularize their deformations. Thus, we propose to pair the implicit representation of the shapes with an explicit, piecewise linear deformation field, learned as an auxiliary function. We demonstrate that, by regularizing these deformation fields, we can encourage the implicit neural representation to induce natural deformations in the learned shape space, such as as-rigid-as-possible deformations.","Authors (format: First Last, First Middle Last, ...)":"Matan Atzmon, David Novotny, Andrea Vedaldi, Yaron Lipman","Bibtex (e.g. @inproceedings...)":"@article{atzmon2021augmenting,\n    journal = {arXiv preprint arXiv:2108.08931},\n    booktitle = {ArXiv Pre-print},\n    author = {Matan Atzmon and David Novotny and Andrea Vedaldi and Yaron Lipman},\n    title = {Augmenting Implicit Neural Shape Representations with Explicit Deformation Fields},\n    year = {2021},\n    url = {http://arxiv.org/abs/2108.08931v1},\n    entrytype = {article},\n    id = {atzmon2021augmenting}\n}","Bibtex Name":"atzmon2021augmenting","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/19/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Human (Body)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2108.08931.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/29/2021 16:44","Title":"Augmenting Implicit Neural Shape Representations with Explicit Deformation Fields","Training time (hr)":"","UID":"232","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present ARCH++, an image-based method to reconstruct 3D avatars with arbitrary clothing styles. Our reconstructed avatars are animation-ready and highly realistic, in both the visible regions from input views and the unseen regions. While prior work shows great promise of reconstructing animatable clothed humans with various topologies, we observe that there exist fundamental limitations resulting in sub-optimal reconstruction quality. In this paper, we revisit the major steps of image-based avatar reconstruction and address the limitations with ARCH++. First, we introduce an end-to-end point based geometry encoder to better describe the semantics of the underlying 3D human body, in replacement of previous hand-crafted features. Second, in order to address the occupancy ambiguity caused by topological changes of clothed humans in the canonical pose, we propose a co-supervising framework with cross-space consistency to jointly estimate the occupancy in both the posed and canonical spaces. Last, we use image-to-image translation networks to further refine detailed geometry and texture on the reconstructed surface, which improves the fidelity and consistency across arbitrary viewpoints. In the experiments, we demonstrate improvements over the state of the art on both public benchmarks and user studies in reconstruction quality and realism.","Authors (format: First Last, First Middle Last, ...)":"Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, Tony Tung","Bibtex (e.g. @inproceedings...)":"@inproceedings{he2021arch++,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Tong He and Yuanlu Xu and Shunsuke Saito and Stefano Soatto and Tony Tung},\n    title = {ARCH++: Animation-Ready Clothed Human Reconstruction Revisited},\n    year = {2021},\n    url = {http://arxiv.org/abs/2108.07845v1},\n    entrytype = {inproceedings},\n    id = {he2021arch++}\n}","Bibtex Name":"he2021arch++","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/17/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Sparse Reconstruction, Editable, Voxel Grid, Data-Driven Method, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"ARCH++","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2108.07845.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/17/2021 14:20","Title":"ARCH++: Animation-Ready Clothed Human Reconstruction Revisited","Training time (hr)":"","UID":"231","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Many computer vision problems face difficulties when imaging through turbulent refractive media (eg, air and water) due to the refraction and scattering of light. These effects cause geometric distortion that requires either handcrafted physical priors or supervised learning methods to remove. In this paper, we present a novel unsupervised network to recover the latent distortion-free image. The key idea is to model non-rigid distortions as deformable grids. Our network consists of a grid deformer that estimates the distortion field and an image","Authors (format: First Last, First Middle Last, ...)":"Nianyi Li, Simron Thapa, Cameron Whyte, Albert Reed, Suren Jayasuriya, Jinwei Ye","Bibtex (e.g. @inproceedings...)":"@inproceedings{li2021unsupervised,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    year = {2021},\n    author = {Nianyi Li and Simron Thapa and Cameron Whyte and Albert Reed and Suren Jayasuriya and Jinwei Ye},\n    title = {Unsupervised Non-Rigid Image Distortion Removal via Grid Deformation},\n    entrytype = {inproceedings},\n    id = {li2021unsupervised}\n}","Bibtex Name":"li2021unsupervised","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/Nianyi-Li/unsupervised-NDIR","Coordinates all at once":"Yes","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/16/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct, Indirect","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"2D Image Neural Fields","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://ivlab.cse.lsu.edu/pub/iccv_21_distortion_removal.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/18/2021 10:30","Title":"Unsupervised Non-Rigid Image Distortion Removal via Grid Deformation","Training time (hr)":"","UID":"230","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recent advances have enabled a single neural network to serve as an implicit scene representation, establishing the mapping function between spatial coordinates and scene properties. In this paper, we make a further step towards continual learning of the implicit scene representation directly from sequential observations, namely Continual Neural Mapping. The proposed problem setting bridges the gap between batch-trained implicit neural representations and commonly used streaming data in robotics and vision communities. We introduce an experience replay approach to tackle an exemplary task of continual neural mapping: approximating a continuous signed distance function (SDF) from sequential depth images as a scene geometry representation. We show for the first time that a single network can represent scene geometry over time continually without catastrophic forgetting, while achieving promising trade-offs between accuracy and efficiency.","Authors (format: First Last, First Middle Last, ...)":"Zike Yan, Yuxin Tian, Xuesong Shi, Ping Guo, Peng Wang, Hongbin Zha","Bibtex (e.g. @inproceedings...)":"@inproceedings{yan2021continual,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Zike Yan and Yuxin Tian and Xuesong Shi and Ping Guo and Peng Wang and Hongbin Zha},\n    title = {Continual Neural Mapping: Learning An Implicit Scene Representation from Sequential Observations},\n    year = {2021},\n    url = {http://arxiv.org/abs/2108.05851v1},\n    entrytype = {inproceedings},\n    id = {yan2021continual}\n}","Bibtex Name":"yan2021continual","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/12/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Robotics, Multi-task/Continual/Transfer learning, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2108.05851.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://zikeyan.github.io/videos/iccv2021.mp4","Timestamp":"8/30/2021 13:31","Title":"Continual Neural Mapping: Learning An Implicit Scene Representation from Sequential Observations","Training time (hr)":"","UID":"229","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present SIDER(Single-Image neural optimization for facial geometric DEtail Recovery), a novel photometric optimization method that recovers detailed facial geometry from a single image in an unsupervised manner. Inspired by classical techniques of coarse-to-fine optimization and recent advances in implicit neural representations of 3D shape, SIDER combines a geometry prior based on statistical models and Signed Distance Functions (SDFs) to recover facial details from single images. First, it estimates a coarse geometry using a morphable model represented as an SDF. Next, it reconstructs facial geometry details by optimizing a photometric loss with respect to the ground truth image. In contrast to prior work, SIDER does not rely on any dataset priors and does not require additional supervision from multiple views, lighting changes or ground truth 3D shape. Extensive qualitative and quantitative evaluation demonstrates that our method achieves state-of-the-art on facial geometric detail recovery, using only a single in-the-wild image.","Authors (format: First Last, First Middle Last, ...)":"Aggelina Chatziagapi, ShahRukh Athar, Francesc Moreno-Noguer, Dimitris Samaras","Bibtex (e.g. @inproceedings...)":"@article{chatziagapi2021sider,\n    journal = {arXiv preprint arXiv:2108.05465},\n    booktitle = {ArXiv Pre-print},\n    author = {Aggelina Chatziagapi and ShahRukh Athar and Francesc Moreno-Noguer and Dimitris Samaras},\n    title = {SIDER: Single-Image Neural Optimization for Facial Geometric Detail Recovery},\n    year = {2021},\n    url = {http://arxiv.org/abs/2108.05465v1},\n    entrytype = {article},\n    id = {chatziagapi2021sider}\n}","Bibtex Name":"chatziagapi2021sider","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/11/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Head), Sparse Reconstruction, Generalization, Data-Driven Method, Coarse-to-Fine","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SIDER","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2108.05465.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/30/2021 11:59","Title":"SIDER: Single-Image Neural Optimization for Facial Geometric Detail Recovery","Training time (hr)":"","UID":"228","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"This paper presents a neural rendering method for controllable portrait video synthesis. Recent advances in volumetric neural rendering, such as neural radiance fields (NeRF), has enabled the photorealistic novel view synthesis of static scenes with impressive results. However, modeling dynamic and controllable objects as part of a scene with such scene representations is still challenging. In this work, we design a system that enables both novel view synthesis for portrait video, including the human subject and the scene background, and explicit control of the facial expressions through a low-dimensional expression representation. We leverage the expression space of a 3D morphable face model (3DMM) to represent the distribution of human facial expressions, and use it to condition the NeRF volumetric function. Furthermore, we impose a spatial prior brought by 3DMM fitting to guide the network to learn disentangled control for scene appearance and facial actions. We demonstrate the effectiveness of our method on free view synthesis of portrait videos with expression controls. To train a scene, our method only requires a short video of a subject captured by a mobile device.","Authors (format: First Last, First Middle Last, ...)":"ShahRukh Athar, Zhixin Shu, Dimitris Samaras","Bibtex (e.g. @inproceedings...)":"@article{athar2021flameinnerf,\n    journal = {arXiv preprint arXiv:2108.04913},\n    booktitle = {ArXiv Pre-print},\n    author = {ShahRukh Athar and Zhixin Shu and Dimitris Samaras},\n    title = {FLAME-in-NeRF : Neural control of Radiance Fields for Free View Face Animation},\n    year = {2021},\n    url = {http://arxiv.org/abs/2108.04913v1},\n    entrytype = {article},\n    id = {athar2021flameinnerf}\n}","Bibtex Name":"athar2021flameinnerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/10/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Human (Head), Generalization, Editable, Global Conditioning, Data-Driven Method, Coarse-to-Fine","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"FLAME-in-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2108.04913.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/30/2021 14:56","Title":"FLAME-in-NeRF: Neural control of Radiance Fields for Free View Face Animation","Training time (hr)":"","UID":"227","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We extend Neural Radiance Fields (NeRF) with a cylindrical parameterization that\n enables rendering photorealistic novel views of 360\u00b0 outward facing scenes. We further\n introduce a learned exposure compensation parameter to account for the varying exposure\n in training images that may occur from casually capturing a scene. We evaluate our\n method on a variety of 360\u00b0 casually captured scenes.","Authors (format: First Last, First Middle Last, ...)":"Wesley Khademi, Jonathan Ventura","Bibtex (e.g. @inproceedings...)":"@article{khademi2021view,\n    journal = {ACM Transactions on Graphics (TOG)},\n    author = {Wesley Khademi and Jonathan Ventura},\n    title = {View Synthesis In Casually Captured Scenes Using a Cylindrical Neural Radiance Field With Exposure Compensation},\n    year = {2021},\n    isbn = {9781450383714},\n    publisher = {Association for Computing Machinery},\n    address = {New York, NY, USA},\n    url = {https://doi.org/10.1145/3450618.3469147},\n    doi = {10.1145/3450618.3469147},\n    booktitle = {ACM SIGGRAPH 2021 Posters},\n    articleno = {28},\n    numpages = {2},\n    location = {Virtual Event, USA},\n    series = {SIGGRAPH '21},\n    entrytype = {article},\n    id = {khademi2021view}\n}","Bibtex Name":"khademi2021view","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/9/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Sampling, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://dl.acm.org/doi/pdf/10.1145/3450618.3469147","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/30/2021 15:08","Title":"View Synthesis In Casually Captured Scenes Using a Cylindrical Neural Radiance Field With Exposure Compensation","Training time (hr)":"","UID":"226","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"SIGGRAPH 2021","Venue no Year":"SIGGRAPH","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose a framework for aligning and fusing multiple images into a single coordinate-based neural representations. Our framework targets burst images that have misalignment due to camera ego motion and small changes in the scene. We describe different strategies for alignment depending on the assumption of the scene motion, namely, perspective planar (i.e., homography), optical flow with minimal scene change, and optical flow with notable occlusion and disocclusion. Our framework effectively combines the multiple inputs into a single neural implicit function without the need for selecting one of the images as a reference frame. We demonstrate how to use this multi-frame fusion framework for various layer separation tasks.","Authors (format: First Last, First Middle Last, ...)":"Seonghyeon Nam, Marcus A. Brubaker, Michael S. Brown","Bibtex (e.g. @inproceedings...)":"@article{nam2021neural,\n    journal = {arXiv preprint arXiv:2108.01199},\n    booktitle = {ArXiv Pre-print},\n    author = {Seonghyeon Nam and Marcus A. Brubaker and Michael S. Brown},\n    title = {Neural Image Representations for Multi-Image Fusion and Layer Separation},\n    year = {2021},\n    url = {http://arxiv.org/abs/2108.01199v2},\n    entrytype = {article},\n    id = {nam2021neural}\n}","Bibtex Name":"nam2021neural","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/2/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, 2D Image Neural Fields","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2108.01199.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/29/2021 16:46","Title":"Neural Image Representations for Multi-Image Fusion and Layer Separation","Training time (hr)":"","UID":"225","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recent learning approaches that implicitly represent surface geometry using coordinate-based neural representations have shown impressive results in the problem of multi-view 3D reconstruction. The effectiveness of these techniques is, however, subject to the availability of a large number (several tens) of input views of the scene, and computationally demanding optimizations. In this paper, we tackle these limitations for the specific problem of few-shot full 3D head reconstruction, by endowing coordinate-based representations with a probabilistic shape prior that enables faster convergence and better generalization when using few input images (down to three). First, we learn a shape model of 3D heads from thousands of incomplete raw scans using implicit representations. At test time, we jointly overfit two coordinate-based neural networks to the scene, one modeling the geometry and another estimating the surface radiance, using implicit differentiable rendering. We devise a two-stage optimization strategy in which the learned prior is used to initialize and constrain the geometry during an initial optimization phase. Then, the prior is unfrozen and fine-tuned to the scene. By doing this, we achieve high-fidelity head reconstructions, including hair and shoulders, and with a high level of detail that consistently outperforms both state-of-the-art 3D Morphable Models methods in the few-shot scenario, and non-parametric methods when large sets of views are available.","Authors (format: First Last, First Middle Last, ...)":"Eduard Ramon, Gil Triginer, Janna Escur, Albert Pumarola, Jaime Garcia, Xavier Giro-i-Nieto, Francesc Moreno-Noguer","Bibtex (e.g. @inproceedings...)":"@article{ramon2021h3dnet,\n    journal = {arXiv preprint arXiv:2107.12512},\n    booktitle = {ArXiv Pre-print},\n    author = {Eduard Ramon and Gil Triginer and Janna Escur and Albert Pumarola and Jaime Garcia and Xavier Giro-i-Nieto and Francesc Moreno-Noguer},\n    title = {H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction},\n    year = {2021},\n    url = {http://arxiv.org/abs/2107.12512v1},\n    entrytype = {article},\n    id = {ramon2021h3dnet}\n}","Bibtex Name":"ramon2021h3dnet","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/CrisalixSA/h3ds","Coordinates all at once":"","Data Release (link)":"https://github.com/CrisalixSA/h3ds","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/26/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Head), Sparse Reconstruction, Data-Driven Method, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"H3D-Net","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2107.12512.pdf","Project webpage link":"https://crisalixsa.github.io/h3d-net/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/29/2021 17:44","Title":"H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction","Training time (hr)":"","UID":"224","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Human portraits exhibit various appearances when observed from different views under different lighting conditions. We can easily imagine how the face will look like in another setup, but computer algorithms still fail on this problem given limited observations. To this end, we present a system for portrait view synthesis and relighting: given multiple portraits, we use a neural network to predict the light-transport field in 3D space, and from the predicted Neural Light-transport Field (NeLF) produce a portrait from a new camera view under a new environmental lighting. Our system is trained on a large number of synthetic models, and can generalize to different synthetic and real portraits under various lighting conditions. Our method achieves simultaneous view synthesis and relighting given multi-view portraits as the input, and achieves state-of-the-art results.","Authors (format: First Last, First Middle Last, ...)":"Tiancheng Sun, Kai-En Lin, Sai Bi, Zexiang Xu, Ravi Ramamoorthi","Bibtex (e.g. @inproceedings...)":"@article{sun2021nelf,\n    publisher = {The Eurographics Association and John Wiley & Sons Ltd.},\n    journal = {Computer Graphics Forum},\n    author = {Tiancheng Sun and Kai-En Lin and Sai Bi and Zexiang Xu and Ravi Ramamoorthi},\n    title = {NeLF: Neural Light-transport Field for Portrait View Synthesis and Relighting},\n    year = {2021},\n    url = {http://arxiv.org/abs/2107.12351v1},\n    entrytype = {article},\n    id = {sun2021nelf}\n}","Bibtex Name":"sun2021nelf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/26/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"Yes","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Head), Material/Lighting Estimation, Data-Driven Method, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeLF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2107.12351.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/17/2021 19:35","Title":"NeLF: Neural Light-transport Field for Portrait View Synthesis and Relighting","Training time (hr)":"","UID":"223","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"EGSR 2021","Venue no Year":"EGSR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural networks that map 3D coordinates to signed distance function (SDF) or occupancy values have enabled high-fidelity implicit representations of object shape. This paper develops a new shape model that allows synthesizing novel distance views by optimizing a continuous signed directional distance function (SDDF). Similar to deep SDF models, our SDDF formulation can represent whole categories of shapes and complete or interpolate across shapes from partial input data. Unlike an SDF, which measures distance to the nearest surface in any direction, an SDDF measures distance in a given direction. This allows training an SDDF model without 3D shape supervision, using only distance measurements, readily available from depth camera or Lidar sensors. Our model also removes post-processing steps like surface extraction or rendering by directly predicting distance at arbitrary locations and viewing directions. Unlike deep view-synthesis techniques, such as Neural Radiance Fields, which train high-capacity black-box models, our model encodes by construction the property that SDDF values decrease linearly along the viewing direction. This structure constraint not only results in dimensionality reduction but also provides analytical confidence about the accuracy of SDDF predictions, regardless of the distance to the object surface.","Authors (format: First Last, First Middle Last, ...)":"Ehsan Zobeidi, Nikolay Atanasov","Bibtex (e.g. @inproceedings...)":"@article{zobeidi2021a,\n    journal = {arXiv preprint arXiv:2107.11024},\n    booktitle = {ArXiv Pre-print},\n    author = {Ehsan Zobeidi and Nikolay Atanasov},\n    title = {A Deep Signed Directional Distance Function for Object Shape Representation},\n    year = {2021},\n    url = {http://arxiv.org/abs/2107.11024v1},\n    entrytype = {article},\n    id = {zobeidi2021a}\n}","Bibtex Name":"zobeidi2021a","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/23/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Signed Directional Distance Field (SDDF)","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2107.11024.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/29/2021 19:19","Title":"A Deep Signed Directional Distance Function for Object Shape Representation","Training time (hr)":"","UID":"222","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce a new neural network architecture that we call \"grid-functioned\" neural networks. It utilises a grid structure of network parameterisations that can be specialised for different subdomains of the problem, while maintaining smooth, continuous behaviour. The grid gives the user flexibility to prevent gross features from overshadowing important minor ones. We present a full characterisation of its computational and spatial complexity, and demonstrate its potential, compared to a traditional architecture, over a set of synthetic regression problems. We further illustrate the benefits through a real-world 3D skeletal animation case study, where it offers the same visual quality as a state-of-the-art model, but with lower computational complexity and better control accuracy.","Authors (format: First Last, First Middle Last, ...)":"Javier Dehesa, Andrew Vidler, Julian Padget, Christof Lutteroth","Bibtex (e.g. @inproceedings...)":"@inproceedings{dehesa2021gfnn,\n    title = {Grid-Functioned Neural Networks},\n    author = {Javier Dehesa and Andrew Vidler and Julian Padget and Christof Lutteroth},\n    booktitle = {Proceedings of the 38th International Conference on Machine Learning},\n    pages = {2559--2567},\n    year = {2021},\n    editor = {Meila, Marina and Zhang, Tong},\n    volume = {139},\n    series = {Proceedings of Machine Learning Research},\n    month = {18--24 Jul},\n    publisher = {PMLR},\n    pdf = {http://proceedings.mlr.press/v139/dehesa21a/dehesa21a.pdf},\n    url = {https://proceedings.mlr.press/v139/dehesa21a.html},\n    entrytype = {inproceedings},\n    id = {dehesa2021gfnn}\n}","Bibtex Name":"dehesa2021gfnn","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/18/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals, Science & Engineering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"GFNN","PDF link (arXiv perferred)":"http://proceedings.mlr.press/v139/dehesa21a/dehesa21a.pdf","Project webpage link":"http://proceedings.mlr.press/v139/dehesa21a.html","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/8/2021 16:41","Title":"Grid-Functioned Neural Networks","Training time (hr)":"","UID":"221","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"PMLR 2021","Venue no Year":"PMLR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We study the problem of inferring an object-centric scene representation from a single image, aiming to derive a representation that explains the image formation process, captures the scene's 3D nature, and is learned without supervision. Most existing methods on scene decomposition lack one or more of these characteristics, due to the fundamental challenge in integrating the complex 3D-to-2D image formation process into powerful inference schemes like deep networks. In this paper, we propose unsupervised discovery of Object Radiance Fields (uORF), integrating recent progresses in neural 3D scene representations and rendering with deep inference networks for unsupervised 3D scene decomposition. Trained on multi-view RGB images without annotations, uORF learns to decompose complex scenes with diverse, textured background from a single image. We show that uORF performs well on unsupervised 3D scene segmentation, novel view synthesis, and scene editing on three datasets.","Authors (format: First Last, First Middle Last, ...)":"Hong-Xing Yu, Leonidas J. Guibas, Jiajun Wu","Bibtex (e.g. @inproceedings...)":"@article{yu2021uorf,\n    journal = {arXiv preprint arXiv:2107.07905},\n    booktitle = {ArXiv Pre-print},\n    author = {Hong-Xing Yu and Leonidas J. Guibas and Jiajun Wu},\n    title = {Unsupervised Discovery of Object Radiance Fields},\n    year = {2021},\n    url = {http://arxiv.org/abs/2107.07905v1},\n    entrytype = {article},\n    id = {yu2021uorf}\n}","Bibtex Name":"yu2021uorf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/KovenYu/uORF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/16/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Editable, Data-Driven Method, Local Conditioning, Object-Centric, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"uORF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2107.07905.pdf","Project webpage link":"https://kovenyu.com/uorf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"https://kovenyu.com/uorf/static/uORF_supp.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=6J9OpvT4dCA","Timestamp":"8/29/2021 20:31","Title":"Unsupervised Discovery of Object Radiance Fields","Training time (hr)":"","UID":"220","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recently, physics-informed neural networks (PINNs) have offered a powerful new paradigm for solving problems relating to differential equations. Compared to classical numerical methods PINNs have several advantages, for example their ability to provide mesh-free solutions of differential equations and their ability to carry out forward and inverse modelling within the same optimisation problem. Whilst promising, a key limitation to date is that PINNs have struggled to accurately and efficiently solve problems with large domains and/or multi-scale solutions, which is crucial for their real-world application. Multiple significant and related factors contribute to this issue, including the increasing complexity of the underlying PINN optimisation problem as the problem size grows and the spectral bias of neural networks. In this work we propose a new, scalable approach for solving large problems relating to differential equations called Finite Basis PINNs (FBPINNs). FBPINNs are inspired by classical finite element methods, where the solution of the differential equation is expressed as the sum of a finite set of basis functions with compact support. In FBPINNs neural networks are used to learn these basis functions, which are defined over small, overlapping subdomains. FBINNs are designed to address the spectral bias of neural networks by using separate input normalisation over each subdomain, and reduce the complexity of the underlying optimisation problem by using many smaller neural networks in a parallel divide-and-conquer approach. Our numerical experiments show that FBPINNs are effective in solving both small and larger, multi-scale problems, outperforming standard PINNs in both accuracy and computational resources required, potentially paving the way to the application of PINNs on large, real-world problems.","Authors (format: First Last, First Middle Last, ...)":"Ben Moseley, Andrew Markham, Tarje Nissen-Meyer","Bibtex (e.g. @inproceedings...)":"@article{moseley2021fbpinns,\n    url = {http://arxiv.org/abs/2107.07871v1},\n    year = {2021},\n    title = {Finite Basis Physics-Informed Neural Networks (FBPINNs): a scalable domain decomposition approach for solving differential equations},\n    author = {Ben Moseley and Andrew Markham and Tarje Nissen-Meyer},\n    booktitle = {ArXiv Pre-print},\n    journal = {arXiv preprint arXiv:2107.07871},\n    entrytype = {article},\n    id = {moseley2021fbpinns}\n}","Bibtex Name":"moseley2021fbpinns","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/benmoseley/FBPINNs","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/16/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"FBPINNs","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2107.07871.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/8/2021 16:34","Title":"Finite Basis Physics-Informed Neural Networks (FBPINNs): a scalable domain decomposition approach for solving differential equations","Training time (hr)":"","UID":"219","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Classic algebraic reconstruction technique (ART) for computed tomography requires pre-determined weights of the voxels for the projected pixel values to build the equations. However, such weights cannot be accurately obtained in the application of chemiluminescence measurements due to the high physical complexity and computation resources required. Moreover, streaks arise in the results from ART method especially with imperfect projections. In this study, we propose a semi-case-wise learning-based method named Weight Encode Reconstruction Network (WERNet) to co-learn the target phantom intensities and the adaptive weight matrix of the case without labeling the target voxel set and thus offers a more applicable solution for computed tomography problems. Both numerical and experimental validations were conducted to evaluate the algorithm. In the numerical test, with the help of gradient normalization, the WERNet reconstructed voxel set with a high accuracy and showed a higher capability of denoising compared to the classic ART methods. In the experimental test, WERNet produces comparable results to the ART method while having a better performance in avoiding the streaks. Furthermore, with the adaptive weight matrix, WERNet is not sensitive to the ensemble intensity of the projection which shows much better robustness than ART method.","Authors (format: First Last, First Middle Last, ...)":"Hujie Pan, Di Xiao, Fuhao Zhang, Xuesong Li, Min Xu","Bibtex (e.g. @inproceedings...)":"@article{pan2021adaptive,\n    author = {Hujie Pan and Di Xiao and Fuhao Zhang and Xuesong Li and Min Xu},\n    journal = {Opt. Express},\n    keywords = {Computational imaging; Computed tomography; Light fields; Light propagation; Neural networks; Propagation methods},\n    number = {15},\n    pages = {23682--23700},\n    publisher = {OSA},\n    title = {Adaptive weight matrix and phantom intensity learning for computed tomography of chemiluminescence},\n    volume = {29},\n    month = {Jul},\n    year = {2021},\n    url = {http://www.opticsexpress.org/abstract.cfm?URI=oe-29-15-23682},\n    doi = {10.1364/OE.427459},\n    entrytype = {article},\n    id = {pan2021adaptive}\n}","Bibtex Name":"pan2021adaptive","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"Yes","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/12/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://www.osapublishing.org/oe/fulltext.cfm?uri=oe-29-15-23682&id=453213","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:43","Title":"Adaptive weight matrix and phantom intensity learning for computed tomography of chemiluminescence","Training time (hr)":"","UID":"218","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"Optics Express 2021","Venue no Year":"Optics Express","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Humans have a strong intuitive understanding of the 3D environment around us. The mental model of the physics in our brain applies to objects of different materials and enables us to perform a wide range of manipulation tasks that are far beyond the reach of current robots. In this work, we desire to learn models for dynamic 3D scenes purely from 2D visual observations. Our model combines Neural Radiance Fields (NeRF) and time contrastive learning with an autoencoding framework, which learns viewpoint-invariant 3D-aware scene representations. We show that a dynamics model, constructed over the learned representation space, enables visuomotor control for challenging manipulation tasks involving both rigid bodies and fluids, where the target is specified in a viewpoint different from what the robot operates on. When coupled with an auto-decoding framework, it can even support goal specification from camera viewpoints that are outside the training distribution. We further demonstrate the richness of the learned 3D dynamics model by performing future prediction and novel view synthesis. Finally, we provide detailed ablation studies regarding different system designs and qualitative analysis of the learned representations.","Authors (format: First Last, First Middle Last, ...)":"Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal, Antonio Torralba","Bibtex (e.g. @inproceedings...)":"@inproceedings{li20213d,\n    booktitle = {Proceedings of Robotics: Science and Systems},\n    author = {Yunzhu Li and Shuang Li and Vincent Sitzmann and Pulkit Agrawal and Antonio Torralba},\n    title = {3D Neural Scene Representations for Visuomotor Control},\n    year = {2021},\n    url = {http://arxiv.org/abs/2107.04004v1},\n    entrytype = {inproceedings},\n    id = {li20213d}\n}","Bibtex Name":"li20213d","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/8/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering, Robotics","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2107.04004.pdf","Project webpage link":"https://3d-representation-learning.github.io/nerf-dy/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"https://www.youtube.com/watch?v=boKF-q6qofQ, https://www.youtube.com/watch?v=GFkb1x6Oxgo, https://www.youtube.com/watch?v=2fSkcTOvl5M, https://www.youtube.com/watch?v=nckvx1S7-cw","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:44","Title":"3D Neural Scene Representations for Visuomotor Control","Training time (hr)":"","UID":"217","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"RSS 2021","Venue no Year":"RSS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"It is well noted that coordinate based MLPs benefit greatly -- in terms of preserving high-frequency information -- through the encoding of coordinate positions as an array of Fourier features. Hitherto, the rationale for the effectiveness of these positional encodings has been solely studied through a Fourier lens. In this paper, we strive to broaden this understanding by showing that alternative non-Fourier embedding functions can indeed be used for positional encoding. Moreover, we show that their performance is entirely determined by a trade-off between the stable rank of the embedded matrix and the distance preservation between embedded coordinates. We further establish that the now ubiquitous Fourier feature mapping of position is a special case that fulfills these conditions. Consequently, we present a more general theory to analyze positional encoding in terms of shifted basis functions. To this end, we develop the necessary theoretical formulae and empirically verify that our theoretical claims hold in practice. Codes available at https://github.com/osiriszjq/Rethinking-positional-encoding.","Authors (format: First Last, First Middle Last, ...)":"Jianqiao Zheng, Sameera Ramasinghe, Simon Lucey","Bibtex (e.g. @inproceedings...)":"@article{zheng2021rethinking,\n    journal = {arXiv preprint arXiv:2107.02561},\n    booktitle = {ArXiv Pre-print},\n    author = {Jianqiao Zheng and Sameera Ramasinghe and Simon Lucey},\n    title = {Rethinking Positional Encoding},\n    year = {2021},\n    url = {http://arxiv.org/abs/2107.02561v2},\n    entrytype = {article},\n    id = {zheng2021rethinking}\n}","Bibtex Name":"zheng2021rethinking","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/osiriszjq/Rethinking-positional-encoding","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/6/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2107.02561.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:20","Title":"Rethinking positional encoding","Training time (hr)":"","UID":"216","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"One common failure mode of Neural Radiance Field (NeRF) models is fitting incorrect geometries when given an insufficient number of input views. We propose DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning neural radiance fields that takes advantage of readily-available depth supervision. Our key insight is that sparse depth supervision can be used to regularize the learned geometry, a crucial component for effectively rendering novel views using NeRF. We exploit the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as ``free\" depth supervision during training: we simply add a loss to ensure that depth rendered along rays that intersect these 3D points is close to the observed depth. We find that DS-NeRF can render more accurate images given fewer training views while training 2-6x faster. With only two training views on real-world images, DS-NeRF significantly outperforms NeRF as well as other sparse-view variants. We show that our loss is compatible with these NeRF models, demonstrating that depth is a cheap and easily digestible supervisory signal. Finally, we show that DS-NeRF supports other types of depth supervision such as scanned depth sensors and RGBD reconstruction outputs.","Authors (format: First Last, First Middle Last, ...)":"Kangle Deng, Andrew Liu, Jun-Yan Zhu, Deva Ramanan","Bibtex (e.g. @inproceedings...)":"@article{deng2021dsnerf,\n    journal = {arXiv preprint arXiv:2107.02791},\n    booktitle = {ArXiv Pre-print},\n    author = {Kangle Deng and Andrew Liu and Jun-Yan Zhu and Deva Ramanan},\n    title = {Depth-supervised NeRF: Fewer Views and Faster Training for Free},\n    year = {2021},\n    url = {http://arxiv.org/abs/2107.02791v1},\n    entrytype = {article},\n    id = {deng2021dsnerf}\n}","Bibtex Name":"deng2021dsnerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/dunbar12138/DSNeRF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/6/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Sparse Reconstruction","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DS-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2107.02791.pdf","Project webpage link":"https://www.cs.cmu.edu/~dsnerf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=84LFxCo7ogk","Timestamp":"7/19/2021 21:49","Title":"Depth-supervised NeRF: Fewer Views and Faster Training for Free","Training time (hr)":"","UID":"215","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"For collecting high-quality high-resolution (HR) MR image, we propose a novel image reconstruction network named IREM, which is trained on multiple low-resolution (LR) MR images and achieve an arbitrary up-sampling rate for HR image reconstruction. In this work, we suppose the desired HR image as an implicit continuous function of the 3D image spatial coordinate and the thick-slice LR images as several sparse discrete samplings of this function. Then the super-resolution (SR) task is to learn the continuous volumetric function from a limited observations using an fully-connected neural network combined with Fourier feature positional encoding. By simply minimizing the error between the network prediction and the acquired LR image intensity across each imaging plane, IREM is trained to represent a continuous model of the observed tissue anatomy. Experimental results indicate that IREM succeeds in representing high frequency image feature, and in real scene data collection, IREM reduces scan time and achieves high-quality high-resolution MR imaging in terms of SNR and local image detail.","Authors (format: First Last, First Middle Last, ...)":"Qing Wu, Yuwei Li, Lan Xu, Ruiming Feng, Hongjiang Wei, Qing Yang, Boliang Yu, Xiaozhao Liu, Jingyi Yu, Yuyao Zhang","Bibtex (e.g. @inproceedings...)":"@article{wu2021irem,\n    author = {Qing Wu and Yuwei Li and Lan Xu and Ruiming Feng and Hongjiang Wei and Qing Yang and Boliang Yu and Xiaozhao Liu and Jingyi Yu and Yuyao Zhang},\n    title = {IREM: High-Resolution Magnetic Resonance (MR) Image Reconstruction via Implicit Neural Representation},\n    year = {2021},\n    month = {Jun},\n    url = {http://arxiv.org/abs/2106.15097v1},\n    entrytype = {article},\n    id = {wu2021irem}\n}","Bibtex Name":"wu2021irem","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/29/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"IREM","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.15097.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:15","Title":"IREM: High-Resolution Magnetic Resonance Image Reconstruction via Implicit Neural Representation","Training time (hr)":"","UID":"214","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"MICCAI 2021","Venue no Year":"MICCAI","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Novel view synthesis is a long-standing problem in machine learning and computer vision. Significant progress has recently been made in developing neural scene representations and rendering techniques that synthesize photorealistic images from arbitrary views. These representations, however, are extremely slow to train and often also slow to render. Inspired by neural variants of image-based rendering, we develop a new neural rendering approach with the goal of quickly learning a high-quality representation which can also be rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a unique combination of a neural shape representation and 2D CNN-based image feature extraction, aggregation, and re-projection. To push representation convergence times down to minutes, we leverage meta learning to learn neural shape and image feature priors which accelerate training. The optimized shape and image features can then be extracted using traditional graphics techniques and rendered in real time. We show that MetaNLR++ achieves similar or better novel view synthesis results in a fraction of the time that competing methods require.","Authors (format: First Last, First Middle Last, ...)":"Alexander W. Bergman, Petr Kellnhofer, Gordon Wetzstein","Bibtex (e.g. @inproceedings...)":"@article{bergman2021metanlr++,\n    journal = {arXiv preprint arXiv:2106.14942},\n    booktitle = {ArXiv Pre-print},\n    author = {Alexander W. Bergman and Petr Kellnhofer and Gordon Wetzstein},\n    title = {Fast Training of Neural Lumigraph Representations using Meta Learning},\n    year = {2021},\n    url = {http://arxiv.org/abs/2106.14942v1},\n    entrytype = {article},\n    id = {bergman2021metanlr++}\n}","Bibtex Name":"bergman2021metanlr++","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/28/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Sinusoidal Activation (SIREN)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Generalization, Image-Based Rendering, Local Conditioning, Hypernetwork/Meta-learning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"MetaNLR++","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.14942.pdf","Project webpage link":"http://www.computationalimaging.org/publications/metanlr/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=5pBFwyUyW6o","Timestamp":"7/19/2021 21:56","Title":"Fast Training of Neural Lumigraph Representations using Meta Learning","Training time (hr)":"","UID":"213","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present animatable neural radiance fields for detailed human avatar creation from monocular videos. Our approach extends neural radiance fields (NeRF) to the dynamic scenes with human movements via introducing explicit pose-guided deformation while learning the scene representation network. In particular, we estimate the human pose for each frame and learn a constant canonical space for the detailed human template, which enables natural shape deformation from the observation space to the canonical space under the explicit control of the pose parameters. To compensate for inaccurate pose estimation, we introduce the pose refinement strategy that updates the initial pose during the learning process, which not only helps to learn more accurate human reconstruction but also accelerates the convergence. In experiments we show that the proposed approach achieves 1) implicit human geometry and appearance reconstruction with high-quality details, 2) photo-realistic rendering of the human from arbitrary views, and 3) animation of the human with arbitrary poses.","Authors (format: First Last, First Middle Last, ...)":"Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao Bao, Huchuan Lu","Bibtex (e.g. @inproceedings...)":"@inproceedings{chen2021animatable,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Jianchuan Chen and Ying Zhang and Di Kang and Xuefei Zhe and Linchao Bao and Xu Jia and Huchuan Lu},\n    title = {Animatable Neural Radiance Fields from Monocular RGB Videos},\n    year = {2021},\n    url = {http://arxiv.org/abs/2106.13629v2},\n    entrytype = {inproceedings},\n    id = {chen2021animatable}\n}","Bibtex Name":"chen2021animatable","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/25/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Human (Body)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.13629.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:14","Title":"Animatable Neural Radiance Fields from Monocular RGB Video","Training time (hr)":"","UID":"212","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprecedented fidelity, and various recent works have extended NeRF to handle dynamic scenes. A common approach to reconstruct such non-rigid scenes is through the use of a learned deformation field mapping from coordinates in each input image into a canonical template coordinate space. However, these deformation-based approaches struggle to model changes in topology, as topological changes require a discontinuity in the deformation field, but these deformation fields are necessarily continuous. We address this limitation by lifting NeRFs into a higher dimensional space, and by representing the 5D radiance field corresponding to each individual input image as a slice through this \"hyper-space\". Our method is inspired by level set methods, which model the evolution of surfaces as slices through a higher dimensional surface. We evaluate our method on two tasks: (i) interpolating smoothly between \"moments\", i.e., configurations of the scene, seen in the input images while maintaining visual plausibility, and (ii) novel-view synthesis at fixed moments. We show that our method, which we dub HyperNeRF, outperforms existing methods on both tasks by significant margins. Compared to Nerfies, HyperNeRF reduces average error rates by 8.6% for interpolation and 8.8% for novel-view synthesis, as measured by LPIPS.","Authors (format: First Last, First Middle Last, ...)":"Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, Steven M. Seitz","Bibtex (e.g. @inproceedings...)":"@article{park2021hypernerf,\n    journal = {arXiv preprint arXiv:2106.13228},\n    booktitle = {ArXiv Pre-print},\n    author = {Keunhong Park and Utkarsh Sinha and Peter Hedman and Jonathan T. Barron and Sofien Bouaziz and Dan B Goldman and Ricardo Martin-Brualla and Steven M. Seitz},\n    title = {HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields},\n    year = {2021},\n    url = {http://arxiv.org/abs/2106.13228v2},\n    entrytype = {article},\n    id = {park2021hypernerf}\n}","Bibtex Name":"park2021hypernerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/24/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Fundamentals, Global Conditioning, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"HyperNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.13228.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 22:00","Title":"HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields","Training time (hr)":"","UID":"211","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a real-time neural radiance caching method for path-traced global illumination. Our system is designed to handle fully dynamic scenes, and makes no assumptions about the lighting, geometry, and materials. The data-driven nature of our approach sidesteps many difficulties of caching algorithms, such as locating, interpolating, and updating cache points. Since pretraining neural networks to handle novel, dynamic scenes is a formidable generalization challenge, we do away with pretraining and instead achieve generalization via adaptation, i.e. we opt for training the radiance cache while rendering. We employ self-training to provide low-noise training targets and simulate infinite-bounce transport by merely iterating few-bounce training updates. The updates and cache queries incur a mild overhead -- about 2.6ms on full HD resolution -- thanks to a streaming implementation of the neural network that fully exploits modern hardware. We demonstrate significant noise reduction at the cost of little induced bias, and report state-of-the-art, real-time performance on a number of challenging scenarios.","Authors (format: First Last, First Middle Last, ...)":"Thomas M\u00fcller, Fabrice Rousselle, Jan Nov\u00e1k, Alexander Keller","Bibtex (e.g. @inproceedings...)":"@article{muller2021realtime,\n    publisher = {Association for Computing Machinery},\n    journal = {ACM Transactions on Graphics (TOG)},\n    author = {Thomas Muller and Fabrice Rousselle and Jan Novak and Alexander Keller},\n    title = {Real-time Neural Radiance Caching for Path Tracing},\n    doi = {10.1145/3450626.3459812},\n    year = {2021},\n    url = {http://arxiv.org/abs/2106.12372v2},\n    entrytype = {article},\n    id = {muller2021realtime}\n}","Bibtex Name":"muller2021realtime","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/23/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Material/Lighting Estimation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.12372.pdf","Project webpage link":"https://tom94.net/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 17:55","Title":"Real-time Neural Radiance Caching for Path Tracing","Training time (hr)":"","UID":"210","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"SIGGRAPH 2021","Venue no Year":"SIGGRAPH","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural volume rendering became increasingly popular recently due to its success in synthesizing novel views of a scene from a sparse set of input images. So far, the geometry learned by neural volume rendering techniques was modeled using a generic density function. Furthermore, the geometry itself was extracted using an arbitrary level set of the density function leading to a noisy, often low fidelity reconstruction. The goal of this paper is to improve geometry representation and reconstruction in neural volume rendering. We achieve that by modeling the volume density as a function of the geometry. This is in contrast to previous work modeling the geometry as a function of the volume density. In more detail, we define the volume density function as Laplace's cumulative distribution function (CDF) applied to a signed distance function (SDF) representation. This simple density representation has three benefits: (i) it provides a useful inductive bias to the geometry learned in the neural volume rendering process; (ii) it facilitates a bound on the opacity approximation error, leading to an accurate sampling of the viewing ray. Accurate sampling is important to provide a precise coupling of geometry and radiance; and (iii) it allows efficient unsupervised disentanglement of shape and appearance in volume rendering. Applying this new density representation to challenging scene multiview datasets produced high quality geometry reconstructions, outperforming relevant baselines. Furthermore, switching shape and appearance between scenes is possible due to the disentanglement of the two.","Authors (format: First Last, First Middle Last, ...)":"Lior Yariv, Jiatao Gu, Yoni Kasten, Yaron Lipman","Bibtex (e.g. @inproceedings...)":"@inproceedings{yariv2021volsdf,\n    url = {http://arxiv.org/abs/2106.12052v1},\n    year = {2021},\n    title = {Volume Rendering of Neural Implicit Surfaces},\n    author = {Lior Yariv and Jiatao Gu and Yoni Kasten and Yaron Lipman},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    publisher = {Curran Associates, Inc.},\n    entrytype = {inproceedings},\n    id = {yariv2021volsdf}\n}","Bibtex Name":"yariv2021volsdf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/22/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF/Density Hybrid","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"VolSDF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.12052.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/29/2021 17:02","Title":"Volume Rendering of Neural Implicit Surfaces","Training time (hr)":"","UID":"209","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2021","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a Hypernetwork/Meta-learning that predicts the parameters of neural SDFs. The Hypernetwork/Meta-learning is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune, compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned Hypernetwork/Meta-learning is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames.","Authors (format: First Last, First Middle Last, ...)":"Shaofei Wang, Marko Mihajlovic, Qianli Ma, Andreas Geiger, Siyu Tang","Bibtex (e.g. @inproceedings...)":"@inproceedings{wang2021metaavatar,\n    publisher = {International Joint Conferences on Artificial Intelligence Organization},\n    booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},\n    author = {Shaofei Wang and Marko Mihajlovic and Qianli Ma and Andreas Geiger and Siyu Tang},\n    title = {MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images},\n    year = {2021},\n    url = {http://arxiv.org/abs/2106.11944v1},\n    entrytype = {inproceedings},\n    id = {wang2021metaavatar}\n}","Bibtex Name":"wang2021metaavatar","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/22/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Sinusoidal Activation (SIREN)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Hypernetwork/Meta-learning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"MetaAvatar","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.11944.pdf","Project webpage link":"https://neuralbodies.github.io/metavatar/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"https://www.youtube.com/watch?v=SXv1sBRwm4U, https://www.youtube.com/watch?v=eLZH-h1VOm8, https://www.youtube.com/watch?v=MMQStRgWJUE","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:50","Title":"MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images","Training time (hr)":"","UID":"208","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"IJCAI 2021","Venue no Year":"IJCAI","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We investigate the use of neural fields for modeling diverse mesoscale structures, such as fur, fabric, and grass. Instead of using classical graphics primitives to model the structure, we propose to employ a versatile volumetric primitive represented by a neural reflectance field (NeRF-Tex), which jointly models the geometry of the material and its response to lighting. The NeRF-Tex primitive can be instantiated over a base mesh to''texture''it with the desired meso and microscale appearance. We condition the reflectance field on user-defined","Authors (format: First Last, First Middle Last, ...)":"Hendrik Baatz, Jonathan Granskog, Marios Papas, Fabrice Rousselle, Jan Nov{\\'a}k","Bibtex (e.g. @inproceedings...)":"@article{baatz2021nerftex,\n    journal = {Computer Graphics Forum},\n    title = {NeRF-Tex: Neural Reflectance Field Textures},\n    author = {Hendrik Baatz and Jonathan Granskog and Marios Papas and Fabrice Rousselle and Jan Nov{\\'a}k},\n    year = {2021},\n    publisher = {The Eurographics Association and John Wiley & Sons Ltd.},\n    entrytype = {article},\n    id = {baatz2021nerftex}\n}","Bibtex Name":"baatz2021nerftex","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/22/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Material/Lighting Estimation, Global Conditioning, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeRF-Tex","PDF link (arXiv perferred)":"https://d1qx31qr3h6wln.cloudfront.net/publications/NeRFTex.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://d1qx31qr3h6wln.cloudfront.net/publications/NeRFTex_video.mp4","Timestamp":"10/2/2021 10:56","Title":"NeRF-Tex: Neural Reflectance Field Textures","Training time (hr)":"","UID":"207","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"EGSR 2021","Venue no Year":"EGSR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural shape representations have recently shown to be effective in shape analysis and reconstruction tasks. Existing neural network methods require point coordinates and corresponding normal vectors to learn the implicit level sets of the shape. Normal vectors are often not provided as raw data, therefore, approximation and reorientation are required as pre-processing stages, both of which can introduce noise. In this paper, we propose a divergence guided shape representation learning approach that does not require normal vectors as input. We show that incorporating a soft constraint on the divergence of the distance function favours smooth solutions that reliably orients gradients to match the unknown normal at each point, in some cases even better than approaches that use ground truth normal vectors directly. Additionally, we introduce a novel geometric initialization method for sinusoidal shape representation networks that further improves convergence to the desired solution. We evaluate the effectiveness of our approach on the task of surface reconstruction and show state-of-the-art performance compared to other unoriented methods and on-par performance compared to oriented methods.","Authors (format: First Last, First Middle Last, ...)":"Yizhak Ben-Shabat, Chamin Hewa Koneputugodage, Stephen Gould","Bibtex (e.g. @inproceedings...)":"@article{ben-shabat2022digs,\n    author = {Yizhak Ben-Shabat and Chamin Hewa Koneputugodage and Stephen Gould},\n    title = {DiGS : Divergence guided shape implicit neural representation for unoriented point clouds},\n    year = {2021},\n    month = {Jun},\n    url = {http://arxiv.org/abs/2106.10811v1}\n}","Bibtex Name":"dblp:journals/corr/abs-2106-10811","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/21/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"sitzikbs@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Sinusoidal Activation (SIREN)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Geometry Only, Coarse-to-Fine","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"DiGS","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.10811.pdf","Project webpage link":"Coming soon","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"3/2/2022 3:27","Title":"DiGS : Divergence guided shape implicit neural representation for unoriented point clouds","Training time (hr)":"","UID":"206","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present Omnidirectional Neural Radiance Fields (OmniNeRF), the first method to the application of parallax-enabled novel panoramic view synthesis. Recent works for novel view synthesis focus on perspective images with limited field-of-view and require sufficient pictures captured in a specific condition. Conversely, OmniNeRF can generate panorama images for unknown viewpoints given a single equirectangular image as training data. To this end, we propose to augment the single RGB-D panorama by projecting back and forth between a 3D world and different 2D panoramic coordinates at different virtual camera positions. By doing so, we are able to optimize an Omnidirectional Neural Radiance Field with visible pixels collecting from omnidirectional viewing angles at a fixed center for the estimation of new viewing angles from varying camera positions. As a result, the proposed OmniNeRF achieves convincing renderings of novel panoramic views that exhibit the parallax effect. We showcase the effectiveness of each of our proposals on both synthetic and real-world datasets.","Authors (format: First Last, First Middle Last, ...)":"Ching-Yu Hsu, Cheng Sun, Hwann-Tzong Chen","Bibtex (e.g. @inproceedings...)":"@article{hsu2021omninerf,\n    journal = {arXiv preprint arXiv:2106.10859},\n    booktitle = {ArXiv Pre-print},\n    author = {Ching-Yu Hsu and Cheng Sun and Hwann-Tzong Chen},\n    title = {Moving in a 360 World: Synthesizing Panoramic Parallaxes from a Single Panorama},\n    year = {2021},\n    url = {http://arxiv.org/abs/2106.10859v1},\n    entrytype = {article},\n    id = {hsu2021omninerf}\n}","Bibtex Name":"hsu2021omninerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/21/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"OmniNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.10859.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:19","Title":"Moving in a 360 World: Synthesizing Panoramic Parallaxes from a Single Panorama","Training time (hr)":"","UID":"205","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a novel neural surface reconstruction method, called NeuS, for reconstructing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as DVR and IDR, require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as NeRF and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formulation that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion.","Authors (format: First Last, First Middle Last, ...)":"Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, Wenping Wang","Bibtex (e.g. @inproceedings...)":"@inproceedings{wang2021neus,\n    publisher = {International Joint Conferences on Artificial Intelligence Organization},\n    booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},\n    author = {Peng Wang and Lingjie Liu and Yuan Liu and Christian Theobalt and Taku Komura and Wenping Wang},\n    title = {NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction},\n    year = {2021},\n    url = {http://arxiv.org/abs/2106.10689v1},\n    entrytype = {inproceedings},\n    id = {wang2021neus}\n}","Bibtex Name":"wang2021neus","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/Totoro97/NeuS","Coordinates all at once":"","Data Release (link)":"https://github.com/Totoro97/NeuS","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/20/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Other","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Sampling, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeuS","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.10689.pdf","Project webpage link":"https://lingjie0206.github.io/papers/NeuS/index.htm","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:16","Title":"NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction","Training time (hr)":"","UID":"204","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"IJCAI 2021","Venue no Year":"IJCAI","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Our goal in this work is to generate realistic videos given just one initial frame as input. Existing unsupervised approaches to this task do not consider the fact that a video typically shows a 3D environment, and that this should remain coherent from frame to frame even as the camera and objects move. We address this by developing a model that first estimates the latent 3D structure of the scene, including the segmentation of any moving objects. It then predicts future frames by simulating the object and camera dynamics, and rendering the resulting views. Importantly, it is trained end-to-end using only the unsupervised objective of predicting future frames, without any 3D information nor segmentation annotations. Experiments on two challenging datasets of natural videos show that our model can estimate 3D structure and motion segmentation from a single frame, and hence generate plausible and varied predictions.","Authors (format: First Last, First Middle Last, ...)":"Paul Henderson, Christoph H. Lampert, Bernd Bickel","Bibtex (e.g. @inproceedings...)":"@article{henderson2021unsupervised,\n    journal = {arXiv preprint arXiv:2106.09051},\n    booktitle = {ArXiv Pre-print},\n    author = {Paul Henderson and Christoph H. Lampert and Bernd Bickel},\n    title = {Unsupervised Video Prediction from a Single Frame by Estimating 3D Dynamic Scene Structure},\n    year = {2021},\n    url = {http://arxiv.org/abs/2106.09051v1},\n    entrytype = {article},\n    id = {henderson2021unsupervised}\n}","Bibtex Name":"henderson2021unsupervised","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/16/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.09051.pdf","Project webpage link":"http://pmh47.net/vipl4s/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 22:06","Title":"Unsupervised Video Prediction from a Single Frame by Estimating 3D Dynamic Scene Structure","Training time (hr)":"","UID":"203","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"\nDeep neural representations of 3D shapes as implicit functions have been shown to produce high fidelity models surpassing the resolution-memory trade-off faced by the explicit representations using meshes and point clouds. However, most such approaches focus on representing closed shapes. Unsigned distance function (UDF) based approaches have been proposed recently as a promising alternative to represent both open and closed shapes. However, since the gradients of UDFs vanish on the surface, it is challenging to estimate local (differential) geometric properties like the normals and tangent planes which are needed for many downstream applications in vision and graphics. There are additional challenges in computing these properties efficiently with a low-memory footprint. This paper presents a novel approach that models such surfaces using a new class of implicit representations called the closest surface-point CSP representation. We show that CSP allows us to represent complex surfaces of any topology (open or closed) with high fidelity. It also allows for accurate and efficient computation of local geometric properties. We further demonstrate that it leads to efficient implementation of downstream algorithms like sphere-tracing for rendering the 3D surface as well as to create explicit mesh-based representations. Extensive experimental evaluation on the ShapeNet dataset validate the above contributions with results surpassing the state-of-the-art. Code and data are available at https://sites.google.com/view/cspnet","Authors (format: First Last, First Middle Last, ...)":"Rahul Venkatesh, Tejan Karmali, Sarthak Sharma, Aurobrata Ghosh, R. Venkatesh Babu, L\u00e1szl\u00f3 A. Jeni, Maneesh Singh","Bibtex (e.g. @inproceedings...)":"@inproceedings{Venkatesh_2021_ICCV,\n    author = {Rahul Venkatesh and Tejan Karmali and Sarthak Sharma and Aurobrata Ghosh and R. Venkatesh Babu and L\\'aszl\\'o A. Jeni and Maneesh Singh},\n    title = {Deep Implicit Surface Point Prediction Networks},\n    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},\n    month = {October},\n    year = {2021},\n    pages = {12653-12662},\n    entrytype = {inproceedings},\n    id = {Venkatesh_2021_ICCV}\n}","Bibtex Name":"venkatesh_2021_iccv","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/rahulvenkk/csp-net","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/10/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"tejankarmali@iisc.ac.in","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Closest-surface point prediction","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Geometry Only","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"CSPNet","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.05779.pdf","Project webpage link":"https://sites.google.com/view/cspnet","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"11/23/2021 13:28","Title":"Deep Implicit Surface Point Prediction Networks","Training time (hr)":"","UID":"202","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Single image pose estimation is a fundamental problem in many vision and robotics tasks, and existing deep learning approaches suffer by not completely modeling and handling: i) uncertainty about the predictions, and ii) symmetric objects with multiple (sometimes infinite) correct poses. To this end, we introduce a method to estimate arbitrary, non-parametric distributions on SO(3). Our key idea is to represent the distributions implicitly, with a neural network that estimates the probability given the input image and a candidate pose. Grid sampling or gradient ascent can be used to find the most likely pose, but it is also possible to evaluate the probability at any pose, enabling reasoning about symmetries and uncertainty. This is the most general way of representing distributions on manifolds, and to showcase the rich expressive power, we introduce a dataset of challenging symmetric and nearly-symmetric objects. We require no supervision on pose uncertainty -- the model trains only with a single pose per example. Nonetheless, our implicit model is highly expressive to handle complex distributions over 3D poses, while still obtaining accurate pose estimation on standard non-ambiguous environments, achieving state-of-the-art performance on Pascal3D+ and ModelNet10-SO(3) benchmarks.","Authors (format: First Last, First Middle Last, ...)":"Kieran Murphy, Carlos Esteves, Varun Jampani, Srikumar Ramalingam, Ameesh Makadia","Bibtex (e.g. @inproceedings...)":"@inproceedings{murphy2021implicitpdf,\n    publisher = {PMLR},\n    booktitle = {International Conference on Machine Learning (ICML)},\n    author = {Kieran Murphy and Carlos Esteves and Varun Jampani and Srikumar Ramalingam and Ameesh Makadia},\n    title = {Implicit-PDF: Non-Parametric Representation of Probability Distributions on the Rotation Manifold},\n    year = {2021},\n    url = {http://arxiv.org/abs/2106.05965v1},\n    entrytype = {inproceedings},\n    id = {murphy2021implicitpdf}\n}","Bibtex Name":"murphy2021implicitpdf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/google-research/google-research/tree/master/implicit_pdf","Coordinates all at once":"","Data Release (link)":"https://www.tensorflow.org/datasets/catalog/symmetric_solids","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/10/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Camera Parameter Estimation, Fundamentals","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Implicit-PDF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.05965.pdf","Project webpage link":"https://implicit-pdf.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=Y-MlRRy0xJA","Timestamp":"9/18/2021 11:01","Title":"Implicit-PDF: Non-Parametric Representation of Probability Distributions on the Rotation Manifold","Training time (hr)":"","UID":"201","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICML 2021","Venue no Year":"ICML","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present implicit displacement fields, a novel representation for detailed 3D geometry. Inspired by a classic surface deformation technique, displacement mapping, our method represents a complex surface as a smooth base surface plus a displacement along the base's normal directions, resulting in a frequency-based shape decomposition, where the high frequency signal is constrained geometrically by the low frequency signal. Importantly, this disentanglement is unsupervised thanks to a tailored architectural design that has an innate frequency hierarchy by construction. We explore implicit displacement field surface reconstruction and detail transfer and demonstrate superior representational power, training stability and generalizability.","Authors (format: First Last, First Middle Last, ...)":"Wang Yifan, Lukas Rahmann, Olga Sorkine-Hornung","Bibtex (e.g. @inproceedings...)":"@inproceedings{yifan2021idf,\n    publisher = {International Joint Conferences on Artificial Intelligence Organization},\n    booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},\n    author = {Wang Yifan and Lukas Rahmann and Olga Sorkine-Hornung},\n    title = {Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields},\n    year = {2021},\n    url = {http://arxiv.org/abs/2106.05187v2},\n    entrytype = {inproceedings},\n    id = {yifan2021idf}\n}","Bibtex Name":"yifan2021idf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/yifita/idf","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/9/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Sinusoidal Activation (SIREN)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals, Data-Driven Method, Coarse-to-Fine, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"IDF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.05187.pdf","Project webpage link":"https://yifita.github.io/publication/idf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=fl4Rje8HM3I","Timestamp":"6/29/2021 15:35","Title":"Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields","Training time (hr)":"","UID":"200","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"IJCAI 2021","Venue no Year":"IJCAI","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Synthesizing novel views of dynamic humans from stationary monocular cameras is a popular scenario. This is particularly attractive as it does not require static scenes, controlled environments, or specialized hardware. In contrast to techniques that exploit multi-view observations to constrain the modeling, given a single fixed viewpoint only, the problem of modeling the dynamic scene is significantly more under-constrained and ill-posed. In this paper, we introduce Neural Motion Consensus Flow (MoCo-Flow), a representation that models the dynamic scene using a 4D continuous time-variant function. The proposed representation is learned by an optimization which models a dynamic scene that minimizes the error of rendering all observation images. At the heart of our work lies a novel optimization formulation, which is constrained by a motion consensus regularization on the motion flow. We extensively evaluate MoCo-Flow on several datasets that contain human motions of varying complexity, and compare, both qualitatively and quantitatively, to several baseline methods and variants of our methods. Pretrained model, code, and data will be released for research purposes upon paper acceptance.","Authors (format: First Last, First Middle Last, ...)":"Xuelin Chen, Weiyu Li, Daniel Cohen-Or, Niloy J. Mitra, Baoquan Chen","Bibtex (e.g. @inproceedings...)":"@article{chen2021mocoflow,\n    journal = {arXiv preprint arXiv:2106.04477},\n    booktitle = {ArXiv Pre-print},\n    author = {Xuelin Chen and Weiyu Li and Daniel Cohen-Or and Niloy J. Mitra and Baoquan Chen},\n    title = {MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in Stationary Monocular Cameras},\n    year = {2021},\n    url = {http://arxiv.org/abs/2106.04477v1},\n    entrytype = {article},\n    id = {chen2021mocoflow}\n}","Bibtex Name":"chen2021mocoflow","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"Coming soon","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/8/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Coarse-to-Fine","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"MoCo-Flow","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.04477.pdf","Project webpage link":"https://wyysf-98.github.io/MoCo_Flow/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:51","Title":"MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in Stationary Monocular Cameras","Training time (hr)":"","UID":"199","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce DoubleField, a novel representation combining the merits of both surface field and radiance field for high-fidelity human rendering. Within DoubleField, the surface field and radiance field are associated together by a shared feature embedding and a surface-guided sampling strategy. In this way, DoubleField has a continuous but disentangled learning space for geometry and appearance modeling, which supports fast training, inference, and finetuning. To achieve high-fidelity free-viewpoint rendering, DoubleField is further augmented to leverage ultra-high-resolution inputs, where a view-to-view transformer and a transfer learning scheme are introduced for more efficient learning and finetuning from sparse-view inputs at original resolutions. The efficacy of DoubleField is validated by the quantitative evaluations on several datasets and the qualitative results in a real-world sparse multi-view system, showing its superior capability for photo-realistic free-viewpoint human rendering. For code and demo video, please refer to our project page: http://www.liuyebin.com/dbfield/dbfield.html.","Authors (format: First Last, First Middle Last, ...)":"Ruizhi Shao, Hongwen Zhang, He Zhang, Yanpei Cao, Tao Yu, Yebin Liu","Bibtex (e.g. @inproceedings...)":"@article{shao2021doublefield,\n    journal = {arXiv preprint arXiv:2106.03798},\n    booktitle = {ArXiv Pre-print},\n    author = {Ruizhi Shao and Hongwen Zhang and He Zhang and Yanpei Cao and Tao Yu and Yebin Liu},\n    title = {DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Rendering},\n    year = {2021},\n    url = {http://arxiv.org/abs/2106.03798v2},\n    entrytype = {article},\n    id = {shao2021doublefield}\n}","Bibtex Name":"shao2021doublefield","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/8/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Human (Body), Sparse Reconstruction, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DoubleField","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.03798.pdf","Project webpage link":"http://www.liuyebin.com/dbfield/dbfield.html","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"http://www.liuyebin.com/dbfield/assets/supp2.mp4","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/28/2021 23:02","Title":"DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Rendering","Training time (hr)":"","UID":"198","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In recent years, neural implicit representations gained popularity in 3D reconstruction due to their expressiveness and flexibility. However, the implicit nature of neural implicit representations results in slow inference time and requires careful initialization. In this paper, we revisit the classic yet ubiquitous point cloud representation and introduce a differentiable point-to-mesh layer using a differentiable formulation of Poisson Surface Reconstruction (PSR) that allows for a GPU-accelerated fast solution of the indicator function given an oriented point cloud. The differentiable PSR layer allows us to efficiently and differentiably bridge the explicit 3D point representation with the 3D mesh via the implicit indicator field, enabling end-to-end optimization of surface reconstruction metrics such as Chamfer distance. This duality between points and meshes hence allows us to represent shapes as oriented point clouds, which are explicit, lightweight and expressive. Compared to neural implicit representations, our Shape-As-Points (SAP) model is more interpretable, lightweight, and accelerates inference time by one order of magnitude. Compared to other explicit representations such as points, patches, and meshes, SAP produces topology-agnostic, watertight manifold surfaces. We demonstrate the effectiveness of SAP on the task of surface reconstruction from unoriented point clouds and learning-based reconstruction.","Authors (format: First Last, First Middle Last, ...)":"Songyou Peng, Chiyu \"Max\" Jiang, Yiyi Liao, Michael Niemeyer, Marc Pollefeys, Andreas Geiger","Bibtex (e.g. @inproceedings...)":"@article{peng2021sap,\n    url = {http://arxiv.org/abs/2106.03452v2},\n    month = {Jun},\n    year = {2021},\n    title = {Shape As Points: A Differentiable Poisson Solver},\n    author = {Songyou Peng and Chiyu \"Max\" Jiang and Yiyi Liao and Michael Niemeyer and Marc Pollefeys and Andreas Geiger},\n    entrytype = {article},\n    id = {peng2021sap}\n}","Bibtex Name":"peng2021sap","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/autonomousvision/shape_as_points","Coordinates all at once":"","Data Release (link)":"https://github.com/autonomousvision/shape_as_points","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/7/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"songyou.peng@inf.ethz.ch","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Points are encoded to an indicator grid with a differentiable Poisson solver using spectral methods.","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Point offset fields and point normal fields","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Geometry Only, Global Conditioning, Local Conditioning, Voxel Grid, Supervision by Gradient (PDE), Coarse-to-Fine, Positional Encoding","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"SAP","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.03452.pdf","Project webpage link":"https://pengsongyou.github.io/sap","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=TgR0NvYty0A","Timestamp":"11/24/2021 3:54","Title":"Shape As Points: A Differentiable Poisson Solver","Training time (hr)":"","UID":"197","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2021","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Implicit representations of geometry, such as occupancy fields or signed distance fields (SDF), have recently re-gained popularity in encoding 3D solid shape in a functional form. In this work, we introduce medial fields: a field function derived from the medial axis transform (MAT) that makes available information about the underlying 3D geometry that is immediately useful for a number of downstream tasks. In particular, the medial field encodes the local thickness of a 3D shape, and enables O(1) projection of a query point onto the medial axis. To construct the medial field we require nothing but the SDF of the shape itself, thus allowing its straightforward incorporation in any application that relies on signed distance fields. Working in unison with the O(1) surface projection supported by the SDF, the medial field opens the door for an entirely new set of efficient, shape-aware operations on implicit representations. We present three such applications, including a modification to sphere tracing that renders implicit representations with better convergence properties, a fast construction method for memory-efficient rigid-body collision proxies, and an efficient approximation of ambient occlusion that remains stable with respect to viewpoint variations.","Authors (format: First Last, First Middle Last, ...)":"Daniel Rebain, Ke Li, Vincent Sitzmann, Soroosh Yazdani, Kwang Moo Yi, Andrea Tagliasacchi","Bibtex (e.g. @inproceedings...)":"@article{rebain2021dmf,\n    journal = {arXiv preprint arXiv:2106.03804},\n    booktitle = {ArXiv Pre-print},\n    author = {Daniel Rebain and Ke Li and Vincent Sitzmann and Soroosh Yazdani and Kwang Moo Yi and Andrea Tagliasacchi},\n    title = {Deep Medial Fields},\n    year = {2021},\n    url = {http://arxiv.org/abs/2106.03804v1},\n    entrytype = {article},\n    id = {rebain2021dmf}\n}","Bibtex Name":"rebain2021dmf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/7/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Medial Field","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DMF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.03804.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/29/2021 15:23","Title":"Deep Medial Fields","Training time (hr)":"","UID":"196","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Inferring representations of 3D scenes from 2D observations is a fundamental problem of computer graphics, computer vision, and artificial intelligence. Emerging 3D-structured neural scene representations are a promising approach to 3D scene understanding. In this work, we propose a novel neural scene representation, Light Field Networks or LFNs, which represent both geometry and appearance of the underlying 3D scene in a 360-degree, four-dimensional light field parameterized via a neural implicit representation. Rendering a ray from an LFN requires only a *single* network evaluation, as opposed to hundreds of evaluations per ray for ray-marching or volumetric based renderers in 3D-structured neural scene representations. In the setting of simple scenes, we leverage meta-learning to learn a prior over LFNs that enables multi-view consistent light field reconstruction from as little as a single image observation. This results in dramatic reductions in time and memory complexity, and enables real-time rendering. The cost of storing a 360-degree light field via an LFN is two orders of magnitude lower than conventional methods such as the Lumigraph. Utilizing the analytical differentiability of neural implicit representations and a novel parameterization of light space, we further demonstrate the extraction of sparse depth maps from LFNs.","Authors (format: First Last, First Middle Last, ...)":"Vincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B. Tenenbaum, Fredo Durand","Bibtex (e.g. @inproceedings...)":"@inproceedings{sitzmann2021lfns,\n    booktitle = {Advances in Neural Information Processing (NeurIPS)},\n    author = {Vincent Sitzmann and Semon Rezchikov and William T. Freeman and Joshua B. Tenenbaum and Fredo Durand},\n    title = {Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering},\n    year = {2021},\n    url = {http://arxiv.org/abs/2106.02634v1},\n    entrytype = {inproceedings},\n    id = {sitzmann2021lfns}\n}","Bibtex Name":"sitzmann2021lfns","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"Coming soon","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/4/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Generalization, Sampling, Global Conditioning, Hypernetwork/Meta-learning, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"LFNs","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.02634.pdf","Project webpage link":"https://vsitzmann.github.io/lfns/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=x3sSreTNFw4","Timestamp":"7/19/2021 21:38","Title":"Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering","Training time (hr)":"","UID":"195","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"IJCAI 2021","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We address the problem of recovering the shape and spatially-varying reflectance of an object from posed multi-view images of the object illuminated by one unknown lighting condition. This enables the rendering of novel views of the object under arbitrary environment lighting and editing of the object's material properties. The key to our approach, which we call Neural Radiance Factorization (NeRFactor), is to distill the volumetric geometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object into a surface representation and then jointly refine the geometry while solving for the spatially-varying reflectance and the environment lighting. Specifically, NeRFactor recovers 3D neural fields of surface normals, light visibility, albedo, and Bidirectional Reflectance Distribution Functions (BRDFs) without any supervision, using only a re-rendering loss, simple smoothness priors, and a data-driven BRDF prior learned from real-world BRDF measurements. By explicitly modeling light visibility, NeRFactor is able to separate shadows from albedo and synthesize realistic soft or hard shadows under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D models for free-viewpoint relighting in this challenging and underconstrained capture setup for both synthetic and real scenes. Qualitative and quantitative experiments show that NeRFactor outperforms classic and deep learning-based state of the art across various tasks. Our code and data are available at people.csail.mit.edu/xiuming/projects/nerfactor/.","Authors (format: First Last, First Middle Last, ...)":"Xiuming Zhang, Pratul P. Srinivasan, Boyang Deng, Paul Debevec, William T. Freeman, Jonathan T. Barron","Bibtex (e.g. @inproceedings...)":"@article{zhang2021nerfactor,\n    author = {Zhang, Xiuming and Srinivasan, Pratul P. and Deng, Boyang and Debevec, Paul and Freeman, William T. and Barron, Jonathan T.},\n    title = {NeRFactor: Neural Factorization of Shape and Reflectance under an Unknown Illumination},\n    year = {2021},\n    issue_date = {December 2021},\n    publisher = {Association for Computing Machinery},\n    address = {New York, NY, USA},\n    volume = {40},\n    number = {6},\n    issn = {0730-0301},\n    url = {https://doi.org/10.1145/3478513.3480496},\n    doi = {10.1145/3478513.3480496},\n    journal = {ACM Trans. Graph.},\n    month = {dec},\n    articleno = {237},\n    numpages = {18}\n}","Bibtex Name":"zhang2021nerfactor","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/google/nerfactor","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/3/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Editable, Material/Lighting Estimation, Data-Driven Method","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeRFactor","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.01970.pdf","Project webpage link":"https://people.csail.mit.edu/xiuming/projects/nerfactor/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=UUVSPJlwhPg","Timestamp":"7/19/2021 21:33","Title":"NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination","Training time (hr)":"","UID":"194","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Multilayer perceptrons (MLPs) have been successfully used to represent 3D shapes implicitly and compactly, by mapping 3D coordinates to the corresponding signed distance values or occupancy values. In this paper, we propose a novel positional encoding scheme, called Spline Positional Encoding, to map the input coordinates to a high dimensional space before passing them to MLPs, for helping to recover 3D signed distance fields with fine-scale geometric details from unorganized 3D point clouds. We verified the superiority of our approach over other positional encoding schemes on tasks of 3D shape reconstruction from input point clouds and shape space learning. The efficacy of our approach extended to image reconstruction is also demonstrated and evaluated.","Authors (format: First Last, First Middle Last, ...)":"Peng-Shuai Wang, Yang Liu, Yu-Qi Yang, Xin Tong","Bibtex (e.g. @inproceedings...)":"@inproceedings{wang2021spline,\n    publisher = {International Joint Conferences on Artificial Intelligence Organization},\n    booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},\n    author = {Peng-Shuai Wang and Yang Liu and Yu-Qi Yang and Xin Tong},\n    title = {Spline Positional Encoding for Learning 3D Implicit Signed Distance Fields},\n    year = {2021},\n    url = {http://arxiv.org/abs/2106.01553v1},\n    entrytype = {inproceedings},\n    id = {wang2021spline}\n}","Bibtex Name":"wang2021spline","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/3/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Other","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals, Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.01553.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:42","Title":"Spline Positional Encoding for Learning 3D Implicit Signed Distance Fields","Training time (hr)":"","UID":"193","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"IJCAI 2021","Venue no Year":"IJCAI","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose Neural Actor (NA), a new method for high-quality synthesis of humans from arbitrary viewpoints and under arbitrary controllable poses. Our method is built upon recent neural scene representation and rendering works which learn representations of geometry and appearance from only 2D images. While existing works demonstrated compelling rendering of static scenes and playback of dynamic scenes, photo-realistic reconstruction and rendering of humans with neural implicit methods, in particular under user-controlled novel poses, is still difficult. To address this problem, we utilize a coarse body model as the proxy to unwarp the surrounding 3D space into a canonical pose. A neural radiance field learns pose-dependent geometric deformations and pose- and view-dependent appearance effects in the canonical space from multi-view video input. To synthesize novel views of high fidelity dynamic geometry and appearance, we leverage 2D texture maps defined on the body model as latent variables for predicting residual deformations and the dynamic appearance. Experiments demonstrate that our method achieves better quality than the state-of-the-arts on playback as well as novel pose synthesis, and can even generalize well to new poses that starkly differ from the training poses. Furthermore, our method also supports body shape control of the synthesized results.","Authors (format: First Last, First Middle Last, ...)":"Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, Christian Theobalt","Bibtex (e.g. @inproceedings...)":"@article{liu2021na,\n    publisher = {Association for Computing Machinery},\n    journal = {ACM Transactions on Graphics (TOG)},\n    author = {Lingjie Liu and Marc Habermann and Viktor Rudnev and Kripasindhu Sarkar and Jiatao Gu and Christian Theobalt},\n    title = {Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control},\n    year = {2021},\n    url = {http://arxiv.org/abs/2106.02019v1},\n    entrytype = {article},\n    id = {liu2021na}\n}","Bibtex Name":"liu2021na","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"Coming soon","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/3/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Editable, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NA","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2106.02019.pdf","Project webpage link":"http://gvv.mpi-inf.mpg.de/projects/NeuralActor/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"http://gvv.mpi-inf.mpg.de/projects/NeuralActor/mp4/main_video_arxiv3.mp4","Timestamp":"7/19/2021 21:46","Title":"Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control","Training time (hr)":"","UID":"192","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"SIGGRAPH 2021","Venue no Year":"SIGGRAPH","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In this work, we aim to address the 3D scene stylization problem - generating stylized images of the scene at arbitrary novel view angles. A straightforward solution is to combine existing novel view synthesis and image/video style transfer approaches, which often leads to blurry results or inconsistent appearance. Inspired by the high quality results of the neural radiance fields (NeRF) method, we propose a joint framework to directly render novel views with the desired style. Our framework consists of two components: an implicit representation of the 3D scene with the neural radiance field model, and a Hypernetwork/Meta-learning to transfer the style information into the scene representation. In particular, our implicit representation model disentangles the scene into the geometry and appearance branches, and the Hypernetwork/Meta-learning learns to predict the parameters of the appearance branch from the reference style image. To alleviate the training difficulties and memory burden, we propose a two-stage training procedure and a patch sub-sampling approach to optimize the style and content losses with the neural radiance field model. After optimization, our model is able to render consistent novel views at arbitrary view angles with arbitrary style. Both quantitative evaluation and human subject study have demonstrated that the proposed method generates faithful stylization results with consistent appearance across different views.","Authors (format: First Last, First Middle Last, ...)":"Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-sheng Lai, Wei-Chen Chiu","Bibtex (e.g. @inproceedings...)":"@article{chiang2021stylizing,\n    journal = {arXiv preprint arXiv:2105.13016},\n    booktitle = {ArXiv Pre-print},\n    author = {Pei-Ze Chiang and Meng-Shiun Tsai and Hung-Yu Tseng and Wei-sheng Lai and Wei-Chen Chiu},\n    title = {Stylizing 3D Scene via Implicit Representation and Hypernetwork/Meta-learning},\n    year = {2021},\n    url = {http://arxiv.org/abs/2105.13016v2},\n    entrytype = {article},\n    id = {chiang2021stylizing}\n}","Bibtex Name":"chiang2021stylizing","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/27/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Editable, Data-Driven Method, Hypernetwork/Meta-learning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2105.13016.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=MJqcI40sXhk","Timestamp":"7/19/2021 21:34","Title":"Stylizing 3D Scene via Implicit Representation and Hypernetwork/Meta-learning","Training time (hr)":"","UID":"191","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a novel approach based on artificial neural networks, so-called geodesyNets, and present compelling evidence of their ability to serve as accurate geodetic models of highly irregular bodies using minimal prior information on the body. The approach does not rely on the body shape information but, if available, can harness it. GeodesyNets learn a three-dimensional, differentiable, function representing the body density, which we call neural density field. The body shape, as well as other geodetic properties, can easily be recovered. We investigate six different shapes including the bodies 101955 Bennu, 67P Churyumov-Gerasimenko, 433 Eros and 25143 Itokawa for which shape models developed during close proximity surveys are available. Both heterogeneous and homogeneous mass distributions are considered. The gravitational acceleration computed from the trained geodesyNets models, as well as the inferred body shape, show great accuracy in all cases with a relative error on the predicted acceleration smaller than 1\\% even close to the asteroid surface. When the body shape information is available, geodesyNets can seamlessly exploit it and be trained to represent a high-fidelity neural density field able to give insights into the internal structure of the body. This work introduces a new unexplored approach to geodesy, adding a powerful tool to consolidated ones based on spherical harmonics, mascon models and polyhedral gravity.","Authors (format: First Last, First Middle Last, ...)":"Dario Izzo, Pablo G\u00f3mez","Bibtex (e.g. @inproceedings...)":"@article{izzo2021geodesynets,\n    journal = {arXiv preprint arXiv:2105.13031},\n    booktitle = {ArXiv Pre-print},\n    author = {Dario Izzo and Pablo Gomez},\n    title = {Geodesy of irregular small bodies via neural density fields: geodesyNets},\n    year = {2021},\n    url = {http://arxiv.org/abs/2105.13031v1},\n    entrytype = {article},\n    id = {izzo2021geodesynets}\n}","Bibtex Name":"izzo2021geodesynets","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/27/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"geodesyNets","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2105.13031.pdf","Project webpage link":"https://github.com/darioizzo/geodesynets","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:41","Title":"Geodesy of irregular small bodies via neural density fields: geodesyNets","Training time (hr)":"","UID":"190","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce Neural Radiosity, an algorithm to solve the rendering equation by minimizing the norm of its residual similar as in traditional radiosity techniques. Traditional basis functions used in radiosity techniques, such as piecewise polynomials or meshless basis functions are typically limited to representing isotropic scattering from diffuse surfaces. Instead, we propose to leverage neural networks to represent the full four-dimensional radiance distribution, directly optimizing network parameters to minimize the norm of the residual. Our approach decouples solving the rendering equation from rendering (perspective) images similar as in traditional radiosity techniques, and allows us to efficiently synthesize arbitrary views of a scene. In addition, we propose a network architecture using geometric learnable features that improves convergence of our solver compared to previous techniques. Our approach leads to an algorithm that is simple to implement, and we demonstrate its effectiveness on a variety of scenes with non-diffuse surfaces.","Authors (format: First Last, First Middle Last, ...)":"Saeed Hadadan, Shuhong Chen, Matthias Zwicker","Bibtex (e.g. @inproceedings...)":"@article{hadadan2021neural,\n    journal = {arXiv preprint arXiv:2105.12319},\n    booktitle = {ArXiv Pre-print},\n    author = {Saeed Hadadan and Shuhong Chen and Matthias Zwicker},\n    title = {Neural Radiosity},\n    year = {2021},\n    url = {http://arxiv.org/abs/2105.12319v1},\n    entrytype = {article},\n    id = {hadadan2021neural}\n}","Bibtex Name":"hadadan2021neural","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/26/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Material/Lighting Estimation, Voxel Grid, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2105.12319.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:36","Title":"Neural Radiosity","Training time (hr)":"","UID":"189","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"View synthesis methods using implicit continuous shape representations learned from a set of images, such as the Neural Radiance Field (NeRF) method, have gained increasing attention due to their high quality imagery and scalability to high resolution. However, the heavy computation required by its volumetric approach prevents NeRF from being useful in practice; minutes are taken to render a single image of a few megapixels. Now, an image of a scene can be rendered in a level-of-detail manner, so we posit that a complicated region of the scene should be represented by a large neural network while a small neural network is capable of encoding a simple region, enabling a balance between efficiency and quality. Recursive-NeRF is our embodiment of this idea, providing an efficient and adaptive rendering and training approach for NeRF. The core of Recursive-NeRF learns uncertainties for query coordinates, representing the quality of the predicted color and volumetric intensity at each level. Only query coordinates with high uncertainties are forwarded to the next level to a bigger neural network with a more powerful representational capability. The final rendered image is a composition of results from neural networks of all levels. Our evaluation on three public datasets shows that Recursive-NeRF is more efficient than NeRF while providing state-of-the-art quality. The code will be available at https://github.com/Gword/Recursive-NeRF.","Authors (format: First Last, First Middle Last, ...)":"Guo-Wei Yang, Wen-Yang Zhou, Hao-Yang Peng, Dun Liang, Tai-Jiang Mu, Shi-Min Hu","Bibtex (e.g. @inproceedings...)":"@article{yang2021recursivenerf,\n    journal = {arXiv preprint arXiv:2105.09103},\n    booktitle = {ArXiv Pre-print},\n    author = {Guo-Wei Yang and Wen-Yang Zhou and Hao-Yang Peng and Dun Liang and Tai-Jiang Mu and Shi-Min Hu},\n    title = {Recursive-NeRF: An Efficient and Dynamically Growing NeRF},\n    year = {2021},\n    url = {http://arxiv.org/abs/2105.09103v1},\n    entrytype = {article},\n    id = {yang2021recursivenerf}\n}","Bibtex Name":"yang2021recursivenerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/Gword/Recursive-NeRF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/19/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Sampling, Coarse-to-Fine","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Recursive-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2105.09103.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 12:29","Title":"Recursive-NeRF: An Efficient and Dynamically Growing NeRF","Training time (hr)":"","UID":"188","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In this paper, we present an efficient and robust deep learning solution for novel view synthesis of complex scenes. In our approach, a 3D scene is represented as a light field, i.e., a set of rays, each of which has a corresponding color when reaching the image plane. For efficient novel view rendering, we adopt a 4D parameterization of the light field, where each ray is characterized by a 4D parameter. We then formulate the light field as a 4D function that maps 4D coordinates to corresponding color values. We train a deep fully connected network to optimize this implicit function and memorize the 3D scene. Then, the scene-specific model is used to synthesize novel views. Different from previous light field approaches which require dense view sampling to reliably render novel views, our method can render novel views by sampling rays and querying the color for each ray from the network directly, thus enabling high-quality light field rendering with a sparser set of training images. Our method achieves state-of-the-art novel view synthesis results while maintaining an interactive frame rate.","Authors (format: First Last, First Middle Last, ...)":"Celong Liu, Zhong Li, Junsong Yuan, Yi Xu","Bibtex (e.g. @inproceedings...)":"@article{liu2021neulf,\n    journal = {arXiv preprint arXiv:2105.07112},\n    booktitle = {ArXiv Pre-print},\n    author = {Celong Liu and Zhong Li and Junsong Yuan and Yi Xu},\n    title = {NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field},\n    year = {2021},\n    url = {http://arxiv.org/abs/2105.07112v4},\n    entrytype = {article},\n    id = {liu2021neulf}\n}","Bibtex Name":"liu2021neulf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/15/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Light Field","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeuLF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2105.07112.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 11:24","Title":"NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field","Training time (hr)":"","UID":"187","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"A neural radiance field (NeRF) is a scene model supporting high-quality view synthesis, optimized per scene. In this paper, we explore enabling user editing of a category-level NeRF - also known as a conditional radiance field - trained on a shape category. Specifically, we introduce a method for propagating coarse 2D user scribbles to the 3D space, to modify the color or shape of a local region. First, we propose a conditional radiance field that incorporates new modular network components, including a shape branch that is shared across object instances. Observing multiple instances of the same category, our model learns underlying part semantics without any supervision, thereby allowing the propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair seat). Next, we propose a hybrid network update strategy that targets specific network components, which balances efficiency and accuracy. During user interaction, we formulate an optimization problem that both satisfies the user's constraints and preserves the original object structure. We demonstrate our approach on various editing tasks over three shape datasets and show that it outperforms prior neural editing approaches. Finally, we edit the appearance and shape of a real photograph and show that the edit propagates to extrapolated novel views.","Authors (format: First Last, First Middle Last, ...)":"Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, Bryan Russell","Bibtex (e.g. @inproceedings...)":"@article{liu2021editing,\n    journal = {arXiv preprint arXiv:2105.06466},\n    booktitle = {ArXiv Pre-print},\n    author = {Steven Liu and Xiuming Zhang and Zhoutong Zhang and Richard Zhang and Jun-Yan Zhu and Bryan Russell},\n    title = {Editing Conditional Radiance Fields},\n    year = {2021},\n    url = {http://arxiv.org/abs/2105.06466v2},\n    entrytype = {article},\n    id = {liu2021editing}\n}","Bibtex Name":"liu2021editing","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/stevliu/editnerf","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/13/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Editable","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2105.06466.pdf","Project webpage link":"http://editnerf.csail.mit.edu/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=9qwRD4ejOpw","Timestamp":"5/23/2021 18:11","Title":"Editing Conditional Radiance Fields","Training time (hr)":"","UID":"186","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present an algorithm for generating novel views at arbitrary viewpoints and any input time step given a monocular video of a dynamic scene. Our work builds upon recent advances in neural implicit representation and uses continuous and differentiable functions for modeling the time-varying structure and the appearance of the scene. We jointly train a time-invariant static NeRF and a time-varying dynamic NeRF, and learn how to blend the results in an unsupervised manner. However, learning this implicit function from a single video is highly ill-posed (with infinitely many solutions that match the input video). To resolve the ambiguity, we introduce regularization losses to encourage a more physically plausible solution. We show extensive quantitative and qualitative results of dynamic view synthesis from casually captured videos.","Authors (format: First Last, First Middle Last, ...)":"Chen Gao, Ayush Saraf, Johannes Kopf, Jia-Bin Huang","Bibtex (e.g. @inproceedings...)":"@article{gao2021dynamic,\n    journal = {arXiv preprint arXiv:2105.06468},\n    booktitle = {ArXiv Pre-print},\n    author = {Chen Gao and Ayush Saraf and Johannes Kopf and Jia-Bin Huang},\n    title = {Dynamic View Synthesis from Dynamic Monocular Video},\n    year = {2021},\n    url = {http://arxiv.org/abs/2105.06468v1},\n    entrytype = {article},\n    id = {gao2021dynamic}\n}","Bibtex Name":"gao2021dynamic","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/13/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2105.06468.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:13","Title":"Dynamic View Synthesis from Dynamic Monocular Video","Training time (hr)":"","UID":"185","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Multi-lead electrocardiogram (ECG) provides clinical information of heartbeats from several fixed viewpoints determined by the lead positioning. However, it is often not satisfactory to visualize ECG signals in these fixed and limited views, as some clinically useful information is represented only from a few specific ECG viewpoints. For the first time, we propose a new concept, Electrocardio Panorama, which allows visualizing ECG signals from any queried viewpoints. To build Electrocardio Panorama, we assume that an underlying electrocardio field exists, representing locations, magnitudes, and directions of ECG signals. We present a Neural electrocardio field Network (Nef-Net), which first predicts the electrocardio field representation by using a sparse set of one or few input ECG views and then synthesizes Electrocardio Panorama based on the predicted representations. Specially, to better disentangle electrocardio field information from viewpoint biases, a new Angular Encoding is proposed to process viewpoint angles. Also, we propose a self-supervised learning approach called Standin Learning, which helps model the electrocardio field without direct supervision. Further, with very few modifications, Nef-Net can also synthesize ECG signals from scratch. Experiments verify that our Nef-Net performs well on Electrocardio Panorama synthesis, and outperforms the previous work on the auxiliary tasks (ECG view transformation and ECG synthesis from scratch). The codes and the division labels of cardiac cycles and ECG deflections on Tianchi ECG and PTB datasets are available at https://github.com/WhatAShot/Electrocardio-Panorama.","Authors (format: First Last, First Middle Last, ...)":"Jintai Chen, Xiangshang Zheng, Hongyun Yu, Danny Z. Chen, Jian Wu","Bibtex (e.g. @inproceedings...)":"@inproceedings{chen2021nefnet,\n    publisher = {International Joint Conferences on Artificial Intelligence Organization},\n    booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},\n    author = {Jintai Chen and Xiangshang Zheng and Hongyun Yu and Danny Z. Chen and Jian Wu},\n    title = {Electrocardio Panorama: Synthesizing New ECG Views with Self-supervision},\n    year = {2021},\n    url = {http://arxiv.org/abs/2105.06293v1},\n    entrytype = {inproceedings},\n    id = {chen2021nefnet}\n}","Bibtex Name":"chen2021nefnet","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/12/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Nef-Net","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2105.06293.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/19/2021 18:01","Title":"Electrocardio Panorama: Synthesizing New ECG Views with Self-supervision","Training time (hr)":"","UID":"184","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"IJCAI 2021","Venue no Year":"IJCAI","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recent approaches to render photorealistic views from a limited set of photographs have pushed the boundaries of our interactions with pictures of static scenes. The ability to recreate moments, that is, time-varying sequences, is perhaps an even more interesting scenario, but it remains largely unsolved. We introduce DCT-NeRF, a coordinatebased neural representation for dynamic scenes. DCTNeRF learns smooth and stable trajectories over the input sequence for each point in space. This allows us to enforce consistency between any two frames in the sequence, which results in high quality reconstruction, particularly in dynamic regions.","Authors (format: First Last, First Middle Last, ...)":"Chaoyang Wang, Ben Eckart, Simon Lucey, Orazio Gallo","Bibtex (e.g. @inproceedings...)":"@article{wang2021dctnerf,\n    journal = {arXiv preprint arXiv:2105.05994},\n    booktitle = {ArXiv Pre-print},\n    author = {Chaoyang Wang and Ben Eckart and Simon Lucey and Orazio Gallo},\n    title = {Neural Trajectory Fields for Dynamic Novel View Synthesis},\n    year = {2021},\n    url = {http://arxiv.org/abs/2105.05994v1},\n    entrytype = {article},\n    id = {wang2021dctnerf}\n}","Bibtex Name":"wang2021dctnerf","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/12/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DCT-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2105.05994.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:13","Title":"Neural Trajectory Fields for Dynamic Novel View Synthesis","Training time (hr)":"","UID":"183","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In advanced mission concepts with high levels of autonomy, spacecraft need to internally model the pose and shape of nearby orbiting objects. Recent works in neural scene representations show promising results for inferring generic three-dimensional scenes from optical images. Neural Radiance Fields (NeRF) have shown success in rendering highly specular surfaces using a large number of images and their pose. More recently, Generative Radiance Fields (GRAF) achieved full volumetric reconstruction of a scene from unposed images only, thanks to the use of an adversarial framework to train a NeRF. In this paper, we compare and evaluate the potential of NeRF and GRAF to render novel views and extract the 3D shape of two different spacecraft, the Soil Moisture and Ocean Salinity satellite of ESA's Living Planet Programme and a generic cube sat. Considering the best performances of both models, we observe that NeRF has the ability to render more accurate images regarding the material specularity of the spacecraft and its pose. For its part, GRAF generates precise novel views with accurate details even when parts of the satellites are shadowed while having the significant advantage of not needing any information about the relative pose.","Authors (format: First Last, First Middle Last, ...)":"Anne Mergy, Gurvan Lecuyer, Dawa Derksen, Dario Izzo","Bibtex (e.g. @inproceedings...)":"@inproceedings{mergy2021visionbased,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Anne Mergy and Gurvan Lecuyer and Dawa Derksen and Dario Izzo},\n    title = {Vision-based Neural Scene Representations for Spacecraft},\n    year = {2021},\n    url = {http://arxiv.org/abs/2105.06405v1},\n    entrytype = {inproceedings},\n    id = {mergy2021visionbased}\n}","Bibtex Name":"mergy2021visionbased","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/11/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Sparse Reconstruction, Science & Engineering, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2105.06405.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:12","Title":"Vision-based Neural Scene Representations for Spacecraft","Training time (hr)":"","UID":"182","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Rendering 3D scenes requires access to arbitrary viewpoints from the scene. Storage of such a 3D scene can be done in two ways; (1) storing 2D images taken from the 3D scene that can reconstruct the scene back through interpolations, or (2) storing a representation of the 3D scene itself that already encodes views from all directions. So far, traditional 3D compression methods have focused on the first type of storage and compressed the original 2D images with image compression techniques. With this approach, the user first decodes the stored 2D images and then renders the 3D scene. However, this separated procedure is inefficient since a large amount of 2D images have to be stored. In this work, we take a different approach and compress a functional representation of 3D scenes. In particular, we introduce a method to compress 3D scenes by compressing the neural networks that represent the scenes as neural radiance fields. Our method provides more efficient storage of 3D scenes since it does not store 2D images -- which are redundant when we render the scene from the neural functional representation.","Authors (format: First Last, First Middle Last, ...)":"Berivan Isik","Bibtex (e.g. @inproceedings...)":"@article{isik2021neural,\n    journal = {arXiv preprint arXiv:2105.03120},\n    booktitle = {ArXiv Pre-print},\n    author = {Berivan Isik},\n    title = {Neural 3D Scene Compression via Model Compression},\n    year = {2021},\n    url = {http://arxiv.org/abs/2105.03120v1},\n    entrytype = {article},\n    id = {isik2021neural}\n}","Bibtex Name":"isik2021neural","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/7/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Compression","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2105.03120.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:17","Title":"Neural 3D Scene Compression via Model Compression","Training time (hr)":"","UID":"181","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"This paper addresses the challenge of reconstructing an animatable human model from a multi-view video. Some recent works have proposed to decompose a dynamic scene into a canonical neural radiance field and a set of deformation fields that map observation-space points to the canonical space, thereby enabling them to learn the dynamic scene from images. However, they represent the deformation field as translational vector field or SE(3) field, which makes the optimization highly under-constrained. Moreover, these representations cannot be explicitly controlled by input motions. Instead, we introduce neural blend weight fields to produce the deformation fields. Based on the skeleton-driven deformation, blend weight fields are used with 3D human skeletons to generate observation-to-canonical and canonical-to-observation correspondences. Since 3D human skeletons are more observable, they can regularize the learning of deformation fields. Moreover, the learned blend weight fields can be combined with input skeletal motions to generate new deformation fields to animate the human model. Experiments show that our approach significantly outperforms recent human synthesis methods. The code will be available at https://zju3dv.github.io/animatable_nerf/.","Authors (format: First Last, First Middle Last, ...)":"Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, Xiaowei Zhou","Bibtex (e.g. @inproceedings...)":"@inproceedings{peng2021animatable,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Sida Peng and Junting Dong and Qianqian Wang and Shangzhan Zhang and Qing Shuai and Hujun Bao and Xiaowei Zhou},\n    title = {Animatable Neural Radiance Fields for Human Body Modeling},\n    year = {2021},\n    url = {http://arxiv.org/abs/2105.02872v1},\n    entrytype = {inproceedings},\n    id = {peng2021animatable}\n}","Bibtex Name":"peng2021animatable","Citation Count":"1","Code Release (Github link, or enter \"Coming soon\")":"https://arxiv.org/pdf/2105.02872.pdf","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/6/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Human (Body)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2105.02872.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=eWOSWbmfJo4","Timestamp":"5/23/2021 18:17","Title":"Animatable Neural Radiance Fields for Human Body Modeling","Training time (hr)":"","UID":"180","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural representations have emerged as a new paradigm for applications in rendering, imaging, geometric modeling, and simulation. Compared to traditional representations such as meshes, point clouds, or volumes they can be flexibly incorporated into differentiable learning-based pipelines. While recent improvements to neural representations now make it possible to represent signals with fine details at moderate resolutions (e.g., for images and 3D shapes), adequately representing large-scale or complex scenes has proven a challenge. Current neural representations fail to accurately represent images at resolutions greater than a megapixel or 3D scenes with more than a few hundred thousand polygons. Here, we introduce a new hybrid implicit-explicit network architecture and training strategy that adaptively allocates resources during training and inference based on the local complexity of a signal of interest. Our approach uses a multiscale block-coordinate decomposition, similar to a quadtree or octree, that is optimized during training. The network architecture operates in two stages: using the bulk of the network parameters, a coordinate encoder generates a feature grid in a single forward pass. Then, hundreds or thousands of samples within each block can be efficiently evaluated using a lightweight feature decoder. With this hybrid implicit-explicit network architecture, we demonstrate the first experiments that fit gigapixel images to nearly 40 dB peak signal-to-noise ratio. Notably this represents an increase in scale of over 1000x compared to the resolution of previously demonstrated image-fitting experiments. Moreover, our approach is able to represent 3D shapes significantly faster and better than previous techniques; it reduces training times from days to hours or minutes and memory requirements by over an order of magnitude.","Authors (format: First Last, First Middle Last, ...)":"Julien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan, Marco Monteiro, Gordon Wetzstein","Bibtex (e.g. @inproceedings...)":"@article{martel2021acorn,\n    publisher = {Association for Computing Machinery},\n    journal = {ACM Transactions on Graphics (TOG)},\n    author = {Julien N. P. Martel and David B. Lindell and Connor Z. Lin and Eric R. Chan and Marco Monteiro and Gordon Wetzstein},\n    title = {ACORN: Adaptive Coordinate Networks for Neural Scene Representation},\n    year = {2021},\n    url = {http://arxiv.org/abs/2105.02788v1},\n    entrytype = {article},\n    id = {martel2021acorn}\n}","Bibtex Name":"martel2021acorn","Citation Count":"2","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/6/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Local Conditioning, Coarse-to-Fine, Voxel Grid, Sampling, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"ACORN","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2105.02788.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/29/2021 16:36","Title":"acorn: Adaptive Coordinate Networks for Neural Scene Representation","Training time (hr)":"","UID":"179","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"SIGGRAPH 2021","Venue no Year":"SIGGRAPH","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Some forms of novel visual media enable the viewer to explore a 3D scene from arbitrary viewpoints, by interpolating between a discrete set of original views. Compared to 2D imagery, these types of applications require much larger amounts of storage space, which we seek to reduce. Existing approaches for compressing 3D scenes are based on a separation of compression and rendering: each of the original views is compressed using traditional 2D image formats; the receiver decompresses the views and then performs the rendering. We unify these steps by directly compressing an implicit representation of the scene, a function that maps spatial coordinates to a radiance vector field, which can then be queried to render arbitrary viewpoints. The function is implemented as a neural network and jointly trained for reconstruction as well as compressibility, in an end-to-end manner, with the use of an entropy penalty on the parameters. Our method significantly outperforms a state-of-the-art conventional approach for scene compression, achieving simultaneously higher quality reconstructions and lower bitrates. Furthermore, we show that the performance at lower bitrates can be improved by jointly representing multiple scenes using a soft form of parameter sharing.","Authors (format: First Last, First Middle Last, ...)":"Thomas Bird, Johannes Ball\u00e9, Saurabh Singh, Philip A. Chou","Bibtex (e.g. @inproceedings...)":"@article{bird2021cnerf,\n    journal = {arXiv preprint arXiv:2104.12456},\n    booktitle = {ArXiv Pre-print},\n    author = {Thomas Bird and Johannes Balle and Saurabh Singh and Philip A. Chou},\n    title = {3D Scene Compression through Entropy Penalized Neural Representation Functions},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.12456v1},\n    entrytype = {article},\n    id = {bird2021cnerf}\n}","Bibtex Name":"bird2021cnerf","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/3/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Compression","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"cNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.12456.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:54","Title":"3D Scene Compression through Entropy Penalized Neural Representation Functions","Training time (hr)":"","UID":"178","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Generating free-viewpoint videos is critical for immersive VR/AR experience but recent neural advances still lack the editing ability to manipulate the visual perception for large dynamic scenes. To fill this gap, in this paper we propose the first approach for editable photo-realistic free-viewpoint video generation for large-scale dynamic scenes using only sparse 16 cameras. The core of our approach is a new layered neural representation, where each dynamic entity including the environment itself is formulated into a space-time coherent neural layered radiance representation called ST-NeRF. Such layered representation supports fully perception and realistic manipulation of the dynamic scene whilst still supporting a free viewing experience in a wide range. In our ST-NeRF, the dynamic entity/layer is represented as continuous functions, which achieves the disentanglement of location, deformation as well as the appearance of the dynamic entity in a continuous and self-supervised manner. We propose a scene parsing 4D label map tracking to disentangle the spatial information explicitly, and a continuous deform module to disentangle the temporal motion implicitly. An object-aware volume rendering scheme is further introduced for the re-assembling of all the neural layers. We adopt a novel layered loss and motion-aware ray sampling strategy to enable efficient training for a large dynamic scene with multiple performers, Our framework further enables a variety of editing functions, i.e., manipulating the scale and location, duplicating or retiming individual neural layers to create numerous visual effects while preserving high realism. Extensive experiments demonstrate the effectiveness of our approach to achieve high-quality, photo-realistic, and editable free-viewpoint video generation for dynamic scenes.","Authors (format: First Last, First Middle Last, ...)":"Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang, Minye Wu, Yingliang Zhang, Lan Xu, Jingyi Yu","Bibtex (e.g. @inproceedings...)":"@article{zhang2021stnerf,\n    publisher = {Association for Computing Machinery},\n    journal = {ACM Transactions on Graphics (TOG)},\n    author = {Jiakai Zhang and Xinhang Liu and Xinyi Ye and Fuqiang Zhao and Yanshun Zhang and Minye Wu and Yingliang Zhang and Lan Xu and Jingyi Yu},\n    title = {Editable Free-viewpoint Video Using a Layered Neural Representation},\n    doi = {10.1145/3450626.3459756},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.14786v1},\n    entrytype = {article},\n    id = {zhang2021stnerf}\n}","Bibtex Name":"zhang2021stnerf","Citation Count":"1","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/30/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Editable, Object-Centric, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"ST-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.14786.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=Wp4HfOwFGP4&feature=emb_logo","Timestamp":"5/23/2021 18:18","Title":"Editable Free-Viewpoint Video using a Layered Neural Representation","Training time (hr)":"","UID":"177","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"SIGGRAPH 2021","Venue no Year":"SIGGRAPH","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recent neural rendering methods have demonstrated accurate view interpolation by predicting volumetric density and color with a neural network. Although such volumetric representations can be supervised on static and dynamic scenes, existing methods implicitly bake the complete scene light transport into a single neural network for a given scene, including surface modeling, bidirectional scattering distribution functions, and indirect lighting effects. In contrast to traditional rendering pipelines, this prohibits changing surface reflectance, illumination, or composing other objects in the scene. In this work, we explicitly model the light transport between scene surfaces and we rely on traditional integration schemes and the rendering equation to reconstruct a scene. The proposed method allows BSDF recovery with unknown light conditions and classic light transports such as pathtracing. By learning decomposed transport with surface representations established in conventional rendering methods, the method naturally facilitates editing shape, reflectance, lighting and scene composition. The method outperforms NeRV for relighting under known lighting conditions, and produces realistic reconstructions for relit and edited scenes. We validate the proposed approach for scene editing, relighting and reflectance estimation learned from synthetic and captured views on a subset of NeRV's datasets.","Authors (format: First Last, First Middle Last, ...)":"Julian Knodt, Seung-Hwan Baek, Felix Heide","Bibtex (e.g. @inproceedings...)":"@article{knodt2021neuralraytracing,\n    journal = {arXiv preprint arXiv:2104.13562},\n    booktitle = {ArXiv Pre-print},\n    author = {Julian Knodt and Seung-Hwan Baek and Felix Heide},\n    title = {Neural Ray-Tracing: Learning Surfaces and Reflectance for Relighting and View Synthesis},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.13562v1},\n    entrytype = {article},\n    id = {knodt2021neuralraytracing}\n}","Bibtex Name":"knodt2021neuralraytracing","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/princeton-computational-imaging/neural_raytracing","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/28/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Editable, Material/Lighting Estimation, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Neural Ray-Tracing","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.13562.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:19","Title":"Neural Ray-Tracing: Learning Surfaces and Reflectance for Relighting and View Synthesis","Training time (hr)":"","UID":"176","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Sampling-based model-predictive control (MPC) is a promising tool for feedback control of robots with complex, non-smooth dynamics, and cost functions. However, the computationally demanding nature of sampling-based MPC algorithms has been a key bottleneck in their application to high-dimensional robotic manipulation problems in the real world. Previous methods have addressed this issue by running MPC in the task space while relying on a low-level operational space controller for joint control. However, by not using the joint space of the robot in the MPC formulation, existing methods cannot directly account for non-task space related constraints such as avoiding joint limits, singular configurations, and link collisions. In this paper, we develop a system for fast, joint space sampling-based MPC for manipulators that is efficiently parallelized using GPUs. Our approach can handle task and joint space constraints while taking less than 8ms~(125Hz) to compute the next control command. Further, our method can tightly integrate perception into the control problem by utilizing learned cost functions from raw sensor data. We validate our approach by deploying it on a Franka Panda robot for a variety of dynamic manipulation tasks. We study the effect of different cost formulations and MPC parameters on the synthesized behavior and provide key insights that pave the way for the application of sampling-based MPC for manipulators in a principled manner. We also provide highly optimized, open-source code to be used by the wider robot learning and control community. Videos of experiments can be found at: https://sites.google.com/view/manipulation-mpc","Authors (format: First Last, First Middle Last, ...)":"Mohak Bhardwaj, Balakumar Sundaralingam, Arsalan Mousavian, Nathan Ratliff, Dieter Fox, Fabio Ramos, Byron Boots","Bibtex (e.g. @inproceedings...)":"@inproceedings{bhardwaj2021storm,\n    publisher = {Curran Associates, Inc.},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    author = {Mohak Bhardwaj and Balakumar Sundaralingam and Arsalan Mousavian and Nathan Ratliff and Dieter Fox and Fabio Ramos and Byron Boots},\n    title = {STORM: An Integrated Framework for Fast Joint-Space Model-Predictive Control for Reactive Manipulation},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.13542v2},\n    entrytype = {inproceedings},\n    id = {bhardwaj2021storm}\n}","Bibtex Name":"bhardwaj2021storm","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/28/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Robotics","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"STORM","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.13542.pdf","Project webpage link":"https://sites.google.com/view/manipulation-mpc","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://vimeo.com/526772348","Timestamp":"9/30/2021 10:56","Title":"STORM: An Integrated Framework for Fast Joint-Space Model-Predictive Control for Reactive Manipulation","Training time (hr)":"","UID":"175","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2021","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Invariance and equivariance to the rotation group have been widely discussed in the 3D deep learning community for pointclouds. Yet most proposed methods either use complex mathematical tools that may limit their accessibility, or are tied to specific input data types and network architectures. In this paper, we introduce a general framework built on top of what we call Vector Neuron representations for creating SO(3)-equivariant neural networks for pointcloud processing. Extending neurons from 1D scalars to 3D vectors, our vector neurons enable a simple mapping of SO(3) actions to latent spaces thereby providing a framework for building equivariance in common neural operations -- including linear layers, non-linearities, pooling, and normalizations. Due to their simplicity, vector neurons are versatile and, as we demonstrate, can be incorporated into diverse network architecture backbones, allowing them to process geometry inputs in arbitrary poses. Despite its simplicity, our method performs comparably well in accuracy and generalization with other more complex and specialized state-of-the-art methods on classification and segmentation tasks. We also show for the first time a rotation equivariant reconstruction network.","Authors (format: First Last, First Middle Last, ...)":"Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, Leonidas Guibas","Bibtex (e.g. @inproceedings...)":"@article{deng2021vectorneurons,\n    journal = {arXiv preprint arXiv:2104.12229},\n    booktitle = {ArXiv Pre-print},\n    author = {Congyue Deng and Or Litany and Yueqi Duan and Adrien Poulenard and Andrea Tagliasacchi and Leonidas Guibas},\n    title = {Vector Neurons: A General Framework for SO(3)-Equivariant Networks},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.12229v1},\n    entrytype = {article},\n    id = {deng2021vectorneurons}\n}","Bibtex Name":"deng2021vectorneurons","Citation Count":"1","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/FlyingGiraffe/vnn, https://github.com/FlyingGiraffe/vnn-neural-implicits/","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/25/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Fundamentals, Data-Driven Method, Symmetry","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Vector Neurons","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.12229.pdf","Project webpage link":"https://cs.stanford.edu/~congyue/vnn/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"Coming soon","Timestamp":"8/29/2021 16:25","Title":"Vector Neurons: A General Framework for SO(3)-Equivariant Networks","Training time (hr)":"","UID":"174","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Reconstructing dynamic, time-varying scenes with computed tomography (4D-CT) is a challenging and ill-posed problem common to industrial and medical settings. Existing 4D-CT reconstructions are designed for sparse sampling schemes that require fast CT scanners to capture multiple, rapid revolutions around the scene in order to generate high quality results. However, if the scene is moving too fast, then the sampling occurs along a limited view and is difficult to reconstruct due to spatiotemporal ambiguities. In this work, we design a reconstruction pipeline using implicit neural representations coupled with a novel parametric motion field warping to perform limited view 4D-CT reconstruction of rapidly deforming scenes. Importantly, we utilize a differentiable analysis-by-synthesis approach to compare with captured x-ray sinogram data in a self-supervised fashion. Thus, our resulting optimization method requires no training data to reconstruct the scene. We demonstrate that our proposed system robustly reconstructs scenes containing deformable and periodic motion and validate against state-of-the-art baselines. Further, we demonstrate an ability to reconstruct continuous spatiotemporal representations of our scenes and upsample them to arbitrary volumes and frame rates post-optimization. This research opens a new avenue for implicit neural representations in computed tomography reconstruction in general.","Authors (format: First Last, First Middle Last, ...)":"Albert W. Reed, Hyojin Kim, Rushil Anirudh, K. Aditya Mohan, Kyle Champley, Jingu Kang, Suren Jayasuriya","Bibtex (e.g. @inproceedings...)":"@article{reed2021inr,\n    journal = {arXiv preprint arXiv:2104.11745},\n    booktitle = {ArXiv Pre-print},\n    author = {Albert W. Reed and Hyojin Kim and Rushil Anirudh and K. Aditya Mohan and Kyle Champley and Jingu Kang and Suren Jayasuriya},\n    title = {Dynamic CT Reconstruction from Limited Views with Implicit Neural Representations and Parametric Motion Fields},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.11745v1},\n    entrytype = {article},\n    id = {reed2021inr}\n}","Bibtex Name":"reed2021inr","Citation Count":"1","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/23/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Science & Engineering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"INR","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.11745.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:21","Title":"Dynamic CT Reconstruction from Limited Views with Implicit Neural Representations and Parametric Motion Fields","Training time (hr)":"","UID":"173","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a new generic method for shadow-aware multi-view satellite photogrammetry of Earth Observation scenes. Our proposed method, the Shadow Neural Radiance Field (S-NeRF) follows recent advances in implicit volumetric representation learning. For each scene, we train S-NeRF using very high spatial resolution optical images taken from known viewing angles. The learning requires no labels or shape priors: it is self-supervised by an image reconstruction loss. To accommodate for changing light source conditions both from a directional light source (the Sun) and a diffuse light source (the sky), we extend the NeRF approach in two ways. First, direct illumination from the Sun is modeled via a local light source visibility field. Second, indirect illumination from a diffuse light source is learned as a non-local color field as a function of the position of the Sun. Quantitatively, the combination of these factors reduces the altitude and color errors in shaded areas, compared to NeRF. The S-NeRF methodology not only performs novel view synthesis and full 3D shape estimation, it also enables shadow detection, albedo synthesis, and transient object filtering, without any explicit shape supervision.","Authors (format: First Last, First Middle Last, ...)":"Dawa Derksen, Dario Izzo","Bibtex (e.g. @inproceedings...)":"@inproceedings{derksen2021snerf,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Dawa Derksen and Dario Izzo},\n    title = {Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.09877v1},\n    entrytype = {inproceedings},\n    id = {derksen2021snerf}\n}","Bibtex Name":"derksen2021snerf","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/20/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Material/Lighting Estimation, Sampling","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"S-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.09877.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/20/2021 15:00","Title":"Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry","Training time (hr)":"","UID":"172","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural implicit 3D representations have emerged as a powerful paradigm for reconstructing surfaces from multi-view images and synthesizing novel views. Unfortunately, existing methods such as DVR or IDR require accurate per-pixel object masks as supervision. At the same time, neural radiance fields have revolutionized novel view synthesis. However, NeRF's estimated volume density does not admit accurate surface reconstruction. Our key insight is that implicit surface models and radiance fields can be formulated in a unified way, enabling both surface and volume rendering using the same model. This unified perspective enables novel, more efficient sampling procedures and the ability to reconstruct accurate surfaces without input masks. We compare our method on the DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments demonstrate that we outperform NeRF in terms of reconstruction quality while performing on par with IDR without requiring masks.","Authors (format: First Last, First Middle Last, ...)":"Michael Oechsle, Songyou Peng, Andreas Geiger","Bibtex (e.g. @inproceedings...)":"@inproceedings{oechsle2021unisurf,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Michael Oechsle and Songyou Peng and Andreas Geiger},\n    title = {UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.10078v1},\n    entrytype = {inproceedings},\n    id = {oechsle2021unisurf}\n}","Bibtex Name":"oechsle2021unisurf","Citation Count":"7","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/20/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"UNISURF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.10078.pdf","Project webpage link":"https://arxiv.org/pdf/2104.10078.pdf","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:32","Title":"UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction","Training time (hr)":"","UID":"171","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We develop a distributed framework for the physics-informed neural networks (PINNs) based on two recent extensions, namely conservative PINNs (cPINNs) and extended PINNs (XPINNs), which employ domain decomposition in space and in time-space, respectively. This domain decomposition endows cPINNs and XPINNs with several advantages over the vanilla PINNs, such as parallelization capacity, large representation capacity, efficient hyperparameter tuning, and is particularly effective for multi-scale and multi-physics problems. Here, we present a parallel algorithm for cPINNs and XPINNs constructed with a hybrid programming model described by MPI $+$ X, where X $\\in \\{\\text{CPUs},~\\text{GPUs}\\}$. The main advantage of cPINN and XPINN over the more classical data and model parallel approaches is the flexibility of optimizing all hyperparameters of each neural network separately in each subdomain. We compare the performance of distributed cPINNs and XPINNs for various forward problems, using both weak and strong scalings. Our results indicate that for space domain decomposition, cPINNs are more efficient in terms of communication cost but XPINNs provide greater flexibility as they can also handle time-domain decomposition for any differential equations, and can deal with any arbitrarily shaped complex subdomains. To this end, we also present an application of the parallel XPINN method for solving an inverse diffusion problem with variable conductivity on the United States map, using ten regions as subdomains.","Authors (format: First Last, First Middle Last, ...)":"Khemraj Shukla, Ameya D. Jagtap, George Em Karniadakis","Bibtex (e.g. @inproceedings...)":"@article{shukla2021parallel,\n    url = {http://arxiv.org/abs/2104.10013v3},\n    year = {2021},\n    title = {Parallel Physics-Informed Neural Networks via Domain Decomposition},\n    author = {Khemraj Shukla and Ameya D. Jagtap and George Em Karniadakis},\n    booktitle = {ArXiv Pre-print},\n    journal = {arXiv preprint arXiv:2104.10013},\n    entrytype = {article},\n    id = {shukla2021parallel}\n}","Bibtex Name":"shukla2021parallel","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/20/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.10013.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/8/2021 16:37","Title":"Parallel Physics-Informed Neural Networks via Domain Decomposition","Training time (hr)":"","UID":"170","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Multilayer-perceptrons (MLP) are known to struggle with learning functions of high-frequencies, and in particular cases with wide frequency bands. We present a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. We demonstrate the advantage of SAPE on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes.","Authors (format: First Last, First Middle Last, ...)":"Amir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung, Daniel Cohen-Or","Bibtex (e.g. @inproceedings...)":"@article{hertz2021sape,\n    journal = {arXiv preprint arXiv:2104.09125},\n    booktitle = {ArXiv Pre-print},\n    author = {Amir Hertz and Or Perel and Raja Giryes and Olga Sorkine-Hornung and Daniel Cohen-Or},\n    title = {SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.09125v2},\n    entrytype = {article},\n    id = {hertz2021sape}\n}","Bibtex Name":"hertz2021sape","Citation Count":"1","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/19/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Other","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals, Coarse-to-Fine, Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SAPE","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.09125.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:59","Title":"SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization","Training time (hr)":"","UID":"169","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We investigate the use of Neural Radiance Fields (NeRF) to learn high quality 3D object category models from collections of input images. In contrast to previous work, we are able to do this whilst simultaneously separating foreground objects from their varying backgrounds. We achieve this via a 2-component NeRF model, FiG-NeRF, that prefers explanation of the scene as a geometrically constant background and a deformable foreground that represents the object category. We show that this method can learn accurate 3D object category models using only photometric supervision and casually captured images of the objects. Additionally, our 2-part decomposition allows the model to perform accurate and crisp amodal segmentation. We quantitatively evaluate our method with view synthesis and image fidelity metrics, using synthetic, lab-captured, and in-the-wild data. Our results demonstrate convincing 3D object category modelling that exceed the performance of existing methods.","Authors (format: First Last, First Middle Last, ...)":"Christopher Xie, Keunhong Park, Ricardo Martin-Brualla, Matthew Brown","Bibtex (e.g. @inproceedings...)":"@article{xie2021fignerf,\n    journal = {arXiv preprint arXiv:2104.08418},\n    booktitle = {ArXiv Pre-print},\n    author = {Christopher Xie and Keunhong Park and Ricardo Martin-Brualla and Matthew Brown},\n    title = {FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category Modelling},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.08418v1},\n    entrytype = {article},\n    id = {xie2021fignerf}\n}","Bibtex Name":"xie2021fignerf","Citation Count":"1","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/17/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Fundamentals, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"FiG-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.08418.pdf","Project webpage link":"https://fig-nerf.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=WtZxuv_hkic","Timestamp":"5/23/2021 18:20","Title":"FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category Modelling","Training time (hr)":"","UID":"168","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Registering point clouds of dressed humans to parametric human models is a challenging task in computer vision. Traditional approaches often rely on heavily engineered pipelines that require accurate manual initialization of human poses and tedious post-processing. More recently, learning-based methods are proposed in hope to automate this process. We observe that pose initialization is key to accurate registration but existing methods often fail to provide accurate pose initialization. One major obstacle is that, regressing joint rotations from point clouds or images of humans is still very challenging. To this end, we propose novel piecewise transformation fields (PTF), a set of functions that learn 3D translation vectors to map any query point in posed space to its correspond position in rest-pose space. We combine PTF with multi-class occupancy networks, obtaining a novel learning-based framework that learns to simultaneously predict shape and per-point correspondences between the posed space and the canonical space for clothed human. Our key insight is that the translation vector for each query point can be effectively estimated using the point-aligned local features; consequently, rigid per bone transformations and joint rotations can be obtained efficiently via a least-square fitting given the estimated point correspondences, circumventing the challenging task of directly regressing joint rotations from neural networks. Furthermore, the proposed PTF facilitate canonicalized occupancy estimation, which greatly improves generalization capability and results in more accurate surface reconstruction with only half of the parameters compared with the state-of-the-art. Both qualitative and quantitative studies show that fitting parametric models with poses initialized by our network results in much better registration quality, especially for extreme poses.","Authors (format: First Last, First Middle Last, ...)":"Shaofei Wang, Andreas Geiger, Siyu Tang","Bibtex (e.g. @inproceedings...)":"@inproceedings{wang2021locally,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Shaofei Wang and Andreas Geiger and Siyu Tang},\n    title = {Locally Aware Piecewise Transformation Fields for 3D Human Mesh Registration},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.08160v1},\n    entrytype = {inproceedings},\n    id = {wang2021locally}\n}","Bibtex Name":"wang2021locally","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/taconite/PTF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/16/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Data-Driven Method, Local Conditioning, Voxel Grid","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.08160.pdf","Project webpage link":"https://taconite.github.io/PTF/website/PTF.html","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=TvLoGLVF70k","Timestamp":"9/17/2021 11:58","Title":"Locally Aware Piecewise Transformation Fields for 3D Human Mesh Registration","Training time (hr)":"","UID":"167","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft. Our method takes a semantic block world as input, where each block is assigned a semantic label such as dirt, grass, or water. We represent the world as a continuous volumetric function and train our model to render view-consistent photorealistic images for a user-controlled camera. In the absence of paired ground truth real images for the block world, we devise a training technique based on pseudo-ground truth and adversarial training. This stands in contrast to prior work on neural rendering for view synthesis, which requires ground truth images to estimate scene geometry and view-dependent appearance. In addition to camera trajectory, GANcraft allows user control over both scene semantics and output style. Experimental results with comparison to strong baselines show the effectiveness of GANcraft on this novel task of photorealistic 3D block world synthesis. The project website is available at https://nvlabs.github.io/GANcraft/ .","Authors (format: First Last, First Middle Last, ...)":"Zekun Hao, Arun Mallya, Serge Belongie, Ming-Yu Liu","Bibtex (e.g. @inproceedings...)":"@inproceedings{hao2021gancraft,\n    url = {http://arxiv.org/abs/2104.07659v1},\n    year = {2021},\n    title = {GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds},\n    author = {Zekun Hao and Arun Mallya and Serge Belongie and Ming-Yu Liu},\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    entrytype = {inproceedings},\n    id = {hao2021gancraft}\n}","Bibtex Name":"hao2021gancraft","Citation Count":"2","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/NVlabs/imaginaire","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/15/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Generative Models, Data-Driven Method, Local Conditioning, Voxel Grid, Large-Scale Scenes","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"GANcraft","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.07659.pdf","Project webpage link":"https://nvlabs.github.io/GANcraft/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=1Hky092CGFQ","Timestamp":"5/23/2021 18:32","Title":"GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds","Training time (hr)":"","UID":"166","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recent work has made significant progress on using implicit functions, as a continuous representation for 3D rigid object shape reconstruction. However, much less effort has been devoted to modeling general articulated objects. Compared to rigid objects, articulated objects have higher degrees of freedom, which makes it hard to generalize to unseen shapes. To deal with the large shape variance, we introduce Articulated Signed Distance Functions (A-SDF) to represent articulated shapes with a disentangled latent space, where we have separate codes for encoding shape and articulation. We assume no prior knowledge on part geometry, articulation status, joint type, joint axis, and joint location. With this disentangled continuous representation, we demonstrate that we can control the articulation input and animate unseen instances with unseen joint angles. Furthermore, we propose a Test-Time Adaptation inference algorithm to adjust our model during inference. We demonstrate our model generalize well to out-of-distribution and unseen data, e.g., partial point clouds and real-world depth images.","Authors (format: First Last, First Middle Last, ...)":"Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille, Nuno Vasconcelos, Xiaolong Wang","Bibtex (e.g. @inproceedings...)":"@article{mu2021asdf,\n    journal = {arXiv preprint arXiv:2104.07645},\n    booktitle = {ArXiv Pre-print},\n    author = {Jiteng Mu and Weichao Qiu and Adam Kortylewski and Alan Yuille and Nuno Vasconcelos and Xiaolong Wang},\n    title = {A-SDF: Learning Disentangled Signed Distance Functions for Articulated Shape Representation},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.07645v1},\n    entrytype = {article},\n    id = {mu2021asdf}\n}","Bibtex Name":"mu2021asdf","Citation Count":"2","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/JitengMu/A-SDF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/15/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Editable","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"A-SDF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.07645.pdf","Project webpage link":"https://jitengmu.github.io/A-SDF/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=P5WTcaXzC7A","Timestamp":"5/23/2021 18:33","Title":"A-SDF: Learning Disentangled Signed Distance Functions for Articulated Shape Representation","Training time (hr)":"","UID":"165","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very computationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based system capable of rendering high fidelity photorealistic images at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each position in space, (ii) efficiently querying that map using ray directions to estimate the pixel values in the rendered image. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and extensibility.","Authors (format: First Last, First Middle Last, ...)":"Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, Julien Valentin","Bibtex (e.g. @inproceedings...)":"@article{garbin2021fastnerf,\n    journal = {arXiv preprint arXiv:2103.10380},\n    booktitle = {ArXiv Pre-print},\n    author = {Stephan J. Garbin and Marek Kowalski and Matthew Johnson and Jamie Shotton and Julien Valentin},\n    title = {FastNeRF: High-Fidelity Neural Rendering at 200FPS},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.10380v2},\n    entrytype = {article},\n    id = {garbin2021fastnerf}\n}","Bibtex Name":"garbin2021fastnerf","Citation Count":"10","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/15/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"FastNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.10380.pdf","Project webpage link":"https://microsoft.github.io/FastNeRF/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:47","Title":"FastNeRF: High-Fidelity Neural Rendering at 200FPS","Training time (hr)":"","UID":"164","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recent neural view synthesis methods have achieved impressive quality and realism, surpassing classical pipelines which rely on multi-view reconstruction. State-of-the-Art methods, such as NeRF, are designed to learn a single scene with a neural network and require dense multi-view inputs. Testing on a new scene requires re-training from scratch, which takes 2-3 days. In this work, we introduce Stereo Radiance Fields (SRF), a neural view synthesis approach that is trained end-to-end, generalizes to new scenes, and requires only sparse views at test time. The core idea is a neural architecture inspired by classical multi-view stereo methods, which estimates surface points by finding similar image regions in stereo images. In SRF, we predict color and density for each 3D point given an encoding of its stereo correspondence in the input images. The encoding is implicitly learned by an ensemble of pair-wise similarities -- emulating classical stereo. Experiments show that SRF learns structure instead of overfitting on a scene. We train on multiple scenes of the DTU dataset and generalize to new ones without re-training, requiring only 10 sparse and spread-out views as input. We show that 10-15 minutes of fine-tuning further improve the results, achieving significantly sharper, more detailed results than scene-specific models. The code, model, and videos are available at https://virtualhumans.mpi-inf.mpg.de/srf/.","Authors (format: First Last, First Middle Last, ...)":"Julian Chibane, Aayush Bansal, Verica Lazova, Gerard Pons-Moll","Bibtex (e.g. @inproceedings...)":"@inproceedings{chibane2021srf,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Julian Chibane and Aayush Bansal and Verica Lazova and Gerard Pons-Moll},\n    title = {Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views of Novel Scenes},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.06935v1},\n    entrytype = {inproceedings},\n    id = {chibane2021srf}\n}","Bibtex Name":"chibane2021srf","Citation Count":"4","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/14/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Sparse Reconstruction, Generalization, Image-Based Rendering, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.06935.pdf","Project webpage link":"https://virtualhumans.mpi-inf.mpg.de/srf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"https://arxiv.org/pdf/2104.06935.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:36","Title":"Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views of Novel Scenes","Training time (hr)":"","UID":"163","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Substantial progress has been made on modeling rigid 3D objects using deep implicit representations. Yet, extending these methods to learn neural models of human shape is still in its infancy. Human bodies are complex and the key challenge is to learn a representation that generalizes such that it can express body shape deformations for unseen subjects in unseen, highly-articulated, poses. To address this challenge, we introduce LEAP (LEarning Articulated occupancy of People), a novel neural occupancy representation of the human body. Given a set of bone transformations (i.e. joint locations and rotations) and a query point in space, LEAP first maps the query point to a canonical space via learned linear blend skinning (LBS) functions and then efficiently queries the occupancy value via an occupancy network that models accurate identity- and pose-dependent deformations in the canonical space. Experiments show that our canonicalized occupancy estimation with the learned LBS functions greatly improves the generalization capability of the learned occupancy representation across various human shapes and poses, outperforming existing solutions in all settings.","Authors (format: First Last, First Middle Last, ...)":"Marko Mihajlovic, Yan Zhang, Michael J. Black, Siyu Tang","Bibtex (e.g. @inproceedings...)":"@inproceedings{mihajlovic2021leap,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Marko Mihajlovic and Yan Zhang and Michael J. Black and Siyu Tang},\n    title = {LEAP: Learning Articulated Occupancy of People},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.06849v1},\n    entrytype = {inproceedings},\n    id = {mihajlovic2021leap}\n}","Bibtex Name":"mihajlovic2021leap","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/neuralbodies/leap","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/14/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Editable, Data-Driven Method, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"LEAP","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.06849.pdf","Project webpage link":"https://neuralbodies.github.io/LEAP/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=UVB8A_T5e3c","Timestamp":"9/17/2021 11:50","Title":"LEAP: Learning Articulated Occupancy of People","Training time (hr)":"","UID":"162","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural Radiance Fields (NeRF) have recently gained a surge of interest within the computer vision community for its power to synthesize photorealistic novel views of real-world scenes. One limitation of NeRF, however, is its requirement of accurate camera poses to learn the scene representations. In this paper, we propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from imperfect (or even unknown) camera poses -- the joint problem of learning neural 3D representations and registering camera frames. We establish a theoretical connection to classical image alignment and show that coarse-to-fine registration is also applicable to NeRF. Furthermore, we show that na\\\"ively applying positional encoding in NeRF has a negative impact on registration with a synthesis-based objective. Experiments on synthetic and real-world data show that BARF can effectively optimize the neural scene representations and resolve large camera pose misalignment at the same time. This enables view synthesis and localization of video sequences from unknown camera poses, opening up new avenues for visual localization systems (e.g. SLAM) and potential applications for dense 3D mapping and reconstruction.","Authors (format: First Last, First Middle Last, ...)":"Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, Simon Lucey","Bibtex (e.g. @inproceedings...)":"@inproceedings{lin2021barf,\n    url = {http://arxiv.org/abs/2104.06405v2},\n    year = {2021},\n    title = {BARF: Bundle-Adjusting Neural Radiance Fields},\n    author = {Chen-Hsuan Lin and Wei-Chiu Ma and Antonio Torralba and Simon Lucey},\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    entrytype = {inproceedings},\n    id = {lin2021barf}\n}","Bibtex Name":"lin2021barf","Citation Count":"2","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/chenhsuanlin/bundle-adjusting-NeRF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/13/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Camera Parameter Estimation, Coarse-to-Fine","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"BARF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.06405.pdf","Project webpage link":"https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:34","Title":"BARF: Bundle-Adjusting Neural Radiance Fields","Training time (hr)":"","UID":"161","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In this paper, we propose StereoPIFu, which integrates the geometric constraints of stereo vision with implicit function representation of PIFu, to recover the 3D shape of the clothed human from a pair of low-cost rectified images. First, we introduce the effective voxel-aligned features from a stereo vision-based network to enable depth-aware reconstruction. Moreover, the novel relative z-offset is employed to associate predicted high-fidelity human depth and occupancy inference, which helps restore fine-level surface details. Second, a network structure that fully utilizes the geometry information from the stereo images is designed to improve the human body reconstruction quality. Consequently, our StereoPIFu can naturally infer the human body's spatial location in camera space and maintain the correct relative position of different parts of the human body, which enables our method to capture human performance. Compared with previous works, our StereoPIFu significantly improves the robustness, completeness, and accuracy of the clothed human reconstruction, which is demonstrated by extensive experimental results.","Authors (format: First Last, First Middle Last, ...)":"Yang Hong, Juyong Zhang, Boyi Jiang, Yudong Guo, Ligang Liu, Hujun Bao","Bibtex (e.g. @inproceedings...)":"@inproceedings{hong2021stereopifu,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Yang Hong and Juyong Zhang and Boyi Jiang and Yudong Guo and Ligang Liu and Hujun Bao},\n    title = {StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.05289v2},\n    entrytype = {inproceedings},\n    id = {hong2021stereopifu}\n}","Bibtex Name":"hong2021stereopifu","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/CrisHY1995/StereoPIFu_Code","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/12/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"Yes","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Data-Driven Method, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"StereoPIFu","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.05289.pdf","Project webpage link":"https://hy1995.top/StereoPIFuProject/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/17/2021 14:04","Title":"StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision","Training time (hr)":"","UID":"160","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present an approach for compressing volumetric scalar fields using implicit neural representations. Our approach represents a scalar field as a learned function, wherein a neural network maps a point in the domain to an output scalar value. By setting the number of weights of the neural network to be smaller than the input size, we achieve compressed representations of scalar fields, thus framing compression as a type of function approximation. Combined with carefully quantizing network weights, we show that this approach yields highly compact representations that outperform state-of-the-art volume compression approaches. The conceptual simplicity of our approach enables a number of benefits, such as support for time-varying scalar fields, optimizing to preserve spatial gradients, and random-access field evaluation. We study the impact of network design choices on compression performance, highlighting how simple network architectures are effective for a broad range of volumes.","Authors (format: First Last, First Middle Last, ...)":"Yuzhe Lu, Kairong Jiang, Joshua A. Levine, Matthew Berger","Bibtex (e.g. @inproceedings...)":"@article{lu2021compressive,\n    author = {Yuzhe Lu and Kairong Jiang and Joshua A. Levine and Matthew Berger},\n    title = {Compressive Neural Representations of Volumetric Scalar Fields},\n    year = {2021},\n    month = {Apr},\n    url = {http://arxiv.org/abs/2104.04523v1},\n    entrytype = {article},\n    id = {lu2021compressive}\n}","Bibtex Name":"lu2021compressive","Citation Count":"1","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/11/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Sinusoidal Activation (SIREN)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Compression, Fundamentals","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.04523.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:33","Title":"Compressive Neural Representations of Volumetric Scalar Fields","Training time (hr)":"","UID":"159","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"EuroVis 2021","Venue no Year":"EuroVis","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance field have shown amazing image synthesis results, but the underlying geometry representation is only a coarse approximation of the real geometry. We demonstrate how depth measurements can be incorporated into the radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone. In contrast to a density field as the underlying geometry representation, we propose to learn a deep neural network which stores a truncated signed distance field. Using this representation, we show that one can still leverage differentiable volume rendering to estimate color values of the observed images during training to compute a reconstruction loss. This is beneficial for learning the signed distance field in regions with missing depth measurements. Furthermore, we correct misalignment errors of the camera, improving the overall reconstruction quality. In several experiments, we showcase our method and compare to existing works on classical RGB-D fusion and learned representations.","Authors (format: First Last, First Middle Last, ...)":"Dejan Azinovi\u0107, Ricardo Martin-Brualla, Dan B Goldman, Matthias Nie\u00dfner, Justus Thies","Bibtex (e.g. @inproceedings...)":"@article{azinovic2021neural,\n    journal = {arXiv preprint arXiv:2104.04532},\n    booktitle = {ArXiv Pre-print},\n    author = {Dejan Azinovic and Ricardo Martin-Brualla and Dan B Goldman and Matthias Niessner and Justus Thies},\n    title = {Neural RGB-D Surface Reconstruction},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.04532v1},\n    entrytype = {article},\n    id = {azinovic2021neural}\n}","Bibtex Name":"azinovic2021neural","Citation Count":"1","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/9/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Camera Parameter Estimation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.04532.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:34","Title":"Neural RGB-D Surface Reconstruction","Training time (hr)":"","UID":"158","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Telecommunication with photorealistic avatars in virtual or augmented reality is a promising path for achieving authentic face-to-face communication in 3D over remote physical distances. In this work, we present the Pixel Codec Avatars (PiCA): a deep generative model of 3D human faces that achieves state of the art reconstruction performance while being computationally efficient and adaptive to the rendering conditions during execution. Our model combines two core ideas: (1) a fully convolutional architecture for decoding spatially varying features, and (2) a rendering-adaptive per-pixel decoder. Both techniques are integrated via a dense surface representation that is learned in a weakly-supervised manner from low-topology mesh tracking over training images. We demonstrate that PiCA improves reconstruction over existing techniques across testing expressions and views on persons of different gender and skin tone. Importantly, we show that the PiCA model is much smaller than the state-of-art baseline model, and makes multi-person telecommunicaiton possible: on a single Oculus Quest 2 mobile VR headset, 5 avatars are rendered in realtime in the same scene.","Authors (format: First Last, First Middle Last, ...)":"Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang, Yuecheng Li, Fernando De La Torre, Yaser Sheikh","Bibtex (e.g. @inproceedings...)":"@inproceedings{ma2021pica,\n    url = {http://arxiv.org/abs/2104.04638v1},\n    year = {2021},\n    title = {Pixel Codec Avatars},\n    author = {Shugao Ma and Tomas Simon and Jason Saragih and Dawei Wang and Yuecheng Li and Fernando De La Torre and Yaser Sheikh},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    entrytype = {inproceedings},\n    id = {ma2021pica}\n}","Bibtex Name":"ma2021pica","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/9/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Head), Generalization, 2D Image Neural Fields, Data-Driven Method, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"PiCA","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.04638.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/4/2021 22:11","Title":"Pixel Codec Avatars","Training time (hr)":"","UID":"157","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a relocalization pipeline, which combines an absolute pose regression (APR) network with a novel view synthesis based direct matching module, offering superior accuracy while maintaining low inference time. Our contribution is twofold: i) we design a direct matching module that supplies a photometric supervision signal to refine the pose regression network via differentiable rendering; ii) we modify the rotation representation from the classical quaternion to SO(3) in pose regression, removing the need for balancing rotation and translation loss terms. As a result, our network Direct-PoseNet achieves state-of-the-art performance among all other single-image APR methods on the 7-Scenes benchmark and the LLFF dataset.","Authors (format: First Last, First Middle Last, ...)":"Shuai Chen, Zirui Wang, Victor Prisacariu","Bibtex (e.g. @inproceedings...)":"@article{chen2021directposenet,\n    journal = {arXiv preprint arXiv:2104.04073},\n    booktitle = {ArXiv Pre-print},\n    author = {Shuai Chen and Zirui Wang and Victor Prisacariu},\n    title = {Direct-PoseNet: Absolute Pose Regression with Photometric Consistency},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.04073v1},\n    entrytype = {article},\n    id = {chen2021directposenet}\n}","Bibtex Name":"chen2021directposenet","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/8/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Camera Parameter Estimation, Coarse-to-Fine","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Direct-PoseNet","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.04073.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:33","Title":"Direct-PoseNet: Absolute Pose Regression with Photometric Consistency","Training time (hr)":"","UID":"156","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural implicit surface representations have emerged as a promising paradigm to capture 3D shapes in a continuous and resolution-independent manner. However, adapting them to articulated shapes is non-trivial. Existing approaches learn a backward warp field that maps deformed to canonical points. However, this is problematic since the backward warp field is pose dependent and thus requires large amounts of data to learn. To address this, we introduce SNARF, which combines the advantages of linear blend skinning (LBS) for polygonal meshes with those of neural implicit surfaces by learning a forward deformation field without direct supervision. This deformation field is defined in canonical, pose-independent space, allowing for generalization to unseen poses. Learning the deformation field from posed meshes alone is challenging since the correspondences of deformed points are defined implicitly and may not be unique under changes of topology. We propose a forward skinning model that finds all canonical correspondences of any deformed point using iterative root finding. We derive analytical gradients via implicit differentiation, enabling end-to-end training from 3D meshes with bone transformations. Compared to state-of-the-art neural implicit representations, our approach generalizes better to unseen poses while preserving accuracy. We demonstrate our method in challenging scenarios on (clothed) 3D humans in diverse and unseen poses.","Authors (format: First Last, First Middle Last, ...)":"Xu Chen, Yufeng Zheng, Michael J. Black, Otmar Hilliges, Andreas Geiger","Bibtex (e.g. @inproceedings...)":"@inproceedings{chen2021snarf,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Xu Chen and Yufeng Zheng and Michael J. Black and Otmar Hilliges and Andreas Geiger},\n    title = {SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.03953v1},\n    entrytype = {inproceedings},\n    id = {chen2021snarf}\n}","Bibtex Name":"chen2021snarf","Citation Count":"1","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/8/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SNARF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.03953.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:35","Title":"SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes","Training time (hr)":"","UID":"155","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Multi-Layer Perceptrons (MLPs) make powerful functional representations for sampling and reconstruction problems involving low-dimensional signals like images,shapes and light fields. Recent works have significantly improved their ability to represent high-frequency content by using periodic activations or positional encodings. This often came at the expense of generalization: modern methods are typically optimized for a single signal. We present a new representation that generalizes to multiple instances and achieves state-of-the-art fidelity. We use a dual-MLP architecture to encode the signals. A synthesis network creates a functional mapping from a low-dimensional input (e.g. pixel-position) to the output domain (e.g. RGB color). A modulation network maps a latent code corresponding to the target signal to parameters that modulate the periodic activations of the synthesis network. We also propose a local-functional representation which enables generalization. The signal's domain is partitioned into a regular grid,with each tile represented by a latent code. At test time, the signal is encoded with high-fidelity by inferring (or directly optimizing) the latent code-book. Our approach produces generalizable functional representations of images, videos and shapes, and achieves higher reconstruction quality than prior works that are optimized for a single signal.","Authors (format: First Last, First Middle Last, ...)":"Ishit Mehta, Micha\u00ebl Gharbi, Connelly Barnes, Eli Shechtman, Ravi Ramamoorthi, Manmohan Chandraker","Bibtex (e.g. @inproceedings...)":"@inproceedings{mehta2021modulated,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Ishit Mehta and Michael Gharbi and Connelly Barnes and Eli Shechtman and Ravi Ramamoorthi and Manmohan Chandraker},\n    title = {Modulated Periodic Activations for Generalizable Local Functional Representations},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.03960v1},\n    entrytype = {inproceedings},\n    id = {mehta2021modulated}\n}","Bibtex Name":"mehta2021modulated","Citation Count":"1","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/8/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Other","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Compression, Fundamentals, Global Conditioning, Hypernetwork/Meta-learning, Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.03960.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:35","Title":"Modulated Periodic Activations for Generalizable Local Functional Representations","Training time (hr)":"","UID":"154","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present Neural Articulated Radiance Field (NARF), a novel deformable 3D representation for articulated objects learned from images. While recent advances in 3D implicit representation have made it possible to learn models of complex objects, learning pose-controllable representations of articulated objects remains a challenge, as current methods require 3D shape supervision and are unable to render appearance. In formulating an implicit representation of 3D articulated objects, our method considers only the rigid transformation of the most relevant object part in solving for the radiance field at each 3D location. In this way, the proposed method represents pose-dependent changes without significantly increasing the computational complexity. NARF is fully differentiable and can be trained from images with pose annotations. Moreover, through the use of an autoencoder, it can learn appearance variations over multiple instances of an object class. Experiments show that the proposed method is efficient and can generalize well to novel poses. The code is available for research purposes at https://github.com/nogu-atsu/NARF","Authors (format: First Last, First Middle Last, ...)":"Atsuhiro Noguchi, Xiao Sun, Stephen Lin, Tatsuya Harada","Bibtex (e.g. @inproceedings...)":"@inproceedings{noguchi2021narf,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Atsuhiro Noguchi and Xiao Sun and Stephen Lin and Tatsuya Harada},\n    title = {Neural Articulated Radiance Field},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.03110v2},\n    entrytype = {inproceedings},\n    id = {noguchi2021narf}\n}","Bibtex Name":"noguchi2021narf","Citation Count":"2","Code Release (Github link, or enter \"Coming soon\")":"https://arxiv.org/pdf/2104.03110.pdf","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/7/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NARF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.03110.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:34","Title":"Neural Articulated Radiance Field","Training time (hr)":"","UID":"153","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present SCANimate, an end-to-end trainable framework that takes raw 3D scans of a clothed human and turns them into an animatable avatar. These avatars are driven by pose parameters and have realistic clothing that moves and deforms naturally. SCANimate does not rely on a customized mesh template or surface mesh registration. We observe that fitting a parametric 3D body model, like SMPL, to a clothed human scan is tractable while surface registration of the body topology to the scan is often not, because clothing can deviate significantly from the body shape. We also observe that articulated transformations are invertible, resulting in geometric cycle consistency in the posed and unposed shapes. These observations lead us to a weakly supervised learning method that aligns scans into a canonical pose by disentangling articulated deformations without template-based surface registration. Furthermore, to complete missing regions in the aligned scans while modeling pose-dependent deformations, we introduce a locally pose-aware implicit function that learns to complete and model geometry with learned pose correctives. In contrast to commonly used global pose embeddings, our local pose conditioning significantly reduces long-range spurious correlations and improves generalization to unseen poses, especially when training data is limited. Our method can be applied to pose-aware appearance modeling to generate a fully textured avatar. We demonstrate our approach on various clothing types with different amounts of training data, outperforming existing solutions and other variants in terms of fidelity and generality in every setting. The code is available at https://scanimate.is.tue.mpg.de.","Authors (format: First Last, First Middle Last, ...)":"Shunsuke Saito, Jinlong Yang, Qianli Ma, Michael J. Black","Bibtex (e.g. @inproceedings...)":"@inproceedings{saito2021scanimate,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Shunsuke Saito and Jinlong Yang and Qianli Ma and Michael J. Black},\n    title = {SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.03313v2},\n    entrytype = {inproceedings},\n    id = {saito2021scanimate}\n}","Bibtex Name":"saito2021scanimate","Citation Count":"7","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/shunsukesaito/SCANimate","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/7/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Human (Body)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SCANimate","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.03313.pdf","Project webpage link":"https://scanimate.is.tue.mpg.de","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=ohavL55Oznw","Timestamp":"7/19/2021 22:03","Title":"SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks","Training time (hr)":"","UID":"152","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Photo-realistic neural reconstruction and rendering of the human portrait are critical for numerous VR/AR applications. Still, existing solutions inherently rely on multi-view capture settings, and the one-shot solution to get rid of the tedious multi-view synchronization and calibration remains extremely challenging. In this paper, we propose MirrorNeRF - a one-shot neural portrait free-viewpoint rendering approach using a catadioptric imaging system with multiple sphere mirrors and a single high-resolution digital camera, which is the first to combine neural radiance field with catadioptric imaging so as to enable one-shot photo-realistic human portrait reconstruction and rendering, in a low-cost and casual capture setting. More specifically, we propose a light-weight catadioptric system design with a sphere mirror array to enable diverse ray sampling in the continuous 3D space as well as an effective online calibration for the camera and the mirror array. Our catadioptric imaging system can be easily deployed with a low budget and the casual capture ability for convenient daily usages. We introduce a novel neural warping radiance field representation to learn a continuous displacement field that implicitly compensates for the misalignment due to our flexible system setting. We further propose a density regularization scheme to leverage the inherent geometry information from the catadioptric data in a self-supervision manner, which not only improves the training efficiency but also provides more effective density supervision for higher rendering quality. Extensive experiments demonstrate the effectiveness and robustness of our scheme to achieve one-shot photo-realistic and high-quality appearance free-viewpoint rendering for human portrait scenes.","Authors (format: First Last, First Middle Last, ...)":"Ziyu Wang, Liao Wang, Fuqiang Zhao, Minye Wu, Lan Xu, Jingyi Yu","Bibtex (e.g. @inproceedings...)":"@article{wang2021mirrornerf,\n    journal = {arXiv preprint arXiv:2104.02607},\n    booktitle = {ArXiv Pre-print},\n    author = {Ziyu Wang and Liao Wang and Fuqiang Zhao and Minye Wu and Lan Xu and Jingyi Yu},\n    title = {MirrorNeRF: One-shot Neural Portrait Radiance Field from Multi-mirror Catadioptric Imaging},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.02607v2},\n    entrytype = {article},\n    id = {wang2021mirrornerf}\n}","Bibtex Name":"wang2021mirrornerf","Citation Count":"1","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/6/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Sparse Reconstruction","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"MirrorNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.02607.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:36","Title":"MirrorNeRF: One-shot Neural Portrait Radiance Field from Multi-mirror Catadioptric Imaging","Training time (hr)":"","UID":"151","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches however fall short in two ways: first, they may lack an underlying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks ($\\pi$-GAN or pi-GAN), for high-quality 3D-aware image synthesis. $\\pi$-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent 3D representations with fine detail. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets.","Authors (format: First Last, First Middle Last, ...)":"Eric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, Gordon Wetzstein","Bibtex (e.g. @inproceedings...)":"@inproceedings{chan2021pigan,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Eric R. Chan and Marco Monteiro and Petr Kellnhofer and Jiajun Wu and Gordon Wetzstein},\n    title = {pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis},\n    year = {2021},\n    url = {http://arxiv.org/abs/2012.00926v2},\n    entrytype = {inproceedings},\n    id = {chan2021pigan}\n}","Bibtex Name":"chan2021pigan","Citation Count":"20","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/marcoamonteiro/pi-GAN","Coordinates all at once":"","Data Release (link)":"Coming soon","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/5/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Sinusoidal Activation (SIREN)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Generative Models, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"pi-GAN","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2012.00926.pdf","Project webpage link":"https://marcoamonteiro.github.io/pi-GAN-website/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=0HCdof9BGtw","Timestamp":"5/23/2021 18:18","Title":"pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis","Training time (hr)":"","UID":"150","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Photo-realistic modeling and rendering of fuzzy objects with complex opacity are critical for numerous immersive VR/AR applications, but it suffers from strong view-dependent brightness, color. In this paper, we propose a novel scheme to generate opacity radiance fields with a convolutional neural renderer for fuzzy objects, which is the first to combine both explicit opacity supervision and convolutional mechanism into the neural radiance field framework so as to enable high-quality appearance and global consistent alpha mattes generation in arbitrary novel views. More specifically, we propose an efficient sampling strategy along with both the camera rays and image plane, which enables efficient radiance field sampling and learning in a patch-wise manner, as well as a novel volumetric feature integration scheme that generates per-patch hybrid feature embeddings to reconstruct the view-consistent fine-detailed appearance and opacity output. We further adopt a patch-wise adversarial training scheme to preserve both high-frequency appearance and opacity details in a self-supervised framework. We also introduce an effective multi-view image capture system to capture high-quality color and alpha maps for challenging fuzzy objects. Extensive experiments on existing and our new challenging fuzzy object dataset demonstrate that our method achieves photo-realistic, globally consistent, and fined detailed appearance and opacity free-viewpoint rendering for various fuzzy objects.","Authors (format: First Last, First Middle Last, ...)":"Haimin Luo, Anpei Chen, Qixuan Zhang, Bai Pang, Minye Wu, Lan Xu, Jingyi Yu","Bibtex (e.g. @inproceedings...)":"@article{luo2021convolutional,\n    author = {Haimin Luo and Anpei Chen and Qixuan Zhang and Bai Pang and Minye Wu and Lan Xu and Jingyi Yu},\n    title = {Convolutional Neural Opacity Radiance Fields},\n    year = {2021},\n    month = {Apr},\n    url = {http://arxiv.org/abs/2104.01772v1},\n    entrytype = {article},\n    id = {luo2021convolutional}\n}","Bibtex Name":"luo2021convolutional","Citation Count":"2","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/5/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Generative Models, Sampling","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.01772.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:35","Title":"Convolutional Neural Opacity Radiance Fields","Training time (hr)":"","UID":"149","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCP 2021","Venue no Year":"ICCP","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Grasp detection in clutter requires the robot to reason about the 3D scene from incomplete and noisy perception. In this work, we draw insight that 3D reconstruction and grasp learning are two intimately connected tasks, both of which require a fine-grained understanding of local geometry details. We thus propose to utilize the synergies between grasp affordance and 3D reconstruction through multi-task learning of a shared representation. Our model takes advantage of deep implicit functions, a continuous and memory-efficient representation, to enable differentiable training of both tasks. We train the model on self-supervised grasp trials data in simulation. Evaluation is conducted on a clutter removal task, where the robot clears cluttered objects by grasping them one at a time. The experimental results in simulation and on the real robot have demonstrated that the use of implicit neural representations and joint learning of grasp affordance and 3D reconstruction have led to state-of-the-art grasping results. Our method outperforms baselines by over 10% in terms of grasp success rate. Additional results and videos can be found at https://sites.google.com/view/rpl-giga2021","Authors (format: First Last, First Middle Last, ...)":"Zhenyu Jiang, Yifeng Zhu, Maxwell Svetlik, Kuan Fang, Yuke Zhu","Bibtex (e.g. @inproceedings...)":"@inproceedings{jiang2021giga,\n    booktitle = {Proceedings of Robotics: Science and Systems},\n    author = {Zhenyu Jiang and Yifeng Zhu and Maxwell Svetlik and Kuan Fang and Yuke Zhu},\n    title = {Synergies Between Affordance and Geometry: 6-DoF Grasp Detection via Implicit Representations},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.01542v2},\n    entrytype = {inproceedings},\n    id = {jiang2021giga}\n}","Bibtex Name":"jiang2021giga","Citation Count":"1","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/UT-Austin-RPL/GIGA","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/4/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Science & Engineering, Robotics, Voxel Grid, Data-Driven Method, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"GIGA","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.01542.pdf","Project webpage link":"https://sites.google.com/view/rpl-giga2021","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:38","Title":"Synergies Between Affordance and Geometry: 6-DoF Grasp Detection via Implicit Representations","Training time (hr)":"","UID":"148","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"RSS 2021","Venue no Year":"RSS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present ObSuRF, a method which turns a single image of a scene into a 3D model represented as a set of Neural Radiance Fields (NeRFs), with each NeRF corresponding to a different object. A single forward pass of an encoder network outputs a set of latent vectors describing the objects in the scene. These vectors are used independently to condition a NeRF decoder, defining the geometry and appearance of each object. We make learning more computationally efficient by deriving a novel loss, which allows training NeRFs on RGB-D inputs without explicit ray marching. After confirming that the model performs equal or better than state of the art on three 2D image segmentation benchmarks, we apply it to two multi-object 3D datasets: A multiview version of CLEVR, and a novel dataset in which scenes are populated by ShapeNet models. We find that after training ObSuRF on RGB-D views of training scenes, it is capable of not only recovering the 3D geometry of a scene depicted in a single input image, but also to segment it into objects, despite receiving no supervision in that regard.","Authors (format: First Last, First Middle Last, ...)":"Karl Stelzner, Kristian Kersting, Adam R. Kosiorek","Bibtex (e.g. @inproceedings...)":"@article{stelzner2021obsurf,\n    journal = {arXiv preprint arXiv:2104.01148},\n    booktitle = {ArXiv Pre-print},\n    author = {Karl Stelzner and Kristian Kersting and Adam R. Kosiorek},\n    title = {Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.01148v1},\n    entrytype = {article},\n    id = {stelzner2021obsurf}\n}","Bibtex Name":"stelzner2021obsurf","Citation Count":"1","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"Coming soon","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/2/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Object-Centric, Hybrid Geometry Representation, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"OBSuRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.01148.pdf","Project webpage link":"https://stelzner.github.io/obsurf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:37","Title":"Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation","Training time (hr)":"","UID":"147","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Parametric 3D models have enabled a wide variety of tasks in computer graphics and vision, such as modeling human bodies, faces, and hands. However, the construction of these parametric models is often tedious, as it requires heavy manual tweaking, and they struggle to represent additional complexity and details such as wrinkles or clothing. To this end, we propose Neural Parametric Models (NPMs), a novel, learned alternative to traditional, parametric 3D models, which does not require hand-crafted, object-specific constraints. In particular, we learn to disentangle 4D dynamics into latent-space representations of shape and pose, leveraging the flexibility of recent developments in learned implicit functions. Crucially, once learned, our neural parametric models of shape and pose enable optimization over the learned spaces to fit to new observations, similar to the fitting of a traditional parametric model, e.g., SMPL. This enables NPMs to achieve a significantly more accurate and detailed representation of observed deformable sequences. We show that NPMs improve notably over both parametric and non-parametric state of the art in reconstruction and tracking of monocular depth sequences of clothed humans and hands. Latent-space interpolation as well as shape/pose transfer experiments further demonstrate the usefulness of NPMs. Code is publicly available at https://pablopalafox.github.io/npms.","Authors (format: First Last, First Middle Last, ...)":"Pablo Palafox, Alja\u017e Bo\u017ei\u010d, Justus Thies, Matthias Nie\u00dfner, Angela Dai","Bibtex (e.g. @inproceedings...)":"@inproceedings{palafox2021npms,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Pablo Palafox and Aljaz Bozic and Justus Thies and Matthias Niessner and Angela Dai},\n    title = {NPMs: Neural Parametric Models for 3D Deformable Shapes},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.00702v2},\n    entrytype = {inproceedings},\n    id = {palafox2021npms}\n}","Bibtex Name":"palafox2021npms","Citation Count":"2","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/1/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NPMs","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.00702.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:37","Title":"NPMs: Neural Parametric Models for 3D Deformable Shapes","Training time (hr)":"","UID":"146","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Majority of the perception methods in robotics require depth information provided by RGB-D cameras. However, standard 3D sensors fail to capture depth of transparent objects due to refraction and absorption of light. In this paper, we introduce a new approach for depth completion of transparent objects from a single RGB-D image. Key to our approach is a local implicit neural representation built on ray-voxel pairs that allows our method to generalize to unseen objects and achieve fast inference speed. Based on this representation, we present a novel framework that can complete missing depth given noisy RGB-D input. We further improve the depth estimation iteratively using a self-correcting refinement model. To train the whole pipeline, we build a large scale synthetic dataset with transparent objects. Experiments demonstrate that our method performs significantly better than the current state-of-the-art methods on both synthetic and real world data. In addition, our approach improves the inference speed by a factor of 20 compared to the previous best method, ClearGrasp. Code and dataset will be released at https://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit.","Authors (format: First Last, First Middle Last, ...)":"Luyang Zhu, Arsalan Mousavian, Yu Xiang, Hammad Mazhar, Jozef van Eenbergen, Shoubhik Debnath, Dieter Fox","Bibtex (e.g. @inproceedings...)":"@inproceedings{zhu2021rgbd,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Luyang Zhu and Arsalan Mousavian and Yu Xiang and Hammad Mazhar and Jozef van Eenbergen and Shoubhik Debnath and Dieter Fox},\n    title = {RGB-D Local Implicit Function for Depth Completion of Transparent Objects},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.00622v1},\n    entrytype = {inproceedings},\n    id = {zhu2021rgbd}\n}","Bibtex Name":"zhu2021rgbd","Citation Count":"1","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/NVlabs/implicit_depth","Coordinates all at once":"","Data Release (link)":"Coming soon","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/1/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Voxel Grid, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.00622.pdf","Project webpage link":"https://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:39","Title":"RGB-D Local Implicit Function for Depth Completion of Transparent Objects","Training time (hr)":"","UID":"145","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present PhySG, an end-to-end inverse rendering pipeline that includes a fully differentiable renderer and can reconstruct geometry, materials, and illumination from scratch from a set of RGB input images. Our framework represents specular BRDFs and environmental illumination using mixtures of spherical Gaussians, and represents geometry as a signed distance function parameterized as a Multi-Layer Perceptron. The use of spherical Gaussians allows us to efficiently solve for approximate light transport, and our method works on scenes with challenging non-Lambertian reflectance captured under natural, static illumination. We demonstrate, with both synthetic and real data, that our reconstructions not only enable rendering of novel viewpoints, but also physics-based appearance editing of materials and illumination.","Authors (format: First Last, First Middle Last, ...)":"Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, Noah Snavely","Bibtex (e.g. @inproceedings...)":"@inproceedings{zhang2021physg,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Kai Zhang and Fujun Luan and Qianqian Wang and Kavita Bala and Noah Snavely},\n    title = {PhySG: Inverse Rendering with Spherical Gaussians for Physics-based Material Editing and Relighting},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.00674v1},\n    entrytype = {inproceedings},\n    id = {zhang2021physg}\n}","Bibtex Name":"zhang2021physg","Citation Count":"2","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/Kai-46/PhySG","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/1/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Material/Lighting Estimation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"PhySG","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.00674.pdf","Project webpage link":"https://kai-46.github.io/PhySG-website/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/25/2021 14:56","Title":"PhySG: Inverse Rendering with Spherical Gaussians for Physics-based Material Editing and Relighting","Training time (hr)":"","UID":"144","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose NeRF-VAE, a 3D scene generative model that incorporates geometric structure via NeRF and differentiable volume rendering. In contrast to NeRF, our model takes into account shared structure across scenes, and is able to infer the structure of a novel scene -- without the need to re-train -- using amortized inference. NeRF-VAE's explicit 3D rendering process further contrasts previous generative models with convolution-based rendering which lacks geometric structure. Our model is a VAE that learns a distribution over radiance fields by conditioning them on a latent scene representation. We show that, once trained, NeRF-VAE is able to infer and render geometrically-consistent scenes from previously unseen 3D environments using very few input images. We further demonstrate that NeRF-VAE generalizes well to out-of-distribution cameras, while convolutional models do not. Finally, we introduce and study an attention-based conditioning mechanism of NeRF-VAE's decoder, which improves model performance.","Authors (format: First Last, First Middle Last, ...)":"Adam R. Kosiorek, Heiko Strathmann, Daniel Zoran, Pol Moreno, Rosalia Schneider, So\u0148a Mokr\u00e1, Danilo J. Rezende","Bibtex (e.g. @inproceedings...)":"@article{kosiorek2021nerfvae,\n    journal = {arXiv preprint arXiv:2104.00587},\n    booktitle = {ArXiv Pre-print},\n    author = {Adam R. Kosiorek and Heiko Strathmann and Daniel Zoran and Pol Moreno and Rosalia Schneider and Sona Mokra and Danilo J. Rezende},\n    title = {NeRF-VAE: A Geometry Aware 3D Scene Generative Model},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.00587v1},\n    entrytype = {article},\n    id = {kosiorek2021nerfvae}\n}","Bibtex Name":"kosiorek2021nerfvae","Citation Count":"6","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/1/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Sparse Reconstruction, Generalization, Generative Models, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeRF-VAE","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.00587.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=f-T3BLVuXkY","Timestamp":"5/23/2021 18:39","Title":"NeRF-VAE: A Geometry Aware 3D Scene Generative Model","Training time (hr)":"","UID":"143","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We tackle the challenge of learning a distribution over complex, realistic, indoor scenes. In this paper, we introduce Generative Scene Networks (GSN), which learns to decompose scenes into a collection of many local radiance fields that can be rendered from a free moving camera. Our model can be used as a prior to generate new scenes, or to complete a scene given only sparse 2D observations. Recent work has shown that generative models of radiance fields can capture properties such as multi-view consistency and view-dependent lighting. However, these models are specialized for constrained viewing of single objects, such as cars or faces. Due to the size and complexity of realistic indoor environments, existing models lack the representational capacity to adequately capture them. Our decomposition scheme scales to larger and more complex scenes while preserving details and diversity, and the learned prior enables high-quality rendering from viewpoints that are significantly different from observed viewpoints. When compared to existing models, GSN produces quantitatively higher-quality scene renderings across several different scene datasets.","Authors (format: First Last, First Middle Last, ...)":"Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, Joshua M. Susskind","Bibtex (e.g. @inproceedings...)":"@inproceedings{devries2021unconstrained,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Terrance DeVries and Miguel Angel Bautista and Nitish Srivastava and Graham W. Taylor and Joshua M. Susskind},\n    title = {Unconstrained Scene Generation with Locally Conditioned Radiance Fields},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.00670v1},\n    entrytype = {inproceedings},\n    id = {devries2021unconstrained}\n}","Bibtex Name":"devries2021unconstrained","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/apple/ml-gsn","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/1/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generative Models","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.00670.pdf","Project webpage link":"https://apple.github.io/ml-gsn/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:40","Title":"Unconstrained Scene Generation with Locally Conditioned Radiance Fields","Training time (hr)":"","UID":"142","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present DietNeRF, a 3D neural scene representation estimated from a few images. Neural Radiance Fields (NeRF) learn a continuous volumetric representation of a scene through multi-view consistency, and can be rendered from novel viewpoints by ray casting. While NeRF has an impressive ability to reconstruct geometry and fine details given many images, up to 100 for challenging 360{\\deg} scenes, it often finds a degenerate solution to its image reconstruction objective when only a few input views are available. To improve few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic consistency loss that encourages realistic renderings at novel poses. DietNeRF is trained on individual scenes to (1) correctly render given input views from the same pose, and (2) match high-level semantic attributes across different, random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary poses. We extract these semantics using a pre-trained visual encoder such as CLIP, a Vision Transformer trained on hundreds of millions of diverse single-view, 2D photographs mined from the web with natural language supervision. In experiments, DietNeRF improves the perceptual quality of few-shot view synthesis when learned from scratch, can render novel views with as few as one observed image when pre-trained on a multi-view dataset, and produces plausible completions of completely unobserved regions.","Authors (format: First Last, First Middle Last, ...)":"Ajay Jain, Matthew Tancik, Pieter Abbeel","Bibtex (e.g. @inproceedings...)":"@article{jain2021dietnerf,\n    journal = {arXiv preprint arXiv:2104.00677},\n    booktitle = {ArXiv Pre-print},\n    author = {Ajay Jain and Matthew Tancik and Pieter Abbeel},\n    title = {Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis},\n    year = {2021},\n    url = {http://arxiv.org/abs/2104.00677v1},\n    entrytype = {article},\n    id = {jain2021dietnerf}\n}","Bibtex Name":"jain2021dietnerf","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/codestella/putting-nerf-on-a-diet","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/1/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Sparse Reconstruction, Data-Driven Method, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DietNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.00677.pdf","Project webpage link":"https://www.ajayj.com/dietnerf","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=RF_3hsNizqw","Timestamp":"5/23/2021 18:41","Title":"Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis","Training time (hr)":"","UID":"141","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Getting representations of multiple objects or scenes is a raising research topic in Machine Learning (ML) community. Here, we propose a multi-scene representation model that can learn the representation of complex scenes and reconstruct them in high resolution given novel viewing directions. Our method represents a single scene with fully-connected layers. Each set of fully-connected layers are controlled by hyper-networks for multiple scenes modeling. For each scene, we take 3D coordinates (x, y, z) and 2D view-point orientations (I,, E,) as inputs. A set of fully-connected layers output volume density and RGB values at given 3D spatial positions. Then, we render the output volume density and RGB values along the camera rays into images using volume density rendering techniques. During training process, we optimize a continuous volume scene function with a small amount of input viewing directions. By designing versatile embedding module and multi-scene representation networks, our model can render photographic images with novel viewing directions for different complex scenes. Experiment results demonstrate the neural rendering and multi-scene representation abilities of our model. Several thorough experiments show that our method outperforms previous model on both reconstruction precision and scenes generation ability from novel viewing directions.","Authors (format: First Last, First Middle Last, ...)":"Bofeng Fu, Zheng Wang","Bibtex (e.g. @inproceedings...)":"@article{fu2021multiscene,\n    doi = {10.1088/1742-6596/1880/1/012034},\n    url = {https://doi.org/10.1088/1742-6596/1880/1/012034},\n    year = {2021},\n    publisher = {{IOP} Publishing},\n    volume = {1880},\n    number = {1},\n    pages = {012034},\n    author = {Bofeng Fu and Zheng Wang},\n    title = {Multi-scene Representation Learning with Neural Radiance Fields},\n    journal = {Journal of Physics: Conference Series},\n    entrytype = {article},\n    id = {fu2021multiscene}\n}","Bibtex Name":"fu2021multiscene","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/1/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://iopscience.iop.org/article/10.1088/1742-6596/1880/1/012034","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:26","Title":"Multi-scene Representation Learning with Neural Radiance Fields","Training time (hr)":"","UID":"140","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"Journal of Physics: Conference Series 2021","Venue no Year":"Journal of Physics: Conference Series","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a novel framework named NeuralRecon for real-time 3D scene reconstruction from a monocular video. Unlike previous methods that estimate single-view depth maps separately on each key-frame and fuse them later, we propose to directly reconstruct local surfaces represented as sparse TSDF volumes for each video fragment sequentially by a neural network. A learning-based TSDF fusion module based on gated recurrent units is used to guide the network to fuse features from previous fragments. This design allows the network to capture local smoothness prior and global shape prior of 3D surfaces when sequentially reconstructing the surfaces, resulting in accurate, coherent, and real-time surface reconstruction. The experiments on ScanNet and 7-Scenes datasets show that our system outperforms state-of-the-art methods in terms of both accuracy and speed. To the best of our knowledge, this is the first learning-based system that is able to reconstruct dense coherent 3D geometry in real-time.","Authors (format: First Last, First Middle Last, ...)":"Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, Hujun Bao","Bibtex (e.g. @inproceedings...)":"@inproceedings{sun2021neuralrecon,\n    url = {http://arxiv.org/abs/2104.00681v1},\n    year = {2021},\n    title = {NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video},\n    author = {Jiaming Sun and Yiming Xie and Linghao Chen and Xiaowei Zhou and Hujun Bao},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    entrytype = {inproceedings},\n    id = {sun2021neuralrecon}\n}","Bibtex Name":"sun2021neuralrecon","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/zju3dv/NeuralRecon","Coordinates all at once":"No","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/1/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"Yes","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"TSDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Robotics, SLAM, Hybrid Geometry Representation, Voxel Grid, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeuralRecon","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2104.00681.pdf","Project webpage link":"https://zju3dv.github.io/neuralrecon/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"https://zju3dv.github.io/neuralrecon/files/NeuralRecon-suppmat.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=wuMPaUTJuO0","Timestamp":"10/4/2021 19:39","Title":"NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video","Training time (hr)":"","UID":"139","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Tremendous progress in deep generative models has led to photorealistic image synthesis. While achieving compelling results, most approaches operate in the two-dimensional image domain, ignoring the three-dimensional nature of our world. Several recent works therefore propose generative models which are 3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably to the image plane. This leads to impressive 3D consistency, but incorporating such a bias comes at a price: the camera needs to be modeled as well. Current approaches assume fixed intrinsics and a predefined prior over camera pose ranges. As a result, parameter tuning is typically required for real-world data, and results degrade if the data distribution is not matched. Our key hypothesis is that learning a camera generator jointly with the image generator leads to a more principled approach to 3D-aware image synthesis. Further, we propose to decompose the scene into a background and foreground model, leading to more efficient and disentangled scene representations. While training from raw, unposed image collections, we learn a 3D- and camera-aware generative model which faithfully recovers not only the image but also the camera data distribution. At test time, our model generates images with explicit control over the camera as well as the shape and appearance of the scene.","Authors (format: First Last, First Middle Last, ...)":"Michael Niemeyer, Andreas Geiger","Bibtex (e.g. @inproceedings...)":"@article{niemeyer2021campari,\n    journal = {arXiv preprint arXiv:2103.17269},\n    booktitle = {ArXiv Pre-print},\n    author = {Michael Niemeyer and Andreas Geiger},\n    title = {CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.17269v1},\n    entrytype = {article},\n    id = {niemeyer2021campari}\n}","Bibtex Name":"niemeyer2021campari","Citation Count":"2","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/31/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generative Models, Data-Driven Method, Local Conditioning, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"CAMPARI","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.17269.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:43","Title":"CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields","Training time (hr)":"","UID":"138","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Maps are arguably one of the most fundamental concepts used to define and operate on manifold surfaces in differentiable geometry. Accordingly, in geometry processing, maps are ubiquitous and are used in many core applications, such as paramterization, shape analysis, remeshing, and deformation. Unfortunately, most computational representations of surface maps do not lend themselves to manipulation and optimization, usually entailing hard, discrete problems. While algorithms exist to solve these problems, they are problem-specific, and a general framework for surface maps is still in need. In this paper, we advocate considering neural networks as encoding surface maps. Since neural networks can be composed on one another and are differentiable, we show it is easy to use them to define surfaces via atlases, compose them for surface-to-surface mappings, and optimize differentiable objectives relating to them, such as any notion of distortion, in a trivial manner. In our experiments, we represent surfaces by generating a neural map that approximates a UV parameterization of a 3D model. Then, we compose this map with other neural maps which we optimize with respect to distortion measures. We show that our formulation enables trivial optimization of rather elusive mapping tasks, such as maps between a collection of surfaces.","Authors (format: First Last, First Middle Last, ...)":"Luca Morreale, Noam Aigerman, Vladimir Kim, Niloy J. Mitra","Bibtex (e.g. @inproceedings...)":"@inproceedings{morreale2021neural,\n    url = {http://arxiv.org/abs/2103.16942v1},\n    year = {2021},\n    title = {Neural Surface Maps},\n    author = {Luca Morreale and Noam Aigerman and Vladimir Kim and Niloy J. Mitra},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    entrytype = {inproceedings},\n    id = {morreale2021neural}\n}","Bibtex Name":"morreale2021neural","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/luca-morreale/neural_surface_maps","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/31/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.16942.pdf","Project webpage link":"http://geometry.cs.ucl.ac.uk/projects/2021/neuralmaps/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=DHkDCwapxc4","Timestamp":"10/8/2021 16:49","Title":"Neural Surface Maps","Training time (hr)":"","UID":"137","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Our goal is to learn a deep network that, given a small number of images of an object of a given category, reconstructs it in 3D. While several recent works have obtained analogous results using synthetic data or assuming the availability of 2D primitives such as keypoints, we are interested in working with challenging real data and with no manual annotations. We thus focus on learning a model from multiple views of a large collection of object instances. We contribute with a new large dataset of object centric videos suitable for training and benchmarking this class of models. We show that existing techniques leveraging meshes, voxels, or implicit surfaces, which work well for reconstructing isolated objects, fail on this challenging data. Finally, we propose a new neural network design, called warp-conditioned ray embedding (WCR), which significantly improves reconstruction while obtaining a detailed implicit representation of the object surface and texture, also compensating for the noise in the initial SfM reconstruction that bootstrapped the learning process. Our evaluation demonstrates performance improvements over several deep monocular reconstruction baselines on existing benchmarks and on our novel dataset.","Authors (format: First Last, First Middle Last, ...)":"Philipp Henzler, Jeremy Reizenstein, Patrick Labatut, Roman Shapovalov, Tobias Ritschel, Andrea Vedaldi, David Novotny","Bibtex (e.g. @inproceedings...)":"@inproceedings{henzler2021unsupervised,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Philipp Henzler and Jeremy Reizenstein and Patrick Labatut and Roman Shapovalov and Tobias Ritschel and Andrea Vedaldi and David Novotny},\n    title = {Unsupervised Learning of 3D Object Categories from Videos in the Wild},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.16552v1},\n    entrytype = {inproceedings},\n    id = {henzler2021unsupervised}\n}","Bibtex Name":"henzler2021unsupervised","Citation Count":"2","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/30/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Data-Driven Method, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.16552.pdf","Project webpage link":"https://henzler.github.io/publication/unsupervised_videos/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:41","Title":"Unsupervised Learning of 3D Object Categories from Videos in the Wild","Training time (hr)":"","UID":"136","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Traditional high-quality 3D graphics requires large volumes of fine-detailed scene data for rendering. This demand compromises computational efficiency and local storage resources. Specifically, it becomes more concerning for future wearable and portable virtual and augmented reality (VR/AR) displays. Recent approaches to combat this problem include remote rendering/streaming and neural representations of 3D assets. These approaches have redefined the traditional local storage-rendering pipeline by distributed computing or compression of large data. However, these methods typically suffer from high latency or low quality for practical visualization of large immersive virtual scenes, notably with extra high resolution and refresh rate requirements for VR applications such as gaming and design. Tailored for the future portable, low-storage, and energy-efficient VR platforms, we present the first gaze-contingent 3D neural representation and view synthesis method. We incorporate the human psychophysics of visual- and stereo-acuity into an egocentric neural representation of 3D scenery. Furthermore, we jointly optimize the latency/performance and visual quality, while mutually bridging human perception and neural scene synthesis, to achieve perceptually high-quality immersive interaction. Both objective analysis and subjective study demonstrate the effectiveness of our approach in significantly reducing local storage volume and synthesis latency (up to 99% reduction in both data size and computational time), while simultaneously presenting high-fidelity rendering, with perceptual quality identical to that of fully locally stored and rendered high-quality imagery.","Authors (format: First Last, First Middle Last, ...)":"Nianchen Deng, Zhenyi He, Jiannan Ye, Praneeth Chakravarthula, Xubo Yang, Qi Sun","Bibtex (e.g. @inproceedings...)":"@article{deng2021foveated,\n    journal = {arXiv preprint arXiv:2103.16365},\n    booktitle = {ArXiv Pre-print},\n    author = {Nianchen Deng and Zhenyi He and Jiannan Ye and Praneeth Chakravarthula and Xubo Yang and Qi Sun},\n    title = {Foveated Neural Radiance Fields for Real-Time and Egocentric Virtual Reality},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.16365v1},\n    entrytype = {article},\n    id = {deng2021foveated}\n}","Bibtex Name":"deng2021foveated","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/30/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Compression, Sampling, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.16365.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:42","Title":"Foveated Neural Radiance Fields for Real-Time and Egocentric Virtual Reality","Training time (hr)":"","UID":"135","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, but the same fully self-supervised approach is not possible for semantics because labels are human-defined properties. We extend neural radiance fields (NeRF) to jointly encode semantics with appearance and geometry, so that complete and accurate 2D semantic labels can be achieved using a small amount of in-place annotations specific to the scene. The intrinsic multi-view consistency and smoothness of NeRF benefit semantics by enabling sparse labels to efficiently propagate. We show the benefit of this approach when labels are either sparse or very noisy in room-scale scenes. We demonstrate its advantageous properties in various interesting applications such as an efficient scene labelling tool, novel semantic view synthesis, label denoising, super-resolution, label interpolation and multi-view semantic label fusion in visual semantic mapping systems.","Authors (format: First Last, First Middle Last, ...)":"Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, Andrew J. Davison","Bibtex (e.g. @inproceedings...)":"@inproceedings{zhi2021semanticnerf,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Shuaifeng Zhi and Tristan Laidlow and Stefan Leutenegger and Andrew J. Davison},\n    title = {In-Place Scene Labelling and Understanding with Implicit Scene Representation},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.15875v2},\n    entrytype = {inproceedings},\n    id = {zhi2021semanticnerf}\n}","Bibtex Name":"zhi2021semanticnerf","Citation Count":"1","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/29/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Semantic-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.15875.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/15/2021 16:07","Title":"In-Place Scene Labelling and Understanding with Implicit Scene Representation","Training time (hr)":"","UID":"134","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present MVSNeRF, a novel neural rendering approach that can efficiently reconstruct neural radiance fields for view synthesis. Unlike prior works on neural radiance fields that consider per-scene optimization on densely captured images, we propose a generic deep neural network that can reconstruct radiance fields from only three nearby input views via fast network inference. Our approach leverages plane-swept cost volumes (widely used in multi-view stereo) for geometry-aware scene reasoning, and combines this with physically based volume rendering for neural radiance field reconstruction. We train our network on real objects in the DTU dataset, and test it on three different datasets to evaluate its effectiveness and generalizability. Our approach can generalize across scenes (even indoor scenes, completely different from our training scenes of objects) and generate realistic view synthesis results using only three input images, significantly outperforming concurrent works on generalizable radiance field reconstruction. Moreover, if dense images are captured, our estimated radiance field representation can be easily fine-tuned; this leads to fast per-scene reconstruction with higher rendering quality and substantially less optimization time than NeRF.","Authors (format: First Last, First Middle Last, ...)":"Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, Hao Su","Bibtex (e.g. @inproceedings...)":"@inproceedings{chen2021mvsnerf,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Anpei Chen and Zexiang Xu and Fuqiang Zhao and Xiaoshuai Zhang and Fanbo Xiang and Jingyi Yu and Hao Su},\n    title = {MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.15595v2},\n    entrytype = {inproceedings},\n    id = {chen2021mvsnerf}\n}","Bibtex Name":"chen2021mvsnerf","Citation Count":"4","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/apchenstu/mvsnerf","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/29/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Sparse Reconstruction, Generalization, Data-Driven Method, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"MVSNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.15595.pdf","Project webpage link":"https://apchenstu.github.io/mvsnerf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=68N21TacPxw","Timestamp":"5/23/2021 18:42","Title":"MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo","Training time (hr)":"","UID":"133","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce GNeRF, a framework to marry Generative Adversarial Networks (GAN) with Neural Radiance Field (NeRF) reconstruction for the complex scenarios with unknown and even randomly initialized camera poses. Recent NeRF-based advances have gained popularity for remarkable realistic novel view synthesis. However, most of them heavily rely on accurate camera poses estimation, while few recent methods can only optimize the unknown camera poses in roughly forward-facing scenes with relatively short camera trajectories and require rough camera poses initialization. Differently, our GNeRF only utilizes randomly initialized poses for complex outside-in scenarios. We propose a novel two-phases end-to-end framework. The first phase takes the use of GANs into the new realm for optimizing coarse camera poses and radiance fields jointly, while the second phase refines them with additional photometric loss. We overcome local minima using a hybrid and iterative optimization scheme. Extensive experiments on a variety of synthetic and natural scenes demonstrate the effectiveness of GNeRF. More impressively, our approach outperforms the baselines favorably in those scenes with repeated patterns or even low textures that are regarded as extremely challenging before.","Authors (format: First Last, First Middle Last, ...)":"Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, Jingyi Yu","Bibtex (e.g. @inproceedings...)":"@article{meng2021gnerf,\n    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},\n    author = {Quan Meng and Anpei Chen and Haimin Luo and Minye Wu and Hao Su and Lan Xu and Xuming He and Jingyi Yu},\n    title = {GNeRF: GAN-based Neural Radiance Field without Posed Camera},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.15606v3},\n    entrytype = {article},\n    id = {meng2021gnerf}\n}","Bibtex Name":"meng2021gnerf","Citation Count":"1","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/MQ66/gnerf","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/29/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Camera Parameter Estimation, Generative Models","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"GNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.15606.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:43","Title":"GNeRF: GAN-based Neural Radiance Field without Posed Camera","Training time (hr)":"","UID":"132","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In this paper, we propose MINE to perform novel view synthesis and depth estimation via dense 3D reconstruction from a single image. Our approach is a continuous depth generalization of the Multiplane Images (MPI) by introducing the NEural radiance fields (NeRF). Given a single image as input, MINE predicts a 4-channel image (RGB and volume density) at arbitrary depth values to jointly reconstruct the camera frustum and fill in occluded contents. The reconstructed and inpainted frustum can then be easily rendered into novel RGB or depth views using differentiable rendering. Extensive experiments on RealEstate10K, KITTI and Flowers Light Fields show that our MINE outperforms state-of-the-art by a large margin in novel view synthesis. We also achieve competitive results in depth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our source code is available at https://github.com/vincentfung13/MINE","Authors (format: First Last, First Middle Last, ...)":"Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu Wang, Gim Hee Lee","Bibtex (e.g. @inproceedings...)":"@inproceedings{li2021mine,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Jiaxin Li and Zijian Feng and Qi She and Henghui Ding and Changhu Wang and Gim Hee Lee},\n    title = {MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.14910v3},\n    entrytype = {inproceedings},\n    id = {li2021mine}\n}","Bibtex Name":"li2021mine","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/vincentfung13/MINE","Coordinates all at once":"No","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/27/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct, Indirect","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"MPI","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Sparse Reconstruction, Generalization, Image-Based Rendering, Data-Driven Method, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"MINE","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.14910.pdf","Project webpage link":"https://vincentfung13.github.io/projects/mine/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/31/2021 16:19","Title":"MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis","Training time (hr)":"","UID":"131","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF's computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. \"bake\") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF's ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.","Authors (format: First Last, First Middle Last, ...)":"Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. Barron, Paul Debevec","Bibtex (e.g. @inproceedings...)":"@article{hedman2021snerg,\n    journal = {arXiv preprint arXiv:2103.14645},\n    booktitle = {ArXiv Pre-print},\n    author = {Peter Hedman and Pratul P. Srinivasan and Ben Mildenhall and Jonathan T. Barron and Paul Debevec},\n    title = {Baking Neural Radiance Fields for Real-Time View Synthesis},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.14645v1},\n    entrytype = {article},\n    id = {hedman2021snerg}\n}","Bibtex Name":"hedman2021snerg","Citation Count":"2","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/26/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Compression, Local Conditioning, Voxel Grid","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SNeRG","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.14645.pdf","Project webpage link":"https://phog.github.io/snerg/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=5jKry8n5YO8","Timestamp":"5/23/2021 18:46","Title":"Baking Neural Radiance Fields for Real-Time View Synthesis","Training time (hr)":"","UID":"130","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800x800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: https://alexyu.net/plenoctrees","Authors (format: First Last, First Middle Last, ...)":"Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, Angjoo Kanazawa","Bibtex (e.g. @inproceedings...)":"@inproceedings{yu2021nerfsh,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Alex Yu and Ruilong Li and Matthew Tancik and Hao Li and Ren Ng and Angjoo Kanazawa},\n    title = {PlenOctrees for Real-time Rendering of Neural Radiance Fields},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.14024v2},\n    entrytype = {inproceedings},\n    id = {yu2021nerfsh}\n}","Bibtex Name":"yu2021nerfsh","Citation Count":"12","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/sxyu/plenoctree, https://github.com/sxyu/volrend","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/25/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Material/Lighting Estimation, Sampling, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeRF-SH, PlenOctrees","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.14024.pdf","Project webpage link":"https://alexyu.net/plenoctrees/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/25/2021 15:13","Title":"PlenOctrees for Real-time Rendering of Neural Radiance Fields","Training time (hr)":"","UID":"129","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that real-time rendering is possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by three orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.","Authors (format: First Last, First Middle Last, ...)":"Christian Reiser, Songyou Peng, Yiyi Liao, Andreas Geiger","Bibtex (e.g. @inproceedings...)":"@inproceedings{reiser2021kilonerf,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Christian Reiser and Songyou Peng and Yiyi Liao and Andreas Geiger},\n    title = {KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.13744v2},\n    entrytype = {inproceedings},\n    id = {reiser2021kilonerf}\n}","Bibtex Name":"reiser2021kilonerf","Citation Count":"6","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/25/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Sampling, Voxel Grid, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"KiloNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.13744.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/5/2021 15:51","Title":"KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs","Training time (hr)":"","UID":"128","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call \"mip-NeRF\" (a la \"mipmap\"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.","Authors (format: First Last, First Middle Last, ...)":"Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan","Bibtex (e.g. @inproceedings...)":"@inproceedings{barron2021mipnerf,\n    url = {http://arxiv.org/abs/2103.13415v3},\n    year = {2021},\n    title = {Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields},\n    author = {Jonathan T. Barron and Ben Mildenhall and Matthew Tancik and Peter Hedman and Ricardo Martin-Brualla and Pratul P. Srinivasan},\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    entrytype = {inproceedings},\n    id = {barron2021mipnerf}\n}","Bibtex Name":"barron2021mipnerf","Citation Count":"3","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/24/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals, Sampling","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Mip-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.13415.pdf","Project webpage link":"https://jonbarron.info/mipnerf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=EpH175PY1A0","Timestamp":"5/23/2021 18:46","Title":"Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields","Training time (hr)":"","UID":"127","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We show for the first time that a multilayer perceptron (MLP) can serve as the only scene representation in a real-time SLAM system for a handheld RGB-D camera. Our network is trained in live operation without prior data, building a dense, scene-specific implicit 3D model of occupancy and colour which is also immediately used for tracking. Achieving real-time SLAM via continual training of a neural network against a live image stream requires significant innovation. Our iMAP algorithm uses a keyframe structure and multi-processing computation flow, with dynamic information-guided pixel sampling for speed, with tracking at 10 Hz and global map updating at 2 Hz. The advantages of an implicit MLP over standard dense SLAM techniques include efficient geometry representation with automatic detail control and smooth, plausible filling-in of unobserved regions such as the back surfaces of objects.","Authors (format: First Last, First Middle Last, ...)":"Edgar Sucar, Shikun Liu, Joseph Ortiz, Andrew J. Davison","Bibtex (e.g. @inproceedings...)":"@inproceedings{sucar2021imap,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Edgar Sucar and Shikun Liu and Joseph Ortiz and Andrew J. Davison},\n    title = {iMAP: Implicit Mapping and Positioning in Real-Time},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.12352v2},\n    entrytype = {inproceedings},\n    id = {sucar2021imap}\n}","Bibtex Name":"sucar2021imap","Citation Count":"3","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/23/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"RGB, Depth","Is the PDF linked to arXiv?":"","Keywords":"Camera Parameter Estimation, Robotics, Multi-task/Continual/Transfer learning, Sampling","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"iMAP","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.12352.pdf","Project webpage link":"https://edgarsucar.github.io/iMAP/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=c-zkKGArl5Y","Timestamp":"5/23/2021 18:47","Title":"iMAP: Implicit Mapping and Positioning in Real-Time","Training time (hr)":"","UID":"126","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Novel view synthesis is a challenging and ill-posed inverse rendering problem. Neural rendering techniques have recently achieved photorealistic image quality for this task. State-of-the-art (SOTA) neural volume rendering approaches, however, are slow to train and require minutes of inference (i.e., rendering) time for high image resolutions. We adopt high-capacity neural scene representations with periodic activations for jointly optimizing an implicit surface and a radiance field of a scene supervised exclusively with posed 2D images. Our neural rendering pipeline accelerates SOTA neural volume rendering by about two orders of magnitude and our implicit surface representation is unique in allowing us to export a mesh with view-dependent texture information. Thus, like other implicit surface representations, ours is compatible with traditional graphics pipelines, enabling real-time rendering rates, while achieving unprecedented image quality compared to other surface methods. We assess the quality of our approach using existing datasets as well as high-quality 3D face data captured with a custom multi-camera rig.","Authors (format: First Last, First Middle Last, ...)":"Petr Kellnhofer, Lars Jebe, Andrew Jones, Ryan Spicer, Kari Pulli, Gordon Wetzstein","Bibtex (e.g. @inproceedings...)":"@inproceedings{kellnhofer2021nlr,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Petr Kellnhofer and Lars Jebe and Andrew Jones and Ryan Spicer and Kari Pulli and Gordon Wetzstein},\n    title = {Neural Lumigraph Rendering},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.11571v1},\n    entrytype = {inproceedings},\n    id = {kellnhofer2021nlr}\n}","Bibtex Name":"kellnhofer2021nlr","Citation Count":"8","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/22/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Image-Based Rendering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NLR","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.11571.pdf","Project webpage link":"http://www.computationalimaging.org/publications/nlr/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"https://openaccess.thecvf.com/content/CVPR2021/supplemental/Kellnhofer_Neural_Lumigraph_Rendering_CVPR_2021_supplemental.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=maVF-7x9644","Timestamp":"6/21/2021 16:43","Title":"Neural Lumigraph Rendering","Training time (hr)":"","UID":"125","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Generating high-fidelity talking head video by fitting with the input audio sequence is a challenging problem that receives considerable attentions recently. In this paper, we address this problem with the aid of neural scene representation networks. Our method is completely different from existing methods that rely on intermediate representations like 2D landmarks or 3D face models to bridge the gap between audio input and video output. Specifically, the feature of input audio signal is directly fed into a conditional implicit function to generate a dynamic neural radiance field, from which a high-fidelity talking-head video corresponding to the audio signal is synthesized using volume rendering. Another advantage of our framework is that not only the head (with hair) region is synthesized as previous methods did, but also the upper body is generated via two individual neural radiance fields. Experimental results demonstrate that our novel framework can (1) produce high-fidelity and natural results, and (2) support free adjustment of audio signals, viewing directions, and background images. Code is available at https://github.com/YudongGuo/AD-NeRF.","Authors (format: First Last, First Middle Last, ...)":"Yudong Guo, Keyu Chen, Sen Liang, Yongjin Liu, Hujun Bao, Juyong Zhang","Bibtex (e.g. @inproceedings...)":"@inproceedings{guo2021adnerf,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Yudong Guo and Keyu Chen and Sen Liang and Yong-Jin Liu and Hujun Bao and Juyong Zhang},\n    title = {AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.11078v3},\n    entrytype = {inproceedings},\n    id = {guo2021adnerf}\n}","Bibtex Name":"guo2021adnerf","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/20/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Audio, Global Conditioning, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"AD-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.11078.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=TQO2EBYXLyU","Timestamp":"5/23/2021 18:44","Title":"AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis","Training time (hr)":"","UID":"124","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In this paper we introduce SMPLicit, a novel generative model to jointly represent body pose, shape and clothing geometry. In contrast to existing learning-based approaches that require training specific models for each type of garment, SMPLicit can represent in a unified manner different garment topologies (e.g. from sleeveless tops to hoodies and to open jackets), while controlling other properties like the garment size or tightness/looseness. We show our model to be applicable to a large variety of garments including T-shirts, hoodies, jackets, shorts, pants, skirts, shoes and even hair. The representation flexibility of SMPLicit builds upon an implicit model conditioned with the SMPL human body parameters and a learnable latent space which is semantically interpretable and aligned with the clothing attributes. The proposed model is fully differentiable, allowing for its use into larger end-to-end trainable systems. In the experimental section, we demonstrate SMPLicit can be readily used for fitting 3D scans and for 3D reconstruction in images of dressed people. In both cases we are able to go beyond state of the art, by retrieving complex garment geometries, handling situations with multiple clothing layers and providing a tool for easy outfit editing. To stimulate further research in this direction, we will make our code and model publicly available at http://www.iri.upc.edu/people/ecorona/smplicit/.","Authors (format: First Last, First Middle Last, ...)":"Enric Corona, Albert Pumarola, Guillem Aleny\u00e0, Gerard Pons-Moll, Francesc Moreno-Noguer","Bibtex (e.g. @inproceedings...)":"@inproceedings{corona2021smplicit,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Enric Corona and Albert Pumarola and Guillem Alenya and Gerard Pons-Moll and Francesc Moreno-Noguer},\n    title = {SMPLicit: Topology-aware Generative Model for Clothed People},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.06871v2},\n    entrytype = {inproceedings},\n    id = {corona2021smplicit}\n}","Bibtex Name":"corona2021smplicit","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/enriccorona/SMPLicit","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/11/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"UDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Editable, Generative Models, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SMPLicit","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.06871.pdf","Project webpage link":"http://www.iri.upc.edu/people/ecorona/smplicit/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/17/2021 11:42","Title":"SMPLicit: Topology-aware Generative Model for Clothed People","Training time (hr)":"","UID":"123","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present NeX, a new approach to novel view synthesis based on enhancements of multiplane image (MPI) that can reproduce next-level view-dependent effects -- in real time. Unlike traditional MPI that uses a set of simple RGB$\\alpha$ planes, our technique models view-dependent effects by instead parameterizing each pixel as a linear combination of basis functions learned from a neural network. Moreover, we propose a hybrid implicit-explicit modeling strategy that improves upon fine detail and produces state-of-the-art results. Our method is evaluated on benchmark forward-facing datasets as well as our newly-introduced dataset designed to test the limit of view-dependent modeling with significantly more challenging effects such as rainbow reflections on a CD. Our method achieves the best overall scores across all major metrics on these datasets with more than 1000$\\times$ faster rendering time than the state of the art. For real-time demos, visit https://nex-mpi.github.io/","Authors (format: First Last, First Middle Last, ...)":"Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, Supasorn Suwajanakorn","Bibtex (e.g. @inproceedings...)":"@inproceedings{wizadwongsa2021nex,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Suttisak Wizadwongsa and Pakkapon Phongthawee and Jiraphon Yenphraphai and Supasorn Suwajanakorn},\n    title = {NeX: Real-time View Synthesis with Neural Basis Expansion},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.05606v2},\n    entrytype = {inproceedings},\n    id = {wizadwongsa2021nex}\n}","Bibtex Name":"wizadwongsa2021nex","Citation Count":"9","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/nex-mpi/nex-code/","Coordinates all at once":"","Data Release (link)":"https://vistec-my.sharepoint.com/personal/pakkapon_p_s19_vistec_ac_th/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fpakkapon%5Fp%5Fs19%5Fvistec%5Fac%5Fth%2FDocuments%2Fpublic%2FVLL%2FNeX%2Fshiny%5Fdatasets&originalPath=aHR0cHM6Ly92aXN0ZWMtbXkuc2hhcmVwb2ludC5jb20vOmY6L2cvcGVyc29uYWwvcGFra2Fwb25fcF9zMTlfdmlzdGVjX2FjX3RoL0VuSVVoc1JWSk9kTnNaXzRzbWRoeWUwQjh6MFZseHFPUjM1SVIzYnAwdUd1cFE%5FcnRpbWU9c1hYTTNEd2UyVWc","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/9/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeX","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.05606.pdf","Project webpage link":"https://nex-mpi.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:48","Title":"NeX: Real-time View Synthesis with Neural Basis Expansion","Training time (hr)":"","UID":"122","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"The recent research explosion around implicit neural representations, such as NeRF, shows that there is immense potential for implicitly storing high-quality scene and lighting information in compact neural networks. However, one major limitation preventing the use of NeRF in real-time rendering applications is the prohibitive computational cost of excessive network evaluations along each view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural representations closer to practical rendering of synthetic content in real-time applications, such as games and virtual reality. We show that the number of samples required for each view ray can be significantly reduced when samples are placed around surfaces in the scene without compromising image quality. To this end, we propose a depth oracle network that predicts ray sample locations for each view ray with a single network evaluation. We show that using a classification network around logarithmically discretized and spherically warped depth values is essential to encode surface locations rather than directly estimating depth. The combination of these techniques leads to DONeRF, our compact dual network design with a depth oracle network as its first step and a locally sampled shading network for ray accumulation. With DONeRF, we reduce the inference costs by up to 48x compared to NeRF when conditioning on available ground truth depth information. Compared to concurrent acceleration methods for raymarching-based neural representations, DONeRF does not require additional memory for explicit caching or acceleration structures, and can render interactively (20 frames per second) on a single GPU.","Authors (format: First Last, First Middle Last, ...)":"Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, Anton Kaplanyan, Markus Steinberger","Bibtex (e.g. @inproceedings...)":"@article{neff2021donerf,\n    publisher = {The Eurographics Association and John Wiley & Sons Ltd.},\n    journal = {Computer Graphics Forum},\n    author = {Thomas Neff and Pascal Stadlbauer and Mathias Parger and Andreas Kurz and Joerg H. Mueller and Chakravarty R. Alla Chaitanya and Anton Kaplanyan and Markus Steinberger},\n    title = {DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks},\n    doi = {10.1111/cgf.14340},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.03231v4},\n    entrytype = {article},\n    id = {neff2021donerf}\n}","Bibtex Name":"neff2021donerf","Citation Count":"6","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/4/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Sampling, Data-Driven Method","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DONeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.03231.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/25/2021 0:11","Title":"DONeRF: Towards Real-Time Rendering of Neural Radiance Fields using Depth Oracle Networks","Training time (hr)":"","UID":"121","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"EGSR 2021","Venue no Year":"EGSR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose a new simple approach for image compression: instead of storing the RGB values for each pixel of an image, we store the weights of a neural network overfitted to the image. Specifically, to encode an image, we fit it with an MLP which maps pixel locations to RGB values. We then quantize and store the weights of this MLP as a code for the image. To decode the image, we simply evaluate the MLP at every pixel location. We found that this simple approach outperforms JPEG at low bit-rates, even without entropy coding or learning a distribution over weights. While our framework is not yet competitive with state of the art compression methods, we show that it has various attractive properties which could make it a viable alternative to other neural data compression approaches.","Authors (format: First Last, First Middle Last, ...)":"Emilien Dupont, Adam Goli\u0144ski, Milad Alizadeh, Yee Whye Teh, Arnaud Doucet","Bibtex (e.g. @inproceedings...)":"@inproceedings{dupont2021coin,\n    booktitle = {International Conference on Learning Representations},\n    author = {Emilien Dupont and Adam Golinski and Milad Alizadeh and Yee Whye Teh and Arnaud Doucet},\n    title = {COIN: COmpression with Implicit Neural representations},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.03123v2},\n    entrytype = {inproceedings},\n    id = {dupont2021coin}\n}","Bibtex Name":"dupont2021coin","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/3/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Sinusoidal Activation (SIREN)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Compression","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"COIN","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.03123.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=FjPurtmqgmw","Timestamp":"5/23/2021 18:50","Title":"COIN: COmpression with Implicit Neural representations","Training time (hr)":"","UID":"120","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICLR 2021","Venue no Year":"ICLR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose a novel approach for 3D video synthesis that is able to represent multi-view video recordings of a dynamic real-world scene in a compact, yet expressive representation that enables high-quality view synthesis and motion interpolation. Our approach takes the high quality and compactness of static neural radiance fields in a new direction: to a model-free, dynamic setting. At the core of our approach is a novel time-conditioned neural radiance fields that represents scene dynamics using a set of compact latent codes. To exploit the fact that changes between adjacent frames of a video are typically small and locally consistent, we propose two novel strategies for efficient training of our neural network: 1) An efficient hierarchical training scheme, and 2) an importance sampling strategy that selects the next rays for training based on the temporal variation of the input videos. In combination, these two strategies significantly boost the training speed, lead to fast convergence of the training process, and enable high quality results. Our learned representation is highly compact and able to represent a 10 second 30 FPS multi-view video recording by 18 cameras with a model size of just 28MB. We demonstrate that our method can render high-fidelity wide-angle novel views at over 1K resolution, even for highly complex and dynamic scenes. We perform an extensive qualitative and quantitative evaluation that shows that our approach outperforms the current state of the art. We include additional video and information at: https://neural-3d-video.github.io/","Authors (format: First Last, First Middle Last, ...)":"Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Zhaoyang Lv","Bibtex (e.g. @inproceedings...)":"@article{li2021dynerf,\n    journal = {arXiv preprint arXiv:2103.02597},\n    booktitle = {ArXiv Pre-print},\n    author = {Tianye Li and Mira Slavcheva and Michael Zollhoefer and Simon Green and Christoph Lassner and Changil Kim and Tanner Schmidt and Steven Lovegrove and Michael Goesele and Zhaoyang Lv},\n    title = {Neural 3D Video Synthesis},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.02597v1},\n    entrytype = {article},\n    id = {li2021dynerf}\n}","Bibtex Name":"li2021dynerf","Citation Count":"3","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"Coming soon","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/3/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Global Conditioning, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DyNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.02597.pdf","Project webpage link":"https://neural-3d-video.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://neural-3d-video.github.io/resources/video.mp4","Timestamp":"5/23/2021 18:50","Title":"Neural 3D Video Synthesis","Training time (hr)":"","UID":"119","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recent work has demonstrated that volumetric scene representations combined with differentiable volume rendering can enable photo-realistic rendering for challenging scenes that mesh reconstruction fails on. However, these methods entangle geometry and appearance in a \"black-box\" volume that cannot be edited. Instead, we present an approach that explicitly disentangles geometry--represented as a continuous 3D volume--from appearance--represented as a continuous 2D texture map. We achieve this by introducing a 3D-to-2D texture mapping (or surface parameterization) network into volumetric representations. We constrain this texture mapping network using an additional 2D-to-3D inverse mapping network and a novel cycle consistency loss to make 3D surface points map to 2D texture points that map back to the original 3D points. We demonstrate that this representation can be reconstructed using only multi-view image supervision and generates high-quality rendering results. More importantly, by separating geometry and texture, we allow users to edit appearance by simply editing 2D texture maps.","Authors (format: First Last, First Middle Last, ...)":"Fanbo Xiang, Zexiang Xu, Milo\u0161 Ha\u0161an, Yannick Hold-Geoffroy, Kalyan Sunkavalli, Hao Su","Bibtex (e.g. @inproceedings...)":"@inproceedings{xiang2021neutex,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Fanbo Xiang and Zexiang Xu and Milos Hasan and Yannick Hold-Geoffroy and Kalyan Sunkavalli and Hao Su},\n    title = {NeuTex: Neural Texture Mapping for Volumetric Neural Rendering},\n    year = {2021},\n    url = {http://arxiv.org/abs/2103.00762v1},\n    entrytype = {inproceedings},\n    id = {xiang2021neutex}\n}","Bibtex Name":"xiang2021neutex","Citation Count":"2","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/1/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeuTex","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2103.00762.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:56","Title":"NeuTex: Neural Texture Mapping for Volumetric Neural Rendering","Training time (hr)":"","UID":"118","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a method that synthesizes novel views of complex scenes by interpolating a sparse set of nearby views. The core of our method is a network architecture that includes a multilayer perceptron and a ray transformer that estimates radiance and volume density at continuous 5D locations (3D spatial locations and 2D viewing directions), drawing appearance information on the fly from multiple source views. By drawing on source views at render time, our method hearkens back to classic work on image-based rendering (IBR), and allows us to render high-resolution imagery. Unlike neural scene representation work that optimizes per-scene functions for rendering, we learn a generic view interpolation function that generalizes to novel scenes. We render images using classic volume rendering, which is fully differentiable and allows us to train using only multi-view posed images as supervision. Experiments show that our method outperforms recent novel view synthesis methods that also seek to generalize to novel scenes. Further, if fine-tuned on each scene, our method is competitive with state-of-the-art single-scene neural rendering methods. Project page: https://ibrnet.github.io/","Authors (format: First Last, First Middle Last, ...)":"Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, Thomas Funkhouser","Bibtex (e.g. @inproceedings...)":"@inproceedings{wang2021ibrnet,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Qianqian Wang and Zhicheng Wang and Kyle Genova and Pratul Srinivasan and Howard Zhou and Jonathan T. Barron and Ricardo Martin-Brualla and Noah Snavely and Thomas Funkhouser},\n    title = {IBRNet: Learning Multi-View Image-Based Rendering},\n    year = {2021},\n    url = {http://arxiv.org/abs/2102.13090v2},\n    entrytype = {inproceedings},\n    id = {wang2021ibrnet}\n}","Bibtex Name":"wang2021ibrnet","Citation Count":"20","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"2/25/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Generalization, Image-Based Rendering, Data-Driven Method, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"IBRNet","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2102.13090.pdf","Project webpage link":"https://ibrnet.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:52","Title":"IBRNet: Learning Multi-View Image-Based Rendering","Training time (hr)":"","UID":"117","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recent advances in implicit neural representations show great promise when it comes to generating numerical solutions to partial differential equations (PDEs). Compared to conventional alternatives, such representations employ parameterized neural networks to define, in a mesh-free manner, signals that are highly-detailed, continuous, and fully differentiable. Most prior works aim to exploit these benefits in order to solve PDE-governed forward problems, or associated inverse problems that are defined by a small number of parameters. In this work, we present a novel machine learning approach to tackle topology optimization (TO) problems. Topology optimization refers to an important class of inverse problems that typically feature very high-dimensional parameter spaces and objective landscapes which are highly non-linear. To effectively leverage neural representations in the context of TO problems, we use multilayer perceptrons (MLPs) to parameterize both density and displacement fields. Using sensitivity analysis with a moving mean squared error, we show that our formulation can be used to efficiently minimize traditional structural compliance objectives. As we show through our experiments, a major benefit of our approach is that it enables self-supervised learning of continuous solution spaces to topology optimization problems.","Authors (format: First Last, First Middle Last, ...)":"Jonas Zehnder, Yue Li, Stelian Coros, Bernhard Thomaszewski","Bibtex (e.g. @inproceedings...)":"@article{zehnder2021ntopo,\n    journal = {arXiv preprint arXiv:2102.10782},\n    booktitle = {ArXiv Pre-print},\n    author = {Jonas Zehnder and Yue Li and Stelian Coros and Bernhard Thomaszewski},\n    title = {NTopo: Mesh-free Topology Optimization using Implicit Neural Representations},\n    year = {2021},\n    url = {http://arxiv.org/abs/2102.10782v1},\n    entrytype = {article},\n    id = {zehnder2021ntopo}\n}","Bibtex Name":"zehnder2021ntopo","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"2/22/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering, Supervision by Gradient (PDE)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NTopo","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2102.10782.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:53","Title":"NTopo: Mesh-free Topolibogy Optimization using Implicit Neural Representations","Training time (hr)":"","UID":"116","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a method for estimating neural scenes representations of objects given only a single image. The core of our method is the estimation of a geometric scaffold for the object and its use as a guide for the reconstruction of the underlying radiance field. Our formulation is based on a generative process that first maps a latent code to a voxelized shape, and then renders it to an image, with the object appearance being controlled by a second latent code. During inference, we optimize both the latent codes and the networks to fit a test image of a new object. The explicit disentanglement of shape and appearance allows our model to be fine-tuned given a single image. We can then render new views in a geometrically consistent manner and they represent faithfully the input object. Additionally, our method is able to generalize to images outside of the training domain (more realistic renderings and even real photographs). Finally, the inferred geometric scaffold is itself an accurate estimate of the object's 3D shape. We demonstrate in several experiments the effectiveness of our approach in both synthetic and real images.","Authors (format: First Last, First Middle Last, ...)":"Konstantinos Rematas, Ricardo Martin-Brualla, Vittorio Ferrari","Bibtex (e.g. @inproceedings...)":"@inproceedings{rematas2021sharf,\n    publisher = {PMLR},\n    booktitle = {International Conference on Machine Learning (ICML)},\n    author = {Konstantinos Rematas and Ricardo Martin-Brualla and Vittorio Ferrari},\n    title = {ShaRF: Shape-conditioned Radiance Fields from a Single View},\n    year = {2021},\n    url = {http://arxiv.org/abs/2102.08860v2},\n    entrytype = {inproceedings},\n    id = {rematas2021sharf}\n}","Bibtex Name":"rematas2021sharf","Citation Count":"5","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"2/17/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Sparse Reconstruction, Generalization, Generative Models, Data-Driven Method, Local Conditioning, Voxel Grid, Sampling","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"ShaRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2102.08860.pdf","Project webpage link":"http://www.krematas.com/sharf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/26/2021 14:37","Title":"ShaRF: Shape-conditioned Radiance Fields from a Single View","Training time (hr)":"","UID":"115","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICML 2021","Venue no Year":"ICML","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"This paper tackles the problem of novel view synthesis (NVS) from 2D images without known camera poses and intrinsics. Among various NVS techniques, Neural Radiance Field (NeRF) has recently gained popularity due to its remarkable synthesis quality. Existing NeRF-based approaches assume that the camera parameters associated with each input image are either directly accessible at training, or can be accurately estimated with conventional techniques based on correspondences, such as Structure-from-Motion. In this work, we propose an end-to-end framework, termed NeRF--, for training NeRF models given only RGB images, without pre-computed camera parameters. Specifically, we show that the camera parameters, including both intrinsics and extrinsics, can be automatically discovered via joint optimisation during the training of the NeRF model. On the standard LLFF benchmark, our model achieves comparable novel view synthesis results compared to the baseline trained with COLMAP pre-computed camera parameters. We also conduct extensive analyses to understand the model behaviour under different camera trajectories, and show that in scenarios where COLMAP fails, our model still produces robust results.","Authors (format: First Last, First Middle Last, ...)":"Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, Victor Adrian Prisacariu","Bibtex (e.g. @inproceedings...)":"@article{wang2021nerf--,\n    journal = {arXiv preprint arXiv:2102.07064},\n    booktitle = {ArXiv Pre-print},\n    author = {Zirui Wang and Shangzhe Wu and Weidi Xie and Min Chen and Victor Adrian Prisacariu},\n    title = {NeRF--: Neural Radiance Fields Without Known Camera Parameters},\n    year = {2021},\n    url = {http://arxiv.org/abs/2102.07064v3},\n    entrytype = {article},\n    id = {wang2021nerf--}\n}","Bibtex Name":"wang2021nerf--","Citation Count":"9","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/ActiveVisionLab/nerfmm","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"2/14/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Camera Parameter Estimation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeRF\u2212\u2212","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2102.07064.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:54","Title":"NeRF\u2212\u2212: Neural Radiance Fields Without Known Camera Parameters","Training time (hr)":"","UID":"114","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Controlled capture of real-world material appearance yields tabulated sets of highly realistic reflectance data. In practice, however, its high memory footprint requires compressing into a representation that can be used efficiently in rendering while remaining faithful to the original. Previous works in appearance encoding often prioritised one of these requirements at the expense of the other, by either applying high-fidelity array compression strategies not suited for efficient queries during rendering, or by fitting a compact analytic model that lacks expressiveness. We present a compact neural network-based representation of BRDF data that combines high-accuracy reconstruction with efficient practical rendering via built-in interpolation of reflectance. We encode BRDFs as lightweight networks, and propose a training scheme with adaptive angular sampling, critical for the accurate reconstruction of specular highlights. Additionally, we propose a novel approach to make our representation amenable to importance sampling: rather than inverting the trained networks, we learn to encode them in a more compact embedding that can be mapped to parameters of an analytic BRDF for which importance sampling is known. We evaluate encoding results on isotropic and anisotropic BRDFs from multiple real-world datasets, and importance sampling performance for isotropic BRDFs mapped to two different analytic models.","Authors (format: First Last, First Middle Last, ...)":"Alejandro Sztrajman, Gilles Rainer, Tobias Ritschel, Tim Weyrich","Bibtex (e.g. @inproceedings...)":"@article{sztrajman2021neural,\n    author = {Alejandro Sztrajman and Gilles Rainer and Tobias Ritschel and Tim Weyrich},\n    title = {Neural BRDF Representation and Importance Sampling},\n    year = {2021},\n    month = {Feb},\n    url = {http://arxiv.org/abs/2102.05963v3}\n}","Bibtex Name":"sztrajman2021neural","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"http://www0.cs.ucl.ac.uk/staff/A.Sztrajman/webpage/publications/nbrdf2021/nbrdf.html","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"2/11/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"qian_zhang@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Material/Lighting Estimation, Compression, Sampling","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2102.05963.pdf","Project webpage link":"http://www0.cs.ucl.ac.uk/staff/A.Sztrajman/webpage/publications/nbrdf2021/nbrdf.html","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"3/21/2022 10:04","Title":"Neural BRDF Representation and Importance Sampling","Training time (hr)":"","UID":"113","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"Computer Grahics Forum 2021","Venue no Year":"Computer Grahics Forum","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"While deep learning has reshaped the classical motion capture pipeline, generative, analysis-by-synthesis elements are still in use to recover fine details if a high-quality 3D model of the user is available. Unfortunately, obtaining such a model for every user a priori is challenging, time-consuming, and limits the application scenarios. We propose a novel test-time optimization approach for monocular motion capture that learns a volumetric body model of the user in a self-supervised manner. To this end, our approach combines the advantages of neural radiance fields with an articulated skeleton representation. Our proposed skeleton embedding serves as a common reference that links constraints across time, thereby reducing the number of required camera views from traditionally dozens of calibrated cameras, down to a single uncalibrated one. As a starting point, we employ the output of an off-the-shelf model that predicts the 3D skeleton pose. The volumetric body shape and appearance is then learned from scratch, while jointly refining the initial pose estimate. Our approach is self-supervised and does not require any additional ground truth labels for appearance, pose, or 3D shape. We demonstrate that our novel combination of a discriminative pose estimation technique with surface-free analysis-by-synthesis outperforms purely discriminative monocular pose estimation approaches and generalizes well to multiple views.","Authors (format: First Last, First Middle Last, ...)":"Shih-Yang Su, Frank Yu, Michael Zollhoefer, Helge Rhodin","Bibtex (e.g. @inproceedings...)":"@article{su2021anerf,\n    journal = {arXiv preprint arXiv:2102.06199},\n    booktitle = {ArXiv Pre-print},\n    author = {Shih-Yang Su and Frank Yu and Michael Zollhoefer and Helge Rhodin},\n    title = {A-NeRF: Surface-free Human 3D Pose Refinement via Neural Rendering},\n    year = {2021},\n    url = {http://arxiv.org/abs/2102.06199v1},\n    entrytype = {article},\n    id = {su2021anerf}\n}","Bibtex Name":"su2021anerf","Citation Count":"1","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"2/11/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Human (Body)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"A-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2102.06199.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:54","Title":"A-NeRF: Surface-free Human 3D Pose Refinement via Neural Rendering","Training time (hr)":"","UID":"112","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose Coordinate-based Internal Learning (CoIL) as a new deep-learning (DL) methodology for continuous representation of measurements. Unlike traditional DL methods that learn a mapping from the measurements to the desired image, CoIL trains a multilayer perceptron (MLP) to encode the complete measurement field by mapping the coordinates of the measurements to their responses. CoIL is a self-supervised method that requires no training examples besides the measurements of the test object itself. Once the MLP is trained, CoIL generates new measurements that can be used within most image reconstruction methods. We validate CoIL on sparse-view computed tomography using several widely-used reconstruction methods, including purely model-based methods and those based on DL. Our results demonstrate the ability of CoIL to consistently improve the performance of all the considered methods by providing high-fidelity measurement fields.","Authors (format: First Last, First Middle Last, ...)":"Yu Sun, Jiaming Liu, Mingyang Xie, Brendt Wohlberg, Ulugbek S. Kamilov","Bibtex (e.g. @inproceedings...)":"@article{9606601,\n    doi = {10.1109/TCI.2021.3125564},\n    pages = {1400-1412},\n    number = {},\n    volume = {7},\n    year = {2021},\n    title = {CoIL: Coordinate-Based Internal Learning for Tomographic Imaging},\n    journal = {IEEE Transactions on Computational Imaging},\n    author = {Sun, Yu and Liu, Jiaming and Xie, Mingyang and Wohlberg, Brendt and Kamilov, Ulugbek S.},\n    entrytype = {article},\n    id = {9606601}\n}","Bibtex Name":"sun2021coil","Citation Count":"3","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/wustl-cig/Cooridnate-based-Internal-Learning","Coordinates all at once":"","Data Release (link)":"https://www.youtube.com/watch?v=7LXagKec31U","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"2/9/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Linear encoding","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering, Positional Encoding","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"CoIL","PDF link (arXiv perferred)":"https://ieeexplore.ieee.org/document/9606601","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:41","Title":"CoIL: Coordinate-based Internal Learning for Imaging Inverse Problems","Training time (hr)":"","UID":"111","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"IEEE Transactions on Computational Imaging, vol. 7, pp. 1400-1412 2021","Venue no Year":"IEEE Transactions on Computational Imaging, vol. 7, pp. 1400-1412","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural implicit representations have shown substantial improvements in efficiently storing 3D data, when compared to conventional formats. However, the focus of existing work has mainly been on storage and subsequent reconstruction. In this work, we show that training neural representations for reconstruction tasks alongside conventional tasks can produce more general encodings that admit equal quality reconstructions to single task training, whilst improving results on conventional tasks when compared to single task encodings. We reformulate the semantic segmentation task, creating a more representative task for implicit representation contexts, and through multi-task experiments on reconstruction, classification, and segmentation, show our approach learns feature rich encodings that admit equal performance for each task.","Authors (format: First Last, First Middle Last, ...)":"Theo W. Costain, Victor Adrian Prisacariu","Bibtex (e.g. @inproceedings...)":"@article{costain2021towards,\n    journal = {arXiv preprint arXiv:2101.12690},\n    booktitle = {ArXiv Pre-print},\n    author = {Theo W. Costain and Victor Adrian Prisacariu},\n    title = {Towards Generalising Neural Implicit Representations},\n    year = {2021},\n    url = {http://arxiv.org/abs/2101.12690v2},\n    entrytype = {article},\n    id = {costain2021towards}\n}","Bibtex Name":"costain2021towards","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"1/29/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2101.12690.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:53","Title":"Towards Generalising Neural Implicit Representations","Training time (hr)":"","UID":"110","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2-3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.","Authors (format: First Last, First Middle Last, ...)":"Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, Sanja Fidler","Bibtex (e.g. @inproceedings...)":"@inproceedings{takikawa2021nglod,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Towaki Takikawa and Joey Litalien and Kangxue Yin and Karsten Kreis and Charles Loop and Derek Nowrouzezahrai and Alec Jacobson and Morgan McGuire and Sanja Fidler},\n    title = {Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes},\n    year = {2021},\n    url = {http://arxiv.org/abs/2101.10994v1},\n    entrytype = {inproceedings},\n    id = {takikawa2021nglod}\n}","Bibtex Name":"takikawa2021nglod","Citation Count":"12","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/nv-tlabs/nglod","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"1/26/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Generalization, Coarse-to-Fine, Voxel Grid, Sampling, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NGLOD","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2101.10994.pdf","Project webpage link":"nv-tlabs.github.io/nglod","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=Pi7W6XrFtMs","Timestamp":"6/29/2021 15:40","Title":"Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes","Training time (hr)":"","UID":"109","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Constructing and animating humans is an important component for building virtual worlds in a wide variety of applications such as virtual reality or robotics testing in simulation. As there are exponentially many variations of humans with different shape, pose and clothing, it is critical to develop methods that can automatically reconstruct and animate humans at scale from real world data. Towards this goal, we represent the pedestrian's shape, pose and skinning weights as neural implicit functions that are directly learned from data. This representation enables us to handle a wide variety of different pedestrian shapes and poses without explicitly fitting a human parametric body model, allowing us to handle a wider range of human geometries and topologies. We demonstrate the effectiveness of our approach on various datasets and show that our reconstructions outperform existing state-of-the-art methods. Furthermore, our re-animation experiments show that we can generate 3D human animations at scale from a single RGB image (and/or an optional LiDAR sweep) as input.","Authors (format: First Last, First Middle Last, ...)":"Ze Yang, Shenlong Wang, Sivabalan Manivasagam, Zeng Huang, Wei-Chiu Ma, Xinchen Yan, Ersin Yumer, Raquel Urtasun","Bibtex (e.g. @inproceedings...)":"@inproceedings{yang2021s3,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Ze Yang and Shenlong Wang and Sivabalan Manivasagam and Zeng Huang and Wei-Chiu Ma and Xinchen Yan and Ersin Yumer and Raquel Urtasun},\n    title = {S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling},\n    year = {2021},\n    url = {http://arxiv.org/abs/2101.06571v1},\n    entrytype = {inproceedings},\n    id = {yang2021s3}\n}","Bibtex Name":"yang2021s3","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"1/17/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"No","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Editable, Voxel Grid, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"S3","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2101.06571.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/17/2021 11:46","Title":"S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling","Training time (hr)":"","UID":"108","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Acquisition and rendering of photo-realistic human heads is a highly challenging research problem of particular importance for virtual telepresence. Currently, the highest quality is achieved by volumetric approaches trained in a person specific manner on multi-view data. These models better represent fine structure, such as hair, compared to simpler mesh-based models. Volumetric models typically employ a global code to represent facial expressions, such that they can be driven by a small set of animation parameters. While such architectures achieve impressive rendering quality, they can not easily be extended to the multi-identity setting. In this paper, we devise a novel approach for predicting volumetric avatars of the human head given just a small number of inputs. We enable generalization across identities by a novel parameterization that combines neural radiance fields with local, pixel-aligned features extracted directly from the inputs, thus sidestepping the need for very deep or complex networks. Our approach is trained in an end-to-end manner solely based on a photometric re-rendering loss without requiring explicit 3D supervision.We demonstrate that our approach outperforms the existing state of the art in terms of quality and is able to generate faithful facial expressions in a multi-identity setting.","Authors (format: First Last, First Middle Last, ...)":"Amit Raj, Michael Zollhoefer, Tomas Simon, Jason Saragih, Shunsuke Saito, James Hays, Stephen Lombardi","Bibtex (e.g. @inproceedings...)":"@inproceedings{raj2021pva,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Amit Raj and Michael Zollhoefer and Tomas Simon and Jason Saragih and Shunsuke Saito and James Hays and Stephen Lombardi},\n    title = {PVA: Pixel-aligned Volumetric Avatars},\n    year = {2021},\n    url = {http://arxiv.org/abs/2101.02697v1},\n    entrytype = {inproceedings},\n    id = {raj2021pva}\n}","Bibtex Name":"raj2021pva","Citation Count":"6","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"1/7/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Sparse Reconstruction, Generalization, Image-Based Rendering, Data-Driven Method, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"PVA","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2101.02697.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:56","Title":"Pixel-aligned Volumetric Avatars","Training time (hr)":"","UID":"107","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a neural modeling framework for Non-Line-of-Sight (NLOS) imaging. Previous solutions have sought to explicitly recover the 3D geometry (e.g., as point clouds) or voxel density (e.g., within a pre-defined volume) of the hidden scene. In contrast, inspired by the recent Neural Radiance Field (NeRF) approach, we use a multi-layer perceptron (MLP) to represent the neural transient field or NeTF. However, NeTF measures the transient over spherical wavefronts rather than the radiance along lines. We therefore formulate a spherical volume NeTF reconstruction pipeline, applicable to both confocal and non-confocal setups. Compared with NeRF, NeTF samples a much sparser set of viewpoints (scanning spots) and the sampling is highly uneven. We thus introduce a Monte Carlo technique to improve the robustness in the reconstruction. Comprehensive experiments on synthetic and real datasets demonstrate NeTF provides higher quality reconstruction and preserves fine details largely missing in the state-of-the-art.","Authors (format: First Last, First Middle Last, ...)":"Siyuan Shen, Zi Wang, Ping Liu, Zhengqing Pan, Ruiqian Li, Tian Gao, Shiying Li, Jingyi Yu","Bibtex (e.g. @inproceedings...)":"@article{shen2021nonlineofsight,\n    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n    author = {Siyuan Shen and Zi Wang and Ping Liu and Zhengqing Pan and Ruiqian Li and Tian Gao and Shiying Li and Jingyi Yu},\n    title = {Non-line-of-Sight Imaging via Neural Transient Fields},\n    year = {2021},\n    url = {http://arxiv.org/abs/2101.00373v3},\n    entrytype = {article},\n    id = {shen2021nonlineofsight}\n}","Bibtex Name":"shen2021nonlineofsight","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/zeromakerplus/NeTF_public","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"1/2/2021","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2101.00373.pdf","Project webpage link":"https://sci2020.github.io/paper/2021/01/05/Non-line-of-Sight-Imaging-via-Neural-Transient-Fields.html","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:43","Title":"Non-line-of-Sight Imaging via Neural Transient Fields","Training time (hr)":"","UID":"106","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"TPAMI 2021","Venue no Year":"TPAMI","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"This paper addresses the challenge of novel view synthesis for a human performer from a very sparse set of camera views. Some recent works have shown that learning implicit neural representations of 3D scenes achieves remarkable view synthesis quality given dense input views. However, the representation learning will be ill-posed if the views are highly sparse. To solve this ill-posed problem, our key idea is to integrate observations over video frames. To this end, we propose Neural Body, a new human body representation which assumes that the learned neural representations at different frames share the same set of latent codes anchored to a deformable mesh, so that the observations across frames can be naturally integrated. The deformable mesh also provides geometric guidance for the network to learn 3D representations more efficiently. To evaluate our approach, we create a multi-view dataset named ZJU-MoCap that captures performers with complex motions. Experiments on ZJU-MoCap show that our approach outperforms prior works by a large margin in terms of novel view synthesis quality. We also demonstrate the capability of our approach to reconstruct a moving person from a monocular video on the People-Snapshot dataset. The code and dataset are available at https://zju3dv.github.io/neuralbody/.","Authors (format: First Last, First Middle Last, ...)":"Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, Xiaowei Zhou","Bibtex (e.g. @inproceedings...)":"@inproceedings{peng2021neuralbody,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Sida Peng and Yuanqing Zhang and Yinghao Xu and Qianqian Wang and Qing Shuai and Hujun Bao and Xiaowei Zhou},\n    title = {Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans},\n    year = {2021},\n    url = {http://arxiv.org/abs/2012.15838v2},\n    entrytype = {inproceedings},\n    id = {peng2021neuralbody}\n}","Bibtex Name":"peng2021neuralbody","Citation Count":"23","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/zju3dv/neuralbody","Coordinates all at once":"","Data Release (link)":"https://zjueducn-my.sharepoint.com/:f:/g/personal/pengsida_zju_edu_cn/Eo9zn4x_xcZKmYHZNjzel7gBdWf_d4m-pISHhPWB-GZBYw?e=Hf4mz7","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/31/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Human (Body)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Neural Body","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2012.15838.pdf","Project webpage link":"https://zju3dv.github.io/neuralbody/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"https://github.com/zju3dv/neuralbody/blob/master/supplementary_material.md","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:58","Title":"Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans","Training time (hr)":"","UID":"105","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and novel view synthesis approach for general non-rigid dynamic scenes. Our approach takes RGB images of a dynamic scene as input, e.g., from a monocular video recording, and creates a high-quality space-time geometry and appearance representation. In particular, we show that even a single handheld consumer-grade camera is sufficient to synthesize sophisticated renderings of a dynamic scene from novel virtual camera views, for example a `bullet-time' video effect. Our method disentangles the dynamic scene into a canonical volume and its deformation. Scene deformation is implemented as ray bending, where straight rays are deformed non-rigidly to represent scene motion. We also propose a novel rigidity regression network that enables us to better constrain rigid regions of the scene, which leads to more stable results. The ray bending and rigidity network are trained without any explicit supervision. In addition to novel view synthesis, our formulation enables dense correspondence estimation across views and time, as well as compelling video editing applications such as motion exaggeration. We demonstrate the effectiveness of our method using extensive evaluations, including ablation studies and comparisons to the state of the art. We urge the reader to watch the supplemental video for qualitative results. Our code will be open sourced.","Authors (format: First Last, First Middle Last, ...)":"Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollh\u00f6fer, Christoph Lassner, Christian Theobalt","Bibtex (e.g. @inproceedings...)":"@inproceedings{tretschk2021nrnerf,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Edgar Tretschk and Ayush Tewari and Vladislav Golyanik and Michael Zollhofer and Christoph Lassner and Christian Theobalt},\n    title = {Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video},\n    year = {2021},\n    url = {http://arxiv.org/abs/2012.12247v4},\n    entrytype = {inproceedings},\n    id = {tretschk2021nrnerf}\n}","Bibtex Name":"tretschk2021nrnerf","Citation Count":"18","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/22/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NR-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2012.12247.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:16","Title":"Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video","Training time (hr)":"","UID":"104","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present STaR, a novel method that performs Self-supervised Tracking and Reconstruction of dynamic scenes with rigid motion from multi-view RGB videos without any manual annotation. Recent work has shown that neural networks are surprisingly effective at the task of compressing many views of a scene into a learned function which maps from a viewing ray to an observed radiance value via volume rendering. Unfortunately, these methods lose all their predictive power once any object in the scene has moved. In this work, we explicitly model rigid motion of objects in the context of neural representations of radiance fields. We show that without any additional human specified supervision, we can reconstruct a dynamic scene with a single rigid object in motion by simultaneously decomposing it into its two constituent parts and encoding each with its own neural representation. We achieve this by jointly optimizing the parameters of two neural radiance fields and a set of rigid poses which align the two fields at each frame. On both synthetic and real world datasets, we demonstrate that our method can render photorealistic novel views, where novelty is measured on both spatial and temporal axes. Our factored representation furthermore enables animation of unseen object motion.","Authors (format: First Last, First Middle Last, ...)":"Wentao Yuan, Zhaoyang Lv, Tanner Schmidt, Steven Lovegrove","Bibtex (e.g. @inproceedings...)":"@inproceedings{yuan2021star,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Wentao Yuan and Zhaoyang Lv and Tanner Schmidt and Steven Lovegrove},\n    title = {STaR: Self-supervised Tracking and Reconstruction of Rigid Objects in Motion with Neural Rendering},\n    year = {2021},\n    url = {http://arxiv.org/abs/2101.01602v1},\n    entrytype = {inproceedings},\n    id = {yuan2021star}\n}","Bibtex Name":"yuan2021star","Citation Count":"3","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/22/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"STaR","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2101.01602.pdf","Project webpage link":"https://wentaoyuan.github.io/star/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:56","Title":"STaR: Self-supervised Tracking and Reconstruction of Rigid Objects in Motion with Neural Rendering","Training time (hr)":"","UID":"103","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a method, Neural Radiance Flow (NeRFlow),to learn a 4D spatial-temporal representation of a dynamic scene from a set of RGB images. Key to our approach is the use of a neural implicit representation that learns to capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing consistency across different modalities, our representation enables multi-view rendering in diverse dynamic scenes, including water pouring, robotic interaction, and real images, outperforming state-of-the-art methods for spatial-temporal view synthesis. Our approach works even when inputs images are captured with only one camera. We further demonstrate that the learned representation can serve as an implicit scene prior, enabling video processing tasks such as image super-resolution and de-noising without any additional supervision.","Authors (format: First Last, First Middle Last, ...)":"Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B. Tenenbaum, Jiajun Wu","Bibtex (e.g. @inproceedings...)":"@inproceedings{du2021nerflow,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Yilun Du and Yinan Zhang and Hong-Xing Yu and Joshua B. Tenenbaum and Jiajun Wu},\n    title = {Neural Radiance Flow for 4D View Synthesis and Video Processing},\n    year = {2021},\n    url = {http://arxiv.org/abs/2012.09790v2},\n    entrytype = {inproceedings},\n    id = {du2021nerflow}\n}","Bibtex Name":"du2021nerflow","Citation Count":"5","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/yilundu/nerflow","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/17/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeRFlow","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2012.09790.pdf","Project webpage link":"https://yilundu.github.io/nerflow/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:14","Title":"Neural Radiance Flow for 4D View Synthesis and Video Processing","Training time (hr)":"","UID":"102","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Photorealistic rendering of dynamic humans is an important ability for telepresence systems, virtual shopping, synthetic data generation, and more. Recently, neural rendering methods, which combine techniques from computer graphics and machine learning, have created high-fidelity models of humans and objects. Some of these methods do not produce results with high-enough fidelity for driveable human models (Neural Volumes) whereas others have extremely long rendering times (NeRF). We propose a novel compositional 3D representation that combines the best of previous methods to produce both higher-resolution and faster results. Our representation bridges the gap between discrete and continuous volumetric representations by combining a coarse 3D-structure-aware grid of animation codes with a continuous learned scene function that maps every position and its corresponding local animation code to its view-dependent emitted radiance and local volume density. Differentiable volume rendering is employed to compute photo-realistic novel views of the human head and upper body as well as to train our novel representation end-to-end using only 2D supervision. In addition, we show that the learned dynamic radiance field can be used to synthesize novel unseen expressions based on a global animation code. Our approach achieves state-of-the-art results for synthesizing novel views of dynamic human heads and the upper body.","Authors (format: First Last, First Middle Last, ...)":"Ziyan Wang, Timur Bagautdinov, Stephen Lombardi, Tomas Simon, Jason Saragih, Jessica Hodgins, Michael Zollh\u00f6fer","Bibtex (e.g. @inproceedings...)":"@inproceedings{wang2021hybridnerf,\n    url = {http://arxiv.org/abs/2012.09955v1},\n    year = {2021},\n    title = {Learning Compositional Radiance Fields of Dynamic Human Heads},\n    author = {Ziyan Wang and Timur Bagautdinov and Stephen Lombardi and Tomas Simon and Jason Saragih and Jessica Hodgins and Michael Zollhofer},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    entrytype = {inproceedings},\n    id = {wang2021hybridnerf}\n}","Bibtex Name":"wang2021hybridnerf","Citation Count":"5","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/17/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Human (Head), Generalization, Sampling, Voxel Grid, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"HybridNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2012.09955.pdf","Project webpage link":"https://ziyanw1.github.io/hybrid_nerf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"https://openaccess.thecvf.com/content/CVPR2021/supplemental/Wang_Learning_Compositional_Radiance_CVPR_2021_supplemental.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 16:53","Title":"Learning Compositional Radiance Fields of Dynamic Human Heads","Training time (hr)":"","UID":"101","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"How to represent an image? While the visual world is presented in a continuous manner, machines store and see the images in a discrete way with 2D arrays of pixels. In this paper, we seek to learn a continuous representation for images. Inspired by the recent progress in 3D reconstruction with implicit neural representation, we propose Local Implicit Image Function (LIIF), which takes an image coordinate and the 2D deep features around the coordinate as inputs, predicts the RGB value at a given coordinate as an output. Since the coordinates are continuous, LIIF can be presented in arbitrary resolution. To generate the continuous representation for images, we train an encoder with LIIF representation via a self-supervised task with super-resolution. The learned continuous representation can be presented in arbitrary resolution even extrapolate to x30 higher resolution, where the training tasks are not provided. We further show that LIIF representation builds a bridge between discrete and continuous representation in 2D, it naturally supports the learning tasks with size-varied image ground-truths and significantly outperforms the method with resizing the ground-truths.","Authors (format: First Last, First Middle Last, ...)":"Yinbo Chen, Sifei Liu, Xiaolong Wang","Bibtex (e.g. @inproceedings...)":"@inproceedings{chen2021liif,\n    url = {http://arxiv.org/abs/2012.09161v2},\n    year = {2021},\n    title = {Learning Continuous Image Representation with Local Implicit Image Function},\n    author = {Yinbo Chen and Sifei Liu and Xiaolong Wang},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    entrytype = {inproceedings},\n    id = {chen2021liif}\n}","Bibtex Name":"chenoralliif","Citation Count":"10","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/yinboc/liif","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/16/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Sinusoidal Activation (SIREN)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"2D Image Neural Fields, Fundamentals, Data-Driven Method, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"LIIF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2012.09161.pdf","Project webpage link":"https://yinboc.github.io/liif/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=6f2roieSY_8","Timestamp":"8/29/2021 17:03","Title":"Learning Continuous Image Representation with Local Implicit Image Function","Training time (hr)":"","UID":"100","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a method for composing photorealistic scenes from captured images of objects. Our work builds upon neural radiance fields (NeRFs), which implicitly model the volumetric density and directionally-emitted radiance of a scene. While NeRFs synthesize realistic pictures, they only model static scenes and are closely tied to specific imaging conditions. This property makes NeRFs hard to generalize to new scenarios, including new lighting or new arrangements of objects. Instead of learning a scene radiance field as a NeRF does, we propose to learn object-centric neural scattering functions (OSFs), a representation that models per-object light transport implicitly using a lighting- and view-dependent neural network. This enables rendering scenes even when objects or lights move, without retraining. Combined with a volumetric path tracing procedure, our framework is capable of rendering both intra- and inter-object light transport effects including occlusions, specularities, shadows, and indirect illumination. We evaluate our approach on scene composition and show that it generalizes to novel illumination conditions, producing photorealistic, physically accurate renderings of multi-object scenes.","Authors (format: First Last, First Middle Last, ...)":"Michelle Guo, Alireza Fathi, Jiajun Wu, Thomas Funkhouser","Bibtex (e.g. @inproceedings...)":"@article{guo2020osfs,\n    journal = {arXiv preprint arXiv:2012.08503},\n    booktitle = {ArXiv Pre-print},\n    author = {Michelle Guo and Alireza Fathi and Jiajun Wu and Thomas Funkhouser},\n    title = {Object-Centric Neural Scene Rendering},\n    year = {2020},\n    url = {http://arxiv.org/abs/2012.08503v1},\n    entrytype = {article},\n    id = {guo2020osfs}\n}","Bibtex Name":"guo2020osfs","Citation Count":"6","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/15/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Editable, Material/Lighting Estimation, Object-Centric, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"OSFs","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2012.08503.pdf","Project webpage link":"https://www.shellguo.com/osf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=NtR7xgxSL1U","Timestamp":"5/23/2021 18:37","Title":"Object-Centric Neural Scene Rendering","Training time (hr)":"","UID":"99","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2020","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Many learning-based approaches have difficulty scaling to unseen data, as the generality of its learned prior is limited to the scale and variations of the training samples. This holds particularly true with 3D learning tasks, given the sparsity of 3D datasets available. We introduce a new learning framework for 3D modeling and reconstruction that greatly improves the generalization ability of a deep generator. Our approach strives to connect the good ends of both learning-based and optimization-based methods. In particular, unlike the common practice that fixes the pre-trained priors at test time, we propose to further optimize the learned prior and latent code according to the input physical measurements after the training. We show that the proposed strategy effectively breaks the barriers constrained by the pre-trained priors and could lead to high-quality adaptation to unseen data. We realize our framework using the implicit surface representation and validate the efficacy of our approach in a variety of challenging tasks that take highly sparse or collapsed observations as input. Experimental results show that our approach compares favorably with the state-of-the-art methods in terms of both generality and accuracy.","Authors (format: First Last, First Middle Last, ...)":"Mingyue Yang, Yuxin Wen, Weikai Chen, Yongwei Chen, Kui Jia","Bibtex (e.g. @inproceedings...)":"@inproceedings{yang2021deep,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Mingyue Yang and Yuxin Wen and Weikai Chen and Yongwei Chen and Kui Jia},\n    title = {Deep Optimized Priors for 3D Shape Modeling and Reconstruction},\n    year = {2021},\n    url = {http://arxiv.org/abs/2012.07241v1},\n    entrytype = {inproceedings},\n    id = {yang2021deep}\n}","Bibtex Name":"yang2021deep","Citation Count":"3","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/14/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Data-Driven Method, Global Conditioning, Coarse-to-Fine","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2012.07241.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:48","Title":"Deep Optimized Priors for 3D Shape Modeling and Reconstruction","Training time (hr)":"","UID":"98","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural implicit functions have emerged as a powerful representation for surfaces in 3D. Such a function can encode a high quality surface with intricate details into the parameters of a deep neural network. However, optimizing for the parameters for accurate and robust reconstructions remains a challenge, especially when the input data is noisy or incomplete. In this work, we develop a hybrid neural surface representation that allows us to impose geometry-aware sampling and regularization, which significantly improves the fidelity of reconstructions. We propose to use \\emph{iso-points} as an explicit representation for a neural implicit function. These points are computed and updated on-the-fly during training to capture important geometric features and impose geometric constraints on the optimization. We demonstrate that our method can be adopted to improve state-of-the-art techniques for reconstructing neural implicit surfaces from multi-view images or point clouds. Quantitative and qualitative evaluations show that, compared with existing sampling and optimization methods, our approach allows faster convergence, better generalization, and accurate recovery of details and topology.","Authors (format: First Last, First Middle Last, ...)":"Wang Yifan, Shihao Wu, Cengiz Oztireli, Olga Sorkine-Hornung","Bibtex (e.g. @inproceedings...)":"@inproceedings{yifan2021isopoints,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Wang Yifan and Shihao Wu and Cengiz Oztireli and Olga Sorkine-Hornung},\n    title = {Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid Representations},\n    year = {2021},\n    url = {http://arxiv.org/abs/2012.06434v2},\n    entrytype = {inproceedings},\n    id = {yifan2021isopoints}\n}","Bibtex Name":"yifan2021isopoints","Citation Count":"1","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/11/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Sampling, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Iso-Points","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2012.06434.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:59","Title":"Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid Representations","Training time (hr)":"","UID":"97","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present iNeRF, a framework that performs mesh-free pose estimation by \"inverting\" a Neural RadianceField (NeRF). NeRFs have been shown to be remarkably effective for the task of view synthesis - synthesizing photorealistic novel views of real-world scenes or objects. In this work, we investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free, RGB-only 6DoF pose estimation - given an image, find the translation and rotation of a camera relative to a 3D object or scene. Our method assumes that no object mesh models are available during either training or test time. Starting from an initial pose estimate, we use gradient descent to minimize the residual between pixels rendered from a NeRF and pixels in an observed image. In our experiments, we first study 1) how to sample rays during pose refinement for iNeRF to collect informative gradients and 2) how different batch sizes of rays affect iNeRF on a synthetic dataset. We then show that for complex real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating the camera poses of novel images and using these images as additional training data for NeRF. Finally, we show iNeRF can perform category-level object pose estimation, including object instances not seen during training, with RGB images by inverting a NeRF model inferred from a single view.","Authors (format: First Last, First Middle Last, ...)":"Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto Rodriguez, Phillip Isola, Tsung-Yi Lin","Bibtex (e.g. @inproceedings...)":"@article{yen-chen2021inerf,\n    author = {Lin Yen-Chen and Pete Florence and Jonathan T. Barron and Alberto Rodriguez and Phillip Isola and Tsung-Yi Lin},\n    title = {INeRF: Inverting Neural Radiance Fields for Pose Estimation},\n    year = {2020},\n    month = {Dec},\n    url = {http://arxiv.org/abs/2012.05877v3},\n    entrytype = {article},\n    id = {yen-chen2021inerf}\n}","Bibtex Name":"yen-chen2021inerf","Citation Count":"9","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/10/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Camera Parameter Estimation, Sampling","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"iNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2012.05877.pdf","Project webpage link":"https://yenchenlin.me/inerf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=eQuCZaQN0tI","Timestamp":"5/23/2021 18:55","Title":"iNeRF: Inverting Neural Radiance Fields for Pose Estimation","Training time (hr)":"","UID":"96","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"IROS 2021","Venue no Year":"IROS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a method for estimating Neural Radiance Fields (NeRF) from a single headshot portrait. While NeRF has demonstrated high-quality view synthesis, it requires multiple images of static scenes and thus impractical for casual captures and moving subjects. In this work, we propose to pretrain the weights of a multilayer perceptron (MLP), which implicitly models the volumetric density and colors, with a meta-learning framework using a light stage portrait dataset. To improve the generalization to unseen faces, we train the MLP in the canonical coordinate space approximated by 3D face morphable models. We quantitatively evaluate the method using controlled captures and demonstrate the generalization to real portrait images, showing favorable results against state-of-the-arts.","Authors (format: First Last, First Middle Last, ...)":"Chen Gao, Yichang Shih, Wei-Sheng Lai, Chia-Kai Liang, Jia-Bin Huang","Bibtex (e.g. @inproceedings...)":"@article{gao2020portraitnerf,\n    journal = {arXiv preprint arXiv:2012.05903},\n    booktitle = {ArXiv Pre-print},\n    author = {Chen Gao and Yichang Shih and Wei-Sheng Lai and Chia-Kai Liang and Jia-Bin Huang},\n    title = {Portrait Neural Radiance Fields from a Single Image},\n    year = {2020},\n    url = {http://arxiv.org/abs/2012.05903v2},\n    entrytype = {article},\n    id = {gao2020portraitnerf}\n}","Bibtex Name":"gao2020portraitnerf","Citation Count":"6","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"https://portrait-nerf.github.io/#","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/10/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Head), Sparse Reconstruction, Generalization, Data-Driven Method","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"PortraitNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2012.05903.pdf","Project webpage link":"https://portrait-nerf.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:25","Title":"Portrait Neural Radiance Fields from a Single Image","Training time (hr)":"","UID":"95","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2020","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a method that takes as input a set of images of a scene illuminated by unconstrained known lighting, and produces as output a 3D representation that can be rendered from novel viewpoints under arbitrary lighting conditions. Our method represents the scene as a continuous volumetric function parameterized as MLPs whose inputs are a 3D location and whose outputs are the following scene properties at that input location: volume density, surface normal, material parameters, distance to the first surface intersection in any direction, and visibility of the external environment in any direction. Together, these allow us to render novel views of the object under arbitrary lighting, including indirect illumination effects. The predicted visibility and surface intersection fields are critical to our model's ability to simulate direct and indirect illumination during training, because the brute-force techniques used by prior work are intractable for lighting conditions outside of controlled setups with a single light. Our method outperforms alternative approaches for recovering relightable 3D scene representations, and performs well in complex lighting settings that have posed a significant challenge to prior work.","Authors (format: First Last, First Middle Last, ...)":"Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, Jonathan T. Barron","Bibtex (e.g. @inproceedings...)":"@inproceedings{srinivasan2021nerv,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Pratul P. Srinivasan and Boyang Deng and Xiuming Zhang and Matthew Tancik and Ben Mildenhall and Jonathan T. Barron},\n    title = {NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis},\n    year = {2021},\n    url = {http://arxiv.org/abs/2012.03927v1},\n    entrytype = {inproceedings},\n    id = {srinivasan2021nerv}\n}","Bibtex Name":"srinivasan2021nerv","Citation Count":"21","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/7/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Material/Lighting Estimation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeRV","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2012.03927.pdf","Project webpage link":"https://pratulsrinivasan.github.io/nerv/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=4XyDdvhhjVo","Timestamp":"5/23/2021 19:07","Title":"NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis","Training time (hr)":"","UID":"94","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Decomposing a scene into its shape, reflectance, and illumination is a challenging but essential problem in computer vision and graphics. This problem is inherently more challenging when the illumination is not a single light source under laboratory conditions but is instead an unconstrained environmental illumination. Though recent work has shown that implicit representations can be used to model the radiance field of an object, these techniques only enable view synthesis and not relighting. Additionally, evaluating these radiance fields is resource and time-intensive. By decomposing a scene into explicit representations, any rendering framework can be leveraged to generate novel views under any illumination in real-time. NeRD is a method that achieves this decomposition by introducing physically-based rendering to neural radiance fields. Even challenging non-Lambertian reflectances, complex geometry, and unknown illumination can be decomposed into high-quality models. The datasets and code is available on the project page: https://markboss.me/publication/2021-nerd/","Authors (format: First Last, First Middle Last, ...)":"Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu, Hendrik P. A. Lensch","Bibtex (e.g. @inproceedings...)":"@inproceedings{boss2021nerd,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Mark Boss and Raphael Braun and Varun Jampani and Jonathan T. Barron and Ce Liu and Hendrik P. A. Lensch},\n    title = {NeRD: Neural Reflectance Decomposition from Image Collections},\n    year = {2021},\n    url = {http://arxiv.org/abs/2012.03918v4},\n    entrytype = {inproceedings},\n    id = {boss2021nerd}\n}","Bibtex Name":"boss2021nerd","Citation Count":"3","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition","Coordinates all at once":"","Data Release (link)":"https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition/blob/master/download_datasets.py","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/7/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Material/Lighting Estimation, Sampling","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeRD","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2012.03918.pdf","Project webpage link":"https://markboss.me/publication/2021-nerd/?s=09","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=JL-qMTXw9VU, https://www.youtube.com/watch?v=IM9OgMwHNTI","Timestamp":"5/23/2021 19:01","Title":"NeRD: Neural Reflectance Decomposition from Image Collections","Training time (hr)":"","UID":"93","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications. Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoints or head-poses is required. In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks. To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions. We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup. In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.","Authors (format: First Last, First Middle Last, ...)":"Guy Gafni, Justus Thies, Michael Zollh\u00f6fer, Matthias Nie\u00dfner","Bibtex (e.g. @inproceedings...)":"@inproceedings{gafni2021nerface,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Guy Gafni and Justus Thies and Michael Zollhofer and Matthias Niessner},\n    title = {Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction},\n    year = {2021},\n    url = {http://arxiv.org/abs/2012.03065v1},\n    entrytype = {inproceedings},\n    id = {gafni2021nerface}\n}","Bibtex Name":"gafni2021nerface","Citation Count":"18","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/gafniguy/4D-Facial-Avatars","Coordinates all at once":"","Data Release (link)":"Available upon request","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/5/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Head), Data-Driven Method, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NerFACE","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2012.03065.pdf","Project webpage link":"https://gafniguy.github.io/4D-Facial-Avatars/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=XihxC65tmyA, https://www.youtube.com/watch?v=m7oROLdQnjk","Timestamp":"7/19/2021 21:30","Title":"Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction","Training time (hr)":"","UID":"92","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce a new generator architecture, aimed at fast and efficient high-resolution image-to-image translation. We design the generator to be an extremely lightweight function of the full-resolution image. In fact, we use pixel-wise networks; that is, each pixel is processed independently of others, through a composition of simple affine transformations and nonlinearities. We take three important steps to equip such a seemingly simple function with adequate expressivity. First, the parameters of the pixel-wise networks are spatially varying so they can represent a broader function class than simple 1x1 convolutions. Second, these parameters are predicted by a fast convolutional network that processes an aggressively low-resolution representation of the input; Third, we augment the input image with a sinusoidal encoding of spatial coordinates, which provides an effective inductive bias for generating realistic novel high-frequency image content. As a result, our model is up to 18x faster than state-of-the-art baselines. We achieve this speedup while generating comparable visual quality across different image resolutions and translation domains.","Authors (format: First Last, First Middle Last, ...)":"Tamar Rott Shaham, Michael Gharbi, Richard Zhang, Eli Shechtman, Tomer Michaeli","Bibtex (e.g. @inproceedings...)":"@inproceedings{shaham2021asapnet,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Tamar Rott Shaham and Michael Gharbi and Richard Zhang and Eli Shechtman and Tomer Michaeli},\n    title = {Spatially-Adaptive Pixelwise Networks for Fast Image Translation},\n    year = {2021},\n    url = {http://arxiv.org/abs/2012.02992v1},\n    entrytype = {inproceedings},\n    id = {shaham2021asapnet}\n}","Bibtex Name":"shaham2021asapnet","Citation Count":"2","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/tamarott/ASAPNet","Coordinates all at once":"Yes","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/5/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct, Indirect","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, 2D Image Neural Fields, Data-Driven Method","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"ASAPNet","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2012.02992.pdf","Project webpage link":"https://tamarott.github.io/ASAPNet_web/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=6-OfZ32CoBE","Timestamp":"8/29/2021 20:39","Title":"Spatially-Adaptive Pixelwise Networks for Fast Image Translation","Training time (hr)":"","UID":"91","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixelNeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pixelNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website: https://alexyu.net/pixelnerf","Authors (format: First Last, First Middle Last, ...)":"Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa","Bibtex (e.g. @inproceedings...)":"@inproceedings{yu2021pixelnerf,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Alex Yu and Vickie Ye and Matthew Tancik and Angjoo Kanazawa},\n    title = {pixelNeRF: Neural Radiance Fields from One or Few Images},\n    year = {2021},\n    url = {http://arxiv.org/abs/2012.02190v3},\n    entrytype = {inproceedings},\n    id = {yu2021pixelnerf}\n}","Bibtex Name":"yu2021pixelnerf","Citation Count":"37","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/sxyu/pixel-nerf","Coordinates all at once":"","Data Release (link)":"https://drive.google.com/drive/folders/1PsT3uKwqHHD2bEEHkIXB99AlIjtmrEiR","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/3/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Sparse Reconstruction, Generalization, Data-Driven Method, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"pixelNeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2012.02190.pdf","Project webpage link":"https://alexyu.net/pixelnerf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=voebZx7f32g","Timestamp":"5/23/2021 18:50","Title":"pixelNeRF: Neural Radiance Fields from One or Few Images","Training time (hr)":"","UID":"90","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Coordinate-based neural representations have shown significant promise as an alternative to discrete, array-based representations for complex low dimensional signals. However, optimizing a coordinate-based network from randomly initialized weights for each new signal is inefficient. We propose applying standard meta-learning algorithms to learn the initial weight parameters for these fully-connected networks based on the underlying class of signals being represented (e.g., images of faces or 3D models of chairs). Despite requiring only a minor change in implementation, using these learned initial weights enables faster convergence during optimization and can serve as a strong prior over the signal class being modeled, resulting in better generalization when only partial observations of a given signal are available. We explore these benefits across a variety of tasks, including representing 2D images, reconstructing CT scans, and recovering 3D shapes and scenes from 2D image observations.","Authors (format: First Last, First Middle Last, ...)":"Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P. Srinivasan, Jonathan T. Barron, Ren Ng","Bibtex (e.g. @inproceedings...)":"@inproceedings{tancik2021learned,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Matthew Tancik and Ben Mildenhall and Terrance Wang and Divi Schmidt and Pratul P. Srinivasan and Jonathan T. Barron and Ren Ng},\n    title = {Learned Initializations for Optimizing Coordinate-Based Neural Representations},\n    year = {2021},\n    url = {http://arxiv.org/abs/2012.02189v2},\n    entrytype = {inproceedings},\n    id = {tancik2021learned}\n}","Bibtex Name":"tancik2021learned","Citation Count":"12","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/3/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Fundamentals, Data-Driven Method, Hypernetwork/Meta-learning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2012.02189.pdf","Project webpage link":"https://www.matthewtancik.com/learnit","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:53","Title":"Learned Initializations for Optimizing Coordinate-Based Neural Representations","Training time (hr)":"","UID":"89","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Numerical integration is a foundational technique in scientific computing and is at the core of many computer vision applications. Among these applications, neural volume rendering has recently been proposed as a new paradigm for view synthesis, achieving photorealistic image quality. However, a fundamental obstacle to making these methods practical is the extreme computational and memory requirements caused by the required volume integrations along the rendered rays during training and inference. Millions of rays, each requiring hundreds of forward passes through a neural network are needed to approximate those integrations with Monte Carlo sampling. Here, we propose automatic integration, a new framework for learning efficient, closed-form solutions to integrals using coordinate-based neural networks. For training, we instantiate the computational graph corresponding to the derivative of the network. The graph is fitted to the signal to integrate. After optimization, we reassemble the graph to obtain a network that represents the antiderivative. By the fundamental theorem of calculus, this enables the calculation of any definite integral in two evaluations of the network. Applying this approach to neural rendering, we improve a tradeoff between rendering speed and image quality: improving render times by greater than 10 times with a tradeoff of slightly reduced image quality.","Authors (format: First Last, First Middle Last, ...)":"David B. Lindell, Julien N. P. Martel, Gordon Wetzstein","Bibtex (e.g. @inproceedings...)":"@inproceedings{lindell2021autoint,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {David B. Lindell and Julien N. P. Martel and Gordon Wetzstein},\n    title = {AutoInt: Automatic Integration for Fast Neural Volume Rendering},\n    year = {2021},\n    url = {http://arxiv.org/abs/2012.01714v2},\n    entrytype = {inproceedings},\n    id = {lindell2021autoint}\n}","Bibtex Name":"lindell2021autoint","Citation Count":"16","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/3/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Fundamentals, Sampling","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"AutoInt","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2012.01714.pdf","Project webpage link":"http://www.computationalimaging.org/publications/automatic-integration/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 18:59","Title":"AutoInt: Automatic Integration for Fast Neural Volume Rendering","Training time (hr)":"","UID":"88","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D reconstruction of non-rigid objects. Specifically, we implicitly model a deformation graph via a deep neural network. This neural deformation graph does not rely on any object-specific structure and, thus, can be applied to general non-rigid deformation tracking. Our method globally optimizes this neural graph on a given sequence of depth camera observations of a non-rigidly moving object. Based on explicit viewpoint consistency as well as inter-frame graph and surface consistency constraints, the underlying network is trained in a self-supervised fashion. We additionally optimize for the geometry of the object with an implicit deformable multi-MLP shape representation. Our approach does not assume sequential input data, thus enabling robust tracking of fast motions or even temporally disconnected recordings. Our experiments demonstrate that our Neural Deformation Graphs outperform state-of-the-art non-rigid reconstruction approaches both qualitatively and quantitatively, with 64% improved reconstruction and 62% improved deformation tracking performance.","Authors (format: First Last, First Middle Last, ...)":"Alja\u017e Bo\u017ei\u010d, Pablo Palafox, Michael Zollh\u00f6fer, Justus Thies, Angela Dai, Matthias Nie\u00dfner","Bibtex (e.g. @inproceedings...)":"@inproceedings{bozic2021neuraldeformationgraphs,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Aljaz Bozic and Pablo Palafox and Michael Zollhofer and Justus Thies and Angela Dai and Matthias Niessner},\n    title = {Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction},\n    year = {2021},\n    url = {http://arxiv.org/abs/2012.01451v1},\n    entrytype = {inproceedings},\n    id = {bozic2021neuraldeformationgraphs}\n}","Bibtex Name":"bozic2021neuraldeformationgraphs","Citation Count":"1","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/2/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Human (Body)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Neural Deformation Graphs","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2012.01451.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 22:05","Title":"Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction","Training time (hr)":"","UID":"87","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present the first deep implicit 3D morphable model (i3DMM) of full heads. Unlike earlier morphable face models it not only captures identity-specific geometry, texture, and expressions of the frontal face, but also models the entire head, including hair. We collect a new dataset consisting of 64 people with different expressions and hairstyles to train i3DMM. Our approach has the following favorable properties: (i) It is the first full head morphable model that includes hair. (ii) In contrast to mesh-based models it can be trained on merely rigidly aligned scans, without requiring difficult non-rigid registration. (iii) We design a novel architecture to decouple the shape model into an implicit reference shape and a deformation of this reference shape. With that, dense correspondences between shapes can be learned implicitly. (iv) This architecture allows us to semantically disentangle the geometry and color components, as color is learned in the reference space. Geometry is further disentangled as identity, expressions, and hairstyle, while color is disentangled as identity and hairstyle components. We show the merits of i3DMM using ablation studies, comparisons to state-of-the-art models, and applications such as semantic head editing and texture transfer. We will make our model publicly available.","Authors (format: First Last, First Middle Last, ...)":"Tarun Yenamandra, Ayush Tewari, Florian Bernard, Hans-Peter Seidel, Mohamed Elgharib, Daniel Cremers, Christian Theobalt","Bibtex (e.g. @inproceedings...)":"@inproceedings{yenamandra2021i3dmm,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Tarun Yenamandra and Ayush Tewari and Florian Bernard and Hans-Peter Seidel and Mohamed Elgharib and Daniel Cremers and Christian Theobalt},\n    title = {i3DMM: Deep Implicit 3D Morphable Model of Human Heads},\n    year = {2021},\n    url = {http://arxiv.org/abs/2011.14143v1},\n    entrytype = {inproceedings},\n    id = {yenamandra2021i3dmm}\n}","Bibtex Name":"yenamandra2021i3dmm","Citation Count":"3","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/tarun738/i3DMM","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/28/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Human (Head), Editable, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"i3DMM","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2011.14143.pdf","Project webpage link":"https://vcai.mpi-inf.mpg.de/projects/i3DMM/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=4pYzV3ButPY","Timestamp":"7/19/2021 21:22","Title":"i3DMM: Deep Implicit 3D Morphable Model of Human Heads","Training time (hr)":"","UID":"86","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF), which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions from a \\emph{single} camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be released.","Authors (format: First Last, First Middle Last, ...)":"Albert Pumarola, Enric Corona, Gerard Pons-Moll, Francesc Moreno-Noguer","Bibtex (e.g. @inproceedings...)":"@inproceedings{pumarola2021dnerf,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Albert Pumarola and Enric Corona and Gerard Pons-Moll and Francesc Moreno-Noguer},\n    title = {D-NeRF: Neural Radiance Fields for Dynamic Scenes},\n    year = {2021},\n    url = {http://arxiv.org/abs/2011.13961v1},\n    entrytype = {inproceedings},\n    id = {pumarola2021dnerf}\n}","Bibtex Name":"pumarola2021dnerf","Citation Count":"41","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/albertpumarola/D-NeRF","Coordinates all at once":"","Data Release (link)":"https://www.dropbox.com/s/0bf6fl0ye2vz3vr/data.zip?dl=0","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/27/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"D-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2011.13961.pdf","Project webpage link":"https://www.albertpumarola.com/research/D-NeRF/index.html","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=lSgzmgi2JPw","Timestamp":"5/23/2021 18:16","Title":"D-NeRF: Neural Radiance Fields for Dynamic Scenes","Training time (hr)":"","UID":"85","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Existing image generator networks rely heavily on spatial convolutions and, optionally, self-attention blocks in order to gradually synthesize images in a coarse-to-fine manner. Here, we present a new architecture for image generators, where the color value at each pixel is computed independently given the value of a random latent vector and the coordinate of that pixel. No spatial convolutions or similar operations that propagate information across pixels are involved during the synthesis. We analyze the modeling capabilities of such generators when trained in an adversarial fashion, and observe the new generators to achieve similar generation quality to state-of-the-art convolutional generators. We also investigate several interesting properties unique to the new architecture.","Authors (format: First Last, First Middle Last, ...)":"Ivan Anokhin, Kirill Demochkin, Taras Khakhulin, Gleb Sterkin, Victor Lempitsky, Denis Korzhenkov","Bibtex (e.g. @inproceedings...)":"@inproceedings{anokhin2021cips,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Ivan Anokhin and Kirill Demochkin and Taras Khakhulin and Gleb Sterkin and Victor Lempitsky and Denis Korzhenkov},\n    title = {Image Generators with Conditionally-Independent Pixel Synthesis},\n    year = {2021},\n    url = {http://arxiv.org/abs/2011.13775v1},\n    entrytype = {inproceedings},\n    id = {anokhin2021cips}\n}","Bibtex Name":"anokhin2021cips","Citation Count":"11","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/saic-mdal/CIPS","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/27/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"2D Image Neural Fields","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"CIPS","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2011.13775.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/29/2021 20:48","Title":"Image Generators with Conditionally-Independent Pixel Synthesis","Training time (hr)":"","UID":"84","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a method to perform novel view and time synthesis of dynamic scenes, requiring only a monocular video with known camera poses as input. To do this, we introduce Neural Scene Flow Fields, a new representation that models the dynamic scene as a time-variant continuous function of appearance, geometry, and 3D scene motion. Our representation is optimized through a neural network to fit the observed input views. We show that our representation can be used for complex dynamic scenes, including thin structures, view-dependent effects, and natural degrees of motion. We conduct a number of experiments that demonstrate our approach significantly outperforms recent monocular view synthesis methods, and show qualitative results of space-time view synthesis on a variety of real-world videos.","Authors (format: First Last, First Middle Last, ...)":"Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang","Bibtex (e.g. @inproceedings...)":"@inproceedings{li2021nsff,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Zhengqi Li and Simon Niklaus and Noah Snavely and Oliver Wang},\n    title = {Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes},\n    year = {2021},\n    url = {http://arxiv.org/abs/2011.13084v3},\n    entrytype = {inproceedings},\n    id = {li2021nsff}\n}","Bibtex Name":"li2021nsff","Citation Count":"36","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/zhengqili/Neural-Scene-Flow-Fields","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/26/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NSFF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2011.13084.pdf","Project webpage link":"http://www.cs.cornell.edu/~zl548/NSFF/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"https://www.cs.cornell.edu/~zl548/NSFF/NSFF_supp.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 19:03","Title":"Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes","Training time (hr)":"","UID":"83","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub \"nerfies.\" We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.","Authors (format: First Last, First Middle Last, ...)":"Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, Ricardo Martin-Brualla","Bibtex (e.g. @inproceedings...)":"@inproceedings{park2021nerfies,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Keunhong Park and Utkarsh Sinha and Jonathan T. Barron and Sofien Bouaziz and Dan B Goldman and Steven M. Seitz and Ricardo Martin-Brualla},\n    title = {Nerfies: Deformable Neural Radiance Fields},\n    year = {2021},\n    url = {http://arxiv.org/abs/2011.12948v5},\n    entrytype = {inproceedings},\n    id = {park2021nerfies}\n}","Bibtex Name":"park2021nerfies","Citation Count":"54","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/25/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Global Conditioning, Coarse-to-Fine","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Nerfies","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2011.12948.pdf","Project webpage link":"https://nerfies.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=MrKrnHhk8IA","Timestamp":"5/23/2021 19:01","Title":"Nerfies: Deformable Neural Radiance Fields","Training time (hr)":"","UID":"82","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"With the advent of Neural Radiance Fields (NeRF), neural networks can now render novel views of a 3D scene with quality that fools the human eye. Yet, generating these images is very computationally intensive, limiting their applicability in practical scenarios. In this paper, we propose a technique based on spatial decomposition capable of mitigating this issue. Our key observation is that there are diminishing returns in employing larger (deeper and/or wider) networks. Hence, we propose to spatially decompose a scene and dedicate smaller networks for each decomposed part. When working together, these networks can render the whole scene. This allows us near-constant inference time regardless of the number of decomposed parts. Moreover, we show that a Voronoi spatial decomposition is preferable for this purpose, as it is provably compatible with the Painter's Algorithm for efficient and GPU-friendly rendering. Our experiments show that for real-world scenes, our method provides up to 3x more efficient inference than NeRF (with the same rendering quality), or an improvement of up to 1.0~dB in PSNR (for the same inference cost).","Authors (format: First Last, First Middle Last, ...)":"Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, Andrea Tagliasacchi","Bibtex (e.g. @inproceedings...)":"@inproceedings{rebain2021derf,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Daniel Rebain and Wei Jiang and Soroosh Yazdani and Ke Li and Kwang Moo Yi and Andrea Tagliasacchi},\n    title = {DeRF: Decomposed Radiance Fields},\n    year = {2021},\n    url = {http://arxiv.org/abs/2011.12490v1},\n    entrytype = {inproceedings},\n    id = {rebain2021derf}\n}","Bibtex Name":"rebain2021derf","Citation Count":"17","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/25/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2011.12490.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 19:01","Title":"DeRF: Decomposed Radiance Fields","Training time (hr)":"","UID":"81","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a method that learns a spatiotemporal neural irradiance field for dynamic scenes from a single video. Our learned representation enables free-viewpoint rendering of the input video. Our method builds upon recent advances in implicit representations. Learning a spatiotemporal irradiance field from a single video poses significant challenges because the video contains only one observation of the scene at any point in time. The 3D geometry of a scene can be legitimately represented in numerous ways since varying geometry (motion) can be explained with varying appearance and vice versa. We address this ambiguity by constraining the time-varying geometry of our dynamic scene representation using the scene depth estimated from video depth estimation methods, aggregating contents from individual frames into a single global representation. We provide an extensive quantitative evaluation and demonstrate compelling free-viewpoint rendering results.","Authors (format: First Last, First Middle Last, ...)":"Wenqi Xian, Jia-Bin Huang, Johannes Kopf, Changil Kim","Bibtex (e.g. @inproceedings...)":"@inproceedings{xian2021spacetime,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Wenqi Xian and Jia-Bin Huang and Johannes Kopf and Changil Kim},\n    title = {Space-time Neural Irradiance Fields for Free-Viewpoint Video},\n    year = {2021},\n    url = {http://arxiv.org/abs/2011.12950v2},\n    entrytype = {inproceedings},\n    id = {xian2021spacetime}\n}","Bibtex Name":"xian2021spacetime","Citation Count":"28","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/25/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2011.12950.pdf","Project webpage link":"https://video-nerf.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=2tN8ghNu2sI","Timestamp":"5/23/2021 19:02","Title":"Space-time Neural Irradiance Fields for Free-Viewpoint Video","Training time (hr)":"","UID":"80","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects' shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.","Authors (format: First Last, First Middle Last, ...)":"Michael Niemeyer, Andreas Geiger","Bibtex (e.g. @inproceedings...)":"@inproceedings{niemeyer2021giraffe,\n    url = {http://arxiv.org/abs/2011.12100v2},\n    year = {2021},\n    title = {GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields},\n    author = {Michael Niemeyer and Andreas Geiger},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    entrytype = {inproceedings},\n    id = {niemeyer2021giraffe}\n}","Bibtex Name":"niemeyer2021giraffe","Citation Count":"21","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/24/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generative Models, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"GIRAFFE","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2011.12100.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 19:03","Title":"GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields","Training time (hr)":"","UID":"79","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In most existing learning systems, images are typically viewed as 2D pixel arrays. However, in another paradigm gaining popularity, a 2D image is represented as an implicit neural representation (INR) - an MLP that predicts an RGB pixel value given its (x,y) coordinate. In this paper, we propose two novel architectural techniques for building INR-based image decoders: factorized multiplicative modulation and multi-scale INRs, and use them to build a state-of-the-art continuous image GAN. Previous attempts to adapt INRs for image generation were limited to MNIST-like datasets and do not scale to complex real-world data. Our proposed INR-GAN architecture improves the performance of continuous image generators by several times, greatly reducing the gap between continuous image GANs and pixel-based ones. Apart from that, we explore several exciting properties of the INR-based decoders, like out-of-the-box superresolution, meaningful image-space interpolation, accelerated inference of low-resolution images, an ability to extrapolate outside of image boundaries, and strong geometric prior. The project page is located at https://universome.github.io/inr-gan.","Authors (format: First Last, First Middle Last, ...)":"Ivan Skorokhodov, Savva Ignatyev, Mohamed Elhoseiny","Bibtex (e.g. @inproceedings...)":"@inproceedings{skorokhodov2021inrgan,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Ivan Skorokhodov and Savva Ignatyev and Mohamed Elhoseiny},\n    title = {Adversarial Generation of Continuous Images},\n    year = {2021},\n    url = {http://arxiv.org/abs/2011.12026v2},\n    entrytype = {inproceedings},\n    id = {skorokhodov2021inrgan}\n}","Bibtex Name":"skorokhodov2021inrgan","Citation Count":"7","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/universome/inr-gan","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/24/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, 2D Image Neural Fields, Generative Models, Global Conditioning, Hypernetwork/Meta-learning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"INR-GAN","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2011.12026.pdf","Project webpage link":"https://universome.github.io/inr-gan","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/29/2021 20:43","Title":"Adversarial Generation of Continuous Images","Training time (hr)":"","UID":"78","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Reconstructing continuous surfaces from 3D point clouds is a fundamental operation in 3D\ngeometry processing. Several recent state-of-theart methods address this problem using neural networks to learn signed distance functions (SDFs). In this paper, we introduce Neural-Pull, a new approach that is simple and leads to high quality SDFs. Specifically, we train a neural network to pull query 3D locations to their closest neighbors on the surface using the predicted signed distance values and the gradient at the query locations, both\nof which are computed by the network itself. The pulling operation moves each query location with a stride given by the distance predicted by the network. Based on the sign of the distance, this may move the query location along or against the direction of the gradient of the SDF. This is a differentiable operation that allows us to update the signed distance value and the gradient simultaneously during training. Our outperforming results under widely used benchmarks demonstrate that we can learn SDFs more accurately and flexibly for surface reconstruction and single image reconstruction than the state-of-the-art methods.","Authors (format: First Last, First Middle Last, ...)":"Baorui, Ma and Zhizhong, Han and Yu-shen, Liu and Matthias, Zwicker","Bibtex (e.g. @inproceedings...)":"@inproceedings{NeuralPull,\n    year = {2021},\n    booktitle = {International Conference on Machine Learning (ICML)},\n    author = {Baorui, Ma and Zhizhong, Han and Yu-shen, Liu and Matthias, Zwicker},\n    title = {Neural-Pull: Learning Signed Distance Functions from Point Clouds by Learning to Pull Space onto Surfaces},\n    entrytype = {inproceedings},\n    id = {NeuralPull}\n}","Bibtex Name":"NeuralPull","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/mabaorui/NeuralPull","Coordinates all at once":"","Data Release (link)":"https://drive.google.com/drive/folders/1qre9mgJNCKiX11HnZO10qMZMmPv_gnh3","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/20/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"mbr18@mails.tsinghua.edu.cn","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"2D Image Neural Fields, Global Conditioning","Lighting":"","New entry or update existing?":"Update existing entry","Nickname (e.g. DeepSDF)":"Neural-Pull","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2011.13495.pdf","Project webpage link":"https://github.com/mabaorui/NeuralPull","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"http://cgcad.thss.tsinghua.edu.cn/liuyushen/main/pdf/LiuYS_ICML21_NeuralPull_Supp.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"11/29/2021 22:57","Title":"Neural-Pull: Learning Signed Distance Functions from Point Clouds by Learning to Pull Space onto Surfaces","Training time (hr)":"","UID":"77","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICML 2021","Venue no Year":"ICML","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recent implicit neural rendering methods have demonstrated that it is possible to learn accurate view synthesis for complex scenes by predicting their volumetric density and color supervised solely by a set of RGB images. However, existing methods are restricted to learning efficient representations of static scenes that encode all scene objects into a single neural network, and lack the ability to represent dynamic scenes and decompositions into individual scene objects. In this work, we present the first neural rendering method that decomposes dynamic scenes into scene graphs. We propose a learned scene graph representation, which encodes object transformation and radiance, to efficiently render novel arrangements and views of the scene. To this end, we learn implicitly encoded scenes, combined with a jointly learned latent representation to describe objects with a single implicit function. We assess the proposed method on synthetic and real automotive data, validating that our approach learns dynamic scenes -- only by observing a video of this scene -- and allows for rendering novel photo-realistic views of novel scene compositions with unseen sets of objects at unseen poses.","Authors (format: First Last, First Middle Last, ...)":"Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, Felix Heide","Bibtex (e.g. @inproceedings...)":"@inproceedings{ost2021neural,\n    url = {http://arxiv.org/abs/2011.10379v3},\n    year = {2021},\n    title = {Neural Scene Graphs for Dynamic Scenes},\n    author = {Julian Ost and Fahim Mannan and Nils Thuerey and Julian Knodt and Felix Heide},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    entrytype = {inproceedings},\n    id = {ost2021neural}\n}","Bibtex Name":"ost2021neural","Citation Count":"12","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/20/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Object-Centric, Global Conditioning, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2011.10379.pdf","Project webpage link":"https://light.princeton.edu/publication/neural-scene-graphs/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"https://light.cs.princeton.edu/wp-content/uploads/2021/02/NeuralSceneGraphs_Supplement.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=ea4Y6P0Hk3o","Timestamp":"6/21/2021 16:40","Title":"Neural Scene Graphs for Dynamic Scenes","Training time (hr)":"","UID":"76","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Control barrier functions are widely used to enforce safety properties in robot motion planning and control. However, the problem of constructing barrier functions online and synthesizing safe controllers that can deal with the associated uncertainty has received little attention. This paper investigates safe navigation in unknown environments, using onboard range sensing to construct control barrier functions online. To represent different objects in the environment, we use the distance measurements to train neural network approximations of the signed distance functions incrementally with replay memory. This allows us to formulate a novel robust control barrier safety constraint which takes into account the error in the estimated distance fields and its gradient. Our formulation leads to a second-order cone program, enabling safe and stable control synthesis in a priori unknown environments.","Authors (format: First Last, First Middle Last, ...)":"Kehan Long, Cheng Qian, Jorge Cort\u00e9s, Nikolay Atanasov","Bibtex (e.g. @inproceedings...)":"@article{long2021learning,\n    author = {Kehan Long and Cheng Qian and Jorge Cortes and Nikolay Atanasov},\n    title = {Learning Barrier Functions with Memory for Robust Safe Navigation},\n    year = {2020},\n    month = {Nov},\n    url = {http://arxiv.org/abs/2011.01899v2},\n    entrytype = {article},\n    id = {long2021learning}\n}","Bibtex Name":"long2021learning","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/3/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Robotics, Multi-task/Continual/Transfer learning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2011.01899.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/30/2021 8:45","Title":"Learning Barrier Functions with Memory for Robust Safe Navigation","Training time (hr)":"","UID":"75","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"RAL 2021","Venue no Year":"RAL","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Modern 3D printers are capable of printing large-size light-field displays at high-resolutions. However, optimizing such displays in full 3D volume for a given light-field imagery is still a challenging task. Existing light field displays optimize over relatively small resolutions using","Authors (format: First Last, First Middle Last, ...)":"Quan Zheng, Vahid Babaei, Gordon Wetzstein, Hans-Peter Seidel, Matthias Zwicker, Gurprit Singh","Bibtex (e.g. @inproceedings...)":"@article{zheng2020neural,\n    year = {2020},\n    author = {Quan Zheng and Vahid Babaei and Gordon Wetzstein and Hans-Peter Seidel and Matthias Zwicker and Gurprit Singh},\n    journal = {ACM Transactions on Graphics (TOG)},\n    number = {6},\n    pages = {1--12},\n    publisher = {Association for Computing Machinery},\n    title = {Neural light field 3D printing},\n    volume = {39},\n    entrytype = {article},\n    id = {zheng2020neural}\n}","Bibtex Name":"zheng2020neural","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"Coming soon","Coordinates all at once":"","Data Release (link)":"https://drive.google.com/uc?id=1EGwwQsAlw4C1IS6jgK_4P9uP0Rf4ICug&export=download","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/1/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://dl.acm.org/doi/pdf/10.1145/3414685.3417879","Project webpage link":"https://quan-zheng.github.io/publication/neuralLF3Dprinting20/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"https://quan-zheng.github.io/publication/NeuralLightField3DPrinting-supp.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/17/2021 11:30","Title":"Neural Light Field 3D Printing","Training time (hr)":"","UID":"74","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"SIGGRAPH 2020","Venue no Year":"SIGGRAPH","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose a generalized space-time domain decomposition approach for the physics-informed neural networks (PINNs) to solve nonlinear partial differential equations (PDEs) on arbitrary complex-geometry domains. The proposed framework, named eXtended PINNs (XPINNs), further pushes the boundaries of both PINNs as well as conservative PINNs (cPINNs), which is a recently proposed domain decomposition approach in the PINN framework tailored to conservation laws. Compared to PINN, the XPINN method has large representation and parallelization capacity due to the inherent property of deployment of multiple neural networks in the smaller subdomains. Unlike cPINN, XPINN can be extended to any type of PDEs. Moreover, the domain can be decomposed in any arbitrary way (in space and time), which is not possible in cPINN. Thus, XPINN offers both space and time parallelization, thereby reducing the training cost more effectively. In each subdomain, a separate neural network is employed with optimally selected hyperparameters, e.g., depth/width of the network, number and location of residual points, activation function, optimization method, etc. A deep network can be employed in a subdomain with complex solution, whereas a shallow neural network can be used in a subdomain with relatively simple and smooth solutions. We demonstrate the versatility of XPINN by solving both forward and inverse PDE problems, ranging from one-dimensional to three-dimensional problems, from time-dependent to time-independent problems, and from continuous to discontinuous problems, which clearly shows that the XPINN method is promising in many practical problems. The proposed XPINN method is the generalization of PINN and cPINN methods, both in terms of applicability as well as domain decomposition approach, which efficiently lends itself to parallelized computation. The XPINN code is available on https://github.com/AmeyaJagtap/XPINNs.","Authors (format: First Last, First Middle Last, ...)":"Ameya D Jagtap, George Em Karniadakis","Bibtex (e.g. @inproceedings...)":"@article{jagtap2020extended,\n    title = {Extended physics-informed neural networks (xpinns): A generalized space-time domain decomposition based deep learning framework for nonlinear partial differential equations},\n    author = {Ameya D Jagtap and George Em Karniadakis},\n    journal = {Communications in Computational Physics},\n    volume = {28},\n    number = {5},\n    pages = {2002--2041},\n    year = {2020},\n    entrytype = {article},\n    id = {jagtap2020extended}\n}","Bibtex Name":"jagtap2020extended","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/1/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"XPINNs","PDF link (arXiv perferred)":"https://doc.global-sci.org/uploads/Issue/CiCP/shortpdf/v28n5/285_2002.pdf","Project webpage link":"https://github.com/AmeyaJagtap/XPINNs","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/8/2021 16:31","Title":"Extended Physics-InformedNeuralNetworks (XPINNs): A Generalized Space-Time Domain Decomposition Based Deep Learning Framework for Nonlinear Partial Differential Equations","Training time (hr)":"","UID":"73","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"Computer Physics Communications 2020","Venue no Year":"Computer Physics Communications","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In this work we target a learnable output representation that allows continuous, high resolution outputs of arbitrary shape. Recent works represent 3D surfaces implicitly with a Neural Network, thereby breaking previous barriers in resolution, and ability to represent diverse topologies. However, neural implicit representations are limited to closed surfaces, which divide the space into inside and outside. Many real world objects such as walls of a scene scanned by a sensor, clothing, or a car with inner structures are not closed. This constitutes a significant barrier, in terms of data pre-processing (objects need to be artificially closed creating artifacts), and the ability to output open surfaces. In this work, we propose Neural Distance Fields (NDF), a neural network based model which predicts the unsigned distance field for arbitrary 3D shapes given sparse point clouds. NDF represent surfaces at high resolutions as prior implicit models, but do not require closed surface data, and significantly broaden the class of representable shapes in the output. NDF allow to extract the surface as very dense point clouds and as meshes. We also show that NDF allow for surface normal calculation and can be rendered using a slight modification of sphere tracing. We find NDF can be used for multi-target regression (multiple outputs for one input) with techniques that have been exclusively used for rendering in graphics. Experiments on ShapeNet show that NDF, while simple, is the state-of-the art, and allows to reconstruct shapes with inner structures, such as the chairs inside a bus. Notably, we show that NDF are not restricted to 3D shapes, and can approximate more general open surfaces such as curves, manifolds, and functions. Code is available for research at https://virtualhumans.mpi-inf.mpg.de/ndf/.","Authors (format: First Last, First Middle Last, ...)":"Julian Chibane, Aymen Mir, Gerard Pons-Moll","Bibtex (e.g. @inproceedings...)":"@inproceedings{chibane2020ndf,\n    publisher = {Curran Associates, Inc.},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    author = {Julian Chibane and Aymen Mir and Gerard Pons-Moll},\n    title = {Neural Unsigned Distance Fields for Implicit Function Learning},\n    year = {2020},\n    url = {http://arxiv.org/abs/2010.13938v1},\n    entrytype = {inproceedings},\n    id = {chibane2020ndf}\n}","Bibtex Name":"chibane2020ndf","Citation Count":"17","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/jchibane/ndf/","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/26/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"UDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Data-Driven Method","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NDF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2010.13938.pdf","Project webpage link":"http://virtualhumans.mpi-inf.mpg.de/ndf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"http://virtualhumans.mpi-inf.mpg.de/papers/chibane2020ndf/chibane2020ndf-supp.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=_xsLdVzX8DY","Timestamp":"6/29/2021 15:14","Title":"Neural Unsigned Distance Fields for Implicit Function Learning","Training time (hr)":"","UID":"72","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2020","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We address the problem of fitting 3D human models to 3D scans of dressed humans. Classical methods optimize both the data-to-model correspondences and the human model parameters (pose and shape), but are reliable only when initialized close to the solution. Some methods initialize the optimization based on fully supervised correspondence predictors, which is not differentiable end-to-end, and can only process a single scan at a time. Our main contribution is LoopReg, an end-to-end learning framework to register a corpus of scans to a common 3D human model. The key idea is to create a self-supervised loop. A backward map, parameterized by a Neural Network, predicts the correspondence from every scan point to the surface of the human model. A forward map, parameterized by a human model, transforms the corresponding points back to the scan based on the model parameters (pose and shape), thus closing the loop. Formulating this closed loop is not straightforward because it is not trivial to force the output of the NN to be on the surface of the human model - outside this surface the human model is not even defined. To this end, we propose two key innovations. First, we define the canonical surface implicitly as the zero level set of a distance field in R3, which in contrast to morecommon UV parameterizations, does not require cutting the surface, does not have discontinuities, and does not induce distortion. Second, we diffuse the human model to the 3D domain R3. This allows to map the NN predictions forward,even when they slightly deviate from the zero level set. Results demonstrate that we can train LoopRegmainly self-supervised - following a supervised warm-start, the model becomes increasingly more accurate as additional unlabelled raw scans are processed. Our code and pre-trained models can be downloaded for research.","Authors (format: First Last, First Middle Last, ...)":"Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian Theobalt, Gerard Pons-Moll","Bibtex (e.g. @inproceedings...)":"@inproceedings{bhatnagar2020loopreg,\n    publisher = {Curran Associates, Inc.},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    author = {Bharat Lal Bhatnagar and Cristian Sminchisescu and Christian Theobalt and Gerard Pons-Moll},\n    title = {LoopReg: Self-supervised Learning of Implicit Surface Correspondences, Pose and Shape for 3D Human Mesh Registration},\n    year = {2020},\n    url = {http://arxiv.org/abs/2010.12447v1},\n    entrytype = {inproceedings},\n    id = {bhatnagar2020loopreg}\n}","Bibtex Name":"bhatnagar2020loopreg","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/bharat-b7/LoopReg","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/23/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"LoopReg","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2010.12447.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=fIhm_tWG_X8","Timestamp":"9/17/2021 13:19","Title":"LoopReg: Self-supervised Learning of Implicit Surface Correspondences, Pose and Shape for 3D Human Mesh Registration","Training time (hr)":"","UID":"71","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2020","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Dense 3D object reconstruction from a single image has recently witnessed remarkable advances, but supervising neural networks with ground-truth 3D shapes is impractical due to the laborious process of creating paired image-shape datasets. Recent efforts have turned to learning 3D reconstruction without 3D supervision from RGB images with annotated 2D silhouettes, dramatically reducing the cost and effort of annotation. These techniques, however, remain impractical as they still require multi-view annotations of the same object instance during training. As a result, most experimental efforts to date have been limited to synthetic datasets. In this paper, we address this issue and propose SDF-SRN, an approach that requires only a single view of objects at training time, offering greater utility for real-world scenarios. SDF-SRN learns implicit 3D shape representations to handle arbitrary shape topologies that may exist in the datasets. To this end, we derive a novel differentiable rendering formulation for learning signed distance functions (SDF) from 2D silhouettes. Our method outperforms the state of the art under challenging single-view supervision settings on both synthetic and real-world datasets.","Authors (format: First Last, First Middle Last, ...)":"Chen-Hsuan Lin, Chaoyang Wang, Simon Lucey","Bibtex (e.g. @inproceedings...)":"@inproceedings{lin2020sdfsrn,\n    publisher = {Curran Associates, Inc.},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    author = {Chen-Hsuan Lin and Chaoyang Wang and Simon Lucey},\n    title = {SDF-SRN: Learning Signed Distance 3D Object Reconstruction from Static Images},\n    year = {2020},\n    url = {http://arxiv.org/abs/2010.10505v1},\n    entrytype = {inproceedings},\n    id = {lin2020sdfsrn}\n}","Bibtex Name":"lin2020sdfsrn","Citation Count":"7","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/chenhsuanlin/signed-distance-SRN","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/20/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Data-Driven Method, Hypernetwork/Meta-learning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SDF-SRN","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2010.10505.pdf","Project webpage link":"https://chenhsuanlin.bitbucket.io/signed-distance-SRN/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"https://chenhsuanlin.bitbucket.io/signed-distance-SRN/supplementary.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://chenhsuanlin.bitbucket.io/signed-distance-SRN/video.mp4, https://chenhsuanlin.bitbucket.io/signed-distance-SRN/shorttalk.mp4","Timestamp":"7/19/2021 21:13","Title":"SDF-SRN: Learning Signed Distance 3D Object Reconstruction from Static Images","Training time (hr)":"","UID":"70","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2020","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF's success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at https://github.com/Kai-46/nerfplusplus.","Authors (format: First Last, First Middle Last, ...)":"Kai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun","Bibtex (e.g. @inproceedings...)":"@article{zhang2020nerf++,\n    journal = {arXiv preprint arXiv:2010.07492},\n    booktitle = {ArXiv Pre-print},\n    author = {Kai Zhang and Gernot Riegler and Noah Snavely and Vladlen Koltun},\n    title = {NeRF++: Analyzing and Improving Neural Radiance Fields},\n    year = {2020},\n    url = {http://arxiv.org/abs/2010.07492v2},\n    entrytype = {article},\n    id = {zhang2020nerf++}\n}","Bibtex Name":"zhang2020nerf++","Citation Count":"48","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/15/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals, Sampling","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeRF++","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2010.07492.pdf","Project webpage link":"https://github.com/Kai-46/nerfplusplus","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=Rd0nBO6--bM&feature=youtu.be&t=1992","Timestamp":"5/23/2021 19:04","Title":"NeRF++: Analyzing and Improving Neural Radiance Fields","Training time (hr)":"","UID":"69","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2020","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a simple yet powerful neural network that implicitly represents and renders 3D objects and scenes only from 2D observations. The network models 3D geometries as a general radiance field, which takes a set of 2D images with camera poses and intrinsics as input, constructs an internal representation for each point of the 3D space, and then renders the corresponding appearance and geometry of that point viewed from an arbitrary position. The key to our approach is to learn local features for each pixel in 2D images and to then project these features to 3D points, thus yielding general and rich point representations. We additionally integrate an attention mechanism to aggregate pixel features from multiple 2D views, such that visual occlusions are implicitly taken into account. Extensive experiments demonstrate that our method can generate high-quality and realistic novel views for novel objects, unseen categories and challenging real-world scenes.","Authors (format: First Last, First Middle Last, ...)":"Alex Trevithick, Bo Yang","Bibtex (e.g. @inproceedings...)":"@inproceedings{trevithick2021grf,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Alex Trevithick and Bo Yang},\n    title = {GRF: Learning a General Radiance Field for 3D Representation and Rendering},\n    year = {2021},\n    url = {http://arxiv.org/abs/2010.04595v3},\n    entrytype = {inproceedings},\n    id = {trevithick2021grf}\n}","Bibtex Name":"trevithick2021grf","Citation Count":"25","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/alextrevithick/GRF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/9/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Generalization, Image-Based Rendering, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"GRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2010.04595.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"https://drive.google.com/file/d/1H2FNeAsKoQqCsO0n7PiA1HcT1ingnwJd/view","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/20/2021 10:58","Title":"General Radiance Field","Training time (hr)":"","UID":"68","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2021","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We suggest to represent an X-Field -a set of 2D images taken across different view, time or illumination conditions, i.e., video, light field, reflectance fields or combinations thereof-by learning a neural network (NN) to map their view, time or light coordinates to 2D images. Executing this NN at new coordinates results in joint view, time or light interpolation. The key idea to make this workable is a NN that already knows the \"basic tricks\" of graphics (lighting, 3D projection, occlusion) in a hard-coded and differentiable form. The NN represents the input to that rendering as an implicit map, that for any view, time, or light coordinate and for any pixel can quantify how it will move if view, time or light coordinates change (Jacobian of pixel position with respect to view, time, illumination, etc.). Our X-Field representation is trained for one scene within minutes, leading to a compact set of trainable parameters and hence real-time navigation in view, time and illumination.","Authors (format: First Last, First Middle Last, ...)":"Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, Tobias Ritschel","Bibtex (e.g. @inproceedings...)":"@article{bemana2020xfields,\n    publisher = {Association for Computing Machinery},\n    journal = {ACM Transactions on Graphics (TOG)},\n    author = {Mojtaba Bemana and Karol Myszkowski and Hans-Peter Seidel and Tobias Ritschel},\n    title = {X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation},\n    year = {2020},\n    url = {http://arxiv.org/abs/2010.00450v1},\n    entrytype = {article},\n    id = {bemana2020xfields}\n}","Bibtex Name":"bemana2020xfields","Citation Count":"17","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/m-bemana/xfields","Coordinates all at once":"Yes","Data Release (link)":"https://xfields.mpi-inf.mpg.de/dataset/view_light_time.zip","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/1/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct, Indirect","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, 2D Image Neural Fields, Editable, Material/Lighting Estimation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"X-Fields","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2010.00450.pdf","Project webpage link":"https://xfields.mpi-inf.mpg.de/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=0tsw7yJGfFI","Timestamp":"8/29/2021 21:03","Title":"X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation","Training time (hr)":"","UID":"67","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"SIGGRAPH 2020","Venue no Year":"SIGGRAPH","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Constrained robot motion planning is a widely used technique to solve complex robot tasks. We consider the problem of learning representations of constraints from demonstrations with a deep neural network, which we call Equality Constraint Manifold Neural Network (ECoMaNN). The key idea is to learn a level-set function of the constraint suitable for integration into a constrained sampling-based motion planner. Learning proceeds by aligning subspaces in the network with subspaces of the data. We combine both learned constraints and analytically described constraints into the planner and use a projection-based strategy to find valid points. We evaluate ECoMaNN on its representation capabilities of constraint manifolds, the impact of its individual loss terms, and the motions produced when incorporated into a planner.","Authors (format: First Last, First Middle Last, ...)":"Giovanni Sutanto, Isabel M. Rayas Fern\u00e1ndez, Peter Englert, Ragesh K. Ramachandran, Gaurav S. Sukhatme","Bibtex (e.g. @inproceedings...)":"@inproceedings{sutanto2020learning,\n    booktitle = {Proceedings of the Conference on Robot Learning (CoRL)},\n    author = {Giovanni Sutanto and Isabel M. Rayas Fernandez and Peter Englert and Ragesh K. Ramachandran and Gaurav S. Sukhatme},\n    title = {Learning Equality Constraints for Motion Planning on Manifolds},\n    year = {2020},\n    url = {http://arxiv.org/abs/2009.11852v1},\n    entrytype = {inproceedings},\n    id = {sutanto2020learning}\n}","Bibtex Name":"sutanto2020learning","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/gsutanto/smp_manifold_learning","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/24/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals, Science & Engineering, Robotics","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2009.11852.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=WoC7nqp4XNk","Timestamp":"9/30/2021 11:06","Title":"Learning Equality Constraints for Motion Planning on Manifolds","Training time (hr)":"","UID":"66","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CoRL 2020","Venue no Year":"CoRL","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Common methods for learning robot dynamics assume motion is continuous, causing unrealistic model predictions for systems undergoing discontinuous impact and stiction behavior. In this work, we resolve this conflict with a smooth, implicit encoding of the structure inherent to contact-induced discontinuities. Our method, ContactNets, learns parameterizations of inter-body signed distance and contact-frame Jacobians, a representation that is compatible with many simulation, control, and planning environments for robotics. We furthermore circumvent the need to differentiate through stiff or non-smooth dynamics with a novel loss function inspired by the principles of complementarity and maximum dissipation. Our method can predict realistic impact, non-penetration, and stiction when trained on 60 seconds of real-world data.","Authors (format: First Last, First Middle Last, ...)":"Samuel Pfrommer, Mathew Halm, Michael Posa","Bibtex (e.g. @inproceedings...)":"@inproceedings{pfrommer2020contactnets,\n    booktitle = {Proceedings of the Conference on Robot Learning (CoRL)},\n    author = {Samuel Pfrommer and Mathew Halm and Michael Posa},\n    title = {ContactNets: Learning Discontinuous Contact Dynamics with Smooth, Implicit Representations},\n    year = {2020},\n    url = {http://arxiv.org/abs/2009.11193v2},\n    entrytype = {inproceedings},\n    id = {pfrommer2020contactnets}\n}","Bibtex Name":"pfrommer2020contactnets","Citation Count":"0","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/DAIRLab/contact-nets","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/23/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering, Robotics, Data-Driven Method, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"ContactNets","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2009.11193.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=I6p8JrIp1Es","Timestamp":"9/1/2021 14:36","Title":"ContactNets: Learning Discontinuous Contact Dynamics with Smooth, Implicit Representations","Training time (hr)":"","UID":"65","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CoRL 2020","Venue no Year":"CoRL","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Prior work to infer 3D texture use either texture atlases, which require uv-mappings and hence have discontinuities, or colored voxels, which are memory inefficient and limited in resolution. Recent work, predicts RGB color at every XYZ coordinate forming a texture field, but focus on completing texture given a single 2D image. Instead, we focus on 3D texture and geometry completion from partial and incomplete 3D scans. IF-Nets have recently achieved state-of-the-art results on 3D geometry completion using a multi-scale deep feature encoding, but the outputs lack texture. In this work, we generalize IF-Nets to texture completion from partial textured scans of humans and arbitrary objects. Our key insight is that 3D texture completion benefits from incorporating local and global deep features extracted from both the 3D partial texture and completed geometry. Specifically, given the partial 3D texture and the 3D geometry completed with IF-Nets, our model successfully in-paints the missing texture parts in consistence with the completed geometry. Our model won the SHARP ECCV'20 challenge, achieving highest performance on all challenges.","Authors (format: First Last, First Middle Last, ...)":"Julian Chibane, Gerard Pons-Moll","Bibtex (e.g. @inproceedings...)":"@inproceedings{chibane2020ifnettexture,\n    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},\n    author = {Julian Chibane and Gerard Pons-Moll},\n    title = {Implicit Feature Networks for Texture Completion from Partial 3D Data},\n    year = {2020},\n    url = {http://arxiv.org/abs/2009.09458v1},\n    entrytype = {inproceedings},\n    id = {chibane2020ifnettexture}\n}","Bibtex Name":"chibane2020ifnettexture","Citation Count":"6","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/jchibane/if-net_texture","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/20/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"Yes","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Data-Driven Method","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"IF-Net-Texture","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2009.09458.pdf","Project webpage link":"https://virtualhumans.mpi-inf.mpg.de/ifnets/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:09","Title":"Implicit Feature Networks for Texture Completion from Partial 3D Data","Training time (hr)":"","UID":"64","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2020","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"A neural implicit outputs a number indicating whether the given query point in space is inside, outside, or on a surface. Many prior works have focused on _latent-encoded_ neural implicits, where a latent vector encoding of a specific shape is also fed as input. While affording latent-space interpolation, this comes at the cost of reconstruction accuracy for any _single_ shape. Training a specific network for each 3D shape, a _weight-encoded_ neural implicit may forgo the latent vector and focus reconstruction accuracy on the details of a single shape. While previously considered as an intermediary representation for 3D scanning tasks or as a toy-problem leading up to latent-encoding tasks, weight-encoded neural implicits have not yet been taken seriously as a 3D shape representation. In this paper, we establish that weight-encoded neural implicits meet the criteria of a first-class 3D shape representation. We introduce a suite of technical contributions to improve reconstruction accuracy, convergence, and robustness when learning the signed distance field induced by a polygonal mesh -- the _de facto_ standard representation. Viewed as a lossy compression, our conversion outperforms standard techniques from geometry processing. Compared to previous latent- and weight-encoded neural implicits we demonstrate superior robustness, scalability, and performance.","Authors (format: First Last, First Middle Last, ...)":"Thomas Davies, Derek Nowrouzezahrai, Alec Jacobson","Bibtex (e.g. @inproceedings...)":"@inproceedings{davies2021on,\n    publisher = {PMLR},\n    booktitle = {International Conference on Machine Learning (ICML)},\n    author = {Thomas Davies and Derek Nowrouzezahrai and Alec Jacobson},\n    title = {On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes},\n    year = {2021},\n    url = {http://arxiv.org/abs/2009.09808v3},\n    entrytype = {inproceedings},\n    id = {davies2021on}\n}","Bibtex Name":"davies2021on","Citation Count":"5","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/u2ni/ICML2021","Coordinates all at once":"","Data Release (link)":"https://ten-thousand-models.appspot.com/","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/17/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Compression, Fundamentals, Sampling","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2009.09808.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/29/2021 15:53","Title":"On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes","Training time (hr)":"","UID":"63","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICML 2021","Venue no Year":"ICML","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We investigate the problem of learning to generate 3D parametric surface representations for novel object instances, as seen from one or more views. Previous work on learning shape reconstruction from multiple views uses discrete representations such as point clouds or voxels, while continuous surface generation approaches lack multi-view consistency. We address these issues by designing neural networks capable of generating high-quality parametric 3D surfaces which are also consistent between views. Furthermore, the generated 3D surfaces preserve accurate image pixel to 3D surface point correspondences, allowing us to lift texture information to reconstruct shapes with rich geometry and appearance. Our method is supervised and trained on a public dataset of shapes from common object categories. Quantitative results indicate that our method significantly outperforms previous work, while qualitative results demonstrate the high quality of our reconstructions.","Authors (format: First Last, First Middle Last, ...)":"Jiahui Lei, Srinath Sridhar, Paul Guerrero, Minhyuk Sung, Niloy Mitra, Leonidas J. Guibas","Bibtex (e.g. @inproceedings...)":"@inproceedings{lei2020pix2surf,\n    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},\n    author = {Jiahui Lei and Srinath Sridhar and Paul Guerrero and Minhyuk Sung and Niloy Mitra and Leonidas J. Guibas},\n    title = {Pix2Surf: Learning Parametric 3D Surface Models of Objects from Images},\n    year = {2020},\n    url = {http://arxiv.org/abs/2008.07760v1},\n    entrytype = {inproceedings},\n    id = {lei2020pix2surf}\n}","Bibtex Name":"lei2020pix2surf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/JiahuiLei/Pix2Surf","Coordinates all at once":"No","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/18/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Atlas, Explicit","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Sparse Reconstruction, Generalization, Generative Models, Global Conditioning, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Pix2Surf","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2008.07760.pdf","Project webpage link":"https://geometry.stanford.edu/projects/pix2surf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"https://geometry.stanford.edu/projects/pix2surf/pub/pix2surf_supp.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=jaxB0VSuvms","Timestamp":"9/18/2021 10:22","Title":"Pix2Surf: Learning Parametric 3D Surface Models of Objects from Images","Training time (hr)":"","UID":"62","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2020","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce a novel neural network-based BRDF model and a Bayesian framework for object inverse rendering, i.e., joint estimation of reflectance and natural illumination from a single image of an object of known geometry. The BRDF is expressed with an invertible neural network, namely, normalizing flow, which provides the expressive power of a high-dimensional representation, computational simplicity of a compact analytical model, and physical plausibility of a real-world BRDF. We extract the latent space of real-world reflectance by conditioning this model, which directly results in a strong reflectance prior. We refer to this model as the invertible neural BRDF model (iBRDF). We also devise a deep illumination prior by leveraging the structural bias of deep neural networks. By integrating this novel BRDF model and reflectance and illumination priors in a MAP estimation formulation, we show that this joint estimation can be computed efficiently with stochastic gradient descent. We experimentally validate the accuracy of the invertible neural BRDF model on a large number of measured data and demonstrate its use in object inverse rendering on a number of synthetic and real images. The results show new ways in which deep neural networks can help solve challenging radiometric inverse problems.","Authors (format: First Last, First Middle Last, ...)":"Zhe Chen, Shohei Nobuhara, Ko Nishino","Bibtex (e.g. @inproceedings...)":"@article{chen2020invertible,\n    author = {Zhe Chen and Shohei Nobuhara and Ko Nishino},\n    title = {Invertible Neural BRDF for Object Inverse Rendering},\n    year = {2020},\n    month = {Aug},\n    url = {http://arxiv.org/abs/2008.04030v2}\n}","Bibtex Name":"chen2020invertible","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/chenzhekl/iBRDF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/10/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"qian_zhang@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Material/Lighting Estimation, Generative Models, Data-Driven Method","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2008.04030.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"3/21/2022 10:01","Title":"Invertible Neural BRDF for Object Inverse Rendering","Training time (hr)":"","UID":"61","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2020","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Robotic grasping of house-hold objects has made remarkable progress in recent years. Yet, human grasps are still difficult to synthesize realistically. There are several key reasons: (1) the human hand has many degrees of freedom (more than robotic manipulators); (2) the synthesized hand should conform to the surface of the object; and (3) it should interact with the object in a semantically and physically plausible manner. To make progress in this direction, we draw inspiration from the recent progress on learning-based implicit representations for 3D object reconstruction. Specifically, we propose an expressive representation for human grasp modelling that is efficient and easy to integrate with deep neural networks. Our insight is that every point in a three-dimensional space can be characterized by the signed distances to the surface of the hand and the object, respectively. Consequently, the hand, the object, and the contact area can be represented by implicit surfaces in a common space, in which the proximity between the hand and the object can be modelled explicitly. We name this 3D to 2D mapping as Grasping Field, parameterize it with a deep neural network, and learn it from data. We demonstrate that the proposed grasping field is an effective and expressive representation for human grasp generation. Specifically, our generative model is able to synthesize high-quality human grasps, given only on a 3D object point cloud. The extensive experiments demonstrate that our generative model compares favorably with a strong baseline and approaches the level of natural human grasps. Our method improves the physical plausibility of the hand-object contact reconstruction and achieves comparable performance for 3D hand reconstruction compared to state-of-the-art methods.","Authors (format: First Last, First Middle Last, ...)":"Korrawe Karunratanakul, Jinlong Yang, Yan Zhang, Michael Black, Krikamol Muandet, Siyu Tang","Bibtex (e.g. @inproceedings...)":"@inproceedings{karunratanakul2020graspingfield,\n    url = {http://arxiv.org/abs/2008.04451v3},\n    year = {2020},\n    title = {Grasping Field: Learning Implicit Representations for Human Grasps},\n    author = {Korrawe Karunratanakul and Jinlong Yang and Yan Zhang and Michael Black and Krikamol Muandet and Siyu Tang},\n    booktitle = {International Conference on 3D Vision (3DV)},\n    organization = {IEEE},\n    entrytype = {inproceedings},\n    id = {karunratanakul2020graspingfield}\n}","Bibtex Name":"karunratanakul2020graspingfield","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/korrawe/grasping_field","Coordinates all at once":"No","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/10/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct, Indirect","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Robotics","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Grasping Field","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2008.04451.pdf","Project webpage link":"https://mano.is.tue.mpg.de/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=_1o21xc3TD0","Timestamp":"9/17/2021 11:38","Title":"Grasping Field: Learning Implicit Representations for Human Grasps","Training time (hr)":"","UID":"60","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"3DV 2020","Venue no Year":"3DV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present Neural Reflectance Fields, a novel deep scene representation that encodes volume density, normal and reflectance properties at any 3D point in a scene using a fully-connected neural network. We combine this representation with a physically-based differentiable ray marching framework that can render images from a neural reflectance field under any viewpoint and light. We demonstrate that neural reflectance fields can be estimated from images captured with a simple collocated camera-light setup, and accurately model the appearance of real-world scenes with complex geometry and reflectance. Once estimated, they can be used to render photo-realistic images under novel viewpoint and (non-collocated) lighting conditions and accurately reproduce challenging effects like specularities, shadows and occlusions. This allows us to perform high-quality view synthesis and relighting that is significantly better than previous methods. We also demonstrate that we can compose the estimated neural reflectance field of a real scene with traditional scene models and render them using standard Monte Carlo rendering engines. Our work thus enables a complete pipeline from high-quality and practical appearance acquisition to 3D scene composition and rendering.","Authors (format: First Last, First Middle Last, ...)":"Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall, Kalyan Sunkavalli, Milo\u0161 Ha\u0161an, Yannick Hold-Geoffroy, David Kriegman, Ravi Ramamoorthi","Bibtex (e.g. @inproceedings...)":"@article{bi2020neural,\n    journal = {arXiv preprint arXiv:2008.03824},\n    booktitle = {ArXiv Pre-print},\n    author = {Sai Bi and Zexiang Xu and Pratul Srinivasan and Ben Mildenhall and Kalyan Sunkavalli and Milos Hasan and Yannick Hold-Geoffroy and David Kriegman and Ravi Ramamoorthi},\n    title = {Neural Reflectance Fields for Appearance Acquisition},\n    year = {2020},\n    url = {http://arxiv.org/abs/2008.03824v2},\n    entrytype = {article},\n    id = {bi2020neural}\n}","Bibtex Name":"bi2020neural","Citation Count":"21","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/9/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Material/Lighting Estimation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2008.03824.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=tQZk5OoFgsc","Timestamp":"5/23/2021 19:10","Title":"Neural Reflectance Fields for Appearance Acquisition","Training time (hr)":"","UID":"59","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2020","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multilayer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art.","Authors (format: First Last, First Middle Last, ...)":"Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, Daniel Duckworth","Bibtex (e.g. @inproceedings...)":"@inproceedings{martin-brualla2021nerfw,\n    url = {http://arxiv.org/abs/2008.02268v3},\n    year = {2021},\n    title = {NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections},\n    author = {Ricardo Martin-Brualla and Noha Radwan and Mehdi S. M. Sajjadi and Jonathan T. Barron and Alexey Dosovitskiy and Daniel Duckworth},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    entrytype = {inproceedings},\n    id = {martin-brualla2021nerfw}\n}","Bibtex Name":"martin-brualla2021nerfw","Citation Count":"78","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/5/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Material/Lighting Estimation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeRF-W","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2008.02268.pdf","Project webpage link":"https://nerf-w.github.io/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 19:07","Title":"NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections","Training time (hr)":"","UID":"58","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Implicit surface representations, such as signed-distance functions, combined with deep learning have led to impressive models which can represent detailed shapes of objects with arbitrary topology. Since a continuous function is learned, the reconstructions can also be extracted at any arbitrary resolution. However, large datasets such as ShapeNet are required to train such models. In this paper, we present a new mid-level patch-based surface representation. At the level of patches, objects across different categories share similarities, which leads to more generalizable models. We then introduce a novel method to learn this patch-based representation in a canonical space, such that it is as object-agnostic as possible. We show that our representation trained on one category of objects from ShapeNet can also well represent detailed shapes from any other category. In addition, it can be trained using much fewer shapes, compared to existing approaches. We show several applications of our new representation, including shape interpolation and partial point cloud completion. Due to explicit control over positions, orientations and scales of patches, our representation is also more controllable compared to object-level representations, which enables us to deform encoded shapes non-rigidly.","Authors (format: First Last, First Middle Last, ...)":"Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollh\u00f6fer, Carsten Stoll, Christian Theobalt","Bibtex (e.g. @inproceedings...)":"@inproceedings{tretschk2020patchnets,\n    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},\n    author = {Edgar Tretschk and Ayush Tewari and Vladislav Golyanik and Michael Zollhofer and Carsten Stoll and Christian Theobalt},\n    title = {PatchNets: Patch-Based Generalizable Deep Implicit 3D Shape Representations},\n    year = {2020},\n    url = {http://arxiv.org/abs/2008.01639v2},\n    entrytype = {inproceedings},\n    id = {tretschk2020patchnets}\n}","Bibtex Name":"tretschk2020patchnets","Citation Count":"16","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/edgar-tr/patchnets","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/4/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Generalization, Data-Driven Method, Global Conditioning, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"PatchNets","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2008.01639.pdf","Project webpage link":"http://gvv.mpi-inf.mpg.de/projects/PatchNets/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_supplemental.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_short.mp4, http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_supplemental.mp4, http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_talk.mp4","Timestamp":"8/29/2021 21:13","Title":"PatchNets: Patch-Based Generalizable Deep Implicit 3D Shape Representations","Training time (hr)":"","UID":"57","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2020","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Novel View Synthesis (NVS) is concerned with synthesizing views under camera viewpoint transformations from one or multiple input images. NVS requires explicit reasoning about 3D object structure and unseen parts of the scene to synthesize convincing results. As a result, current approaches typically rely on supervised training with either ground truth 3D models or multiple target images. We propose Continuous Object Representation Networks (CORN), a conditional architecture that encodes an input image's geometry and appearance that map to a 3D consistent scene representation. We can train CORN with only two source images per object by combining our model with a neural renderer. A key feature of CORN is that it requires no ground truth 3D models or target view supervision. Regardless, CORN performs well on challenging tasks such as novel view synthesis and single-view 3D reconstruction and achieves performance comparable to state-of-the-art approaches that use direct supervision. For up-to-date information, data, and code, please see our project page: https://nicolaihaeni.github.io/corn/.","Authors (format: First Last, First Middle Last, ...)":"Nicolai H\u00e4ni, Selim Engin, Jun-Jee Chao, Volkan Isler","Bibtex (e.g. @inproceedings...)":"@inproceedings{hani2020corn,\n    publisher = {Curran Associates, Inc.},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    author = {Nicolai Hani and Selim Engin and Jun-Jee Chao and Volkan Isler},\n    title = {Continuous Object Representation Networks: Novel View Synthesis without Target View Supervision},\n    year = {2020},\n    url = {http://arxiv.org/abs/2007.15627v2},\n    entrytype = {inproceedings},\n    id = {hani2020corn}\n}","Bibtex Name":"hani2020corn","Citation Count":"5","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/30/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Sparse Reconstruction, Generalization, Image-Based Rendering, Data-Driven Method, Global Conditioning, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"CORN","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2007.15627.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:28","Title":"Continuous Object Representation Networks: Novel View Synthesis without Target View Supervision","Training time (hr)":"","UID":"56","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2020","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present the first approach to volumetric performance capture and novel-view rendering at real-time speed from monocular video, eliminating the need for expensive multi-view systems or cumbersome pre-acquisition of a personalized template model. Our system reconstructs a fully textured 3D human from each frame by leveraging Pixel-Aligned Implicit Function (PIFu). While PIFu achieves high-resolution reconstruction in a memory-efficient manner, its computationally expensive inference prevents us from deploying such a system for real-time applications. To this end, we propose a novel hierarchical surface localization algorithm and a direct rendering method without explicitly extracting surface meshes. By culling unnecessary regions for evaluation in a coarse-to-fine manner, we successfully accelerate the reconstruction by two orders of magnitude from the baseline without compromising the quality. Furthermore, we introduce an Online Hard Example Mining (OHEM) technique that effectively suppresses failure modes due to the rare occurrence of challenging examples. We adaptively update the sampling probability of the training data based on the current reconstruction accuracy, which effectively alleviates reconstruction artifacts. Our experiments and evaluations demonstrate the robustness of our system to various challenging angles, illuminations, poses, and clothing styles. We also show that our approach compares favorably with the state-of-the-art monocular performance capture. Our proposed approach removes the need for multi-view studio settings and enables a consumer-accessible solution for volumetric capture.","Authors (format: First Last, First Middle Last, ...)":"Ruilong Li, Yuliang Xiu, Shunsuke Saito, Zeng Huang, Kyle Olszewski, Hao Li","Bibtex (e.g. @inproceedings...)":"@inproceedings{li2020monoport,\n    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},\n    author = {Ruilong Li and Yuliang Xiu and Shunsuke Saito and Zeng Huang and Kyle Olszewski and Hao Li},\n    title = {Monocular Real-Time Volumetric Performance Capture},\n    year = {2020},\n    url = {http://arxiv.org/abs/2007.13988v1},\n    entrytype = {inproceedings},\n    id = {li2020monoport}\n}","Bibtex Name":"li2020monoport","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/Project-Splinter/MonoPort","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/28/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Monoport","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2007.13988.pdf","Project webpage link":"https://project-splinter.github.io/monoport/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://github.com/Project-Splinter/MonoPort","Timestamp":"9/17/2021 14:11","Title":"Monocular Real-Time Volumetric Performance Capture","Training time (hr)":"","UID":"55","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2020","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Deep implicit field regression methods are effective for 3D reconstruction from single-view images. However, the impact of different sampling patterns on the reconstruction quality is not well-understood. In this work, we first study the effect of point set discrepancy on the network training. Based on Farthest Point Sampling algorithm, we propose a sampling scheme that theoretically encourages better generalization performance, and results in fast convergence for SGD-based optimization algorithms. Secondly, based on the reflective symmetry of an object, we propose a feature fusion method that alleviates issues due to self-occlusions which makes it difficult to utilize local image features. Our proposed system Ladybird is able to create high quality 3D object reconstructions from a single input image. We evaluate Ladybird on a large scale 3D dataset (ShapeNet) demonstrating highly competitive results in terms of Chamfer distance, Earth Mover's distance and Intersection Over Union (IoU).","Authors (format: First Last, First Middle Last, ...)":"Yifan Xu, Tianqi Fan, Yi Yuan, Gurprit Singh","Bibtex (e.g. @inproceedings...)":"@inproceedings{xu2020ladybird,\n    url = {http://arxiv.org/abs/2007.13393v1},\n    year = {2020},\n    title = {Ladybird: Quasi-Monte Carlo Sampling for Deep Implicit Field Based 3D Reconstruction with Symmetry},\n    author = {Yifan Xu and Tianqi Fan and Yi Yuan and Gurprit Singh},\n    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},\n    entrytype = {inproceedings},\n    id = {xu2020ladybird}\n}","Bibtex Name":"xuoralladybird","Citation Count":"6","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/FuxiCV/Ladybird","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/27/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Sparse Reconstruction, Fundamentals, Sampling, Symmetry","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Ladybird","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2007.13393.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/29/2021 16:29","Title":"Ladybird: Quasi-Monte Carlo Sampling for Deep Implicit Field Based 3D Reconstruction with Symmetry","Training time (hr)":"","UID":"54","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2020","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a differentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is typically over 10 times faster than the state-of-the-art (namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering. Code and data are available at our website: https://github.com/facebookresearch/NSVF.","Authors (format: First Last, First Middle Last, ...)":"Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, Christian Theobalt","Bibtex (e.g. @inproceedings...)":"@inproceedings{liu2020nsvf,\n    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},\n    author = {Lingjie Liu and Jiatao Gu and Kyaw Zaw Lin and Tat-Seng Chua and Christian Theobalt},\n    title = {Neural Sparse Voxel Fields},\n    year = {2020},\n    url = {http://arxiv.org/abs/2007.11571v2},\n    entrytype = {inproceedings},\n    id = {liu2020nsvf}\n}","Bibtex Name":"liu2020nsvf","Citation Count":"90","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/facebookresearch/NSVF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/22/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"Yes","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Speed & Computational Efficiency, Local Conditioning, Coarse-to-Fine, Voxel Grid, Sampling, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NSVF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2007.11571.pdf","Project webpage link":"https://lingjie0206.github.io/papers/NSVF/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=RFqPwH7QFEI","Timestamp":"5/23/2021 19:11","Title":"Neural Sparse Voxel Fields","Training time (hr)":"","UID":"53","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2020","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Implicit functions represented as deep learning approximations are powerful for reconstructing 3D surfaces. However, they can only produce static surfaces that are not controllable, which provides limited ability to modify the resulting model by editing its pose or shape parameters. Nevertheless, such features are essential in building flexible models for both computer graphics and computer vision. In this work, we present methodology that combines detail-rich implicit functions and parametric representations in order to reconstruct 3D models of people that remain controllable and accurate even in the presence of clothing. Given sparse 3D point clouds sampled on the surface of a dressed person, we use an Implicit Part Network (IP-Net)to jointly predict the outer 3D surface of the dressed person, the and inner body surface, and the semantic correspondences to a parametric body model. We subsequently use correspondences to fit the body model to our inner surface and then non-rigidly deform it (under a parametric body + displacement model) to the outer surface in order to capture garment, face and hair detail. In quantitative and qualitative experiments with both full body data and hand scans we show that the proposed methodology generalizes, and is effective even given incomplete point clouds collected from single-view depth images. Our models and code can be downloaded from http://virtualhumans.mpi-inf.mpg.de/ipnet.","Authors (format: First Last, First Middle Last, ...)":"Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian Theobalt, Gerard Pons-Moll","Bibtex (e.g. @inproceedings...)":"@inproceedings{bhatnagar2020ipnet,\n    url = {http://arxiv.org/abs/2007.11432v1},\n    year = {2020},\n    title = {Combining Implicit Function Learning and Parametric Models for 3D Human Reconstruction},\n    author = {Bharat Lal Bhatnagar and Cristian Sminchisescu and Christian Theobalt and Gerard Pons-Moll},\n    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},\n    entrytype = {inproceedings},\n    id = {bhatnagar2020ipnet}\n}","Bibtex Name":"bhatnagar2020ipnet","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/bharat-b7/IPNet","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/22/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"Yes","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Local Conditioning, Voxel Grid, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"IP-Net","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2007.11432.pdf","Project webpage link":"https://virtualhumans.mpi-inf.mpg.de/ipnet/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://virtualhumans.mpi-inf.mpg.de/ipnet/ECCV_short.mp4","Timestamp":"9/17/2021 13:11","Title":"Combining Implicit Function Learning and Parametric Models for 3D Human Reconstruction","Training time (hr)":"","UID":"52","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2020","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose a novel neural architecture for representing 3D surfaces, which harnesses two complementary shape representations: (i) an explicit representation via an atlas, i.e., embeddings of 2D domains into 3D; (ii) an implicit-function representation, i.e., a scalar function over the 3D volume, with its levels denoting surfaces. We make these two representations synergistic by introducing novel consistency losses that ensure that the surface created from the atlas aligns with the level-set of the implicit function. Our hybrid architecture outputs results which are superior to the output of the two equivalent single-representation networks, yielding smoother explicit surfaces with more accurate normals, and a more accurate implicit occupancy function. Additionally, our surface reconstruction step can directly leverage the explicit atlas-based representation. This process is computationally efficient, and can be directly used by differentiable rasterizers, enabling training our hybrid representation with image-based losses.","Authors (format: First Last, First Middle Last, ...)":"Omid Poursaeed, Matthew Fisher, Noam Aigerman, Vladimir G. Kim","Bibtex (e.g. @inproceedings...)":"@inproceedings{poursaeed2020hybridnet,\n    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},\n    author = {Omid Poursaeed and Matthew Fisher and Noam Aigerman and Vladimir G. Kim},\n    title = {Coupling Explicit and Implicit Surface Representations for Generative 3D Modeling},\n    year = {2020},\n    url = {http://arxiv.org/abs/2007.10294v2},\n    entrytype = {inproceedings},\n    id = {poursaeed2020hybridnet}\n}","Bibtex Name":"poursaeed2020hybridnet","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"No","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/20/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy, Atlas","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Geometry Only, Global Conditioning, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"HybridNet","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2007.10294.pdf","Project webpage link":"https://omidpoursaeed.github.io/publication/hybrid/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"https://omidpoursaeed.github.io/pdf/HybridNet_Supp.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://drive.google.com/file/d/1wwnp6HlDdfYw19__ESxWdTQfmc8Bf--v/view","Timestamp":"10/2/2021 20:25","Title":"Coupling Explicit and Implicit Surface Representations for Generative 3D Modeling","Training time (hr)":"","UID":"51","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2020","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"This paper proposes a new type of generative model that is able to quickly learn a latent representation without an encoder. This is achieved using empirical Bayes to calculate the expectation of the posterior, which is implemented by initialising a latent vector with zeros, then using the gradient of the log-likelihood of the data with respect to this zero vector as new latent points. The approach has similar characteristics to autoencoders, but with a simpler architecture, and is demonstrated in a variational autoencoder equivalent that permits sampling. This also allows implicit representation networks to learn a space of implicit functions without requiring a hypernetwork, retaining their representation advantages across datasets. The experiments show that the proposed method converges faster, with significantly lower reconstruction error than autoencoders, while requiring half the parameters.","Authors (format: First Last, First Middle Last, ...)":"Sam Bond-Taylor & Chris G. Willcocks","Bibtex (e.g. @inproceedings...)":"@inproceedings{bond2020gradient,\n    title = {Gradient Origin Networks},\n    author = {Sam Bond-Taylor and Chris G. Willcocks},\n    booktitle = {International Conference on Learning Representations},\n    year = {2021},\n    url = {https://openreview.net/pdf?id=0O_cQfw6uEh},\n    entrytype = {inproceedings},\n    id = {bond2020gradient}\n}","Bibtex Name":"bond2020gradient","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/cwkx/GON","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/6/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"chris.willcocks@gmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Sinusoidal Activation (SIREN)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"None, N/A","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"Speed & Computational Efficiency, 2D Image Neural Fields, Compression, Generative Models, Generalization, Sampling","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"GON","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2007.02798.pdf","Project webpage link":"https://cwkx.github.io/data/GON/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=v-ZxzTSpmk4","Timestamp":"11/24/2021 5:24","Title":"Gradient Origin Networks","Training time (hr)":"","UID":"50","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICLR 2021","Venue no Year":"ICLR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"While 2D generative adversarial networks have enabled high-resolution image synthesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper, we propose a generative model for radiance fields which have recently proven successful for novel view synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high fidelity.","Authors (format: First Last, First Middle Last, ...)":"Katja Schwarz, Yiyi Liao, Michael Niemeyer, Andreas Geiger","Bibtex (e.g. @inproceedings...)":"@inproceedings{schwarz2020graf,\n    publisher = {Curran Associates, Inc.},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    author = {Katja Schwarz and Yiyi Liao and Michael Niemeyer and Andreas Geiger},\n    title = {GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis},\n    year = {2020},\n    url = {http://arxiv.org/abs/2007.02442v4},\n    entrytype = {inproceedings},\n    id = {schwarz2020graf}\n}","Bibtex Name":"schwarz2020graf","Citation Count":"61","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/autonomousvision/graf","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/5/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Generative Models, Data-Driven Method, Global Conditioning, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"GRAF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2007.02442.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=akQf7WaCOHo","Timestamp":"5/23/2021 19:12","Title":"GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis","Training time (hr)":"","UID":"49","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2020","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present Neural Splines, a technique for 3D surface reconstruction that is based on random feature kernels arising from infinitely-wide shallow ReLU networks. Our method achieves state-of-the-art results, outperforming recent neural network-based techniques and widely used Poisson Surface Reconstruction (which, as we demonstrate, can also be viewed as a type of kernel method). Because our approach is based on a simple kernel formulation, it is easy to analyze and can be accelerated by general techniques designed for kernel-based learning. We provide explicit analytical expressions for our kernel and argue that our formulation can be seen as a generalization of cubic spline interpolation to higher dimensions. In particular, the RKHS norm associated with Neural Splines biases toward smooth interpolants.","Authors (format: First Last, First Middle Last, ...)":"Francis Williams, Matthew Trager, Joan Bruna, Denis Zorin","Bibtex (e.g. @inproceedings...)":"@inproceedings{williams2021neuralsplines,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Francis Williams and Matthew Trager and Joan Bruna and Denis Zorin},\n    title = {Neural Splines: Fitting 3D Surfaces with Infinitely-Wide Neural Networks},\n    year = {2021},\n    url = {http://arxiv.org/abs/2006.13782v3},\n    entrytype = {inproceedings},\n    id = {williams2021neuralsplines}\n}","Bibtex Name":"williams2021neuralsplines","Citation Count":"2","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/fwilliams/neural-splines","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/24/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Neural Splines","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2006.13782.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/29/2021 16:22","Title":"Neural Splines: Fitting 3D Surfaces with Infinitely-Wide Neural Networks","Training time (hr)":"","UID":"48","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2021","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a deep learning approach to reconstruct scene appearance from unstructured images captured under collocated point lighting. At the heart of Deep Reflectance Volumes is a novel volumetric scene representation consisting of opacity, surface normal and reflectance voxel grids. We present a novel physically-based differentiable volume ray marching framework to render these scene volumes under arbitrary viewpoint and lighting. This allows us to optimize the scene volumes to minimize the error between their rendered images and the captured images. Our method is able to reconstruct real scenes with challenging non-Lambertian reflectance and complex geometry with occlusions and shadowing. Moreover, it accurately generalizes to novel viewpoints and lighting, including non-collocated lighting, rendering photorealistic images that are significantly better than state-of-the-art mesh-based methods. We also show that our learned reflectance volumes are editable, allowing for modifying the materials of the captured scenes.","Authors (format: First Last, First Middle Last, ...)":"Sai Bi, Zexiang Xu, Kalyan Sunkavalli, Milo\u0161 Ha\u0161an, Yannick Hold-Geoffroy, David Kriegman, Ravi Ramamoorthi","Bibtex (e.g. @inproceedings...)":"@inproceedings{bi2020deep,\n    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},\n    author = {Sai Bi and Zexiang Xu and Kalyan Sunkavalli and Milos Hasan and Yannick Hold-Geoffroy and David Kriegman and Ravi Ramamoorthi},\n    title = {Deep Reflectance Volumes: Relightable Reconstructions from Multi-View Photometric Images},\n    year = {2020},\n    url = {http://arxiv.org/abs/2007.09892v1},\n    entrytype = {inproceedings},\n    id = {bi2020deep}\n}","Bibtex Name":"bi2020deep","Citation Count":"18","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/20/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Material/Lighting Estimation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2007.09892.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"https://drive.google.com/file/d/12IAg73kWtGtvKp2RNeaJ8WrmlsTehV4U/view","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://drive.google.com/file/d/1JEbeIrIttznaowJJcBGZD56KR22j635q/view","Timestamp":"5/23/2021 19:11","Title":"Deep Reflectance Volumes","Training time (hr)":"","UID":"47","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2020","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.","Authors (format: First Last, First Middle Last, ...)":"Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, Ren Ng","Bibtex (e.g. @inproceedings...)":"@inproceedings{tancik2020ffn,\n    publisher = {Curran Associates, Inc.},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    author = {Matthew Tancik and Pratul P. Srinivasan and Ben Mildenhall and Sara Fridovich-Keil and Nithin Raghavan and Utkarsh Singhal and Ravi Ramamoorthi and Jonathan T. Barron and Ren Ng},\n    title = {Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains},\n    year = {2020},\n    url = {http://arxiv.org/abs/2006.10739v1},\n    entrytype = {inproceedings},\n    id = {tancik2020ffn}\n}","Bibtex Name":"tancik2020ffn","Citation Count":"135","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/tancik/fourier-feature-networks","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/18/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"FFN","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2006.10739.pdf","Project webpage link":"https://bmild.github.io/fourfeat/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=iKyIJ_EtSkw, https://www.youtube.com/watch?v=h0SXP6lJxak","Timestamp":"6/23/2021 13:23","Title":"Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains","Training time (hr)":"","UID":"46","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2020","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with Hypernetwork/Meta-learnings to learn priors over the space of Siren functions.","Authors (format: First Last, First Middle Last, ...)":"Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, Gordon Wetzstein","Bibtex (e.g. @inproceedings...)":"@inproceedings{sitzmann2020siren,\n    publisher = {Curran Associates, Inc.},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    author = {Vincent Sitzmann and Julien N. P. Martel and Alexander W. Bergman and David B. Lindell and Gordon Wetzstein},\n    title = {Implicit Neural Representations with Periodic Activation Functions},\n    year = {2020},\n    url = {http://arxiv.org/abs/2006.09661v1},\n    entrytype = {inproceedings},\n    id = {sitzmann2020siren}\n}","Bibtex Name":"sitzmann2020siren","Citation Count":"185","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/vsitzmann/siren","Coordinates all at once":"","Data Release (link)":"https://drive.google.com/drive/u/1/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/17/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Sinusoidal Activation (SIREN)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Fundamentals, Audio, Supervision by Gradient (PDE), Hypernetwork/Meta-learning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SIREN","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2006.09661.pdf","Project webpage link":"https://vsitzmann.github.io/siren/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/23/2021 13:23","Title":"Implicit Neural Representations with Periodic Activation Functions","Training time (hr)":"","UID":"45","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2020","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Neural implicit shape representations are an emerging paradigm that offers many potential benefits over conventional discrete representations, including memory efficiency at a high spatial resolution. Generalizing across shapes with such neural implicit representations amounts to learning priors over the respective function space and enables geometry reconstruction from partial or noisy observations. Existing generalization methods rely on conditioning a neural network on a low-dimensional latent code that is either regressed by an encoder or jointly optimized in the auto-decoder framework. Here, we formalize learning of a shape space as a meta-learning problem and leverage gradient-based meta-learning algorithms to solve this task. We demonstrate that this approach performs on par with auto-decoder based approaches while being an order of magnitude faster at test-time inference. We further demonstrate that the proposed gradient-based method outperforms encoder-decoder based methods that leverage pooling-based set encoders.","Authors (format: First Last, First Middle Last, ...)":"Vincent Sitzmann, Eric R. Chan, Richard Tucker, Noah Snavely, Gordon Wetzstein","Bibtex (e.g. @inproceedings...)":"@inproceedings{sitzmann2020metasdf,\n    publisher = {Curran Associates, Inc.},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    author = {Vincent Sitzmann and Eric R. Chan and Richard Tucker and Noah Snavely and Gordon Wetzstein},\n    title = {MetaSDF: Meta-learning Signed Distance Functions},\n    year = {2020},\n    url = {http://arxiv.org/abs/2006.09662v1},\n    entrytype = {inproceedings},\n    id = {sitzmann2020metasdf}\n}","Bibtex Name":"sitzmann2020metasdf","Citation Count":"25","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/vsitzmann/metasdf","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/17/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Data-Driven Method, Hypernetwork/Meta-learning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"MetaSDF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2006.09662.pdf","Project webpage link":"https://vsitzmann.github.io/metasdf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 17:57","Title":"MetaSDF: Meta-learning Signed Distance Functions","Training time (hr)":"","UID":"44","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2020","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose Geo-PIFu, a method to recover a 3D mesh from a monocular color image of a clothed person. Our method is based on a deep implicit function-based representation to learn latent voxel features using a structure-aware 3D U-Net, to constrain the model in two ways: first, to resolve feature ambiguities in query point encoding, second, to serve as a coarse human shape proxy to regularize the high-resolution mesh and encourage global shape regularity. We show that, by both encoding query points and constraining global shape using latent voxel features, the reconstruction we obtain for clothed human meshes exhibits less shape distortion and improved surface details compared to competing methods. We evaluate Geo-PIFu on a recent human mesh public dataset that is $10 \\times$ larger than the private commercial dataset used in PIFu and previous derivative work. On average, we exceed the state of the art by $42.7\\%$ reduction in Chamfer and Point-to-Surface Distances, and $19.4\\%$ reduction in normal estimation errors.","Authors (format: First Last, First Middle Last, ...)":"Tong He, John Collomosse, Hailin Jin, Stefano Soatto","Bibtex (e.g. @inproceedings...)":"@inproceedings{he2020geopifu,\n    publisher = {Curran Associates, Inc.},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    author = {Tong He and John Collomosse and Hailin Jin and Stefano Soatto},\n    title = {Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction},\n    year = {2020},\n    url = {http://arxiv.org/abs/2006.08072v2},\n    entrytype = {inproceedings},\n    id = {he2020geopifu}\n}","Bibtex Name":"he2020geopifu","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/simpleig/Geo-PIFu","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/15/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"Yes","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Voxel Grid, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Geo-PIFu","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2006.08072.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/17/2021 14:03","Title":"Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction","Training time (hr)":"","UID":"43","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2020","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose MeshfreeFlowNet, a novel deep learning-based super-resolution framework to generate continuous (grid-free) spatio-temporal solutions from the low-resolution inputs. While being computationally efficient, MeshfreeFlowNet accurately recovers the fine-scale quantities of interest. MeshfreeFlowNet allows for: (i) the output to be sampled at all spatio-temporal resolutions, (ii) a set of Partial Differential Equation (PDE) constraints to be imposed, and (iii) training on fixed-size inputs on arbitrarily sized spatio-temporal domains owing to its fully convolutional encoder. We empirically study the performance of MeshfreeFlowNet on the task of super-resolution of turbulent flows in the Rayleigh-Benard convection problem. Across a diverse set of evaluation metrics, we show that MeshfreeFlowNet significantly outperforms existing baselines. Furthermore, we provide a large scale implementation of MeshfreeFlowNet and show that it efficiently scales across large clusters, achieving 96.80% scaling efficiency on up to 128 GPUs and a training time of less than 4 minutes.","Authors (format: First Last, First Middle Last, ...)":"Chiyu Max Jiang, Soheil Esmaeilzadeh, Kamyar Azizzadenesheli, Karthik Kashinath, Mustafa Mustafa, Hamdi A. Tchelepi, Philip Marcus, Prabhat, Anima Anandkumar","Bibtex (e.g. @inproceedings...)":"@article{jiang2020meshfreeflownet,\n    author = {Chiyu Max Jiang and Soheil Esmaeilzadeh and Kamyar Azizzadenesheli and Karthik Kashinath and Mustafa Mustafa and Hamdi A. Tchelepi and Philip Marcus and Prabhat and Anima Anandkumar},\n    title = {MeshfreeFlowNet: A Physics-Constrained Deep Continuous Space-Time Super-Resolution Framework},\n    year = {2020},\n    month = {May},\n    url = {http://arxiv.org/abs/2005.01463v2},\n    entrytype = {article},\n    id = {jiang2020meshfreeflownet}\n}","Bibtex Name":"jiang2020meshfreeflownet","Citation Count":"11","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/maxjiang93/space_time_pde","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/1/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering, Supervision by Gradient (PDE)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"MeshfreeFlowNet","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2005.01463.pdf","Project webpage link":"http://www.maxjiang.ml/proj/meshfreeflownet","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=mjqwPch9gDo, https://www.youtube.com/watch?v=anZ_gLrvnYs&t=538s","Timestamp":"8/29/2021 20:52","Title":"MeshfreeFlowNet: A Physics-Constrained Deep Continuous Space-Time Super-Resolution Framework","Training time (hr)":"","UID":"42","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"SC20: International Conference for High Performance Computing, Networking, Storage and Analysis 2020","Venue no Year":"SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In this paper, we propose ARCH (Animatable Reconstruction of Clothed Humans), a novel end-to-end framework for accurate reconstruction of animation-ready 3D clothed humans from a monocular image. Existing approaches to digitize 3D humans struggle to handle pose variations and recover details. Also, they do not produce models that are animation ready. In contrast, ARCH is a learned pose-aware model that produces detailed 3D rigged full-body human avatars from a single unconstrained RGB image. A Semantic Space and a Semantic Deformation Field are created using a parametric 3D body estimator. They allow the transformation of 2D/3D clothed humans into a canonical space, reducing ambiguities in geometry caused by pose variations and occlusions in training data. Detailed surface geometry and appearance are learned using an implicit function representation with spatial local features. Furthermore, we propose additional per-pixel supervision on the 3D reconstruction using opacity-aware differentiable rendering. Our experiments indicate that ARCH increases the fidelity of the reconstructed humans. We obtain more than 50% lower reconstruction errors for standard metrics compared to state-of-the-art methods on public datasets. We also show numerous qualitative examples of animated, high-quality reconstructed avatars unseen in the literature so far.","Authors (format: First Last, First Middle Last, ...)":"Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, Tony Tung","Bibtex (e.g. @inproceedings...)":"@inproceedings{huang2020arch,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Zeng Huang and Yuanlu Xu and Christoph Lassner and Hao Li and Tony Tung},\n    title = {ARCH: Animatable Reconstruction of Clothed Humans},\n    year = {2020},\n    url = {http://arxiv.org/abs/2004.04572v2},\n    entrytype = {inproceedings},\n    id = {huang2020arch}\n}","Bibtex Name":"huang2020arch","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/8/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Data-Driven Method, Local Conditioning, Voxel Grid","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"ARCH","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2004.04572.pdf","Project webpage link":"https://vgl.ict.usc.edu/Research/ARCH/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=DG3QNMcmTvo","Timestamp":"9/17/2021 14:27","Title":"ARCH: Animatable Reconstruction of Clothed Humans","Training time (hr)":"","UID":"41","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2020","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We are seeing a Cambrian explosion of 3D shape representations for use in machine learning. Some representations seek high expressive power in capturing high-resolution detail. Other approaches seek to represent shapes as compositions of simple parts, which are intuitive for people to understand and easy to edit and manipulate. However, it is difficult to achieve both fidelity and interpretability in the same representation. We propose DualSDF, a representation expressing shapes at two levels of granularity, one capturing fine details and the other representing an abstracted proxy shape using simple and semantically consistent shape primitives. To achieve a tight coupling between the two representations, we use a variational objective over a shared latent space. Our two-level model gives rise to a new shape manipulation technique in which a user can interactively manipulate the coarse proxy shape and see the changes instantly mirrored in the high-resolution shape. Moreover, our model actively augments and guides the manipulation towards producing semantically meaningful shapes, making complex manipulations possible with minimal user input.","Authors (format: First Last, First Middle Last, ...)":"Zekun Hao, Hadar Averbuch-Elor, Noah Snavely, Serge Belongie","Bibtex (e.g. @inproceedings...)":"@inproceedings{hao2020dualsdf,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Zekun Hao and Hadar Averbuch-Elor and Noah Snavely and Serge Belongie},\n    title = {DualSDF: Semantic Shape Manipulation using a Two-Level Representation},\n    year = {2020},\n    url = {http://arxiv.org/abs/2004.02869v1},\n    entrytype = {inproceedings},\n    id = {hao2020dualsdf}\n}","Bibtex Name":"hao2020dualsdf","Citation Count":"21","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/zekunhao1995/DualSDF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/6/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Editable, Data-Driven Method, Global Conditioning, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DualSDF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2004.02869.pdf","Project webpage link":"https://www.cs.cornell.edu/~hadarelor/dualsdf/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"https://www.youtube.com/watch?v=u40ZwDINz0A","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=pAszEMLd5Xk","Timestamp":"8/29/2021 16:20","Title":"DualSDF: Semantic Shape Manipulation using a Two-Level Representation","Training time (hr)":"","UID":"40","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2020","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In this paper, we propose an efficient method for robust 3D self-portraits using a single RGBD camera. Benefiting from the proposed PIFusion and lightweight bundle adjustment algorithm, our method can generate detailed 3D self-portraits in seconds and shows the ability to handle subjects wearing extremely loose clothes. To achieve highly efficient and robust reconstruction, we propose PIFusion, which combines learning-based 3D recovery with volumetric non-rigid fusion to generate accurate sparse partial scans of the subject. Moreover, a non-rigid volumetric deformation method is proposed to continuously refine the learned shape prior. Finally, a lightweight bundle adjustment algorithm is proposed to guarantee that all the partial scans can not only \"loop\" with each other but also remain consistent with the selected live key observations. The results and experiments show that the proposed method achieves more robust and efficient 3D self-portraits compared with state-of-the-art methods.","Authors (format: First Last, First Middle Last, ...)":"Zhe Li, Tao Yu, Chuanyu Pan, Zerong Zheng, Yebin Liu","Bibtex (e.g. @inproceedings...)":"@inproceedings{li2020pifusion,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Zhe Li and Tao Yu and Chuanyu Pan and Zerong Zheng and Yebin Liu},\n    title = {Robust 3D Self-portraits in Seconds},\n    year = {2020},\n    url = {http://arxiv.org/abs/2004.02460v1},\n    entrytype = {inproceedings},\n    id = {li2020pifusion}\n}","Bibtex Name":"li2020pifusion","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/6/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"Yes","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"PIFusion","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2004.02460.pdf","Project webpage link":"http://www.liuyebin.com/portrait/portrait.html","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"http://www.liuyebin.com/portrait/assets/portrait.mp4","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=tayZT0exfVA","Timestamp":"9/17/2021 13:51","Title":"Robust 3D Self-portraits in Seconds","Training time (hr)":"","UID":"39","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2020","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recent advances in image-based 3D human shape estimation have been driven by the significant improvement in representation power afforded by deep neural networks. Although current approaches have demonstrated the potential in real world settings, they still fail to produce reconstructions with the level of detail often present in the input images. We argue that this limitation stems primarily form two conflicting requirements; accurate predictions require large context, but precise predictions require high resolution. Due to memory limitations in current hardware, previous approaches tend to take low resolution images as input to cover large spatial context, and produce less precise (or low resolution) 3D estimates as a result. We address this limitation by formulating a multi-level architecture that is end-to-end trainable. A coarse level observes the whole image at lower resolution and focuses on holistic reasoning. This provides context to an fine level which estimates highly detailed geometry by observing higher-resolution images. We demonstrate that our approach significantly outperforms existing state-of-the-art techniques on single image human shape reconstruction by fully leveraging 1k-resolution input images.","Authors (format: First Last, First Middle Last, ...)":"Shunsuke Saito, Tomas Simon, Jason Saragih, Hanbyul Joo","Bibtex (e.g. @inproceedings...)":"@inproceedings{saito2020pifuhd,\n    url = {http://arxiv.org/abs/2004.00452v1},\n    year = {2020},\n    title = {PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization},\n    author = {Shunsuke Saito and Tomas Simon and Jason Saragih and Hanbyul Joo},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    entrytype = {inproceedings},\n    id = {saito2020pifuhd}\n}","Bibtex Name":"saito2020pifuhd","Citation Count":"115","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/facebookresearch/pifuhd","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"4/1/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Sparse Reconstruction, Generalization, Image-Based Rendering, Data-Driven Method, Coarse-to-Fine, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"PIFuHD","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2004.00452.pdf","Project webpage link":"https://shunsukesaito.github.io/PIFuHD/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=uEDqCxvF5yc, https://www.youtube.com/watch?v=-1XYTmm8HhE","Timestamp":"6/29/2021 16:42","Title":"PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization","Training time (hr)":"","UID":"38","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2020","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"The recent success of implicit neural scene representations has presented a viable new method for how we capture and store 3D scenes. Unlike conventional 3D representations, such as point clouds, which explicitly store scene properties in discrete, localized units, these implicit representations encode a scene in the weights of a neural network which can be queried at any coordinate to produce these same scene properties. Thus far, implicit representations have primarily been optimized to estimate only the appearance and/or 3D geometry information in a scene. We take the next step and demonstrate that an existing implicit representation (SRNs) is actually multi-modal; it can be further leveraged to perform per-point semantic segmentation while retaining its ability to represent appearance and geometry. To achieve this multi-modal behavior, we utilize a semi-supervised learning strategy atop the existing pre-trained scene representation. Our method is simple, general, and only requires a few tens of labeled 2D segmentation masks in order to achieve dense 3D semantic segmentation. We explore two novel applications for this semantically aware implicit neural scene representation: 3D novel view and semantic label synthesis given only a single input RGB image or 2D label mask, as well as 3D interpolation of appearance and semantics.","Authors (format: First Last, First Middle Last, ...)":"Amit Kohli, Vincent Sitzmann, Gordon Wetzstein","Bibtex (e.g. @inproceedings...)":"@inproceedings{kohli2020semantic,\n    url = {http://arxiv.org/abs/2003.12673v2},\n    year = {2020},\n    title = {Semantic Implicit Neural Scene Representations With Semi-Supervised Training},\n    author = {Amit Kohli and Vincent Sitzmann and Gordon Wetzstein},\n    booktitle = {International Conference on 3D Vision (3DV)},\n    organization = {IEEE},\n    entrytype = {inproceedings},\n    id = {kohli2020semantic}\n}","Bibtex Name":"kohli2020semantic","Citation Count":"7","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/28/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generative Models, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2003.12673.pdf","Project webpage link":"http://www.computationalimaging.org/publications/semantic-srn/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=iVubC_ymE5w","Timestamp":"5/23/2021 19:16","Title":"Semantic Implicit Neural Scene Representations With Semi-Supervised Training","Training time (hr)":"","UID":"37","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"3DV 2020","Venue no Year":"3DV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"The recent deep learning revolution has created an enormous opportunity for accelerating compute capabilities in the context of physics-based simulations. Here, we propose EikoNet, a deep learning approach to solving the Eikonal equation, which characterizes the first-arrival-time field in heterogeneous 3D velocity structures. Our grid-free approach allows for rapid determination of the travel time between any two points within a continuous 3D domain. These travel time solutions are allowed to violate the differential equation - which casts the problem as one of optimization - with the goal of finding network parameters that minimize the degree to which the equation is violated. In doing so, the method exploits the differentiability of neural networks to calculate the spatial gradients analytically, meaning the network can be trained on its own without ever needing solutions from a finite difference algorithm. EikoNet is rigorously tested on several velocity models and sampling methods to demonstrate robustness and versatility. Training and inference are highly parallelized, making the approach well-suited for GPUs. EikoNet has low memory overhead, and further avoids the need for travel-time lookup tables. The developed approach has important applications to earthquake hypocenter inversion, ray multi-pathing, and tomographic modeling, as well as to other fields beyond seismology where ray tracing is essential.","Authors (format: First Last, First Middle Last, ...)":"Jonathan D. Smith, Kamyar Azizzadenesheli, Zachary E. Ross","Bibtex (e.g. @inproceedings...)":"@article{smith2020eikonet,\n    author = {Jonathan D. Smith and Kamyar Azizzadenesheli and Zachary E. Ross},\n    title = {EikoNet: Solving the Eikonal equation with Deep Neural Networks},\n    year = {2020},\n    month = {Mar},\n    url = {http://arxiv.org/abs/2004.00361v3},\n    entrytype = {article},\n    id = {smith2020eikonet}\n}","Bibtex Name":"smith2020eikonet","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/Ulvetanna/EikoNet","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/25/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering, Supervision by Gradient (PDE)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"EikoNet","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2004.00361.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/18/2021 9:39","Title":"EikoNet: Solving the Eikonal equation with Deep Neural Networks","Training time (hr)":"","UID":"36","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"IEEE Transactions on Geoscience and Remote Sensing 2020","Venue no Year":"IEEE Transactions on Geoscience and Remote Sensing","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Efficiently reconstructing complex and intricate surfaces at scale is a long-standing goal in machine perception. To address this problem we introduce Deep Local Shapes (DeepLS), a deep shape representation that enables encoding and reconstruction of high-quality 3D shapes without prohibitive memory requirements. DeepLS replaces the dense volumetric signed distance function (SDF) representation used in traditional surface reconstruction systems with a set of locally learned continuous SDFs defined by a neural network, inspired by recent work such as DeepSDF. Unlike DeepSDF, which represents an object-level SDF with a neural network and a single latent code, we store a grid of independent latent codes, each responsible for storing information about surfaces in a small local neighborhood. This decomposition of scenes into local shapes simplifies the prior distribution that the network must learn, and also enables efficient inference. We demonstrate the effectiveness and generalization power of DeepLS by showing object shape encoding and reconstructions of full scenes, where DeepLS delivers high compression, accuracy, and local shape completion.","Authors (format: First Last, First Middle Last, ...)":"Rohan Chabra, Jan Eric Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove, Richard Newcombe","Bibtex (e.g. @inproceedings...)":"@inproceedings{chabra2020deepls,\n    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},\n    author = {Rohan Chabra and Jan Eric Lenssen and Eddy Ilg and Tanner Schmidt and Julian Straub and Steven Lovegrove and Richard Newcombe},\n    title = {Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction},\n    year = {2020},\n    url = {http://arxiv.org/abs/2003.10983v3},\n    entrytype = {inproceedings},\n    id = {chabra2020deepls}\n}","Bibtex Name":"chabra2020deepls","Citation Count":"61","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/24/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Voxel Grid, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DeepLS","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2003.10983.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/29/2021 16:58","Title":"Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction","Training time (hr)":"","UID":"35","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2020","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In this work we address the challenging problem of multiview 3D surface reconstruction. We introduce a neural network architecture that simultaneously learns the unknown geometry, camera parameters, and a neural renderer that approximates the light reflected from the surface towards the camera. The geometry is represented as a zero level-set of a neural network, while the neural renderer, derived from the rendering equation, is capable of (implicitly) modeling a wide set of lighting conditions and materials. We trained our network on real world 2D images of objects with different material properties, lighting conditions, and noisy camera initializations from the DTU MVS dataset. We found our model to produce state of the art 3D surface reconstructions with high fidelity, resolution and detail.","Authors (format: First Last, First Middle Last, ...)":"Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Ronen Basri, Yaron Lipman","Bibtex (e.g. @inproceedings...)":"@inproceedings{yariv2020idr,\n    url = {http://arxiv.org/abs/2003.09852v3},\n    year = {2020},\n    title = {Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance},\n    author = {Lior Yariv and Yoni Kasten and Dror Moran and Meirav Galun and Matan Atzmon and Ronen Basri and Yaron Lipman},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    publisher = {Curran Associates, Inc.},\n    entrytype = {inproceedings},\n    id = {yariv2020idr}\n}","Bibtex Name":"yariv2020idr","Citation Count":"52","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/lioryariv/idr","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/22/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Material/Lighting Estimation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"IDR","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2003.09852.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 19:12","Title":"Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance","Training time (hr)":"","UID":"34","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2020","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.","Authors (format: First Last, First Middle Last, ...)":"Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng","Bibtex (e.g. @inproceedings...)":"@inproceedings{mildenhall2020nerf,\n    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},\n    author = {Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},\n    title = {NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},\n    year = {2020},\n    url = {http://arxiv.org/abs/2003.08934v2},\n    entrytype = {inproceedings},\n    id = {mildenhall2020nerf}\n}","Bibtex Name":"mildenhall2020nerf","Citation Count":"366","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/bmild/nerf","Coordinates all at once":"","Data Release (link)":"https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/19/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Sampling","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2003.08934.pdf","Project webpage link":"https://www.matthewtancik.com/nerf","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=JuH79E8rdKc","Timestamp":"5/23/2021 19:13","Title":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis","Training time (hr)":"","UID":"33","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2020","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fully-connected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases, enabling structured reasoning in 3D space. We investigate the effectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically find that our method enables the fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data.","Authors (format: First Last, First Middle Last, ...)":"Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, Andreas Geiger","Bibtex (e.g. @inproceedings...)":"@inproceedings{peng2020convolutional,\n    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},\n    author = {Songyou Peng and Michael Niemeyer and Lars Mescheder and Marc Pollefeys and Andreas Geiger},\n    title = {Convolutional Occupancy Networks},\n    year = {2020},\n    url = {http://arxiv.org/abs/2003.04618v2},\n    entrytype = {inproceedings},\n    id = {peng2020convolutional}\n}","Bibtex Name":"peng2020convolutional","Citation Count":"99","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/autonomousvision/convolutional_occupancy_networks","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/10/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2003.04618.pdf","Project webpage link":"https://pengsongyou.github.io/conv_onet","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"http://www.cvlibs.net/publications/Peng2020ECCV_supplementary.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=k0monzIcjUo, https://www.youtube.com/watch?v=EmauovgrDSM","Timestamp":"5/23/2021 19:21","Title":"Convolutional Occupancy Networks","Training time (hr)":"","UID":"32","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2020","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"While many works focus on 3D reconstruction from images, in this paper, we focus on 3D shape reconstruction and completion from a variety of 3D inputs, which are deficient in some respect: low and high resolution voxels, sparse and dense point clouds, complete or incomplete. Processing of such 3D inputs is an increasingly important problem as they are the output of 3D scanners, which are becoming more accessible, and are the intermediate output of 3D computer vision algorithms. Recently, learned implicit functions have shown great promise as they produce continuous reconstructions. However, we identified two limitations in reconstruction from 3D inputs: 1) details present in the input data are not retained, and 2) poor reconstruction of articulated humans. To solve this, we propose Implicit Feature Networks (IF-Nets), which deliver continuous outputs, can handle multiple topologies, and complete shapes for missing or sparse input data retaining the nice properties of recent learned implicit functions, but critically they can also retain detail when it is present in the input data, and can reconstruct articulated humans. Our work differs from prior work in two crucial aspects. First, instead of using a single vector to encode a 3D shape, we extract a learnable 3-dimensional multi-scale tensor of deep features, which is aligned with the original Euclidean space embedding the shape. Second, instead of classifying x-y-z point coordinates directly, we classify deep features extracted from the tensor at a continuous query point. We show that this forces our model to make decisions based on global and local shape structure, as opposed to point coordinates, which are arbitrary under Euclidean transformations. Experiments demonstrate that IF-Nets clearly outperform prior work in 3D object reconstruction in ShapeNet, and obtain significantly more accurate 3D human reconstructions.","Authors (format: First Last, First Middle Last, ...)":"Julian Chibane, Thiemo Alldieck, Gerard Pons-Moll","Bibtex (e.g. @inproceedings...)":"@inproceedings{chibane2020ifnet,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Julian Chibane and Thiemo Alldieck and Gerard Pons-Moll},\n    title = {Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion},\n    year = {2020},\n    url = {http://arxiv.org/abs/2003.01456v2},\n    entrytype = {inproceedings},\n    id = {chibane2020ifnet}\n}","Bibtex Name":"chibane2020ifnet","Citation Count":"86","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/jchibane/if-net","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"3/3/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"Yes","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Data-Driven Method","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"IF-Net","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2003.01456.pdf","Project webpage link":"https://virtualhumans.mpi-inf.mpg.de/ifnets/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"http://virtualhumans.mpi-inf.mpg.de/papers/chibane20ifnet/chibane20ifnet_supp.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=cko07jINRZg","Timestamp":"7/19/2021 21:08","Title":"Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion","Training time (hr)":"","UID":"31","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2020","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Representing shapes as level sets of neural networks has been recently proved to be useful for different shape analysis and reconstruction tasks. So far, such representations were computed using either: (i) pre-computed implicit shape representations; or (ii) loss functions explicitly defined over the neural level sets. In this paper we offer a new paradigm for computing high fidelity implicit neural representations directly from raw data (i.e., point clouds, with or without normal information). We observe that a rather simple loss function, encouraging the neural network to vanish on the input point cloud and to have a unit norm gradient, possesses an implicit geometric regularization property that favors smooth and natural zero level set surfaces, avoiding bad zero-loss solutions. We provide a theoretical analysis of this property for the linear case, and show that, in practice, our method leads to state of the art implicit neural representations with higher level-of-details and fidelity compared to previous methods.","Authors (format: First Last, First Middle Last, ...)":"Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, Yaron Lipman","Bibtex (e.g. @inproceedings...)":"@inproceedings{gropp2020igr,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Amos Gropp and Lior Yariv and Niv Haim and Matan Atzmon and Yaron Lipman},\n    title = {Implicit Geometric Regularization for Learning Shapes},\n    year = {2020},\n    url = {http://arxiv.org/abs/2002.10099v2},\n    entrytype = {inproceedings},\n    id = {gropp2020igr}\n}","Bibtex Name":"gropp2020igr","Citation Count":"65","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/amosgropp/IGR","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"2/24/2020","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Generalization, Fundamentals","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"IGR","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2002.10099.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=6cOvBGBQF9g","Timestamp":"6/29/2021 16:32","Title":"Implicit Geometric Regularization for Learning Shapes","Training time (hr)":"","UID":"30","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2020","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.","Authors (format: First Last, First Middle Last, ...)":"Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger","Bibtex (e.g. @inproceedings...)":"@inproceedings{niemeyer2020dvr,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Michael Niemeyer and Lars Mescheder and Michael Oechsle and Andreas Geiger},\n    title = {Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision},\n    year = {2020},\n    url = {http://arxiv.org/abs/1912.07372v2},\n    entrytype = {inproceedings},\n    id = {niemeyer2020dvr}\n}","Bibtex Name":"niemeyer2020dvr","Citation Count":"142","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/autonomousvision/differentiable_volumetric_rendering","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/16/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DVR","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1912.07372.pdf","Project webpage link":"https://www.youtube.com/watch?v=U_jIN3qWVEw","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"http://www.cvlibs.net/publications/Niemeyer2020CVPR_supplementary.pdf","Supplement video (link, comma separated if multiple exists)":"https://www.youtube.com/watch?v=lcub1KH-mmk","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=U_jIN3qWVEw","Timestamp":"7/7/2021 17:49","Title":"Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision","Training time (hr)":"","UID":"29","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2020","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"The goal of this project is to learn a 3D shape representation that enables accurate surface reconstruction, compact storage, efficient computation, consistency for similar shapes, generalization across diverse shape categories, and inference from depth camera observations. Towards this end, we introduce Local Deep Implicit Functions (LDIF), a 3D shape representation that decomposes space into a structured set of learned implicit functions. We provide networks that infer the space decomposition and local deep implicit functions from a 3D mesh or posed depth image. During experiments, we find that it provides 10.3 points higher surface reconstruction accuracy (F-Score) than the state-of-the-art (OccNet), while requiring fewer than 1 percent of the network parameters. Experiments on posed depth image completion and generalization to unseen classes show 15.8 and 17.8 point improvements over the state-of-the-art, while producing a structured 3D representation for each input with consistency across diverse shape collections.","Authors (format: First Last, First Middle Last, ...)":"Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, Thomas Funkhouser","Bibtex (e.g. @inproceedings...)":"@inproceedings{genova2020ldif,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Kyle Genova and Forrester Cole and Avneesh Sud and Aaron Sarna and Thomas Funkhouser},\n    title = {Local Deep Implicit Functions for 3D Shape},\n    year = {2020},\n    url = {http://arxiv.org/abs/1912.06126v2},\n    entrytype = {inproceedings},\n    id = {genova2020ldif}\n}","Bibtex Name":"genova2020ldif","Citation Count":"70","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/google/ldif","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/12/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Data-Driven Method, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"LDIF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1912.06126.pdf","Project webpage link":"https://ldif.cs.princeton.edu/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=3RAITzNWVJs","Timestamp":"6/29/2021 16:55","Title":"Local Deep Implicit Functions for 3D Shape","Training time (hr)":"","UID":"28","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2020","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose a generative model of 2D and 3D natural textures with diversity, visual fidelity and at high computational efficiency. This is enabled by a family of methods that extend ideas from classic stochastic procedural texturing (Perlin noise) to learned, deep, non-linearities. The key idea is a hard-coded, tunable and differentiable step that feeds multiple transformed random 2D or 3D fields into an MLP that can be sampled over infinite domains. Our model encodes all exemplars from a diverse set of textures without a need to be re-trained for each exemplar. Applications include texture interpolation, and learning 3D textures from 2D exemplars.","Authors (format: First Last, First Middle Last, ...)":"Philipp Henzler, Niloy J. Mitra, Tobias Ritschel","Bibtex (e.g. @inproceedings...)":"@inproceedings{henzler2020neuraltexture,\n    url = {http://arxiv.org/abs/1912.04158v2},\n    year = {2020},\n    title = {Learning a Neural 3D Texture Space from 2D Exemplars},\n    author = {Philipp Henzler and Niloy J. Mitra and Tobias Ritschel},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    entrytype = {inproceedings},\n    id = {henzler2020neuraltexture}\n}","Bibtex Name":"henzler2020neuraltexture","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/henzler/neuraltexture","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/9/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"2D Image Neural Fields","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Neural Texture","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1912.04158.pdf","Project webpage link":"https://geometry.cs.ucl.ac.uk/group_website/projects/2020/neuraltexture/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=it5y2qaONBE","Timestamp":"10/8/2021 16:47","Title":"Learning a Neural 3D Texture Space from 2D Exemplars","Training time (hr)":"","UID":"27","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2020","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Efficient representation of articulated objects such as human bodies is an important problem in computer vision and graphics. To efficiently simulate deformation, existing approaches represent 3D objects using polygonal meshes and deform them using skinning techniques. This paper introduces neural articulated shape approximation (NASA), an alternative framework that enables efficient representation of articulated deformable objects using neural indicator functions that are conditioned on pose. Occupancy testing using NASA is straightforward, circumventing the complexity of meshes and the issue of water-tightness. We demonstrate the effectiveness of NASA for 3D tracking applications, and discuss other potential extensions.","Authors (format: First Last, First Middle Last, ...)":"Boyang Deng, JP Lewis, Timothy Jeruzalski, Gerard Pons-Moll, Geoffrey Hinton, Mohammad Norouzi, Andrea Tagliasacchi","Bibtex (e.g. @inproceedings...)":"@inproceedings{deng2020nasa,\n    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},\n    author = {Boyang Deng and JP Lewis and Timothy Jeruzalski and Gerard Pons-Moll and Geoffrey Hinton and Mohammad Norouzi and Andrea Tagliasacchi},\n    title = {NASA: Neural Articulated Shape Approximation},\n    year = {2020},\n    url = {http://arxiv.org/abs/1912.03207v4},\n    entrytype = {inproceedings},\n    id = {deng2020nasa}\n}","Bibtex Name":"deng2020nasa","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/6/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NASA","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1912.03207.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/17/2021 11:55","Title":"NASA: Neural Articulated Shape Approximation","Training time (hr)":"","UID":"26","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ECCV 2020","Venue no Year":"ECCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose a differentiable sphere tracing algorithm to bridge the gap between inverse graphics methods and the recently proposed deep learning based implicit signed distance function. Due to the nature of the implicit function, the rendering process requires tremendous function queries, which is particularly problematic when the function is represented as a neural network. We optimize both the forward and backward passes of our rendering layer to make it run efficiently with affordable memory consumption on a commodity graphics card. Our rendering method is fully differentiable such that losses can be directly computed on the rendered 2D observations, and the gradients can be propagated backwards to optimize the 3D geometry. We show that our rendering method can effectively reconstruct accurate 3D shapes from various inputs, such as sparse depth and multi-view images, through inverse optimization. With the geometry based reasoning, our 3D shape prediction methods show excellent generalization capability and robustness against various noises.","Authors (format: First Last, First Middle Last, ...)":"Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc Pollefeys, Zhaopeng Cui","Bibtex (e.g. @inproceedings...)":"@inproceedings{liu2020dist,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Shaohui Liu and Yinda Zhang and Songyou Peng and Boxin Shi and Marc Pollefeys and Zhaopeng Cui},\n    title = {DIST: Rendering Deep Implicit Signed Distance Function with Differentiable Sphere Tracing},\n    year = {2020},\n    url = {http://arxiv.org/abs/1911.13225v2},\n    entrytype = {inproceedings},\n    id = {liu2020dist}\n}","Bibtex Name":"liu2020dist","Citation Count":"68","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/B1ueber2y/DIST-Renderer","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/29/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals, Data-Driven Method","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DIST","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1911.13225.pdf","Project webpage link":"http://b1ueber2y.me/projects/DIST-Renderer/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"http://b1ueber2y.me/projects/DIST-Renderer/dist-supp.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=KjfNS1mnqoM","Timestamp":"8/29/2021 20:20","Title":"DIST: Rendering Deep Implicit Signed Distance Function with Differentiable Sphere Tracing","Training time (hr)":"","UID":"25","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2020","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recently, neural networks have been used as implicit representations for surface reconstruction, modelling, learning, and generation. So far, training neural networks to be implicit representations of surfaces required training data sampled from a ground-truth signed implicit functions such as signed distance or occupancy functions, which are notoriously hard to compute. In this paper we introduce Sign Agnostic Learning (SAL), a deep learning approach for learning implicit shape representations directly from raw, unsigned geometric data, such as point clouds and triangle soups. We have tested SAL on the challenging problem of surface reconstruction from an un-oriented point cloud, as well as end-to-end human shape space learning directly from raw scans dataset, and achieved state of the art reconstructions compared to current approaches. We believe SAL opens the door to many geometric deep learning applications with real-world data, alleviating the usual painstaking, often manual pre-process.","Authors (format: First Last, First Middle Last, ...)":"Matan Atzmon, Yaron Lipman","Bibtex (e.g. @inproceedings...)":"@inproceedings{atzmon2020sal,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Matan Atzmon and Yaron Lipman},\n    title = {SAL: Sign Agnostic Learning of Shapes from Raw Data},\n    year = {2020},\n    url = {http://arxiv.org/abs/1911.10414v2},\n    entrytype = {inproceedings},\n    id = {atzmon2020sal}\n}","Bibtex Name":"atzmon2020sal","Citation Count":"68","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/matanatz/SAL","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/23/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SAL","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1911.10414.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/29/2021 16:19","Title":"SAL: Sign Agnostic Learning of Shapes from Raw Data","Training time (hr)":"","UID":"24","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2020","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Recent advances in 3D deep learning have shown that it is possible to train highly effective deep models for 3D shape generation, directly from 2D images. This is particularly interesting since the availability of 3D models is still limited compared to the massive amount of accessible 2D images, which is invaluable for training. The representation of 3D surfaces itself is a key factor for the quality and resolution of the 3D output. While explicit representations, such as point clouds and voxels, can span a wide range of shape variations, their resolutions are often limited. Mesh-based representations are more efficient but are limited by their ability to handle varying topologies. Implicit surfaces, however, can robustly handle complex shapes, topologies, and also provide flexible resolution control. We address the fundamental problem of learning implicit surfaces for shape inference without the need of 3D supervision. Despite their advantages, it remains nontrivial to (1) formulate a differentiable connection between implicit surfaces and their 2D renderings, which is needed for image-based supervision; and (2) ensure precise geometric properties and control, such as local smoothness. In particular, sampling implicit surfaces densely is also known to be a computationally demanding and very slow operation. To this end, we propose a novel ray-based field probing technique for efficient image-to-field supervision, as well as a general geometric regularizer for implicit surfaces, which provides natural shape priors in unconstrained regions. We demonstrate the effectiveness of our framework on the task of single-view image-based 3D shape digitization and show how we outperform state-of-the-art techniques both quantitatively and qualitatively.","Authors (format: First Last, First Middle Last, ...)":"Shichen Liu, Shunsuke Saito, Weikai Chen, Hao Li","Bibtex (e.g. @inproceedings...)":"@inproceedings{liu2019learning,\n    publisher = {Curran Associates, Inc.},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    author = {Shichen Liu and Shunsuke Saito and Weikai Chen and Hao Li},\n    title = {Learning to Infer Implicit Surfaces without 3D Supervision},\n    year = {2019},\n    url = {http://arxiv.org/abs/1911.00767v1},\n    entrytype = {inproceedings},\n    id = {liu2019learning}\n}","Bibtex Name":"liu2019learning","Citation Count":"69","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/2/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals, Sampling, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1911.00767.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:31","Title":"Learning to Infer Implicit Surfaces without 3D Supervision","Training time (hr)":"","UID":"23","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2019","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2019"},{"":"ICLR 2022","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"","Authors (format: First Last, First Middle Last, ...)":"","Bibtex (e.g. @inproceedings...)":"@article{mitchell2022hof,\n    url = {http://arxiv.org/abs/1907.10388v2},\n    month = {Jul},\n    year = {2019},\n    title = {Higher-Order Function Networks for Learning Composable 3D Object Representations},\n    author = {Eric Mitchell and Selim Engin and Volkan Isler and Daniel D Lee},\n    entrytype = {article},\n    id = {mitchell2022hof}\n}","Bibtex Name":"Eric Mitchell, Selim Engin, Volkan Isler, Daniel D Lee","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"We present a new approach to 3D object representation where a neural network encodes the geometry of an object directly into the weights and biases of a second 'mapping' network. This mapping network can be used to reconstruct an object by applying its encoded transformation to points randomly sampled from a simple geometric space, such as the unit sphere. We study the effectiveness of our method through various experiments on subsets of the ShapeNet dataset. We find that the proposed approach can reconstruct encoded objects with accuracy equal to or exceeding state-of-the-art methods with orders of magnitude fewer parameters. Our smallest mapping network has only about 7000 parameters and shows reconstruction quality on par with state-of-the-art object decoder architectures with millions of parameters. Further experiments on feature mixing through the composition of learned functions show that the encoding captures a meaningful subspace of objects.","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/7/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"isler@umn.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Direct mapping","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Geometry Only, Hypernetwork/Meta-learning","Lighting":"","New entry or update existing?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Nickname (e.g. DeepSDF)":"HOF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1907.10388.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":" Surface-HOF (AAAI 2020) produces complete geodesics","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"11/23/2021 23:23","Title":"Higher-Order Function Networks for Learning Composable 3D Object Representations","Training time (hr)":"No","UID":"22","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"New entry","Venue no Year":"ICLR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Deep learning based 3D reconstruction techniques have recently achieved impressive results. However, while stateof-the-art methods are able to output complex 3D geometry, it is not clear how to extend these results to time-varying topologies. Approaches treating each time step individually lack continuity and exhibit slow inference, while traditional 4D reconstruction methods often utilize a template model or discretize the 4D space at fixed resolution. In this work, we present Occupancy Flow, a novel spatio-temporal representation of time-varying 3D geometry with implicit correspondences. Towards this goal, we learn a temporally and spatially continuous vector field which assigns a motion vector to every point in space and time. In order to perform dense 4D reconstruction from images or sparse point clouds, we combine our method with a continuous 3D representation. Implicitly, our model yields correspondences over time, thus enabling fast inference while providing a sound physical description of the temporal dynamics. We show that our method can be used for interpolation and reconstruction tasks, and demonstrate the accuracy of the learned correspondences. We believe that Occupancy Flow is a promising new 4D representation which will be useful for a variety of spatio-temporal reconstruction tasks.","Authors (format: First Last, First Middle Last, ...)":"Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger","Bibtex (e.g. @inproceedings...)":"@inproceedings{niemeyer2019occupancyflow,\n    title = {Occupancy flow: 4d reconstruction by learning particle dynamics},\n    author = {Michael Niemeyer and Lars Mescheder and Michael Oechsle and Andreas Geiger},\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    pages = {5379--5389},\n    year = {2019},\n    entrytype = {inproceedings},\n    id = {niemeyer2019occupancyflow}\n}","Bibtex Name":"niemeyer2019occupancyflow","Citation Count":"65","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/autonomousvision/occupancy_flow","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"10/1/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Human (Body)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Occupancy Flow","PDF link (arXiv perferred)":"https://openaccess.thecvf.com/content_ICCV_2019/papers/Niemeyer_Occupancy_Flow_4D_Reconstruction_by_Learning_Particle_Dynamics_ICCV_2019_paper.pdf","Project webpage link":"https://avg.is.tuebingen.mpg.de/publications/niemeyer2019iccv","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"http://www.cvlibs.net/publications/Niemeyer2019ICCV_supplementary.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=c0yOugTgrWc","Timestamp":"8/29/2021 21:39","Title":"Occupancy Flow: 4D Reconstruction by Learning Particle Dynamics","Training time (hr)":"","UID":"21","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2019","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2019"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Compositional Pattern Producing Networks (CPPNs) are differentiable networks that independently map (x, y) pixel coordinates to (r, g, b) colour values. Recently, CPPNs have been used for creating interesting imagery for creative purposes, e.g., neural art. However their architecture biases generated images to be overly smooth, lacking high-frequency detail. In this work, we extend CPPNs to explicitly model the frequency information for each pixel output, capturing frequencies beyond the DC component. We show that our Fourier-CPPNs (F-CPPNs) provide improved visual detail for image synthesis.","Authors (format: First Last, First Middle Last, ...)":"Mattie Tesfaldet, Xavier Snelgrove, David Vazquez","Bibtex (e.g. @inproceedings...)":"@article{tesfaldet2019fcppn,\n    url = {http://arxiv.org/abs/1909.09273v1},\n    month = {Sep},\n    year = {2019},\n    title = {Fourier-CPPNs for Image Synthesis},\n    author = {Mattie Tesfaldet and Xavier Snelgrove and David Vazquez},\n    entrytype = {article},\n    id = {tesfaldet2019fcppn}\n}","Bibtex Name":"tesfaldet2019fcppn","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/tesfaldet/fourier-cppn/tree/master","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/20/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"Yes","Email Address":"tesfaldet@hotmail.com","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Windowed Fourier Coefficients as output","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"None, N/A","Inputs":"","Is the PDF linked to arXiv?":"Yes (No need to fill out \"Date\", \"Authors\", \"Abstract\", \"Bibtex Citation\")","Keywords":"2D Image Neural Fields, Fundamentals, Image-Based Rendering, Fourier, Positional Encoding","Lighting":"","New entry or update existing?":"New entry","Nickname (e.g. DeepSDF)":"F-CPPN","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1909.09273.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"11/23/2021 22:10","Title":"Fourier-CPPNs for Image Synthesis","Training time (hr)":"","UID":"20","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV Workshop 2019","Venue no Year":"ICCV Workshop","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2019"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Cryo-electron microscopy (cryo-EM) is a powerful technique for determining the structure of proteins and other macromolecular complexes at near-atomic resolution. In single particle cryo-EM, the central problem is to reconstruct the three-dimensional structure of a macromolecule from $10^{4-7}$ noisy and randomly oriented two-dimensional projections. However, the imaged protein complexes may exhibit structural variability, which complicates reconstruction and is typically addressed using discrete clustering approaches that fail to capture the full range of protein dynamics. Here, we introduce a novel method for cryo-EM reconstruction that extends naturally to modeling continuous generative factors of structural heterogeneity. This method encodes structures in Fourier space using coordinate-based deep neural networks, and trains these networks from unlabeled 2D cryo-EM images by combining exact inference over image orientation with variational inference for structural heterogeneity. We demonstrate that the proposed method, termed cryoDRGN, can perform ab initio reconstruction of 3D protein complexes from simulated and real 2D cryo-EM image data. To our knowledge, cryoDRGN is the first neural network-based approach for cryo-EM reconstruction and the first end-to-end method for directly reconstructing continuous ensembles of protein structures from cryo-EM images.","Authors (format: First Last, First Middle Last, ...)":"Ellen D. Zhong, Tristan Bepler, Joseph H. Davis, Bonnie Berger","Bibtex (e.g. @inproceedings...)":"@inproceedings{zhong2020cryodrgn,\n    booktitle = {International Conference on Learning Representations},\n    author = {Ellen D. Zhong and Tristan Bepler and Joseph H. Davis and Bonnie Berger},\n    title = {Reconstructing continuous distributions of 3D protein structure from cryo-EM images},\n    year = {2020},\n    url = {http://arxiv.org/abs/1909.05215v3},\n    entrytype = {inproceedings},\n    id = {zhong2020cryodrgn}\n}","Bibtex Name":"zhong2020cryodrgn","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/zhonge/cryodrgn","Coordinates all at once":"No","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"9/11/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"Fourier Feature (NeRF)","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Electron density","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Science & Engineering, Data-Driven Method, Global Conditioning, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"cryoDRGN","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1909.05215.pdf","Project webpage link":"http://cb.csail.mit.edu/cb/cryodrgn/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=zd6YcUyDhPE","Timestamp":"9/19/2021 15:43","Title":"Reconstructing continuous distributions of 3D protein structure from cryo-EM images","Training time (hr)":"","UID":"19","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICLR 2020","Venue no Year":"ICLR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a new method for 3D shape reconstruction from a single image, in which a deep neural network directly maps an image to a vector of network weights. The network \\textcolor{black}{parametrized by} these weights represents a 3D shape by classifying every point in the volume as either within or outside the shape. The new representation has virtually unlimited capacity and resolution, and can have an arbitrary topology. Our experiments show that it leads to more accurate shape inference from a 2D projection than the existing methods, including voxel-, silhouette-, and mesh-based methods. The code is available at: https://github.com/gidilittwin/Deep-Meta","Authors (format: First Last, First Middle Last, ...)":"Gidi Littwin, Lior Wolf","Bibtex (e.g. @inproceedings...)":"@inproceedings{littwin2019deep,\n    url = {http://arxiv.org/abs/1908.06277v1},\n    year = {2019},\n    title = {Deep Meta Functionals for Shape Representation},\n    author = {Gidi Littwin and Lior Wolf},\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    entrytype = {inproceedings},\n    id = {littwin2019deep}\n}","Bibtex Name":"littwin2019deep","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/gidilittwin/Deep-Meta","Coordinates all at once":"No","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/17/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"No","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Hypernetwork/Meta-learning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1908.06277.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/8/2021 14:17","Title":"Deep Meta Functionals for Shape Representation","Training time (hr)":"","UID":"18","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2019","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2019"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We propose to represent shapes as the deformation and combination of learnable elementary 3D structures, which are primitives resulting from training over a collection of shape. We demonstrate that the learned elementary 3D structures lead to clear improvements in 3D shape generation and matching. More precisely, we present two complementary approaches for learning elementary structures: (i) patch deformation learning and (ii) point translation learning. Both approaches can be extended to abstract structures of higher dimensions for improved results. We evaluate our method on two tasks: reconstructing ShapeNet objects and estimating dense correspondences between human scans (FAUST inter challenge). We show 16% improvement over surface deformation approaches for shape reconstruction and outperform FAUST inter challenge state of the art by 6%.","Authors (format: First Last, First Middle Last, ...)":"Theo Deprelle, Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry","Bibtex (e.g. @inproceedings...)":"@inproceedings{deprelle2019learning,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Theo Deprelle and Thibault Groueix and Matthew Fisher and Vladimir G. Kim and Bryan C. Russell and Mathieu Aubry},\n    title = {Learning elementary structures for 3D shape generation and matching},\n    year = {2019},\n    url = {http://arxiv.org/abs/1908.04725v2},\n    entrytype = {inproceedings},\n    id = {deprelle2019learning}\n}","Bibtex Name":"deprelle2019learning","Citation Count":"61","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/13/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Atlas","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Geometry Only, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1908.04725.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"7/19/2021 21:58","Title":"Learning elementary structures for 3D shape generation and matching","Training time (hr)":"","UID":"17","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2019","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2019"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Modeling and rendering of dynamic scenes is challenging, as natural scenes often contain complex phenomena such as thin structures, evolving topology, translucency, scattering, occlusion, and biological motion. Mesh-based reconstruction and tracking often fail in these cases, and other approaches (e.g., light field video) typically rely on constrained viewing conditions, which limit interactivity. We circumvent these difficulties by presenting a learning-based approach to representing dynamic objects inspired by the integral projection model used in tomographic imaging. The approach is supervised directly from 2D images in a multi-view capture setting and does not require explicit reconstruction or tracking of the object. Our method has two primary components: an encoder-decoder network that transforms input images into a 3D volume representation, and a differentiable ray-marching operation that enables end-to-end training. By virtue of its 3D representation, our construction extrapolates better to novel viewpoints compared to screen-space rendering techniques. The encoder-decoder architecture learns a latent representation of a dynamic scene that enables us to produce novel content sequences not seen during training. To overcome memory limitations of voxel-based representations, we learn a dynamic irregular grid structure implemented with a warp field during ray-marching. This structure greatly improves the apparent resolution and reduces grid-like artifacts and jagged motion. Finally, we demonstrate how to incorporate surface-based representations into our volumetric-learning framework for applications where the highest resolution is required, using facial performance capture as a case in point.","Authors (format: First Last, First Middle Last, ...)":"Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, Yaser Sheikh","Bibtex (e.g. @inproceedings...)":"@article{lombardi2019nv,\n    publisher = {Association for Computing Machinery},\n    journal = {ACM Transactions on Graphics (TOG)},\n    author = {Stephen Lombardi and Tomas Simon and Jason Saragih and Gabriel Schwartz and Andreas Lehrmann and Yaser Sheikh},\n    title = {Neural Volumes: Learning Dynamic Renderable Volumes from Images},\n    doi = {10.1145/3306346.3323020},\n    year = {2019},\n    url = {http://arxiv.org/abs/1906.07751v1},\n    entrytype = {article},\n    id = {lombardi2019nv}\n}","Bibtex Name":"lombardi2019nv","Citation Count":"161","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/facebookresearch/neuralvolumes","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/18/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Dynamic/Temporal, Local Conditioning, Voxel Grid, Global Conditioning, Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"NV","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1906.07751.pdf","Project webpage link":"https://stephenlombardi.github.io/projects/neuralvolumes/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://youtu.be/JlyGNvbGKB8?t=5347, https://crossminds.ai/video/neural-volumes-learning-dynamic-renderable-volumes-from-images-606f94d175292b321dd0906f/","Timestamp":"5/23/2021 19:19","Title":"Neural Volumes: Learning Dynamic Renderable Volumes from Images","Training time (hr)":"","UID":"16","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"SIGGRAPH 2019","Venue no Year":"SIGGRAPH","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2019"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3D-structure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model.","Authors (format: First Last, First Middle Last, ...)":"Vincent Sitzmann, Michael Zollh\u00f6fer, Gordon Wetzstein","Bibtex (e.g. @inproceedings...)":"@inproceedings{sitzmann2019srn,\n    url = {http://arxiv.org/abs/1906.01618v2},\n    year = {2019},\n    title = {Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations},\n    author = {Vincent Sitzmann and Michael Zollhofer and Gordon Wetzstein},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    publisher = {Curran Associates, Inc.},\n    entrytype = {inproceedings},\n    id = {sitzmann2019srn}\n}","Bibtex Name":"sitzmann2020srn","Citation Count":"262","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/vsitzmann/scene-representation-networks","Coordinates all at once":"","Data Release (link)":"https://drive.google.com/drive/folders/1OkYgeRcIcLOFu1ft5mRODWNQaPJ0ps90","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/4/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Global Conditioning, Hypernetwork/Meta-learning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SRN","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1906.01618.pdf","Project webpage link":"https://vsitzmann.github.io/srns/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=6vMEBWD8O20, https://slideslive.com/38922305/scene-representation-networks-continuous-3dstructureaware-neural-scene-representations","Timestamp":"5/23/2021 19:15","Title":"Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations","Training time (hr)":"","UID":"15","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2019","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2019"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"The level sets of neural networks represent fundamental properties such as decision boundaries of classifiers and are used to model non-linear manifold data such as curves and surfaces. Thus, methods for controlling the neural level sets could find many applications in machine learning. In this paper we present a simple and scalable approach to directly control level sets of a deep neural network. Our method consists of two parts: (i) sampling of the neural level sets, and (ii) relating the samples' positions to the network parameters. The latter is achieved by a sample network that is constructed by adding a single fixed linear layer to the original network. In turn, the sample network can be used to incorporate the level set samples into a loss function of interest. We have tested our method on three different learning tasks: improving generalization to unseen data, training networks robust to adversarial attacks, and curve and surface reconstruction from point clouds. For surface reconstruction, we produce high fidelity surfaces directly from raw 3D point clouds. When training small to medium networks to be robust to adversarial attacks we obtain robust accuracy comparable to state-of-the-art methods.","Authors (format: First Last, First Middle Last, ...)":"Matan Atzmon, Niv Haim, Lior Yariv, Ofer Israelov, Haggai Maron, Yaron Lipman","Bibtex (e.g. @inproceedings...)":"@inproceedings{atzmon2019controlling,\n    publisher = {Curran Associates, Inc.},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    author = {Matan Atzmon and Niv Haim and Lior Yariv and Ofer Israelov and Haggai Maron and Yaron Lipman},\n    title = {Controlling Neural Level Sets},\n    year = {2019},\n    url = {http://arxiv.org/abs/1905.11911v2},\n    entrytype = {inproceedings},\n    id = {atzmon2019controlling}\n}","Bibtex Name":"atzmon2019controlling","Citation Count":"30","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"http://faust.is.tue.mpg.de/","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/28/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Fundamentals, Sampling","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1905.11911.pdf","Project webpage link":"https://github.com/matanatz/ControllingNeuralLevelsets","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/29/2021 17:38","Title":"Controlling Neural Level Sets","Training time (hr)":"","UID":"14","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2019","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2019"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Reconstructing 3D shapes from single-view images has been a long-standing research problem. In this paper, we present DISN, a Deep Implicit Surface Network which can generate a high-quality detail-rich 3D mesh from an 2D image by predicting the underlying signed distance fields. In addition to utilizing global image features, DISN predicts the projected location for each 3D point on the 2D image, and extracts local features from the image feature maps. Combining global and local features significantly improves the accuracy of the signed distance field prediction, especially for the detail-rich areas. To the best of our knowledge, DISN is the first method that constantly captures details such as holes and thin structures present in 3D shapes from single-view images. DISN achieves the state-of-the-art single-view reconstruction performance on a variety of shape categories reconstructed from both synthetic and real images. Code is available at https://github.com/xharlie/DISN The supplementary can be found at https://xharlie.github.io/images/neurips_2019_supp.pdf","Authors (format: First Last, First Middle Last, ...)":"Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, Ulrich Neumann","Bibtex (e.g. @inproceedings...)":"@inproceedings{xu2019disn,\n    url = {http://arxiv.org/abs/1905.10711v2},\n    year = {2019},\n    title = {DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction},\n    author = {Qiangeng Xu and Weiyue Wang and Duygu Ceylan and Radomir Mech and Ulrich Neumann},\n    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n    publisher = {Curran Associates, Inc.},\n    entrytype = {inproceedings},\n    id = {xu2019disn}\n}","Bibtex Name":"xu2019disn","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/laughtervv/DISN","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/26/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DISN","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1905.10711.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/8/2021 14:20","Title":"DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction","Training time (hr)":"","UID":"13","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"NeurIPS 2019","Venue no Year":"NeurIPS","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2019"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In recent years, substantial progress has been achieved in learning-based reconstruction of 3D objects. At the same time, generative models were proposed that can generate highly realistic images. However, despite this success in these closely related tasks, texture reconstruction of 3D objects has received little attention from the research community and state-of-the-art methods are either limited to comparably low resolution or constrained experimental setups. A major reason for these limitations is that common representations of texture are inefficient or hard to interface for modern deep learning techniques. In this paper, we propose Texture Fields, a novel texture representation which is based on regressing a continuous 3D function parameterized with a neural network. Our approach circumvents limiting factors like shape discretization and parameterization, as the proposed texture representation is independent of the shape representation of the 3D object. We show that Texture Fields are able to represent high frequency texture and naturally blend with modern deep learning techniques. Experimentally, we find that Texture Fields compare favorably to state-of-the-art methods for conditional texture reconstruction of 3D objects and enable learning of probabilistic generative models for texturing unseen 3D models. We believe that Texture Fields will become an important building block for the next generation of generative 3D models.","Authors (format: First Last, First Middle Last, ...)":"Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, Andreas Geiger","Bibtex (e.g. @inproceedings...)":"@inproceedings{oechsle2019texturefields,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Michael Oechsle and Lars Mescheder and Michael Niemeyer and Thilo Strauss and Andreas Geiger},\n    title = {Texture Fields: Learning Texture Representations in Function Space},\n    year = {2019},\n    url = {http://arxiv.org/abs/1905.07259v1},\n    entrytype = {inproceedings},\n    id = {oechsle2019texturefields}\n}","Bibtex Name":"oechsle2019texturefields","Citation Count":"77","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/autonomousvision/texture_fields","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/17/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Generative Models, Data-Driven Method, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Texture Fields","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1905.07259.pdf","Project webpage link":"https://autonomousvision.github.io/texture-fields/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"http://www.cvlibs.net/publications/Oechsle2019ICCV_supplementary.pdf","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=pbfeE0qmD2E","Timestamp":"8/29/2021 21:45","Title":"Texture Fields: Learning Texture Representations in Function Space","Training time (hr)":"","UID":"12","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2019","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2019"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce Pixel-aligned Implicit Function (PIFu), a highly effective implicit representation that locally aligns pixels of 2D images with the global context of their corresponding 3D object. Using PIFu, we propose an end-to-end deep learning method for digitizing highly detailed clothed humans that can infer both 3D surface and texture from a single image, and optionally, multiple input images. Highly intricate shapes, such as hairstyles, clothing, as well as their variations and deformations can be digitized in a unified way. Compared to existing representations used for 3D deep learning, PIFu can produce high-resolution surfaces including largely unseen regions such as the back of a person. In particular, it is memory efficient unlike the voxel representation, can handle arbitrary topology, and the resulting surface is spatially aligned with the input image. Furthermore, while previous techniques are designed to process either a single image or multiple views, PIFu extends naturally to arbitrary number of views. We demonstrate high-resolution and robust reconstructions on real world images from the DeepFashion dataset, which contains a variety of challenging clothing types. Our method achieves state-of-the-art performance on a public benchmark and outperforms the prior work for clothed human digitization from a single image.","Authors (format: First Last, First Middle Last, ...)":"Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, Hao Li","Bibtex (e.g. @inproceedings...)":"@inproceedings{saito2019pifu,\n    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},\n    author = {Shunsuke Saito and Zeng Huang and Ryota Natsume and Shigeo Morishima and Angjoo Kanazawa and Hao Li},\n    title = {PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization},\n    year = {2019},\n    url = {http://arxiv.org/abs/1905.05172v3},\n    entrytype = {inproceedings},\n    id = {saito2019pifu}\n}","Bibtex Name":"saito2019pifu","Citation Count":"295","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/shunsukesaito/PIFu","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/13/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Human (Body), Sparse Reconstruction, Generalization, Image-Based Rendering, Data-Driven Method, Local Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"PIFu","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1905.05172.pdf","Project webpage link":"https://shunsukesaito.github.io/PIFu/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=S1FpjwKqtPs","Timestamp":"6/29/2021 16:44","Title":"PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization","Training time (hr)":"","UID":"11","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICCV 2019","Venue no Year":"ICCV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2019"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a learning based approach for multi-view stereopsis (MVS). While current deep MVS methods achieve impressive results, they crucially rely on ground-truth 3D training data, and acquisition of such precise 3D geometry for supervision is a major hurdle. Our framework instead leverages photometric consistency between multiple views as supervisory signal for learning depth prediction in a wide baseline MVS setup. However, naively applying photo consistency constraints is undesirable due to occlusion and lighting changes across views. To overcome this, we propose a robust loss formulation that: a) enforces first order consistency and b) for each point, selectively enforces consistency with some views, thus implicitly handling occlusions. We demonstrate our ability to learn MVS without 3D supervision using a real dataset, and show that each component of our proposed robust loss results in a significant improvement. We qualitatively observe that our reconstructions are often more complete than the acquired ground truth, further showing the merits of this approach. Lastly, our learned model generalizes to novel settings, and our approach allows adaptation of existing CNNs to datasets without ground-truth 3D by unsupervised finetuning. Project webpage: https://tejaskhot.github.io/unsup_mvs","Authors (format: First Last, First Middle Last, ...)":"Tejas Khot, Shubham Agrawal, Shubham Tulsiani, Christoph Mertz, Simon Lucey, Martial Hebert","Bibtex (e.g. @inproceedings...)":"@article{khot2021surrf,\n    url = {http://arxiv.org/abs/1905.02706v2},\n    year = {2021},\n    title = {Learning Unsupervised Multi-View Stereopsis via Robust Photometric Consistency},\n    author = {Tejas Khot and Shubham Agrawal and Shubham Tulsiani and Christoph Mertz and Simon Lucey and Martial Hebert},\n    booktitle = {ArXiv Pre-print},\n    journal = {arXiv preprint arXiv:1905.02706},\n    entrytype = {article},\n    id = {khot2021surrf}\n}","Bibtex Name":"khot2021surrf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/tejaskhot/unsup_mvs","Coordinates all at once":"No","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"5/7/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"Direct","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"No","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Other","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Hybrid Geometry Representation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"SurRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1905.02706.pdf","Project webpage link":"https://tejaskhot.github.io/unsup_mvs/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"No","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/4/2021 22:06","Title":"SurRF: Unsupervised Multi-view Stereopsis by Learning Surface Radiance Field","Training time (hr)":"","UID":"10","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2021","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2021"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.","Authors (format: First Last, First Middle Last, ...)":"Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove","Bibtex (e.g. @inproceedings...)":"@inproceedings{park2019deepsdf,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Jeong Joon Park and Peter Florence and Julian Straub and Richard Newcombe and Steven Lovegrove},\n    title = {DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation},\n    year = {2019},\n    url = {http://arxiv.org/abs/1901.05103v1},\n    entrytype = {inproceedings},\n    id = {park2019deepsdf}\n}","Bibtex Name":"park2019deepsdf","Citation Count":"593","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/facebookresearch/DeepSDF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"1/16/2019","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"SDF","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Generative Models","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DeepSDF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1901.05103.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"5/23/2021 19:20","Title":"DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation","Training time (hr)":"","UID":"9","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2019","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2019"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"Deep image generation is becoming a tool to enhance artists and designers creativity potential. In this paper, we aim at making the generation process more structured and easier to interact with. Inspired by vector graphics systems, we propose a new deep image reconstruction paradigm where the outputs are composed from simple layers, defined by their color and a vector transparency mask. This presents a number of advantages compared to the commonly used convolutional network architectures. In particular, our layered decomposition allows simple user interaction, for example to update a given mask, or change the color of a selected layer. From a compact code, our architecture also generates vector images with a virtually infinite resolution, the color at each point in an image being a parametric function of its coordinates. We validate the efficiency of our approach by comparing reconstructions with state-of-the-art baselines given similar memory resources on CelebA and ImageNet datasets. Most importantly, we demonstrate several applications of our new image representation obtained in an unsupervised manner, including editing, vectorization and image search.","Authors (format: First Last, First Middle Last, ...)":"Othman Sbai, Camille Couprie, Mathieu Aubry","Bibtex (e.g. @inproceedings...)":"@article{sbai2020unsupervised,\n    author = {Othman Sbai and Camille Couprie and Mathieu Aubry},\n    title = {Unsupervised Image Decomposition in Vector Layers},\n    year = {2018},\n    month = {Dec},\n    url = {http://arxiv.org/abs/1812.05484v2}\n}","Bibtex Name":"sbai2020unsupervised","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/facebookresearch/pix2vec","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/13/2018","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"qian_zhang@brown.edu","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"2D Image Neural Fields, Editable, Data-Driven Method","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1812.05484.pdf","Project webpage link":"http://imagine.enpc.fr/~sbaio/publications/pix2vec/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"3/21/2022 9:58","Title":"Unsupervised Image Decomposition in Vector Layers","Training time (hr)":"","UID":"8","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ICIP 2020","Venue no Year":"ICIP","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2020"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.","Authors (format: First Last, First Middle Last, ...)":"Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, Andreas Geiger","Bibtex (e.g. @inproceedings...)":"@inproceedings{mescheder2019occupancynetworks,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Lars Mescheder and Michael Oechsle and Michael Niemeyer and Sebastian Nowozin and Andreas Geiger},\n    title = {Occupancy Networks: Learning 3D Reconstruction in Function Space},\n    year = {2019},\n    url = {http://arxiv.org/abs/1812.03828v2},\n    entrytype = {inproceedings},\n    id = {mescheder2019occupancynetworks}\n}","Bibtex Name":"mescheder2019occupancynetworks","Citation Count":"540","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/autonomousvision/occupancy_networks","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/10/2018","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Sampling, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"Occupancy Networks","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1812.03828.pdf","Project webpage link":"https://avg.is.tuebingen.mpg.de/publications/occupancy-networks","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=w1Qo3bOiPaE","Timestamp":"5/23/2021 19:22","Title":"Occupancy Networks: Learning 3D Reconstruction in Function Space","Training time (hr)":"","UID":"7","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2019","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2019"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We advocate the use of implicit fields for learning generative models of shapes and introduce an implicit field decoder, called IM-NET, for shape generation, aimed at improving the visual quality of the generated shapes. An implicit field assigns a value to each point in 3D space, so that a shape can be extracted as an iso-surface. IM-NET is trained to perform this assignment by means of a binary classifier. Specifically, it takes a point coordinate, along with a feature vector encoding a shape, and outputs a value which indicates whether the point is outside the shape or not. By replacing conventional decoders by our implicit decoder for representation learning (via IM-AE) and shape generation (via IM-GAN), we demonstrate superior results for tasks such as generative shape modeling, interpolation, and single-view 3D reconstruction, particularly in terms of visual quality. Code and supplementary material are available at https://github.com/czq142857/implicit-decoder.","Authors (format: First Last, First Middle Last, ...)":"Zhiqin Chen, Hao Zhang","Bibtex (e.g. @inproceedings...)":"@inproceedings{chen2019imnet,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Zhiqin Chen and Hao Zhang},\n    title = {Learning Implicit Fields for Generative Shape Modeling},\n    year = {2019},\n    url = {http://arxiv.org/abs/1812.02822v5},\n    entrytype = {inproceedings},\n    id = {chen2019imnet}\n}","Bibtex Name":"chen2019imnet","Citation Count":"324","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/czq142857/implicit-decoder","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"12/6/2018","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Occupancy","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Generalization, Generative Models, Data-Driven Method, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"IM-NET","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1812.02822.pdf","Project webpage link":"https://www.sfu.ca/~zhiqinc/imgan/Readme.html","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/29/2021 15:38","Title":"Learning Implicit Fields for Generative Shape Modeling","Training time (hr)":"","UID":"6","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2019","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2019"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"The reconstruction of a discrete surface from a point cloud is a fundamental geometry processing problem that has been studied for decades, with many methods developed. We propose the use of a deep neural network as a geometric prior for surface reconstruction. Specifically, we overfit a neural network representing a local chart parameterization to part of an input point cloud using the Wasserstein distance as a measure of approximation. By jointly fitting many such networks to overlapping parts of the point cloud, while enforcing a consistency condition, we compute a manifold atlas. By sampling this atlas, we can produce a dense reconstruction of the surface approximating the input cloud. The entire procedure does not require any training data or explicit regularization, yet, we show that it is able to perform remarkably well: not introducing typical overfitting artifacts, and approximating sharp features closely at the same time. We experimentally show that this geometric prior produces good results for both man-made objects containing sharp features and smoother organic objects, as well as noisy inputs. We compare our method with a number of well-known reconstruction methods on a standard surface reconstruction benchmark.","Authors (format: First Last, First Middle Last, ...)":"Francis Williams, Teseo Schneider, Claudio Silva, Denis Zorin, Joan Bruna, Daniele Panozzo","Bibtex (e.g. @inproceedings...)":"@inproceedings{williams2019deep,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Francis Williams and Teseo Schneider and Claudio Silva and Denis Zorin and Joan Bruna and Daniele Panozzo},\n    title = {Deep Geometric Prior for Surface Reconstruction},\n    year = {2019},\n    url = {http://arxiv.org/abs/1811.10943v2},\n    entrytype = {inproceedings},\n    id = {williams2019deep}\n}","Bibtex Name":"williams2019deep","Citation Count":"64","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/fwilliams/deep-geometric-prior","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"11/27/2018","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Atlas","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1811.10943.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/29/2021 15:56","Title":"Deep Geometric Prior for Surface Reconstruction","Training time (hr)":"","UID":"5","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2019","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2019"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce physics-informed neural networks-neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and","Authors (format: First Last, First Middle Last, ...)":"Maziar Raissi, Paris Perdikaris, George E Karniadakis","Bibtex (e.g. @inproceedings...)":"@article{raissi2019physicsinformed,\n    author = {Maziar Raissi and Paris Perdikaris and George E Karniadakis},\n    journal = {Journal of Computational Physics},\n    pages = {686--707},\n    year = {2019},\n    publisher = {Elsevier},\n    title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},\n    volume = {378},\n    entrytype = {article},\n    id = {raissi2019physicsinformed}\n}","Bibtex Name":"raissi2019physicsinformed","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/maziarraissi/PINNs","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/13/2018","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals, Science & Engineering, Supervision by Gradient (PDE)","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://www.sciencedirect.com/science/article/pii/S0021999118307125","Project webpage link":"https://maziarraissi.github.io/PINNs/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"9/17/2021 22:42","Title":"Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations","Training time (hr)":"","UID":"4","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"Journal of Computational Physics 2019","Venue no Year":"Journal of Computational Physics","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2019"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We introduce a method for learning to generate the surface of 3D shapes. Our approach represents a 3D shape as a collection of parametric surface elements and, in contrast to methods generating voxel grids or point clouds, naturally infers a surface representation of the shape. Beyond its novelty, our new shape generation framework, AtlasNet, comes with significant advantages, such as improved precision and generalization capabilities, and the possibility to generate a shape of arbitrary resolution without memory issues. We demonstrate these benefits and compare to strong baselines on the ShapeNet benchmark for two applications: (i) auto-encoding shapes, and (ii) single-view reconstruction from a still image. We also provide results showing its potential for other applications, such as morphing, parametrization, super-resolution, matching, and co-segmentation.","Authors (format: First Last, First Middle Last, ...)":"Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry","Bibtex (e.g. @inproceedings...)":"@inproceedings{groueix2018atlasnet,\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    author = {Thibault Groueix and Matthew Fisher and Vladimir G. Kim and Bryan C. Russell and Mathieu Aubry},\n    title = {AtlasNet: A Papier-Mache Approach to Learning 3D Surface Generation},\n    year = {2018},\n    url = {http://arxiv.org/abs/1802.05384v3},\n    entrytype = {inproceedings},\n    id = {groueix2018atlasnet}\n}","Bibtex Name":"groueix2018atlasnet","Citation Count":"6","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/ThibaultGROUEIX/AtlasNet","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"2/15/2018","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Sampling, Data-Driven Method, Global Conditioning","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"AtlasNet","PDF link (arXiv perferred)":"https://arxiv.org/pdf/1802.05384.pdf","Project webpage link":"http://imagine.enpc.fr/~groueixt/atlasnet/","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"http://imagine.enpc.fr/~groueixt/atlasnet/atlasnet_slides_spotlight_CVPR.pptx, http://imagine.enpc.fr/~groueixt/atlasnet/atlasnet_poster.pdf","Timestamp":"5/23/2021 19:23","Title":"AtlasNet: A Papier-M\u00e2ch\u00e9 Approach to Learning 3D Surface Generation","Training time (hr)":"","UID":"3","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"CVPR 2019","Venue no Year":"CVPR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2019"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present radiance regression functions for fast rendering of global illumination in scenes with dynamic local light sources. A radiance regression function (RRF) represents a non-linear mapping from local and contextual attributes of surface points, such as position, viewing direction, and lighting condition, to their indirect illumination values. The RRF is obtained from precomputed shading samples through regression analysis, which determines a function that best fits the shading data. For a given scene, the shading samples are precomputed by an offline renderer.\n The key idea behind our approach is to exploit the nonlinear coherence of the indirect illumination data to make the RRF both compact and fast to evaluate. We model the RRF as a multilayer acyclic feed-forward neural network, which provides a close functional approximation of the indirect illumination and can be efficiently evaluated at run time. To effectively model scenes with spatially variant material properties, we utilize an augmented set of attributes as input to the neural network RRF to reduce the amount of inference that the network needs to perform. To handle scenes with greater geometric complexity, we partition the input space of the RRF model and represent the subspaces with separate, smaller RRFs that can be evaluated more rapidly. As a result, the RRF model scales well to increasingly complex scene geometry and material variation. Because of its compactness and ease of evaluation, the RRF model enables real-time rendering with full global illumination effects, including changing caustics and multiple-bounce high-frequency glossy interreflections.","Authors (format: First Last, First Middle Last, ...)":"Peiran Ren, Jiaping Wang, Minmin Gong, Stephen Lin, Xin Tong, Baining Guo","Bibtex (e.g. @inproceedings...)":"@article{ren2013global,\n    author = {Peiran Ren and Jiaping Wang and Minmin Gong and Stephen Lin and Xin Tong and Baining Guo},\n    title = {Global Illumination with Radiance Regression Functions},\n    year = {2013},\n    date = {July 2013},\n    publisher = {Association for Computing Machinery},\n    address = {New York, NY, USA},\n    volume = {32},\n    number = {4},\n    issn = {0730-0301},\n    url = {https://doi.org/10.1145/2461912.2462009},\n    doi = {10.1145/2461912.2462009},\n    journal = {ACM Trans. Graph.},\n    month = {jul},\n    articleno = {130},\n    numpages = {12},\n    keywords = {real time rendering, global illumination, neural network, non-linear regression},\n    entrytype = {article},\n    id = {ren2013global}\n}","Bibtex Name":"ren2013global","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"7/1/2013","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Material/Lighting Estimation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://cseweb.ucsd.edu//~ravir/274/15/papers/a130-ren.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"10/8/2021 16:45","Title":"Global Illumination with Radiance Regression Functions","Training time (hr)":"","UID":"2","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"SIGGRAPH 2018","Venue no Year":"SIGGRAPH","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2018"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"3D object reconstruction is frequent used in various fields such as product design, engineering, medical and artistic applications. Numerous reconstruction techniques and software were introduced and developed. However, the purpose of this paper is to fully integrate an adaptive artificial neural network (ANN) based method in reconstructing and representing 3D objects. This study explores the ability of neural networks in learning through experience when reconstructing an object by estimating it\u2019s z-coordinate. Neural networks\u2019 capability in representing most classes of 3D objects used in computer graphics is also proven. Simple affined transformation is applied on different objects using this approach and compared with the real objects. The results show that neural network is a promising approach for reconstruction and representation of 3D objects.","Authors (format: First Last, First Middle Last, ...)":"Lim Wen Peng, Siti Mariyam Shamsuddin","Bibtex (e.g. @inproceedings...)":"@book{lim20043d,\n    title = {3D Object Reconstruction and Representation Using Neural Networks},\n    author = {Wen Peng Lim and Siti Mariyam Shamsuddin},\n    year = {2004},\n    publisher = {Universiti Teknologi Malaysia},\n    booktitle = {International Conference on Computer Graphics and Interactive Techniques in Australasia and South East Asia},\n    entrytype = {book},\n    id = {lim20043d}\n}","Bibtex Name":"lim20043d","Citation Count":"23","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/15/2004","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"Other","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Fundamentals","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.6810&rep=rep1&type=pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"Yes, geometry only","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"6/29/2021 15:19","Title":"3D Object Reconstruction and Representation Using Neural Networks","Training time (hr)":"","UID":"1","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"GRAPHITE 2004","Venue no Year":"GRAPHITE","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2004"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"We present a new representation for the storage and reconstruction of arbitrary reflectance functions. This non-linear representation, based on a neural network model, accurately captures the spectral and spatial variation of these functions. It is both computationally efficient and concise, yet expressive. We reconstruct the subtle reflection characteristics of an analytic reflection model as well as measured and simulated reflection data","Authors (format: First Last, First Middle Last, ...)":"David Gargan, Francis Neelamkavil","Bibtex (e.g. @inproceedings...)":"@article{gargan1998approximating,\n    publisher = {The Eurographics Association and John Wiley & Sons Ltd.},\n    journal = {Computer Graphics Forum},\n    title = {Approximating reflectance functions using neural networks},\n    author = {David Gargan and Francis Neelamkavil},\n    booktitle = {Eurographics Workshop on Rendering Techniques},\n    pages = {23--34},\n    year = {1998},\n    organization = {Springer},\n    entrytype = {article},\n    id = {gargan1998approximating}\n}","Bibtex Name":"gargan1998approximating","Citation Count":"11","Code Release (Github link, or enter \"Coming soon\")":"","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"6/29/1998","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"None","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"","Keywords":"Material/Lighting Estimation","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"","PDF link (arXiv perferred)":"https://link.springer.com/chapter/10.1007/978-3-7091-6453-2_3","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"","Timestamp":"8/29/2021 20:09","Title":"Approximating Reflectance Functions using Neural Networks","Training time (hr)":"","UID":"0","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"EGSR 1998","Venue no Year":"EGSR","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"1998"},{"":"","# of input views (e.g. 18 for 18-camera system)":"","Abstract":"In this paper, we study the problem of 3D scene geometry decomposition and manipulation from 2D views. By leveraging the recent implicit neural representation techniques, particularly the appealing neural radiance fields, we introduce an object field component to learn unique codes for all individual objects in 3D space only from 2D supervision. The key to this component is a series of carefully designed loss functions to enable every 3D point, especially in non-occupied space, to be effectively optimized even without 3D labels. In addition, we introduce an inverse query algorithm to freely manipulate any specified 3D object shape in the learned scene representation. Notably, our manipulation algorithm can explicitly tackle key issues such as object collisions and visual occlusions. Our method, called DM-NeRF, is among the first to simultaneously reconstruct, decompose, manipulate and render complex 3D scenes in a single pipeline. Extensive experiments on three datasets clearly show that our method can accurately decompose all 3D objects from 2D views, allowing any interested object to be freely manipulated in 3D space such as translation, rotation, size adjustment, and deformation.","Authors (format: First Last, First Middle Last, ...)":"Bing Wang, Lu Chen, Bo Yang","Bibtex (e.g. @inproceedings...)":"  @article{wang2022dmnerf,\n  title={DM-NeRF: 3D Scene Geometry Decomposition and Manipulation from 2D Images},\n  author={Bing, Wang and Chen, Lu and Yang, Bo},\n  journal={arXiv preprint arXiv:2208.07227},\n  year={2022}\n}","Bibtex Name":"wang2022dmnerf","Citation Count":"","Code Release (Github link, or enter \"Coming soon\")":"https://github.com/vLAR-group/DM-NeRF","Coordinates all at once":"","Data Release (link)":"","Dataset(s) used (e.g. Tanks and Temples)":"","Date released":"8/15/2022","Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z)":"","Does your work use coordinate(s) as neural network input(s)?":"","Email Address":"n.tsagkas@ed.ac.uk","Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)":"","Frequency/Positional Encoding":"","Geometry proxy (for non-visual computing papers, choose \"N/A\")":"","Inputs":"","Is the PDF linked to arXiv?":"Yes (almost done)","Keywords":"Editable, Image-Based Rendering","Lighting":"","New entry or update existing?":"","Nickname (e.g. DeepSDF)":"DM-NeRF","PDF link (arXiv perferred)":"https://arxiv.org/pdf/2208.07227.pdf","Project webpage link":"","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose \"N/A\")":"","Rendering time (FPS)":"","Supplement PDF (link)":"","Supplement video (link, comma separated if multiple exists)":"","Talk/Video (link, Youtube preferred)":"https://www.youtube.com/watch?v=iE0RwmdLIzk","Timestamp":"01/03/23 04:19","Title":"DM-NeRF: 3D Scene Geometry Decomposition and Manipulation from 2D Images","Training time (hr)":"","UID":"410","Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)":"ARXIV 2022","Venue no Year":"ARXIV","Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put \"2022\" for this entry, and \"2021\" for the above)":"2022"}]
