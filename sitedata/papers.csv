Timestamp,Email Address,Does your work use coordinate(s) as neural network input(s)?,Title,Nickname (e.g. DeepSDF),Venue no Year,Date released,"Year (corresponding to venue e.g. released in 2021, accepted to CVPR 2022, then put ""2022"" for this entry, and ""2021"" for the above)",Bibtex (e.g. @inproceedings...),PDF link (arXiv perferred),Project webpage link,"Code Release (Github link, or enter ""Coming soon"")",Data Release (link),"Talk/Video (link, Youtube preferred)",Supplement PDF (link),"Supplement video (link, comma separated if multiple exists)",Keywords,Frequency/Positional Encoding,"Geometry proxy (for non-visual computing papers, choose ""N/A"")","Reconstructs Geometry Only (i.e. no color texture) (for non-visual computing papers, choose ""N/A"")",Training time (hr),Rendering time (FPS),Dataset(s) used (e.g. Tanks and Temples),# of input views (e.g. 18 for 18-camera system),Inputs,Lighting,"Authors (format: First Last, First Middle Last, ...)",Bibtex Name,UID,Citation Count,Abstract,Coordinates all at once,"Feature-as-input (coordinate samples feature grid, but coordinate is not supplied as input)",Direct/Indirect Neural Field (one or more dimension built into the network e.g. 2D CNN + z),Is the PDF linked to arXiv?,New entry or update existing?,"Venue & Year (e.g. NeurIPS 2022, ARXIV 2021)",,,,,,
3/21/2022 9:58,qian_zhang@brown.edu,,Unsupervised Image Decomposition in Vector Layers,,ICIP,12/13/2018,2020,"@article{sbai2020unsupervised,
    AUTHOR = {Othman Sbai and Camille Couprie and Mathieu Aubry},
    TITLE = {Unsupervised Image Decomposition in Vector Layers},
    YEAR = {2018},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/1812.05484v2}
}",https://arxiv.org/pdf/1812.05484.pdf,http://imagine.enpc.fr/~sbaio/publications/pix2vec/,https://github.com/facebookresearch/pix2vec,,,,,"2D Image Neural Fields, Editable, Data-Driven Method",,,,,,,,,,"Othman Sbai, Camille Couprie, Mathieu Aubry",sbai2020unsupervised,00000281,,"Deep image generation is becoming a tool to enhance artists and designers creativity potential. In this paper, we aim at making the generation process more structured and easier to interact with. Inspired by vector graphics systems, we propose a new deep image reconstruction paradigm where the outputs are composed from simple layers, defined by their color and a vector transparency mask. This presents a number of advantages compared to the commonly used convolutional network architectures. In particular, our layered decomposition allows simple user interaction, for example to update a given mask, or change the color of a selected layer. From a compact code, our architecture also generates vector images with a virtually infinite resolution, the color at each point in an image being a parametric function of its coordinates. We validate the efficiency of our approach by comparing reconstructions with state-of-the-art baselines given similar memory resources on CelebA and ImageNet datasets. Most importantly, we demonstrate several applications of our new image representation obtained in an unsupervised manner, including editing, vectorization and image search.",,,,Yes (almost done),,ICIP 2020,,,,,,
3/21/2022 10:01,qian_zhang@brown.edu,,Invertible Neural BRDF for Object Inverse Rendering,,ECCV,08/10/2020,2020,"@article{chen2020invertible,
    AUTHOR = {Zhe Chen and Shohei Nobuhara and Ko Nishino},
    TITLE = {Invertible Neural BRDF for Object Inverse Rendering},
    YEAR = {2020},
    MONTH = {Aug},
    URL = {http://arxiv.org/abs/2008.04030v2}
}",https://arxiv.org/pdf/2008.04030.pdf,,https://github.com/chenzhekl/iBRDF,,,,,"Material/Lighting Estimation, Generative Models, Data-Driven Method",,,,,,,,,,"Zhe Chen, Shohei Nobuhara, Ko Nishino",chen2020invertible,00000282,,"We introduce a novel neural network-based BRDF model and a Bayesian framework for object inverse rendering, i.e., joint estimation of reflectance and natural illumination from a single image of an object of known geometry. The BRDF is expressed with an invertible neural network, namely, normalizing flow, which provides the expressive power of a high-dimensional representation, computational simplicity of a compact analytical model, and physical plausibility of a real-world BRDF. We extract the latent space of real-world reflectance by conditioning this model, which directly results in a strong reflectance prior. We refer to this model as the invertible neural BRDF model (iBRDF). We also devise a deep illumination prior by leveraging the structural bias of deep neural networks. By integrating this novel BRDF model and reflectance and illumination priors in a MAP estimation formulation, we show that this joint estimation can be computed efficiently with stochastic gradient descent. We experimentally validate the accuracy of the invertible neural BRDF model on a large number of measured data and demonstrate its use in object inverse rendering on a number of synthetic and real images. The results show new ways in which deep neural networks can help solve challenging radiometric inverse problems.",,,,Yes (almost done),,ECCV 2020,,,,,,
3/21/2022 10:04,qian_zhang@brown.edu,,Neural BRDF Representation and Importance Sampling,,Computer Grahics Forum,02/11/2021,2021,"@article{sztrajman2021neural,
    AUTHOR = {Alejandro Sztrajman and Gilles Rainer and Tobias Ritschel and Tim Weyrich},
    TITLE = {Neural BRDF Representation and Importance Sampling},
    YEAR = {2021},
    MONTH = {Feb},
    URL = {http://arxiv.org/abs/2102.05963v3}
}",https://arxiv.org/pdf/2102.05963.pdf,http://www0.cs.ucl.ac.uk/staff/A.Sztrajman/webpage/publications/nbrdf2021/nbrdf.html,http://www0.cs.ucl.ac.uk/staff/A.Sztrajman/webpage/publications/nbrdf2021/nbrdf.html,,,,,"Material/Lighting Estimation, Compression, Sampling",,,,,,,,,,"Alejandro Sztrajman, Gilles Rainer, Tobias Ritschel, Tim Weyrich",sztrajman2021neural,00000283,,"Controlled capture of real-world material appearance yields tabulated sets of highly realistic reflectance data. In practice, however, its high memory footprint requires compressing into a representation that can be used efficiently in rendering while remaining faithful to the original. Previous works in appearance encoding often prioritised one of these requirements at the expense of the other, by either applying high-fidelity array compression strategies not suited for efficient queries during rendering, or by fitting a compact analytic model that lacks expressiveness. We present a compact neural network-based representation of BRDF data that combines high-accuracy reconstruction with efficient practical rendering via built-in interpolation of reflectance. We encode BRDFs as lightweight networks, and propose a training scheme with adaptive angular sampling, critical for the accurate reconstruction of specular highlights. Additionally, we propose a novel approach to make our representation amenable to importance sampling: rather than inverting the trained networks, we learn to encode them in a more compact embedding that can be mapped to parameters of an analytic BRDF for which importance sampling is known. We evaluate encoding results on isotropic and anisotropic BRDFs from multiple real-world datasets, and importance sampling performance for isotropic BRDFs mapped to two different analytic models.",,,,Yes (almost done),,Computer Grahics Forum 2021,,,,,,
3/21/2022 10:54,yiheng_xie@brown.edu,,Instant Neural Graphics Primitives with a Multiresolution Hash Encoding,Instant-NGP,ARXIV,01/16/2022,2022,"@article{muller2022instantngp,
    AUTHOR = {Thomas Muller and Alex Evans and Christoph Schied and Alexander Keller},
    TITLE = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
    YEAR = {2022},
    MONTH = {Jan},
    URL = {http://arxiv.org/abs/2201.05989v1}
}",https://arxiv.org/pdf/2201.05989.pdf,https://nvlabs.github.io/instant-ngp/,https://github.com/NVlabs/instant-ngp,,https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.mp4,,,"Speed & Computational Efficiency, Fundamentals, Generalization, Hybrid Geometry Parameterization, Positional Encoding",,,,,,,,,,"Thomas Müller, Alex Evans, Christoph Schied, Alexander Keller",muller2022instantngp,00000284,,"Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of ${1920\!\times\!1080}$.",,,,Yes (almost done),,ARXIV 2022,,,,,,
3/21/2022 11:02,yiheng_xie@brown.edu,,Learning Smooth Neural Functions via Lipschitz Regularization,,ARXIV,02/16/2022,2022,"@article{liu2022learning,
    AUTHOR = {Hsueh-Ti Derek Liu and Francis Williams and Alec Jacobson and Sanja Fidler and Or Litany},
    TITLE = {Learning Smooth Neural Functions via Lipschitz Regularization},
    YEAR = {2022},
    MONTH = {Feb},
    URL = {http://arxiv.org/abs/2202.08345v1}
}",https://arxiv.org/pdf/2202.08345.pdf,,,,,,,"Surface Reconstruction, Fundamentals, Generalization, Global Conditioning, Supervision by Gradient (PDE), Regularization",,,,,,,,,,"Hsueh-Ti Derek Liu, Francis Williams, Alec Jacobson, Sanja Fidler, Or Litany",liu2022learning,00000285,,"Neural implicit fields have recently emerged as a useful representation for 3D shapes. These fields are commonly represented as neural networks which map latent descriptors and 3D coordinates to implicit function values. The latent descriptor of a neural field acts as a deformation handle for the 3D shape it represents. Thus, smoothness with respect to this descriptor is paramount for performing shape-editing operations. In this work, we introduce a novel regularization designed to encourage smooth latent spaces in neural fields by penalizing the upper bound on the field's Lipschitz constant. Compared with prior Lipschitz regularized networks, ours is computationally fast, can be implemented in four lines of code, and requires minimal hyperparameter tuning for geometric applications. We demonstrate the effectiveness of our approach on shape interpolation and extrapolation as well as partial shape reconstruction from 3D point clouds, showing both qualitative and quantitative improvements over existing state-of-the-art and non-regularized baselines.",,,,Yes (almost done),,ARXIV 2022,,,,,,
3/21/2022 11:05,yiheng_xie@brown.edu,,Block-NeRF: Scalable Large Scene Neural View Synthesis,Block-NeRF,ARXIV,02/10/2022,2022,"@article{tancik2022blocknerf,
    AUTHOR = {Matthew Tancik and Vincent Casser and Xinchen Yan and Sabeek Pradhan and Ben Mildenhall and Pratul P. Srinivasan and Jonathan T. Barron and Henrik Kretzschmar},
    TITLE = {Block-NeRF: Scalable Large Scene Neural View Synthesis},
    YEAR = {2022},
    MONTH = {Feb},
    URL = {http://arxiv.org/abs/2202.05263v1}
}",https://arxiv.org/pdf/2202.05263.pdf,https://waymo.com/research/block-nerf/,,,https://www.youtube.com/watch?v=6lGMCAzBzOQ,,,"Voxel Grid, Large-Scale Scenes",,,,,,,,,,"Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P. Srinivasan, Jonathan T. Barron, Henrik Kretzschmar",tancik2022blocknerf,00000286,,"We present Block-NeRF, a variant of Neural Radiance Fields that can represent large-scale environments. Specifically, we demonstrate that when scaling NeRF to render city-scale scenes spanning multiple blocks, it is vital to decompose the scene into individually trained NeRFs. This decomposition decouples rendering time from scene size, enables rendering to scale to arbitrarily large environments, and allows per-block updates of the environment. We adopt several architectural changes to make NeRF robust to data captured over months under different environmental conditions. We add appearance embeddings, learned pose refinement, and controllable exposure to each individual NeRF, and introduce a procedure for aligning appearance between adjacent NeRFs so that they can be seamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to create the largest neural scene representation to date, capable of rendering an entire neighborhood of San Francisco.",,,,Yes (almost done),,ARXIV 2022,,,,,,
3/21/2022 13:06,yiheng_xie@brown.edu,,Mega-NeRF: Scalable Construction of Large-Scale NeRFs for Virtual Fly-Throughs,Mega-NeRF,ARXIV,12/20/2021,2022,"@article{turki2022meganerf,
    AUTHOR = {Haithem Turki and Deva Ramanan and Mahadev Satyanarayanan},
    TITLE = {Mega-NeRF: Scalable Construction of Large-Scale NeRFs for Virtual Fly-Throughs},
    YEAR = {2021},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2112.10703v1}
}",https://arxiv.org/pdf/2112.10703.pdf,https://meganerf.cmusatyalab.org/,https://github.com/cmusatyalab/mega-nerf,,,,,"Speed & Computational Efficiency, Sampling, Large-Scale Scenes",,,,,,,,,,"Haithem Turki, Deva Ramanan, Mahadev Satyanarayanan",turki2022meganerf,00000287,,"We explore how to leverage neural radiance fields (NeRFs) to build interactive 3D environments from large-scale visual captures spanning buildings or even multiple city blocks collected primarily from drone data. In contrast to the single object scenes against which NeRFs have been traditionally evaluated, this setting poses multiple challenges including (1) the need to incorporate thousands of images with varying lighting conditions, all of which capture only a small subset of the scene, (2) prohibitively high model capacity and ray sampling requirements beyond what can be naively trained on a single GPU, and (3) an arbitrarily large number of possible viewpoints that make it unfeasible to precompute all relevant information beforehand (as real-time NeRF renderers typically do). To address these challenges, we begin by analyzing visibility statistics for large-scale scenes, motivating a sparse network structure where parameters are specialized to different regions of the scene. We introduce a simple geometric clustering algorithm that partitions training images (or rather pixels) into different NeRF submodules that can be trained in parallel. We evaluate our approach across scenes taken from the Quad 6k and UrbanScene3D datasets as well as against our own drone footage and show a 3x training speedup while improving PSNR by over 11% on average. We subsequently perform an empirical evaluation of recent NeRF fast renderers on top of Mega-NeRF and introduce a novel method that exploits temporal coherence. Our technique achieves a 40x speedup over conventional NeRF rendering while remaining within 0.5 db in PSNR quality, exceeding the fidelity of existing fast renderers.",,,,Yes (almost done),,ARXIV 2022,,,,,,
3/21/2022 15:50,yiheng_xie@brown.edu,,Intrinsic Neural Fields: Learning Functions on Manifolds,Intrinsic Neural Fields,ARXIV,03/15/2022,2022,"@article{koestler2022intrinsicneuralfields,
    AUTHOR = {Lukas Koestler and Daniel Grittner and Michael Moeller and Daniel Cremers and Zorah Lahner},
    TITLE = {Intrinsic Neural Fields: Learning Functions on Manifolds},
    YEAR = {2022},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2203.07967v3}
}",https://arxiv.org/pdf/2203.07967.pdf,,,,,,,"Fundamentals, Positional Encoding",,,,,,,,,,"Lukas Koestler, Daniel Grittner, Michael Moeller, Daniel Cremers, Zorah Lähner",koestler2022intrinsicneuralfields,00000288,,"Neural fields have gained significant attention in the computer vision community due to their excellent performance in novel view synthesis, geometry reconstruction, and generative modeling. Some of their advantages are a sound theoretic foundation and an easy implementation in current deep learning frameworks. While neural fields have been applied to signals on manifolds, e.g., for texture reconstruction, their representation has been limited to extrinsically embedding the shape into Euclidean space. The extrinsic embedding ignores known intrinsic manifold properties and is inflexible wrt. transfer of the learned function. To overcome these limitations, this work introduces intrinsic neural fields, a novel and versatile representation for neural fields on manifolds. Intrinsic neural fields combine the advantages of neural fields with the spectral properties of the Laplace-Beltrami operator. We show theoretically that intrinsic neural fields inherit many desirable properties of the extrinsic neural field framework but exhibit additional intrinsic qualities, like isometry invariance. In experiments, we show intrinsic neural fields can reconstruct high-fidelity textures from images with state-of-the-art quality and are robust to the discretization of the underlying manifold. We demonstrate the versatility of intrinsic neural fields by tackling various applications: texture transfer between deformed shapes & different shapes, texture reconstruction from real-world images with view dependence, and discretization-agnostic learning on meshes and point clouds.",,,,Yes (almost done),,ARXIV 2022,,,,,,
3/21/2022 15:58,yiheng_xie@brown.edu,,RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs,RegNeRF,CVPR,12/01/2021,2022,"@article{niemeyer2022regnerf,
    AUTHOR = {Michael Niemeyer and Jonathan T. Barron and Ben Mildenhall and Mehdi S. M. Sajjadi and Andreas Geiger and Noha Radwan},
    TITLE = {RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs},
    YEAR = {2021},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2112.00724v1}
}",https://arxiv.org/pdf/2112.00724.pdf,https://m-niemeyer.github.io/regnerf/index.html,https://github.com/google-research/google-research/tree/master/regnerf,,https://www.youtube.com/watch?v=QyyyvA4-Kwc,,,"Sampling, Regularization",,,,,,,,,,"Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi, Andreas Geiger, Noha Radwan",niemeyer2022regnerf,00000289,,"Neural Radiance Fields (NeRF) have emerged as a powerful representation for the task of novel view synthesis due to their simplicity and state-of-the-art performance. Though NeRF can produce photorealistic renderings of unseen viewpoints when many input views are available, its performance drops significantly when this number is reduced. We observe that the majority of artifacts in sparse input scenarios are caused by errors in the estimated scene geometry, and by divergent behavior at the start of training. We address this by regularizing the geometry and appearance of patches rendered from unobserved viewpoints, and annealing the ray sampling space during training. We additionally use a normalizing flow model to regularize the color of unobserved viewpoints. Our model outperforms not only other methods that optimize over a single scene, but in many cases also conditional models that are extensively pre-trained on large multi-view datasets.",,,,Yes (almost done),,CVPR 2022,,,,,,
3/22/2022 8:40,yiheng_xie@brown.edu,,Neural Fields as Learnable Kernels for 3D Reconstruction,Neural Kernel Fields,ARXIV,11/26/2021,2022,"@article{williams2022neuralkernelfields,
    AUTHOR = {Francis Williams and Zan Gojcic and Sameh Khamis and Denis Zorin and Joan Bruna and Sanja Fidler and Or Litany},
    TITLE = {Neural Fields as Learnable Kernels for 3D Reconstruction},
    YEAR = {2021},
    MONTH = {Nov},
    URL = {http://arxiv.org/abs/2111.13674v1}
}",https://arxiv.org/pdf/2111.13674.pdf,https://nv-tlabs.github.io/nkf/,,,,,,"Sparse Reconstruction, Surface Reconstruction, Generalization, Data-Driven Method",,,,,,,,,,"Francis Williams, Zan Gojcic, Sameh Khamis, Denis Zorin, Joan Bruna, Sanja Fidler, Or Litany",williams2022neuralkernelfields,00000290,,"We present Neural Kernel Fields: a novel method for reconstructing implicit 3D shapes based on a learned kernel ridge regression. Our technique achieves state-of-the-art results when reconstructing 3D objects and large scenes from sparse oriented points, and can reconstruct shape categories outside the training set with almost no drop in accuracy. The core insight of our approach is that kernel methods are extremely effective for reconstructing shapes when the chosen kernel has an appropriate inductive bias. We thus factor the problem of shape reconstruction into two parts: (1) a backbone neural network which learns kernel parameters from data, and (2) a kernel ridge regression that fits the input points on-the-fly by solving a simple positive definite linear system using the learned kernel. As a result of this factorization, our reconstruction gains the benefits of data-driven methods under sparse point density while maintaining interpolatory behavior, which converges to the ground truth shape as input sampling density increases. Our experiments demonstrate a strong generalization capability to objects outside the train-set category and scanned scenes. Source code and pretrained models are available at https://nv-tlabs.github.io/nkf.",,,,Yes (almost done),,ARXIV 2022,,,,,,
3/22/2022 8:45,yiheng_xie@brown.edu,,Zero-Shot Text-Guided Object Generation with Dream Fields,Dream Field,ARXIV,12/02/2021,2021,"@article{jain2021dreamfield,
    AUTHOR = {Ajay Jain and Ben Mildenhall and Jonathan T. Barron and Pieter Abbeel and Ben Poole},
    TITLE = {Zero-Shot Text-Guided Object Generation with Dream Fields},
    YEAR = {2021},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2112.01455v1}
}",https://arxiv.org/pdf/2112.01455.pdf,https://ajayj.com/dreamfields,https://github.com/google-research/google-research/tree/master/dreamfields,,https://www.youtube.com/watch?v=1Fke6w46tv4&t=2s,,,"Generative Models, Multi-Modal Signals, Natural Languages",,,,,,,,,,"Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, Ben Poole",jain2021dreamfield,00000291,,"We combine neural rendering with multi-modal image and text representations to synthesize diverse 3D objects solely from natural language descriptions. Our method, Dream Fields, can generate the geometry and color of a wide range of objects without 3D supervision. Due to the scarcity of diverse, captioned 3D data, prior methods only generate objects from a handful of categories, such as ShapeNet. Instead, we guide generation with image-text models pre-trained on large datasets of captioned images from the web. Our method optimizes a Neural Radiance Field from many camera views so that rendered images score highly with a target caption according to a pre-trained CLIP model. To improve fidelity and visual quality, we introduce simple geometric priors, including sparsity-inducing transmittance regularization, scene bounds, and new MLP architectures. In experiments, Dream Fields produce realistic, multi-view consistent object geometry and color from a variety of natural language captions.",,,,Yes (almost done),,ARXIV 2021,,,,,,
3/22/2022 10:17,yiheng_xie@brown.edu,,CoNeRF: Controllable Neural Radiance Fields,CoNeRF,CVPR,12/03/2021,2022,"@article{kania2022conerf,
    AUTHOR = {Kacper Kania and Kwang Moo Yi and Marek Kowalski and Tomasz Trzcinski and Andrea Tagliasacchi},
    TITLE = {CoNeRF: Controllable Neural Radiance Fields},
    YEAR = {2021},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2112.01983v2}
}",https://arxiv.org/pdf/2112.01983.pdf,https://conerf.github.io/,,,https://conerf.github.io/showcase.mp4,,,"Dynamic/Temporal, Human (Head), Editable, Generalization",,,,,,,,,,"Kacper Kania, Kwang Moo Yi, Marek Kowalski, Tomasz Trzciński, Andrea Tagliasacchi",kania2022conerf,00000292,,"We extend neural 3D representations to allow for intuitive and interpretable user control beyond novel view rendering (i.e. camera control). We allow the user to annotate which part of the scene one wishes to control with just a small number of mask annotations in the training images. Our key idea is to treat the attributes as latent variables that are regressed by the neural network given the scene encoding. This leads to a few-shot learning framework, where attributes are discovered automatically by the framework, when annotations are not provided. We apply our method to various scenes with different types of controllable attributes (e.g. expression control on human faces, or state control in movement of inanimate objects). Overall, we demonstrate, to the best of our knowledge, for the first time novel view and novel attribute re-rendering of scenes from a single video.",,,,Yes (almost done),,CVPR 2022,,,,,,
3/2/2022 3:27,sitzikbs@gmail.com,Yes,DiGS : Divergence guided shape implicit neural representation for unoriented point clouds,DiGS,CVPR,6/21/2021,2022,"@article{DBLP:journals/corr/abs-2106-10811,
    BIBSOURCE = {dblp computer science bibliography, https://dblp.org},
    BIBURL = {https://dblp.org/rec/journals/corr/abs-2106-10811.bib},
    TIMESTAMP = {Tue, 29 Jun 2021 16:55:04 +0200},
    EPRINTTYPE = {arXiv},
    URL = {https://arxiv.org/abs/2106.10811},
    YEAR = {2021},
    VOLUME = {abs/2106.10811},
    JOURNAL = {CoRR},
    TITLE = {DiGS : Divergence guided shape implicit neural representation for unoriented point clouds},
    AUTHOR = {Yizhak Ben{-}Shabat and Chamin Hewa Koneputugodage and Stephen Gould}
}",https://arxiv.org/pdf/2106.10811.pdf,Coming soon,Coming soon,,,,,"Surface Reconstruction, Coarse-to-Fine",Sinusoidal Activation (SIREN),SDF,"Yes, geometry only",,,,,,,"Yizhak Ben-Shabat, Chamin Hewa Koneputugodage, Stephen Gould",dblp:journals/corr/abs-2106-10811,00000293,,"Neural shape representations have recently shown to be effective in shape analysis and reconstruction tasks. Existing neural network methods require point coordinates and corresponding normal vectors to learn the implicit level sets of the shape. Normal vectors are often not provided as raw data, therefore, approximation and reorientation are required as pre-processing stages, both of which can introduce noise. In this paper, we propose a divergence guided shape representation learning approach that does not require normal vectors as input. We show that incorporating a soft constraint on the divergence of the distance function favours smooth solutions that reliably orients gradients to match the unknown normal at each point, in some cases even better than approaches that use ground truth normal vectors directly. Additionally, we introduce a novel geometric initialization method for sinusoidal shape representation networks that further improves convergence to the desired solution. We evaluate the effectiveness of our approach on the task of surface reconstruction and show state-of-the-art performance compared to other unoriented methods and on-par performance compared to oriented methods.",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,,,,,,,
3/19/2022 14:19,yiheng_xie@brown.edu,,Learning Positional Embeddings for Coordinate-MLPs,,ARXIV,12/21/2021,2021,"@article{ramasinghe2021learning,
    AUTHOR = {Sameera Ramasinghe and Simon Lucey},
    TITLE = {Learning Positional Embeddings for Coordinate-MLPs},
    YEAR = {2021},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2112.11577v1}
}",https://arxiv.org/pdf/2112.11577.pdf,,,,,,,"2D Image Neural Fields, Fundamentals, Generalization, Global Conditioning, Positional Encoding",,,,,,,,,,"Sameera Ramasinghe, Simon Lucey",ramasinghe2021learning,279,,"We propose a novel method to enhance the performance of coordinate-MLPs by learning instance-specific positional embeddings. End-to-end optimization of positional embedding parameters along with network weights leads to poor generalization performance. Instead, we develop a generic framework to learn the positional embedding based on the classic graph-Laplacian regularization, which can implicitly balance the trade-off between memorization and generalization. This framework is then used to propose a novel positional embedding scheme, where the hyperparameters are learned per coordinate (i.e, instance) to deliver optimal performance. We show that the proposed embedding achieves better performance with higher stability compared to the well-established random Fourier features (RFF). Further, we demonstrate that the proposed embedding scheme yields stable gradients, enabling seamless integration into deep architectures as intermediate layers.",,,,Yes (almost done),,ARXIV 2021,,,,,,
3/19/2022 14:24,yiheng_xie@brown.edu,,"StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2",StyleGAN-V,CVPR,12/29/2021,2022,"@article{skorokhodov2022styleganv,
    url = {http://arxiv.org/abs/2112.14683v1},
    month = {Dec},
    year = {2021},
    title = {StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2},
    author = {Ivan Skorokhodov and Sergey Tulyakov and Mohamed Elhoseiny},
    entrytype = {article},
    id = {skorokhodov2022styleganv}
}",https://arxiv.org/pdf/2112.14683.pdf,https://universome.github.io/stylegan-v,Coming soon,,https://kaust-cair.s3.amazonaws.com/stylegan-v/stylegan-v.mp4,,,"Dynamic/Temporal, 2D Image Neural Fields, Generative Models, Generalization, Global Conditioning, Hypernetwork/Meta-learning, Positional Encoding",,,,,,,,,,"Ivan Skorokhodov, Sergey Tulyakov, Mohamed Elhoseiny",skorokhodov2022styleganv,280,,"Videos show continuous events, yet most - if not all - video synthesis frameworks treat them discretely in time. In this work, we think of videos of what they should be - time-continuous signals, and extend the paradigm of neural representations to build a continuous-time video generator. For this, we first design continuous motion representations through the lens of positional embeddings. Then, we explore the question of training on very sparse videos and demonstrate that a good generator can be learned by using as few as 2 frames per clip. After that, we rethink the traditional image and video discriminators pair and propose to use a single hypernetwork-based one. This decreases the training cost and provides richer learning signal to the generator, making it possible to train directly on 1024$^2$ videos for the first time. We build our model on top of StyleGAN2 and it is just 5% more expensive to train at the same resolution while achieving almost the same image quality. Moreover, our latent space features similar properties, enabling spatial manipulations that our method can propagate in time. We can generate arbitrarily long videos at arbitrary high frame rate, while prior work struggles to generate even 64 frames at a fixed rate. Our model achieves state-of-the-art results on four modern 256$^2$ video synthesis benchmarks and one 1024$^2$ resolution one. Videos and the source code are available at the project website: https://universome.github.io/stylegan-v.",,,,Yes (almost done),,CVPR 2022,,,,,,
3/19/2022 12:29,yiheng_xie@brown.edu,,Spelunking the Deep: Guaranteed Queries for General Neural Implicit Surfaces,Spelunking the Deep,ARXIV,2/5/2022,2022,"@article{sharp2022spelunkingthedeep,
    url = {http://arxiv.org/abs/2202.02444v1},
    month = {Feb},
    year = {2022},
    title = {Spelunking the Deep: Guaranteed Queries for General Neural Implicit Surfaces},
    author = {Nicholas Sharp and Alec Jacobson},
    entrytype = {article},
    id = {sharp2022spelunkingthedeep}
}",https://arxiv.org/pdf/2202.02444.pdf,,,,,,,"Speed & Computational Efficiency, Fundamentals, Sampling, Graphics",,,,,,,,,,"Nicholas Sharp, Alec Jacobson",sharp2022spelunkingthedeep,278,,"Neural implicit representations, which encode a surface as the level set of a neural network applied to spatial coordinates, have proven to be remarkably effective for optimizing, compressing, and generating 3D geometry. Although these representations are easy to fit, it is not clear how to best evaluate geometric queries on the shape, such as intersecting against a ray or finding a closest point. The predominant approach is to encourage the network to have a signed distance property. However, this property typically holds only approximately, leading to robustness issues, and holds only at the conclusion of training, inhibiting the use of queries in loss functions. Instead, this work presents a new approach to perform queries directly on general neural implicit functions for a wide range of existing architectures. Our key tool is the application of range analysis to neural networks, using automatic arithmetic rules to bound the output of a network over a region; we conduct a study of range analysis on neural networks, and identify variants of affine arithmetic which are highly effective. We use the resulting bounds to develop geometric queries including ray casting, intersection testing, constructing spatial hierarchies, fast mesh extraction, closest-point evaluation, evaluating bulk properties, and more. Our queries can be efficiently evaluated on GPUs, and offer concrete accuracy guarantees even on randomly-initialized networks, enabling their use in training objectives and beyond. We also show a preliminary application to inverse rendering.",,,,Yes (almost done),,ARXIV 2022,,,,,,
3/1/2022 16:59,j.m.wolterink@utwente.nl,Yes,Implicit Neural Representations for Deformable Image Registration,IDIR,MIDL,12/17/2021,2022,"@inproceedings{wolterink2021implicit,
    title = {Implicit Neural Representations for Deformable Image Registration},
    author = {Jelmer M Wolterink and Jesse C Zwienenberg and Christoph Brune},
    year = {2022},
    booktitle = {Medical Imaging with Deep Learning},
    entrytype = {inproceedings},
    id = {wolterink2021implicit}
}",https://openreview.net/pdf?id=BP29eKzQBu3,,,,,,,"2D Image Neural Fields, Data-Driven Method",Sinusoidal Activation (SIREN),Deformation vector field,N/A,,,,,,,"Jelmer M. Wolterink, Jesse Zwienenberg, Christoph Brune",wolterink2021implicit,276,,"Deformable medical image registration has in past years been revolutionized by deep learning with convolutional neural networks. These methods surpass conventional image registration techniques in speed but not in accuracy. Here, we present an alternative approach to leveraging neural networks for image registration. Instead of using a neural network to predict the transformation between images, we optimize a neural network to represent this continuous transformation. Using recent insights from differentiable rendering, we show how such an implicit deformable image registration (IDIR) model can be naturally combined with regularization terms based on standard automatic differentiation techniques. We demonstrate the effectiveness of this model on 4D chest CT registration in the DIR-LAB data set and find that a single three-layer multi-layer perceptron with periodic activation functions outperforms all published deep learning-based methods, without any folding and without the need for training data. The model is flexible enough to be extended to include different losses, regularizers, and optimization schemes and is implemented using standard deep learning libraries.",,,,"No (Please fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,MIDL 2022,,,,,,
3/1/2022 19:09,royorel@cs.washington.edu,Yes,StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation,StyleSDF,CVPR,12/21/2021,2022,"@article{or-el2021stylesdf,
    url = {http://arxiv.org/abs/2112.11427v1},
    month = {Dec},
    year = {2021},
    title = {StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation},
    author = {Roy Or-El and Xuan Luo and Mengyi Shan and Eli Shechtman and Jeong Joon Park and Ira Kemelmacher-Shlizerman},
    entrytype = {article},
    id = {or-el2021stylesdf}
}",https://arxiv.org/pdf/2112.11427.pdf,https://stylesdf.github.io/,https://github.com/royorel/StyleSDF,,,,,Generative Models,Sinusoidal Activation (SIREN),SDF,No,,,,,,,"Roy Or-El, Xuan Luo, Mengyi Shan, Eli Shechtman, Jeong Joon Park, Ira Kemelmacher-Shlizerman",or-el2021stylesdf,277,,"We introduce a high resolution, 3D-consistent image and shape generation technique which we call StyleSDF. Our method is trained on single-view RGB data only, and stands on the shoulders of StyleGAN2 for image generation, while solving two main challenges in 3D-aware GANs: 1) high-resolution, view-consistent generation of the RGB images, and 2) detailed 3D shape. We achieve this by merging a SDF-based 3D representation with a style-based 2D generator. Our 3D implicit network renders low-resolution feature maps, from which the style-based network generates view-consistent, 1024x1024 images. Notably, our SDF-based 3D modeling defines detailed 3D surfaces, leading to consistent volume rendering. Our method shows higher quality results compared to state of the art in terms of visual and geometric quality.",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,CVPR 2022,,,,,,
2/8/2022 6:04,qheldiv@gmail.com,Yes,ShapeFormer: Transformer-based Shape Completion via Sparse Representation,ShapeFormer,ARXIV,1/25/2022,2022,"@misc{yan2022shapeformer,
    year = {2022},
    author = {Xingguang Yan and Liqiang Lin and Niloy J. Mitra and Dani Lischinski and Danny Cohen-Or and Hui Huang},
    title = {ShapeFormer: Transformer-based Shape Completion via Sparse Representation},
    entrytype = {misc},
    id = {yan2022shapeformer}
}",https://arxiv.org/pdf/2201.10326.pdf,https://shapeformer.github.io/,https://github.com/QhelDIV/ShapeFormer,,,,,"Surface Reconstruction, Compression, Generative Models, Generalization, Local Conditioning, Data-Driven Method, Voxel Grid",None,Occupancy,"Yes, geometry only",,,,,,,"Xingguang Yan, Liqiang Lin, Niloy J. Mitra, Dani Lischinski, Danny Cohen-Or, Hui Huang",yan2022shapeformer,275,,"We present ShapeFormer, a transformer-based network that produces a distribution of object completions, conditioned on incomplete, and possibly noisy, point clouds. The resultant distribution can then be sampled to generate likely completions, each exhibiting plausible shape details while being faithful to the input. To facilitate the use of transformers for 3D, we introduce a compact 3D representation, vector quantized deep implicit function, that utilizes spatial sparsity to represent a close approximation of a 3D shape by a short sequence of discrete variables. Experiments demonstrate that ShapeFormer outperforms prior art for shape completion from ambiguous partial inputs in terms of both completion quality and diversity. We also show that our approach effectively handles a variety of shape types, incomplete patterns, and real-world scans.",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,ARXIV 2022,,,,,,
1/14/2022 2:35,jaeho-lee@kaist.ac.kr,Yes,Meta-leaning Sparse Implicit Neural Representations,Meta-SparseINR,NeurIPS,10/27/2021,2021,"@article{lee2021metasparseinr,
    author = {Jaeho Lee and Jihoon Tack and Namhoon Lee and Jinwoo Shin},
    title = {Meta-Learning Sparse Implicit Neural Representations},
    year = {2021},
    month = {Oct},
    url = {http://arxiv.org/abs/2110.14678v2},
    entrytype = {article},
    id = {lee2021metasparseinr}
}",https://arxiv.org/pdf/2110.14678.pdf,https://github.com/jaeho-lee/MetaSparseINR,https://github.com/jaeho-lee/MetaSparseINR,,,,,"Speed & Computational Efficiency, 2D Image Neural Fields, Compression, Hypernetwork/Meta-learning",Sinusoidal Activation (SIREN),"None, N/A",N/A,,,,,,,"Jaeho Lee, Jihoon Tack, Namhoon Lee, Jinwoo Shin",lee2021metasparseinr,274,,"Implicit neural representations are a promising new avenue of representing general signals by learning a continuous function that, parameterized as a neural network, maps the domain of a signal to its codomain; the mapping from spatial coordinates of an image to its pixel values, for example. Being capable of conveying fine details in a high dimensional signal, unboundedly of its domain, implicit neural representations ensure many advantages over conventional discrete representations. However, the current approach is difficult to scale for a large number of signals or a data set, since learning a neural representation -- which is parameter heavy by itself -- for each signal individually requires a lot of memory and computations. To address this issue, we propose to leverage a meta-learning approach in combination with network compression under a sparsity constraint, such that it renders a well-initialized sparse parameterization that evolves quickly to represent a set of unseen signals in the subsequent training. We empirically demonstrate that meta-learned sparse neural representations achieve a much smaller loss than dense meta-learned models with the same number of parameters, when trained to fit each signal using the same number of optimization steps.",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,NeurIPS 2021,,,,,,
1/7/2022 6:22,ranahanocka@uchicago.edu,Yes,Text2Mesh: Text-Driven Neural Stylization for Meshes,Text2Mesh,ARXIV,12/6/2021,2022,"@article{michel2022text2mesh,
    author = {Oscar Michel and Roi Bar-On and Richard Liu and Sagie Benaim and Rana Hanocka},
    title = {Text2Mesh: Text-Driven Neural Stylization for Meshes},
    year = {2021},
    month = {Dec},
    url = {http://arxiv.org/abs/2112.03221v1},
    entrytype = {article},
    id = {michel2022text2mesh}
}",https://arxiv.org/pdf/2112.03221.pdf,https://threedle.github.io/text2mesh/,https://github.com/threedle/text2mesh,,,https://arxiv.org/pdf/2112.03221.pdf,,"Editable, Generative Models, Neural Style Field",Fourier Feature (NeRF),Mesh,No,,,,,,,"Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, Rana Hanocka",michel2022text2mesh,273,,"In this work, we develop intuitive controls for editing the style of 3D objects. Our framework, Text2Mesh, stylizes a 3D mesh by predicting color and local geometric details which conform to a target text prompt. We consider a disentangled representation of a 3D object using a fixed mesh input (content) coupled with a learned neural network, which we term neural style field network. In order to modify style, we obtain a similarity score between a text prompt (describing style) and a stylized mesh by harnessing the representational power of CLIP. Text2Mesh requires neither a pre-trained generative model nor a specialized 3D mesh dataset. It can handle low-quality meshes (non-manifold, boundaries, etc.) with arbitrary genus, and does not require UV parameterization. We demonstrate the ability of our technique to synthesize a myriad of styles over a wide variety of 3D meshes.",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,ARXIV 2022,,,,,,
12/21/2021 15:47,srinath@brown.edu,Yes,Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects,Dex-NeRF,CoRL,10/27/2021,2021,"@article{ichnowski2021dexnerf,
    author = {Jeffrey Ichnowski and Yahav Avigal and Justin Kerr and Ken Goldberg},
    title = {Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects},
    year = {2021},
    month = {Oct},
    url = {http://arxiv.org/abs/2110.14217v1},
    entrytype = {article},
    id = {ichnowski2021dexnerf}
}",https://arxiv.org/pdf/2110.14217.pdf,,,,https://www.youtube.com/watch?v=F9R6Nf1d7P4,,,Robotics,Fourier Feature (NeRF),Density,No,,,,,,,"Jeffrey Ichnowski, Yahav Avigal, Justin Kerr, Ken Goldberg",ichnowski2021dexnerf,272,,"The ability to grasp and manipulate transparent objects is a major challenge for robots. Existing depth cameras have difficulty detecting, localizing, and inferring the geometry of such objects. We propose using neural radiance fields (NeRF) to detect, localize, and infer the geometry of transparent objects with sufficient accuracy to find and grasp them securely. We leverage NeRF's view-independent learned density, place lights to increase specular reflections, and perform a transparency-aware depth-rendering that we feed into the Dex-Net grasp planner. We show how additional lights create specular reflections that improve the quality of the depth map, and test a setup for a robot workcell equipped with an array of cameras to perform transparent object manipulation. We also create synthetic and real datasets of transparent objects in real-world settings, including singulated objects, cluttered tables, and the top rack of a dishwasher. In each setting we show that NeRF and Dex-Net are able to reliably compute robust grasps on transparent objects, achieving 90% and 100% grasp success rates in physical experiments on an ABB YuMi, on objects where baseline methods fail.",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,CoRL 2021,,,,,,
12/17/2021 13:16,n-kondo@digitalnature.slis.tsukuba.ac.jp,Yes,VaxNeRF: Revisiting the Classic for Voxel-Accelerated Neural Radiance Field,VaxNeRF,ARXIV,11/25/2021,2021,"@article{kondo2021vaxnerf,
    author = {Naruya Kondo and Yuya Ikeda and Andrea Tagliasacchi and Yutaka Matsuo and Yoichi Ochiai and Shixiang Shane Gu},
    title = {VaxNeRF: Revisiting the Classic for Voxel-Accelerated Neural Radiance Field},
    year = {2021},
    month = {Nov},
    url = {http://arxiv.org/abs/2111.13112v1},
    entrytype = {article},
    id = {kondo2021vaxnerf}
}",https://arxiv.org/pdf/2111.13112.pdf,,https://github.com/naruya/VaxNeRF,,,,,"Speed & Computational Efficiency, Generative Models, Voxel Grid, Object-Centric, Coarse-to-Fine",Fourier Feature (NeRF),Density,No,,,,,,,"Naruya Kondo, Yuya Ikeda, Andrea Tagliasacchi, Yutaka Matsuo, Yoichi Ochiai, Shixiang Shane Gu",kondo2021vaxnerf,271,,"Neural Radiance Field (NeRF) is a popular method in data-driven 3D reconstruction. Given its simplicity and high quality rendering, many NeRF applications are being developed. However, NeRF's big limitation is its slow speed. Many attempts are made to speeding up NeRF training and inference, including intricate code-level optimization and caching, use of sophisticated data structures, and amortization through multi-task and meta learning. In this work, we revisit the basic building blocks of NeRF through the lens of classic techniques before NeRF. We propose Voxel-Accelearated NeRF (VaxNeRF), integrating NeRF with visual hull, a classic 3D reconstruction technique only requiring binary foreground-background pixel labels per image. Visual hull, which can be optimized in about 10 seconds, can provide coarse in-out field separation to omit substantial amounts of network evaluations in NeRF. We provide a clean fully-pythonic, JAX-based implementation on the popular JaxNeRF codebase, consisting of only about 30 lines of code changes and a modular visual hull subroutine, and achieve about 2-8x faster learning on top of the highly-performative JaxNeRF baseline with zero degradation in rendering quality. With sufficient compute, this effectively brings down full NeRF training from hours to 30 minutes. We hope VaxNeRF -- a careful combination of a classic technique with a deep method (that arguably replaced it) -- can empower and accelerate new NeRF extensions and applications, with its simplicity, portability, and reliable performance gains. Codes are available at https://github.com/naruya/VaxNeRF .",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,ARXIV 2021,,,,,,
12/7/2021 16:59,pculbertson@stanford.edu,Yes,Vision-Only Robot Navigation in a Neural Radiance World,,ArXiV (RA-L submission),10/1/2021,2022,"@article{adamkiewicz2022visiononly,
    url = {http://arxiv.org/abs/2110.00168v1},
    month = {Oct},
    year = {2021},
    title = {Vision-Only Robot Navigation in a Neural Radiance World},
    author = {Michal Adamkiewicz and Timothy Chen and Adam Caccavale and Rachel Gardner and Preston Culbertson and Jeannette Bohg and Mac Schwager},
    entrytype = {article},
    id = {adamkiewicz2022visiononly}
}",https://arxiv.org/pdf/2110.00168.pdf,https://mikh3x4.github.io/nerf-navigation,https://github.com/mikh3x4/nerf-navigation,,https://youtu.be/5JjWpv9BaaE,,,"Robotics, Camera Parameter Estimation, Data-Driven Method",Fourier Feature (NeRF),"None, N/A",N/A,,,,,,,"Michal Adamkiewicz, Timothy Chen, Adam Caccavale, Rachel Gardner, Preston Culbertson, Jeannette Bohg, Mac Schwager",adamkiewicz2022visiononly,270,,"Neural Radiance Fields (NeRFs) have recently emerged as a powerful paradigm for the representation of natural, complex 3D scenes. NeRFs represent continuous volumetric density and RGB values in a neural network, and generate photo-realistic images from unseen camera viewpoints through ray tracing. We propose an algorithm for navigating a robot through a 3D environment represented as a NeRF using only an on-board RGB camera for localization. We assume the NeRF for the scene has been pre-trained offline, and the robot's objective is to navigate through unoccupied space in the NeRF to reach a goal pose. We introduce a trajectory optimization algorithm that avoids collisions with high-density regions in the NeRF based on a discrete time version of differential flatness that is amenable to constraining the robot's full pose and control inputs. We also introduce an optimization based filtering method to estimate 6DoF pose and velocities for the robot in the NeRF given only an onboard RGB camera. We combine the trajectory planner with the pose filter in an online replanning loop to give a vision-based robot navigation pipeline. We present simulation results with a quadrotor robot navigating through a jungle gym environment, the inside of a church, and Stonehenge using only an RGB camera. We also demonstrate an omnidirectional ground robot navigating through the church, requiring it to reorient to fit through the narrow gap. Videos of this work can be found at https://mikh3x4.github.io/nerf-navigation/ .",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,ArXiV (RA-L submission) 2022,,,,,,
12/2/2021 16:41,cyc@cs.columbia.edu,Yes,Model Reduction for the Material Point Method via Learning the Deformation map and its Spatial-temporal Gradients,NeuralDefMap,ARXIV,9/25/2021,2021,"@article{chen2021neuraldefmap,
    author = {Peter Yichen Chen and Maurizio Chiaramonte and Eitan Grinspun and Kevin Carlberg},
    title = {Model reduction for the material point method via learning the deformation map and its spatial-temporal gradients},
    year = {2021},
    month = {Sep},
    url = {http://arxiv.org/abs/2109.12390v1},
    entrytype = {article},
    id = {chen2021neuraldefmap}
}",https://arxiv.org/pdf/2109.12390.pdf,https://peterchencyc.com/projects/rom4mpm/,https://github.com/peterchencyc/libNeuralDefMap,,https://peterchencyc.com/files/rom4mpm/supplemental_video.mp4,,,"Dynamic/Temporal, Science & Engineering, Data-Driven Method, Supervision by Gradient (PDE)",,deformation map,"Yes, geometry only",,,,,,,"Peter Yichen Chen, Maurizio Chiaramonte, Eitan Grinspun, Kevin Carlberg",chen2021neuraldefmap,269,,"This work proposes a model-reduction approach for the material point method on nonlinear manifolds. The technique approximates the $\textit{kinematics}$ by approximating the deformation map in a manner that restricts deformation trajectories to reside on a low-dimensional manifold expressed from the extrinsic view via a parameterization function. By explicitly approximating the deformation map and its spatial-temporal gradients, the deformation gradient and the velocity can be computed simply by differentiating the associated parameterization function. Unlike classical model reduction techniques that build a subspace for a finite number of degrees of freedom, the proposed method approximates the entire deformation map with infinite degrees of freedom. Therefore, the technique supports resolution changes in the reduced simulation, attaining the challenging task of zero-shot super-resolution by generating material points unseen in the training data. The ability to generate material points also allows for adaptive quadrature rules for stress update. A family of projection methods is devised to generate $\textit{dynamics}$, i.e., at every time step, the methods perform three steps: (1) generate quadratures in the full space from the reduced space, (2) compute position and velocity updates in the full space, and (3) perform a least-squares projection of the updated position and velocity onto the low-dimensional manifold and its tangent space. Computational speedup is achieved via hyper-reduction, i.e., only a subset of the original material points are needed for dynamics update. Large-scale numerical examples with millions of material points illustrate the method's ability to gain an order-of-magnitude computational-cost saving -- indeed $\textit{real-time simulations}$ in some cases -- with negligible errors.",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,ARXIV 2021,,,,,,
11/29/2021 22:57,mbr18@mails.tsinghua.edu.cn,Yes,Neural-Pull: Learning Signed Distance Functions from Point Clouds by Learning to Pull Space onto Surfaces,Neural-Pull,ICML,11/20/2020,2021,"@inproceedings{NeuralPull,
    year = {2021},
    booktitle = {International Conference on Machine Learning (ICML)},
    author = {Baorui, Ma and Zhizhong, Han and Yu-shen, Liu and Matthias, Zwicker},
    title = {Neural-Pull: Learning Signed Distance Functions from Point Clouds by Learning to Pull Space onto Surfaces},
    entrytype = {inproceedings},
    id = {NeuralPull}
}",https://arxiv.org/pdf/2011.13495.pdf,https://github.com/mabaorui/NeuralPull,https://github.com/mabaorui/NeuralPull,https://drive.google.com/drive/folders/1qre9mgJNCKiX11HnZO10qMZMmPv_gnh3,,http://cgcad.thss.tsinghua.edu.cn/liuyushen/main/pdf/LiuYS_ICML21_NeuralPull_Supp.pdf,,"2D Image Neural Fields, Global Conditioning",None,SDF,"Yes, geometry only",,,,,,,"Baorui, Ma and Zhizhong, Han and Yu-shen, Liu and Matthias, Zwicker",NeuralPull,267,,"Reconstructing continuous surfaces from 3D point clouds is a fundamental operation in 3D
geometry processing. Several recent state-of-theart methods address this problem using neural networks to learn signed distance functions (SDFs). In this paper, we introduce Neural-Pull, a new approach that is simple and leads to high quality SDFs. Specifically, we train a neural network to pull query 3D locations to their closest neighbors on the surface using the predicted signed distance values and the gradient at the query locations, both
of which are computed by the network itself. The pulling operation moves each query location with a stride given by the distance predicted by the network. Based on the sign of the distance, this may move the query location along or against the direction of the gradient of the SDF. This is a differentiable operation that allows us to update the signed distance value and the gradient simultaneously during training. Our outperforming results under widely used benchmarks demonstrate that we can learn SDFs more accurately and flexibly for surface reconstruction and single image reconstruction than the state-of-the-art methods.",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",Update existing entry,ICML 2021,,,,,,
12/2/2021 3:31,xingyu@stu.xjtu.edu.cn,Yes,Hallucinated Neural Radiance Fields in the Wild,Ha-NeRF,ARXIV,11/30/2021,2021,"@article{chen2021hanerf,
    url = {http://arxiv.org/abs/2111.15246v2},
    month = {Nov},
    year = {2021},
    title = {Hallucinated Neural Radiance Fields in the Wild},
    author = {Xingyu Chen and Qi Zhang and Xiaoyu Li and Yue Chen and Ying Feng and Xuan Wang and Jue Wang},
    entrytype = {article},
    id = {chen2021hanerf}
}",https://arxiv.org/pdf/2111.15246.pdf,https://rover-xingyu.github.io/Ha-NeRF/,https://github.com/rover-xingyu/Ha-NeRF,,,,,"Dynamic/Temporal, Editable, Global Conditioning",Fourier Feature (NeRF),Density,No,,,,,,,"Xingyu Chen, Qi Zhang, Xiaoyu Li, Yue Chen, Ying Feng, Xuan Wang, Jue Wang",chen2021hanerf,268,,"Neural Radiance Fields (NeRF) has recently gained popularity for its impressive novel view synthesis ability. This paper studies the problem of hallucinated NeRF: i.e. recovering a realistic NeRF at a different time of day from a group of tourism images. Existing solutions adopt NeRF with a controllable appearance embedding to render novel views under various conditions, but cannot render view-consistent images with an unseen appearance. To solve this problem, we present an end-to-end framework for constructing a hallucinated NeRF, dubbed as Ha-NeRF. Specifically, we propose an appearance hallucination module to handle time-varying appearances and transfer them to novel views. Considering the complex occlusions of tourism images, an anti-occlusion module is introduced to decompose the static subjects for visibility accurately. Experimental results on synthetic data and real tourism photo collections demonstrate that our method can not only hallucinate the desired appearances, but also render occlusion-free images from different views. The project and supplementary materials are available at https://rover-xingyu.github.io/Ha-NeRF/.",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,ARXIV 2021,,,,,,
11/26/2021 16:38,simon.giebenhain@uni-konstanz.de,Yes,AIR-Nets: An Attention-Based Framework for Locally Conditioned Implicit Representations,AIR-Nets,3DV,10/22/2021,2021,"@inproceedings{giebenhain2021airnets,
    organization = {IEEE},
    year = {2021},
    booktitle = {2021 International Conference on 3D Vision (3DV)},
    author = {Simon Giebenhain and Bastian Goldluecke},
    title = {AIR-Nets: An Attention-Based Framework for Locally Conditioned Implicit Representations},
    entrytype = {inproceedings},
    id = {giebenhain2021airnets}
}",https://arxiv.org/pdf/2110.11860.pdf,,https://github.com/SimonGiebenhain/AIR-Nets,,,https://github.com/SimonGiebenhain/AIR-Nets/blob/main/AIR-Nets_supp.pdf,,"Surface Reconstruction, Local Conditioning, Hybrid Geometry Parameterization, Purely Point-Based, Positional Encoding",MLP on relative coordinates,Occupancy,"Yes, geometry only",,,,,,,"Simon Giebenhain, Bastian Goldlücke",giebenhain2021airnets,265,,"This paper introduces Attentive Implicit Representation Networks (AIR-Nets), a simple, but highly effective architecture for 3D reconstruction from point clouds. Since representing 3D shapes in a local and modular fashion increases generalization and reconstruction quality, AIR-Nets encode an input point cloud into a set of local latent vectors anchored in 3D space, which locally describe the object's geometry, as well as a global latent description, enforcing global consistency. Our model is the first grid-free, encoder-based approach that locally describes an implicit function. The vector attention mechanism from [Zhao et al. 2020] serves as main point cloud processing module, and allows for permutation invariance and translation equivariance. When queried with a 3D coordinate, our decoder gathers information from the global and nearby local latent vectors in order to predict an occupancy value. Experiments on the ShapeNet dataset show that AIR-Nets significantly outperform previous state-of-the-art encoder-based, implicit shape learning methods and especially dominate in the sparse setting. Furthermore, our model generalizes well to the FAUST dataset in a zero-shot setting. Finally, since AIR-Nets use a sparse latent representation and follow a simple operating scheme, the model offers several exiting avenues for future work. Our code is available at https://github.com/SimonGiebenhain/AIR-Nets.",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,3DV 2021,,,,,,
11/29/2021 11:40,or.litany@gmail.com,Yes,Neural Fields as Learnable Kernels for 3D Reconstruction,Neural Kernel Fields (NKF),ARXIV,11/26/2021,2021,"@article{williams2021neuralkernelfields(nkf),
    url = {http://arxiv.org/abs/2111.13674v1},
    month = {Nov},
    year = {2021},
    title = {Neural Fields as Learnable Kernels for 3D Reconstruction},
    author = {Francis Williams and Zan Gojcic and Sameh Khamis and Denis Zorin and Joan Bruna and Sanja Fidler and Or Litany},
    entrytype = {article},
    id = {williams2021neuralkernelfields(nkf)}
}",https://arxiv.org/pdf/2111.13674.pdf,https://nv-tlabs.github.io/nkf/,Coming soon,,,,,"Sparse Reconstruction, Surface Reconstruction, Generalization, Global Conditioning, Local Conditioning, Data-Driven Method, Hypernetwork/Meta-learning, Hybrid Geometry Parameterization",Fourier Feature (NeRF),SDF,"Yes, geometry only",,,,,,,"Francis Williams, Zan Gojcic, Sameh Khamis, Denis Zorin, Joan Bruna, Sanja Fidler, Or Litany",williams2021neuralkernelfields(nkf),266,,"We present Neural Kernel Fields: a novel method for reconstructing implicit 3D shapes based on a learned kernel ridge regression. Our technique achieves state-of-the-art results when reconstructing 3D objects and large scenes from sparse oriented points, and can reconstruct shape categories outside the training set with almost no drop in accuracy. The core insight of our approach is that kernel methods are extremely effective for reconstructing shapes when the chosen kernel has an appropriate inductive bias. We thus factor the problem of shape reconstruction into two parts: (1) a backbone neural network which learns kernel parameters from data, and (2) a kernel ridge regression that fits the input points on-the-fly by solving a simple positive definite linear system using the learned kernel. As a result of this factorization, our reconstruction gains the benefits of data-driven methods under sparse point density while maintaining interpolatory behavior, which converges to the ground truth shape as input sampling density increases. Our experiments demonstrate a strong generalization capability to objects outside the train-set category and scanned scenes. Source code and pretrained models are available at https://nv-tlabs.github.io/nkf.",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,ARXIV 2021,,,,,,
11/25/2021 3:55,alldieck@google.com,Yes,H-NeRF: Neural Radiance Fields for Rendering and Temporal Reconstruction of Humans in Motion,H-NeRF,NeurIPS,10/26/2021,2021,"@article{xu2021hnerf,
    url = {http://arxiv.org/abs/2110.13746v2},
    month = {Oct},
    year = {2021},
    title = {H-NeRF: Neural Radiance Fields for Rendering and Temporal Reconstruction of Humans in Motion},
    author = {Hongyi Xu and Thiemo Alldieck and Cristian Sminchisescu},
    entrytype = {article},
    id = {xu2021hnerf}
}",https://arxiv.org/pdf/2110.13746.pdf,,,,,,,"Dynamic/Temporal, Human (Body), Editable",Fourier Feature (NeRF),"Density, SDF",No,,,,,,,"Hongyi Xu, Thiemo Alldieck, Cristian Sminchisescu",xu2021hnerf,264,,"We present neural radiance fields for rendering and temporal (4D) reconstruction of humans in motion (H-NeRF), as captured by a sparse set of cameras or even from a monocular video. Our approach combines ideas from neural scene representation, novel-view synthesis, and implicit statistical geometric human representations, coupled using novel loss functions. Instead of learning a radiance field with a uniform occupancy prior, we constrain it by a structured implicit human body model, represented using signed distance functions. This allows us to robustly fuse information from sparse views and generalize well beyond the poses or views observed in training. Moreover, we apply geometric constraints to co-learn the structure of the observed subject -- including both body and clothing -- and to regularize the radiance field to geometrically plausible solutions. Extensive experiments on multiple datasets demonstrate the robustness and the accuracy of our approach, its generalization capabilities significantly outside a small training set of poses and views, and statistical extrapolation beyond the observed shape.",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,NeurIPS 2021,,,,,,
11/23/2021 10:32,james_tompkin@brown.edu,Yes,Advances in Neural Rendering,,ARXIV,11/10/2021,2021,"@article{tewari2021advances,
    author = {Ayush Tewari and Justus Thies and Ben Mildenhall and Pratul Srinivasan and Edgar Tretschk and Yifan Wang and Christoph Lassner and Vincent Sitzmann and Ricardo Martin-Brualla and Stephen Lombardi and Tomas Simon and Christian Theobalt and Matthias Niessner and Jonathan T. Barron and Gordon Wetzstein and Michael Zollhoefer and Vladislav Golyanik},
    title = {Advances in Neural Rendering},
    year = {2021},
    month = {Nov},
    url = {http://arxiv.org/abs/2111.05849v1},
    entrytype = {article},
    id = {tewari2021advances}
}",https://arxiv.org/pdf/2111.05849.pdf,,,,,,,"2D Image Neural Fields, Image-Based Rendering",,,,,,,,,,"Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, Yifan Wang, Christoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, Tomas Simon, Christian Theobalt, Matthias Niessner, Jonathan T. Barron, Gordon Wetzstein, Michael Zollhoefer, Vladislav Golyanik",tewari2021advances,257,,"Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline. This state-of-the-art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non-rigidly deforming objects...",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,ARXIV 2021,,,,,,
11/23/2021 13:28,tejankarmali@iisc.ac.in,Yes,Deep Implicit Surface Point Prediction Networks,CSPNet,ICCV,6/10/2021,2021,"@inproceedings{Venkatesh_2021_ICCV,
    author = {Rahul Venkatesh and Tejan Karmali and Sarthak Sharma and Aurobrata Ghosh and R. Venkatesh Babu and L\'aszl\'o A. Jeni and Maneesh Singh},
    title = {Deep Implicit Surface Point Prediction Networks},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month = {October},
    year = {2021},
    pages = {12653-12662},
    entrytype = {inproceedings},
    id = {Venkatesh_2021_ICCV}
}",https://arxiv.org/pdf/2106.05779.pdf,https://sites.google.com/view/cspnet ,https://github.com/rahulvenkk/csp-net,,,,,Surface Reconstruction,None,Closest-surface point prediction,"Yes, geometry only",,,,,,,"Rahul Venkatesh, Tejan Karmali, Sarthak Sharma, Aurobrata Ghosh, R. Venkatesh Babu, László A. Jeni, Maneesh Singh",venkatesh_2021_iccv,258,,"
Deep neural representations of 3D shapes as implicit functions have been shown to produce high fidelity models surpassing the resolution-memory trade-off faced by the explicit representations using meshes and point clouds. However, most such approaches focus on representing closed shapes. Unsigned distance function (UDF) based approaches have been proposed recently as a promising alternative to represent both open and closed shapes. However, since the gradients of UDFs vanish on the surface, it is challenging to estimate local (differential) geometric properties like the normals and tangent planes which are needed for many downstream applications in vision and graphics. There are additional challenges in computing these properties efficiently with a low-memory footprint. This paper presents a novel approach that models such surfaces using a new class of implicit representations called the closest surface-point CSP representation. We show that CSP allows us to represent complex surfaces of any topology (open or closed) with high fidelity. It also allows for accurate and efficient computation of local geometric properties. We further demonstrate that it leads to efficient implementation of downstream algorithms like sphere-tracing for rendering the 3D surface as well as to create explicit mesh-based representations. Extensive experimental evaluation on the ShapeNet dataset validate the above contributions with results surpassing the state-of-the-art. Code and data are available at https://sites.google.com/view/cspnet",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,ICCV 2021,,,,,,
11/23/2021 19:00,bchen@cs.columbia.edu,Yes,Full-Body Visual Self-Modeling of Robot Morphologies,Visual Self-Modeling Robot,ARXIV,11/11/2021,2021,"@article{chen2021visualselfmodelingrobot,
    url = {http://arxiv.org/abs/2111.06389v2},
    month = {Nov},
    year = {2021},
    title = {Full-Body Visual Self-Modeling of Robot Morphologies},
    author = {Boyuan Chen and Robert Kwiatkowski and Carl Vondrick and Hod Lipson},
    entrytype = {article},
    id = {chen2021visualselfmodelingrobot}
}",https://arxiv.org/pdf/2111.06389.pdf,https://robot-morphology.cs.columbia.edu/,https://github.com/BoyuanChen/visual-selfmodeling,https://github.com/BoyuanChen/visual-selfmodeling,https://youtu.be/aoCAplokoWE,,https://youtu.be/aoCAplokoWE,"Robotics, Science & Engineering, Data-Driven Method",Sinusoidal Activation (SIREN),SDF,"Yes, geometry only",,,,,,,"Boyuan Chen, Robert Kwiatkowski, Carl Vondrick, Hod Lipson",chen2021visualselfmodelingrobot,259,,"Internal computational models of physical bodies are fundamental to the ability of robots and animals alike to plan and control their actions. These ""self-models"" allow robots to consider outcomes of multiple possible future actions, without trying them out in physical reality. Recent progress in fully data-driven self-modeling has enabled machines to learn their own forward kinematics directly from task-agnostic interaction data. However, forward-kinema\-tics models can only predict limited aspects of the morphology, such as the position of end effectors or velocity of joints and masses. A key challenge is to model the entire morphology and kinematics, without prior knowledge of what aspects of the morphology will be relevant to future tasks. Here, we propose that instead of directly modeling forward-kinematics, a more useful form of self-modeling is one that could answer space occupancy queries, conditioned on the robot's state. Such query-driven self models are continuous in the spatial domain, memory efficient, fully differentiable and kinematic aware. In physical experiments, we demonstrate how a visual self-model is accurate to about one percent of the workspace, enabling the robot to perform various motion planning and control tasks. Visual self-modeling can also allow the robot to detect, localize and recover from real-world damage, leading to improved machine resiliency. Our project website is at: https://robot-morphology.cs.columbia.edu/",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,ARXIV 2021,,,,,,
11/23/2021 22:10,tesfaldet@hotmail.com,Yes,Fourier-CPPNs for Image Synthesis,F-CPPN,ICCV Workshop,9/20/2019,2019,"@article{tesfaldet2019fcppn,
    url = {http://arxiv.org/abs/1909.09273v1},
    month = {Sep},
    year = {2019},
    title = {Fourier-CPPNs for Image Synthesis},
    author = {Mattie Tesfaldet and Xavier Snelgrove and David Vazquez},
    entrytype = {article},
    id = {tesfaldet2019fcppn}
}",https://arxiv.org/pdf/1909.09273.pdf,,https://github.com/tesfaldet/fourier-cppn/tree/master,,,,,"2D Image Neural Fields, Alternative Imaging, Fundamentals, Image-Based Rendering, Fourier, Positional Encoding",Windowed Fourier Coefficients as output,"None, N/A",No,,,,,,,"Mattie Tesfaldet, Xavier Snelgrove, David Vazquez",tesfaldet2019fcppn,260,,"Compositional Pattern Producing Networks (CPPNs) are differentiable networks that independently map (x, y) pixel coordinates to (r, g, b) colour values. Recently, CPPNs have been used for creating interesting imagery for creative purposes, e.g., neural art. However their architecture biases generated images to be overly smooth, lacking high-frequency detail. In this work, we extend CPPNs to explicitly model the frequency information for each pixel output, capturing frequencies beyond the DC component. We show that our Fourier-CPPNs (F-CPPNs) provide improved visual detail for image synthesis.",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,ICCV Workshop 2019,,,,,,
11/23/2021 23:23,isler@umn.edu,Yes,Higher-Order Function Networks for Learning Composable 3D Object Representations,HOF,ICLR,10/7/2019,2022,"@article{mitchell2022hof,
    url = {http://arxiv.org/abs/1907.10388v2},
    month = {Jul},
    year = {2019},
    title = {Higher-Order Function Networks for Learning Composable 3D Object Representations},
    author = {Eric Mitchell and Selim Engin and Volkan Isler and Daniel D Lee},
    entrytype = {article},
    id = {mitchell2022hof}
}",https://arxiv.org/pdf/1907.10388.pdf,,,,,,,"Surface Reconstruction, Hypernetwork/Meta-learning",None,Direct mapping; Surface-HOF (AAAI 2020) produces complete geodesics,No,,,,,,,"Eric Mitchell, Selim Engin, Volkan Isler, Daniel D Lee",mitchell2022hof,261,,"We present a new approach to 3D object representation where a neural network encodes the geometry of an object directly into the weights and biases of a second 'mapping' network. This mapping network can be used to reconstruct an object by applying its encoded transformation to points randomly sampled from a simple geometric space, such as the unit sphere. We study the effectiveness of our method through various experiments on subsets of the ShapeNet dataset. We find that the proposed approach can reconstruct encoded objects with accuracy equal to or exceeding state-of-the-art methods with orders of magnitude fewer parameters. Our smallest mapping network has only about 7000 parameters and shows reconstruction quality on par with state-of-the-art object decoder architectures with millions of parameters. Further experiments on feature mixing through the composition of learned functions show that the encoding captures a meaningful subspace of objects.",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,ICLR 2022,,,,,,
11/24/2021 3:54,songyou.peng@inf.ethz.ch,Yes,Shape As Points: A Differentiable Poisson Solver,SAP,NeurIPS,6/7/2021,2021,"@article{peng2021sap,
    url = {http://arxiv.org/abs/2106.03452v2},
    month = {Jun},
    year = {2021},
    title = {Shape As Points: A Differentiable Poisson Solver},
    author = {Songyou Peng and Chiyu ""Max"" Jiang and Yiyi Liao and Michael Niemeyer and Marc Pollefeys and Andreas Geiger},
    entrytype = {article},
    id = {peng2021sap}
}",https://arxiv.org/pdf/2106.03452.pdf,https://pengsongyou.github.io/sap,https://github.com/autonomousvision/shape_as_points,https://github.com/autonomousvision/shape_as_points,https://www.youtube.com/watch?v=TgR0NvYty0A,,,"Surface Reconstruction, Global Conditioning, Local Conditioning, Voxel Grid, Supervision by Gradient (PDE), Coarse-to-Fine, Positional Encoding",Points are encoded to an indicator grid with a differentiable Poisson solver using spectral methods.,Point offset fields and point normal fields,No,,,,,,,"Songyou Peng, Chiyu ""Max"" Jiang, Yiyi Liao, Michael Niemeyer, Marc Pollefeys, Andreas Geiger",peng2021sap,262,,"In recent years, neural implicit representations gained popularity in 3D reconstruction due to their expressiveness and flexibility. However, the implicit nature of neural implicit representations results in slow inference time and requires careful initialization. In this paper, we revisit the classic yet ubiquitous point cloud representation and introduce a differentiable point-to-mesh layer using a differentiable formulation of Poisson Surface Reconstruction (PSR) that allows for a GPU-accelerated fast solution of the indicator function given an oriented point cloud. The differentiable PSR layer allows us to efficiently and differentiably bridge the explicit 3D point representation with the 3D mesh via the implicit indicator field, enabling end-to-end optimization of surface reconstruction metrics such as Chamfer distance. This duality between points and meshes hence allows us to represent shapes as oriented point clouds, which are explicit, lightweight and expressive. Compared to neural implicit representations, our Shape-As-Points (SAP) model is more interpretable, lightweight, and accelerates inference time by one order of magnitude. Compared to other explicit representations such as points, patches, and meshes, SAP produces topology-agnostic, watertight manifold surfaces. We demonstrate the effectiveness of SAP on the task of surface reconstruction from unoriented point clouds and learning-based reconstruction.",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,NeurIPS 2021,,,,,,
11/24/2021 5:24,chris.willcocks@gmail.com,Yes,Gradient Origin Networks,GON,ICLR,7/6/2020,2021,"@inproceedings{bond2020gradient,
    title = {Gradient Origin Networks},
    author = {Sam Bond-Taylor and Chris G. Willcocks},
    booktitle = {International Conference on Learning Representations},
    year = {2021},
    url = {https://openreview.net/pdf?id=0O_cQfw6uEh},
    entrytype = {inproceedings},
    id = {bond2020gradient}
}",https://arxiv.org/pdf/2007.02798.pdf,https://cwkx.github.io/data/GON/,https://github.com/cwkx/GON,,https://www.youtube.com/watch?v=v-ZxzTSpmk4,,,"Speed & Computational Efficiency, 2D Image Neural Fields, Compression, Generative Models, Generalization, Sampling",Sinusoidal Activation (SIREN),"None, N/A",No,,,,,,,Sam Bond-Taylor & Chris G. Willcocks,bond2020gradient,263,,"This paper proposes a new type of generative model that is able to quickly learn a latent representation without an encoder. This is achieved using empirical Bayes to calculate the expectation of the posterior, which is implemented by initialising a latent vector with zeros, then using the gradient of the log-likelihood of the data with respect to this zero vector as new latent points. The approach has similar characteristics to autoencoders, but with a simpler architecture, and is demonstrated in a variational autoencoder equivalent that permits sampling. This also allows implicit representation networks to learn a space of implicit functions without requiring a hypernetwork, retaining their representation advantages across datasets. The experiments show that the proposed method converges faster, with significantly lower reconstruction error than autoencoders, while requiring half the parameters.",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,ICLR 2021,,,,,,
11/21/2021 16:12,yiheng_xie@brown.edu,Yes,Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition,Neural-PIL,NeurIPS,10/27/2021,2021,"@article{boss2021neuralpil,
    author = {Mark Boss and Varun Jampani and Raphael Braun and Ce Liu and Jonathan T. Barron and Hendrik P. A. Lensch},
    title = {Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition},
    year = {2021},
    month = {Oct},
    url = {http://arxiv.org/abs/2110.14373v1},
    entrytype = {article},
    id = {boss2021neuralpil}
}",https://arxiv.org/pdf/2110.14373.pdf,https://markboss.me/publication/2021-neural-pil/,,,https://www.youtube.com/watch?v=p5cKaNwVp4M,,,"Material/Lighting Estimation, Generative Models, Data-Driven Method, Coarse-to-Fine",Fourier Feature (NeRF),Density,No,,,,,,,"Mark Boss, Varun Jampani, Raphael Braun, Ce Liu, Jonathan T. Barron, Hendrik P. A. Lensch",boss2021neuralpil,253,,"Decomposing a scene into its shape, reflectance and illumination is a fundamental problem in computer vision and graphics. Neural approaches such as NeRF have achieved remarkable success in view synthesis, but do not explicitly perform decomposition and instead operate exclusively on radiance (the product of reflectance and illumination). Extensions to NeRF, such as NeRD, can perform decomposition but struggle to accurately recover detailed illumination, thereby significantly limiting realism. We propose a novel reflectance decomposition network that can estimate shape, BRDF, and per-image illumination given a set of object images captured under varying illumination. Our key technique is a novel illumination integration network called Neural-PIL that replaces a costly illumination integral operation in the rendering with a simple network query. In addition, we also learn deep low-dimensional priors on BRDF and illumination representations using novel smooth manifold auto-encoders. Our decompositions can result in considerably better BRDF and light estimates enabling more accurate novel view-synthesis and relighting compared to prior art. Project page: https://markboss.me/publication/2021-neural-pil/",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,NeurIPS 2021,,,,,,
11/23/2021 0:07,gy46@cornell.edu,Yes,Geometry Processing with Neural Fields,NFGP,NeurIPS,11/10/2021,2021,"@article{yang2021nfgp,
    volume = {34},
    title = {Geometry Processing with Neural Fields},
    year = {2021},
    journal = {Advances in Neural Information Processing Systems},
    author = {Guandao Yang and Serge Belongie and Bharath Hariharan and Vladlen Koltun},
    entrytype = {article},
    id = {yang2021nfgp}
}",https://openreview.net/pdf?id=JG-SlCAx5_K,,,https://github.com/stevenygd/NFGP,,,,Geometry Processing,Sinusoidal Activation (SIREN),SDF,"Yes, geometry only",,,,,,,"Guandao Yang, Serge Belongie, Bharath Hariharan, Vladlen Koltun",yang2021nfgp,254,,"Most existing geometry processing algorithms use meshes as the default shape representation. Manipulating meshes, however, requires one to maintain high quality in the surface discretization. For example, changing the topology of a mesh usually requires additional procedures such as remeshing. This paper instead proposes the use of neural fields for geometry processing. Neural fields can compactly store complicated shapes without spatial discretization. Moreover, neural fields are infinitely differentiable, which allows them to be optimized for objectives that involve higher-order derivatives. This raises the question: can geometry processing be done entirely using neural fields? We introduce loss functions and architectures to show that some of the most challenging geometry processing tasks, such as deformation and filtering, can be done with neural fields. Experimental results show that our methods are on par with the well-established mesh-based methods without committing to a particular surface discretization. Code is available at https://github.com/stevenygd/NFGP.",,,,"No (Please fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,NeurIPS 2021,,,,,,
11/23/2021 4:33,xpan@mpi-inf.mpg.de,Yes,A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis,ShadeGAN,NeurIPS,10/29/2021,2021,"@inproceedings{pan2021shadegan,
    title = {A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis},
    author = {Xingang Pan and Xudong Xu and Chen Change Loy and Christian Theobalt and Bo Dai},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    year = {2021},
    entrytype = {inproceedings},
    id = {pan2021shadegan}
}",https://arxiv.org/pdf/2110.15678.pdf,https://xingangpan.github.io/projects/ShadeGAN.html,https://github.com/XingangPan/ShadeGAN,,,,,"Material/Lighting Estimation, Generative Models, Generalization",Sinusoidal Activation (SIREN),Density,No,,,,,,,"Xingang Pan, Xudong Xu, Chen Change Loy, Christian Theobalt, Bo Dai",pan2021shadegan,255,,"The advancement of generative radiance fields has pushed the boundary of 3D-aware image synthesis. Motivated by the observation that a 3D object should look realistic from multiple viewpoints, these methods introduce a multi-view constraint as regularization to learn valid 3D radiance fields from 2D images. Despite the progress, they often fall short of capturing accurate 3D shapes due to the shape-color ambiguity, limiting their applicability in downstream tasks. In this work, we address this ambiguity by proposing a novel shading-guided generative implicit model that is able to learn a starkly improved shape representation. Our key insight is that an accurate 3D shape should also yield a realistic rendering under different lighting conditions. This multi-lighting constraint is realized by modeling illumination explicitly and performing shading with various lighting conditions. Gradients are derived by feeding the synthesized images to a discriminator. To compensate for the additional computational burden of calculating surface normals, we further devise an efficient volume rendering strategy via surface tracking, reducing the training and inference time by 24% and 48%, respectively. Our experiments on multiple datasets show that the proposed approach achieves photorealistic 3D-aware image synthesis while capturing accurate underlying 3D shapes. We demonstrate improved performance of our approach on 3D shape reconstruction against existing methods, and show its applicability on image relighting. Our code will be released at https://github.com/XingangPan/ShadeGAN.",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,NeurIPS 2021,,,,,,
11/23/2021 4:37,xpan@mpi-inf.mpg.de,Yes,Generative Occupancy Fields for 3D Surface-Aware Image Synthesis,GOF,NeurIPS,11/1/2021,2021,"@inproceedings{xu2021generative,
    title = {Generative Occupancy Fields for 3D Surface-Aware Image Synthesis},
    author = {Xudong Xu and Xingang Pan and Dahua Lin and Bo Dai},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    year = {2021},
    entrytype = {inproceedings},
    id = {xu2021generative}
}",https://arxiv.org/pdf/2111.00969.pdf,https://sheldontsui.github.io/projects/GOF,https://github.com/SheldonTsui/GOF_NeurIPS2021,,,,,"Surface Reconstruction, Generative Models, Generalization",Sinusoidal Activation (SIREN),Occupancy,No,,,,,,,"Xudong Xu, Xingang Pan, Dahua Lin, Bo Dai",xu2021generative,256,,"The advent of generative radiance fields has significantly promoted the development of 3D-aware image synthesis. The cumulative rendering process in radiance fields makes training these generative models much easier since gradients are distributed over the entire volume, but leads to diffused object surfaces. In the meantime, compared to radiance fields occupancy representations could inherently ensure deterministic surfaces. However, if we directly apply occupancy representations to generative models, during training they will only receive sparse gradients located on object surfaces and eventually suffer from the convergence problem. In this paper, we propose Generative Occupancy Fields (GOF), a novel model based on generative radiance fields that can learn compact object surfaces without impeding its training convergence. The key insight of GOF is a dedicated transition from the cumulative rendering in radiance fields to rendering with only the surface points as the learned surface gets more and more accurate. In this way, GOF combines the merits of two representations in a unified framework. In practice, the training-time transition of start from radiance fields and march to occupancy representations is achieved in GOF by gradually shrinking the sampling region in its rendering process from the entire volume to a minimal neighboring region around the surface. Through comprehensive experiments on multiple datasets, we demonstrate that GOF can synthesize high-quality images with 3D consistency and simultaneously learn compact and smooth object surfaces. Code, models, and demo videos are available at https://sheldontsui.github.io/projects/GOF",,,,"Yes (No need to fill out ""Date"", ""Authors"", ""Abstract"", ""Bibtex Citation"")",New entry,NeurIPS 2021,,,,,,
10/8/2021 16:50,,,StyleNeRF: A Style-based 3D Aware Generator for High-resolution Image Synthesis,StyleNeRF,OpenReview,10/5/2021,2021,"@inproceedings{anonymous2022stylenerf,
    title = {StyleNe{RF}: A Style-based 3D Aware Generator for High-resolution Image Synthesis},
    author = {Anonymous},
    booktitle = {Submitted to The Tenth International Conference on Learning Representations },
    year = {2022},
    url = {https://openreview.net/forum?id=iUuzzTMUw9K},
    entrytype = {inproceedings},
    id = {anonymous2022stylenerf}
}",https://openreview.net/pdf?id=iUuzzTMUw9K,,,,,,,,,,,,,,,,,Anonymous,anonymous2022stylenerf,252,,"We propose StyleNeRF, a 3D-aware generative model for photo-realistic highresolution image synthesis with high multi-view consistency, which can be trained on unstructured 2D images. Existing approaches either cannot synthesize highresolution images with fine details or yield clearly noticeable 3D-inconsistent artifacts. In addition, many of them lack control on style attributes and explicit 3D camera poses. To address these issues, StyleNeRF integrates the neural radiance field (NeRF) into a style-based generator to tackle the aforementioned challenges, i.e., improving rendering efficiency and 3D consistency for high-resolution image generation. To address the first issue, we perform volume rendering only to produce a low-resolution feature map, and progressively apply upsampling in 2D. To mitigate the inconsistencies caused by 2D upsampling, we propose multiple designs including a better upsampler choice and a new regularization loss to enforce 3D consistency. With these designs, StyleNeRF is able to synthesize high-resolution images at interactive rates while preserving 3D consistency at high quality. StyleNeRF also enables control of camera poses and different levels of styles, which can generalize to unseen views. It also supports challenging tasks such as style mixing, inversion and simple semantic edits.",,,,,,OpenReview 2021,,,,,,
10/1/2021 13:30,,,TöRF: Time-of-Flight Radiance Fields for Dynamic Scene View Synthesis,TöRF,NeurIPS,9/30/2021,2021,"@inproceedings{attal2021torf,
    publisher = {Curran Associates, Inc.},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    author = {Benjamin Attal and Eliot Laidlaw and Aaron Gokaslan and Changil Kim and Christian Richardt and James Tompkin and Matthew O'Toole},
    title = {ToRF: Time-of-Flight Radiance Fields for Dynamic Scene View Synthesis},
    year = {2021},
    url = {http://arxiv.org/abs/2109.15271v1},
    entrytype = {inproceedings},
    id = {attal2021torf}
}",https://arxiv.org/pdf/2109.15271.pdf,https://imaging.cs.cmu.edu/torf/,,,,,,"Dynamic/Temporal, Alternative Imaging, Global Conditioning, Local Conditioning",Fourier Feature (NeRF),Density,No,,,,,,,"Benjamin Attal, Eliot Laidlaw, Aaron Gokaslan, Changil Kim, Christian Richardt, James Tompkin, Matthew O'Toole",attal2021torf,251,,"Neural networks can represent and accurately reconstruct radiance fields for static 3D scenes (e.g., NeRF). Several works extend these to dynamic scenes captured with monocular video, with promising performance. However, the monocular setting is known to be an under-constrained problem, and so methods rely on data-driven priors for reconstructing dynamic content. We replace these priors with measurements from a time-of-flight (ToF) camera, and introduce a neural representation based on an image formation model for continuous-wave ToF cameras. Instead of working with processed depth maps, we model the raw ToF sensor measurements to improve reconstruction quality and avoid issues with low reflectance regions, multi-path interference, and a sensor's limited unambiguous depth range. We show that this approach improves robustness of dynamic scene reconstruction to erroneous calibration and large motions, and discuss the benefits and limitations of integrating RGB+ToF sensors that are now available on modern smartphones.",No,No,Direct,,,NeurIPS 2021,,,,,,
10/4/2021 22:11,,,Neural Knitworks: Patched Neural Implicit Representation Networks,Neural Knitworks,ARXIV,9/29/2021,2021,"@article{czerkawski2021neuralknitworks,
    url = {http://arxiv.org/abs/2109.14406v1},
    year = {2021},
    title = {Neural Knitworks: Patched Neural Implicit Representation Networks},
    author = {Mikolaj Czerkawski and Javier Cardona and Robert Atkinson and Craig Michie and Ivan Andonovic and Carmine Clemente and Christos Tachtatzis},
    booktitle = {ArXiv Pre-print},
    journal = {arXiv preprint arXiv:2109.14406},
    entrytype = {article},
    id = {czerkawski2021neuralknitworks}
}",https://arxiv.org/pdf/2109.14406.pdf,,,,,,,"2D Image Neural Fields, Generative Models",Fourier Feature (NeRF),,,,,,,,,"Mikolaj Czerkawski, Javier Cardona, Robert Atkinson, Craig Michie, Ivan Andonovic, Carmine Clemente, Christos Tachtatzis",czerkawski2021neuralknitworks,250,,"Coordinate-based Multilayer Perceptron (MLP) networks, despite being capable of learning neural implicit representations, are not performant for internal image synthesis applications. Convolutional Neural Networks (CNNs) are typically used instead for a variety of internal generative tasks, at the cost of a larger model. We propose Neural Knitwork, an architecture for neural implicit representation learning of natural images that achieves image synthesis by optimizing the distribution of image patches in an adversarial manner and by enforcing consistency between the patch predictions. To the best of our knowledge, this is the first implementation of a coordinate-based MLP tailored for synthesis tasks such as image inpainting, super-resolution, and denoising. We demonstrate the utility of the proposed technique by training on these three tasks. The results show that modeling natural images using patches, rather than pixels, produces results of higher fidelity. The resulting model requires 80% fewer parameters than alternative CNN-based solutions while achieving comparable performance and training time.",No,Yes,Indirect,,,ARXIV 2021,,,,,,
10/2/2021 8:15,,,ImplicitVol: Sensorless 3D Ultrasound Reconstruction with Deep Implicit Representation,ImplicitVol,ARXIV,9/24/2021,2021,"@article{yeung2021implicitvol,
    journal = {arXiv preprint arXiv:2109.12108},
    booktitle = {ArXiv Pre-print},
    author = {Pak-Hei Yeung and Linde Hesse and Moska Aliasi and Monique Haak and the INTERGROWTH-21st Consortium and Weidi Xie and Ana I. L. Namburete},
    title = {ImplicitVol: Sensorless 3D Ultrasound Reconstruction with Deep Implicit Representation},
    year = {2021},
    url = {http://arxiv.org/abs/2109.12108v1},
    entrytype = {article},
    id = {yeung2021implicitvol}
}",https://arxiv.org/pdf/2109.12108.pdf,https://pakheiyeung.github.io/ImplicitVol_wp/,https://github.com/pakheiyeung/ImplicitVol,,https://www.youtube.com/watch?v=D4ZCo14mqxs,,,"Alternative Imaging, Science & Engineering",,,,,,,,,,"Pak-Hei Yeung, Linde Hesse, Moska Aliasi, Monique Haak, the INTERGROWTH-21st Consortium, Weidi Xie, Ana I. L. Namburete",yeung2021implicitvol,249,,"The objective of this work is to achieve sensorless reconstruction of a 3D volume from a set of 2D freehand ultrasound images with deep implicit representation. In contrast to the conventional way that represents a 3D volume as a discrete voxel grid, we do so by parameterizing it as the zero level-set of a continuous function, i.e. implicitly representing the 3D volume as a mapping from the spatial coordinates to the corresponding intensity values. Our proposed model, termed as ImplicitVol, takes a set of 2D scans and their estimated locations in 3D as input, jointly re?fing the estimated 3D locations and learning a full reconstruction of the 3D volume. When testing on real 2D ultrasound images, novel cross-sectional views that are sampled from ImplicitVol show significantly better visual quality than those sampled from existing reconstruction approaches, outperforming them by over 30% (NCC and SSIM), between the output and ground-truth on the 3D volume testing data. The code will be made publicly available.",,,,,,ARXIV 2021,,,,,,
9/27/2021 21:57,,,A Skeleton-Driven Neural Occupancy Representation for Articulated Hands,HALO,ARXIV,9/23/2021,2021,"@article{karunratanakul2021halo,
    journal = {arXiv preprint arXiv:2109.11399},
    booktitle = {ArXiv Pre-print},
    author = {Korrawe Karunratanakul and Adrian Spurr and Zicong Fan and Otmar Hilliges and Siyu Tang},
    title = {A Skeleton-Driven Neural Occupancy Representation for Articulated Hands},
    year = {2021},
    url = {http://arxiv.org/abs/2109.11399v1},
    entrytype = {article},
    id = {karunratanakul2021halo}
}",https://arxiv.org/pdf/2109.11399.pdf,,,,,,,Human hand,,Occupancy,"Yes, geometry only",,,,,,,"Korrawe Karunratanakul, Adrian Spurr, Zicong Fan, Otmar Hilliges, Siyu Tang",karunratanakul2021halo,248,,"We present Hand ArticuLated Occupancy (HALO), a novel representation of articulated hands that bridges the advantages of 3D keypoints and neural implicit surfaces and can be used in end-to-end trainable architectures. Unlike existing statistical parametric hand models (e.g.~MANO), HALO directly leverages 3D joint skeleton as input and produces a neural occupancy volume representing the posed hand surface. The key benefits of HALO are (1) it is driven by 3D key points, which have benefits in terms of accuracy and are easier to learn for neural networks than the latent hand-model parameters; (2) it provides a differentiable volumetric occupancy representation of the posed hand; (3) it can be trained end-to-end, allowing the formulation of losses on the hand surface that benefit the learning of 3D keypoints. We demonstrate the applicability of HALO to the task of conditional generation of hands that grasp 3D objects. The differentiable nature of HALO is shown to improve the quality of the synthesized hands both in terms of physical plausibility and user preference.",,,,,,ARXIV 2021,,,,,,
9/27/2021 22:03,,,Layered Neural Atlases for Consistent Video Editing,,SIGGRAPH,9/23/2021,2021,"@article{kasten2021layered,
    publisher = {Association for Computing Machinery},
    journal = {ACM Transactions on Graphics (TOG)},
    author = {Yoni Kasten and Dolev Ofri and Oliver Wang and Tali Dekel},
    title = {Layered Neural Atlases for Consistent Video Editing},
    year = {2021},
    url = {http://arxiv.org/abs/2109.11418v1},
    entrytype = {article},
    id = {kasten2021layered}
}",https://arxiv.org/pdf/2109.11418.pdf,https://layered-neural-atlases.github.io/,Coming soon,,https://www.youtube.com/watch?v=aQhakPFC4oQ,https://layered-neural-atlases.github.io/supplementary/index.html,,"Dynamic/Temporal, 2D Image Neural Fields, Editable",,,,,,,,,,"Yoni Kasten, Dolev Ofri, Oliver Wang, Tali Dekel",kasten2021layered,247,,"We present a method that decomposes, or ""unwraps"", an input video into a set of layered 2D atlases, each providing a unified representation of the appearance of an object (or background) over the video. For each pixel in the video, our method estimates its corresponding 2D coordinate in each of the atlases, giving us a consistent parameterization of the video, along with an associated alpha (opacity) value. Importantly, we design our atlases to be interpretable and semantic, which facilitates easy and intuitive editing in the atlas domain, with minimal manual work required. Edits applied to a single 2D atlas (or input video frame) are automatically and consistently mapped back to the original video frames, while preserving occlusions, deformation, and other complex scene effects such as shadows and reflections. Our method employs a coordinate-based Multilayer Perceptron (MLP) representation for mappings, atlases, and alphas, which are jointly optimized on a per-video basis, using a combination of video reconstruction and regularization losses. By operating purely in 2D, our method does not require any prior 3D knowledge about scene geometry or camera poses, and can handle complex dynamic real world videos. We demonstrate various video editing applications, including texture mapping, video style transfer, image-to-video texture transfer, and segmentation/labeling propagation, all automatically produced by editing a single 2D atlas image.",,,,,,SIGGRAPH 2021,,,,,,
9/27/2021 21:52,,,Implicit Neural Distance Representation for Unsupervised and Supervised Classification of Complex Anatomies,,MICCAI,9/21/2021,2021,"@inproceedings{juhl2021implicit,
    author = {Kristine Aavild Juhl and Xabier Morales and Ole de Backer and Oscar Camara and Rasmus Reinhold Paulsen},
    booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
    organization = {Springer},
    pages = {405--415},
    year = {2021},
    title = {Implicit Neural Distance Representation for Unsupervised and Supervised Classification of Complex Anatomies},
    entrytype = {inproceedings},
    id = {juhl2021implicit}
}",https://link.springer.com/content/pdf/10.1007%2F978-3-030-87196-3.pdf,,https://github.com/kristineaajuhl/Implicit-Neural-Distance-Representation-of-Complex-Anatomie,,,,,"Human (Body), Human (Head), Science & Engineering, Classification, Global Conditioning",,UDF,"Yes, geometry only",,,,,,,"Kristine Aavild Juhl, Xabier Morales, Oscar Camara, Ole de Backer and Rasmus Reinhold Paulsen",juhl2021implicit,246,,"The task of 3D shape classification is closely related to finding a good representation of the shapes. In this study, we focus on surface representations of complex anatomies and on how such representations can be utilized for super-and unsupervised classification. We present a novel Implicit Neural Distance Representation based on unsigned distance fields (UDFs). The UDFs can be embedded into a low-dimensional latent space, which is optimized using only the shape itself. We demonstrate that this self-optimized latent space",No,,,,,MICCAI 2021,,,,,,
10/8/2021 16:39,,,When Do Extended Physics-Informed Neural Networks (XPINNs) Improve Generalization?,,ARXIV,9/20/2021,2021,"@article{hu2021when,
    url = {http://arxiv.org/abs/2109.09444v2},
    year = {2021},
    title = {When Do Extended Physics-Informed Neural Networks (XPINNs) Improve Generalization?},
    author = {Zheyuan Hu and Ameya D. Jagtap and George Em Karniadakis and Kenji Kawaguchi},
    booktitle = {ArXiv Pre-print},
    journal = {arXiv preprint arXiv:2109.09444},
    entrytype = {article},
    id = {hu2021when}
}",https://arxiv.org/pdf/2109.09444.pdf,,,,,,,Science & Engineering,,,,,,,,,,"Zheyuan Hu, Ameya D. Jagtap, George Em Karniadakis, Kenji Kawaguchi",hu2021when,245,,"Physics-informed neural networks (PINNs) have become a popular choice for solving high-dimensional partial differential equations (PDEs) due to their excellent approximation power and generalization ability. Recently, Extended PINNs (XPINNs) based on domain decomposition methods have attracted considerable attention due to their effectiveness in modeling multiscale and multiphysics problems and their parallelization. However, theoretical understanding on their convergence and generalization properties remains unexplored. In this study, we take an initial step towards understanding how and when XPINNs outperform PINNs. Specifically, for general multi-layer PINNs and XPINNs, we first provide a prior generalization bound via the complexity of the target functions in the PDE problem, and a posterior generalization bound via the posterior matrix norms of the networks after optimization. Moreover, based on our bounds, we analyze the conditions under which XPINNs improve generalization. Concretely, our theory shows that the key building block of XPINN, namely the domain decomposition, introduces a tradeoff for generalization. On the one hand, XPINNs decompose the complex PDE solution into several simple parts, which decreases the complexity needed to learn each part and boosts generalization. On the other hand, decomposition leads to less training data being available in each subdomain, and hence such model is typically prone to overfitting and may become less generalizable. Empirically, we choose five PDEs to show when XPINNs perform better than, similar to, or worse than PINNs, hence demonstrating and justifying our new theory.",,,,,,ARXIV 2021,,,,,,
9/18/2021 8:47,,,Neural Cameras: Learning Camera Characteristics for Coherent Mixed Reality Rendering,Neural Cameras,ISMAR,9/18/2021,2021,"@article{mandl2021neuralcameras,
    author = {David Mandl and Peter Mohr and Tobias Langlotz and Christoph Ebner and Shohei Mori and Stefanie Zollmann and Peter M Roth and Denis Kalkofen},
    year = {2021},
    title = {Neural Cameras: Learning Camera Characteristics for Coherent Mixed Reality Rendering},
    booktitle = {IEEE Symp. on Mixed and Augmented Reality (ISMAR)},
    month = {Oct},
    entrytype = {article},
    id = {mandl2021neuralcameras}
}",https://www.hci.otago.ac.nz/papers/MandlIEEEISMAR2021.pdf,,,,,,,Camera Parameter Estimation,,,,,,,,,,"David Mandl, Peter Mohr, Tobias Langlotz, Christoph Ebner, Shohei Mori, Stefanie Zollmann, Peter M Roth, Denis Kalkofen",mandl2021neuralcameras,244,,"Coherent rendering is important for generating plausible Mixed Reality presentations of virtual objects within a user's real-world environment. Besides photo-realistic rendering and correct lighting, visual coherence requires simulating the imaging system that is used to capture the real environment. While existing approaches either focus on a specific camera or a specific component of the imaging system, we introduce Neural Cameras, the first approach that jointly simulates all major components of an arbitrary modern camera using",,,,,,ISMAR 2021,,,,,,
9/26/2021 16:32,,,"ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations",ObjectFolder,CoRL,9/16/2021,2021,"@inproceedings{gao2021objectfolder,
    booktitle = {Proceedings of the Conference on Robot Learning (CoRL)},
    author = {Ruohan Gao and Yen-Yu Chang and Shivani Mall and Li Fei-Fei and Jiajun Wu},
    title = {ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations},
    year = {2021},
    url = {http://arxiv.org/abs/2109.07991v2},
    entrytype = {inproceedings},
    id = {gao2021objectfolder}
}",https://arxiv.org/pdf/2109.07991.pdf,https://ai.stanford.edu/~rhgao/objectfolder/,Coming soon,Coming soon,https://www.youtube.com/watch?v=wQ4o8XeS-X0,https://ai.stanford.edu/~rhgao/objectfolder/ObjectFolder_Supp.pdf,,"Compression, Science & Engineering, Robotics, Audio",,,No,,,,,,,"Ruohan Gao, Yen-Yu Chang, Shivani Mall, Li Fei-Fei, Jiajun Wu",gao2021objectfolder,243,,"Multisensory object-centric perception, reasoning, and interaction have been a key research topic in recent years. However, the progress in these directions is limited by the small set of objects available -- synthetic objects are not realistic enough and are mostly centered around geometry, while real object datasets such as YCB are often practically challenging and unstable to acquire due to international shipping, inventory, and financial cost. We present ObjectFolder, a dataset of 100 virtualized objects that addresses both challenges with two key innovations. First, ObjectFolder encodes the visual, auditory, and tactile sensory data for all objects, enabling a number of multisensory object recognition tasks, beyond existing datasets that focus purely on object geometry. Second, ObjectFolder employs a uniform, object-centric, and implicit representation for each object's visual textures, acoustic simulations, and tactile readings, making the dataset flexible to use and easy to share. We demonstrate the usefulness of our dataset as a testbed for multisensory perception and control by evaluating it on a variety of benchmark tasks, including instance recognition, cross-sensory retrieval, 3D reconstruction, and robotic grasping.",No,,,,,CoRL 2021,,,,,,
9/26/2021 16:35,,,Neural Human Performer: Learning Generalizable Radiance Fields for Human Performance Rendering,Neural Human Performer,NeurIPS,9/15/2021,2021,"@inproceedings{kwon2021neuralhumanperformer,
    url = {http://arxiv.org/abs/2109.07448v1},
    year = {2021},
    title = {Neural Human Performer: Learning Generalizable Radiance Fields for Human Performance Rendering},
    author = {Youngjoong Kwon and Dahun Kim and Duygu Ceylan and Henry Fuchs},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    publisher = {Curran Associates, Inc.},
    entrytype = {inproceedings},
    id = {kwon2021neuralhumanperformer}
}",https://arxiv.org/pdf/2109.07448.pdf,https://youngjoongunc.github.io/nhp/,https://github.com/YoungJoongUNC/Neural_Human_Performer,https://github.com/YoungJoongUNC/Neural_Human_Performer,https://www.youtube.com/watch?v=4b5SPwPOKVo,,,"Dynamic/Temporal, Human (Body), Generalization, Transformer, Local Conditioning",,,,,,,,,,"Youngjoong Kwon, Dahun Kim, Duygu Ceylan, Henry Fuchs",kwon2021neuralhumanperformer,242,,"In this paper, we aim at synthesizing a free-viewpoint video of an arbitrary human performance using sparse multi-view cameras. Recently, several works have addressed this problem by learning person-specific neural radiance fields (NeRF) to capture the appearance of a particular human. In parallel, some work proposed to use pixel-aligned features to generalize radiance fields to arbitrary new scenes and objects. Adopting such generalization approaches to humans, however, is highly challenging due to the heavy occlusions and dynamic articulations of body parts. To tackle this, we propose Neural Human Performer, a novel approach that learns generalizable neural radiance fields based on a parametric human body model for robust performance capture. Specifically, we first introduce a temporal transformer that aggregates tracked visual features based on the skeletal body motion over time. Moreover, a multi-view transformer is proposed to perform cross-attention between the temporally-fused features and the pixel-aligned features at each time step to integrate observations on the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets show that our method significantly outperforms recent generalizable NeRF methods on unseen identities and poses. The video results and code are available at https://youngjoongunc.github.io/nhp.",,Yes,,,,NeurIPS 2021,,,,,,
9/18/2021 8:37,,,Multiresolution Deep Implicit Functions for 3D Shape Representation,MDIF,ICCV,9/12/2021,2021,"@inproceedings{chen2021mdif,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Zhang Chen and Yinda Zhang and Kyle Genova and Sean Fanello and Sofien Bouaziz and Christian Haene and Ruofei Du and Cem Keskin and Thomas Funkhouser and Danhang Tang},
    title = {Multiresolution Deep Implicit Functions for 3D Shape Representation},
    year = {2021},
    url = {http://arxiv.org/abs/2109.05591v2},
    entrytype = {inproceedings},
    id = {chen2021mdif}
}",https://arxiv.org/pdf/2109.05591.pdf,,,,,,,"Surface Reconstruction, Local Conditioning, Coarse-to-Fine, Voxel Grid",,,"Yes, geometry only",,,,,,,"Zhang Chen, Yinda Zhang, Kyle Genova, Sean Fanello, Sofien Bouaziz, Christian Haene, Ruofei Du, Cem Keskin, Thomas Funkhouser, Danhang Tang",chen2021mdif,241,,"We introduce Multiresolution Deep Implicit Functions (MDIF), a hierarchical representation that can recover fine geometry detail, while being able to perform global operations such as shape completion. Our model represents a complex 3D shape with a hierarchy of latent grids, which can be decoded into different levels of detail and also achieve better accuracy. For shape completion, we propose latent grid dropout to simulate partial data in the latent space and therefore defer the completing functionality to the decoder side. This along with our multires design significantly improves the shape completion quality under decoder-only latent optimization. To the best of our knowledge, MDIF is the first deep implicit function model that can at the same time (1) represent different levels of detail and allow progressive decoding; (2) support both encoder-decoder inference and decoder-only latent optimization, and fulfill multiple applications; (3) perform detailed decoder-only shape completion. Experiments demonstrate its superior performance against prior art in various 3D reconstruction tasks.",,Yes,,,,ICCV 2021,,,,,,
9/17/2021 18:10,,,NEAT: Neural Attention Fields for End-to-End Autonomous Driving,NEAT,ICCV,9/9/2021,2021,"@inproceedings{chitta2021neat,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Kashyap Chitta and Aditya Prakash and Andreas Geiger},
    title = {NEAT: Neural Attention Fields for End-to-End Autonomous Driving},
    year = {2021},
    url = {http://arxiv.org/abs/2109.04456v1},
    entrytype = {inproceedings},
    id = {chitta2021neat}
}",https://arxiv.org/pdf/2109.04456.pdf,,https://github.com/autonomousvision/neat,,https://www.youtube.com/watch?v=gtO-ghjKkRs,http://www.cvlibs.net/publications/Chitta2021ICCV_supplementary.pdf,,"Dynamic/Temporal, Science & Engineering, Data-Driven Method",,,,,,,,,,"Kashyap Chitta, Aditya Prakash, Andreas Geiger",chitta2021neat,240,,"Efficient reasoning about the semantic, spatial, and temporal structure of a scene is a crucial prerequisite for autonomous driving. We present NEural ATtention fields (NEAT), a novel representation that enables such reasoning for end-to-end imitation learning models. NEAT is a continuous function which maps locations in Bird's Eye View (BEV) scene coordinates to waypoints and semantics, using intermediate attention maps to iteratively compress high-dimensional 2D image features into a compact representation. This allows our model to selectively attend to relevant regions in the input while ignoring information irrelevant to the driving task, effectively associating the images with the BEV representation. In a new evaluation setting involving adverse environmental conditions and challenging scenarios, NEAT outperforms several strong baselines and achieves driving scores on par with the privileged CARLA expert used to generate its training data. Furthermore, visualizing the attention maps for models with NEAT intermediate representations provides improved interpretability.",,,Direct,,,ICCV 2021,,,,,,
9/17/2021 17:49,,,Stochastic Neural Radiance Fields: Quantifying Uncertainty in Implicit 3D Representations,S-NeRF,ARXIV,9/5/2021,2021,"@article{shen2021snerf,
    journal = {arXiv preprint arXiv:2109.02123},
    booktitle = {ArXiv Pre-print},
    author = {Jianxiong Shen and Adria Ruiz and Antonio Agudo and Francesc Moreno-Noguer},
    title = {Stochastic Neural Radiance Fields: Quantifying Uncertainty in Implicit 3D Representations},
    year = {2021},
    url = {http://arxiv.org/abs/2109.02123v3},
    entrytype = {article},
    id = {shen2021snerf}
}",https://arxiv.org/pdf/2109.02123.pdf,,,,,,,Fundamentals,Fourier Feature (NeRF),Density,No,,,,,,,"Jianxiong Shen, Adria Ruiz, Antonio Agudo, Francesc Moreno",shen2021snerf,239,,"Neural Radiance Fields (NeRF) has become a popular framework for learning implicit 3D representations and addressing different tasks such as novel-view synthesis or depth-map estimation. However, in downstream applications where decisions need to be made based on automatic predictions, it is critical to leverage the confidence associated with the model estimations. Whereas uncertainty quantification is a long-standing problem in Machine Learning, it has been largely overlooked in the recent NeRF literature. In this context, we propose Stochastic Neural Radiance Fields (S-NeRF), a generalization of standard NeRF that learns a probability distribution over all the possible radiance fields modeling the scene. This distribution allows to quantify the uncertainty associated with the scene information provided by the model. S-NeRF optimization is posed as a Bayesian learning problem which is efficiently addressed using the Variational Inference framework. Exhaustive experiments over benchmark datasets demonstrate that S-NeRF is able to provide more reliable predictions and confidence values than generic approaches previously proposed for uncertainty estimation in other domains.",,,Direct,,,ARXIV 2021,,,,,,
9/17/2021 21:31,,,Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering,,ICCV,9/4/2021,2021,"@inproceedings{yang2021learning,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Bangbang Yang and Yinda Zhang and Yinghao Xu and Yijin Li and Han Zhou and Hujun Bao and Guofeng Zhang and Zhaopeng Cui},
    title = {Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering},
    year = {2021},
    url = {http://arxiv.org/abs/2109.01847v1},
    entrytype = {inproceedings},
    id = {yang2021learning}
}",https://arxiv.org/pdf/2109.01847.pdf,https://zju3dv.github.io/object_nerf/,https://github.com/zju3dv/object_nerf,,https://www.youtube.com/watch?v=VTEROu-Yz04,http://www.cad.zju.edu.cn/home/gfzhang/papers/object_nerf/object_nerf_supp.pdf,,"Editable, Local Conditioning, Voxel Grid, Object-Centric, Hybrid Geometry Representation, Global Conditioning",Fourier Feature (NeRF),Density,No,,,,,,,"Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, Zhaopeng Cui",yang2021learning,238,,"Implicit neural rendering techniques have shown promising results for novel view synthesis. However, existing methods usually encode the entire scene as a whole, which is generally not aware of the object identity and limits the ability to the high-level editing tasks such as moving or adding furniture. In this paper, we present a novel neural scene rendering system, which learns an object-compositional neural radiance field and produces realistic rendering with editing capability for a clustered and real-world scene. Specifically, we design a novel two-pathway architecture, in which the scene branch encodes the scene geometry and appearance, and the object branch encodes each standalone object conditioned on learnable object activation codes. To survive the training in heavily cluttered scenes, we propose a scene-guided training strategy to solve the 3D space ambiguity in the occluded regions and learn sharp boundaries for each object. Extensive experiments demonstrate that our system not only achieves competitive performance for static scene novel-view synthesis, but also produces realistic rendering for object-level editing.",,,Direct,,,ICCV 2021,,,,,,
9/17/2021 22:57,,,CodeNeRF: Disentangled Neural Radiance Fields for Object Categories,CodeNeRF,ICCV,9/3/2021,2021,"@inproceedings{jang2021codenerf,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Wonbong Jang and Lourdes Agapito},
    title = {CodeNeRF: Disentangled Neural Radiance Fields for Object Categories},
    year = {2021},
    url = {http://arxiv.org/abs/2109.01750v1},
    entrytype = {inproceedings},
    id = {jang2021codenerf}
}",https://arxiv.org/pdf/2109.01750.pdf,https://sites.google.com/view/wbjang/home/codenerf,https://github.com/wayne1123/code-nerf,,https://user-images.githubusercontent.com/32883157/130004248-0ff74d4e-993e-43f2-91ee-bd25776e65bc.mp4,,,"Generalization, Editable, Data-Driven Method, Global Conditioning",Fourier Feature (NeRF),Density,No,,,,,,,"Wonbong Jang, Lourdes Agapito",jang2021codenerf,237,,"CodeNeRF is an implicit 3D neural representation that learns the variation of object shapes and textures across a category and can be trained, from a set of posed images, to synthesize novel views of unseen objects. Unlike the original NeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture by learning separate embeddings. At test time, given a single unposed image of an unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and appearance codes via optimization. Unseen objects can be reconstructed from a single image, and then rendered from new viewpoints or their shape and texture edited by varying the latent codes. We conduct experiments on the SRN benchmark, which show that CodeNeRF generalises well to unseen objects and achieves on-par performance with methods that require known camera pose at test time. Our results on real-world images demonstrate that CodeNeRF can bridge the sim-to-real gap. Project page: \url{https://github.com/wayne1123/code-nerf}",,,Direct,,,ICCV 2021,,,,,,
9/18/2021 8:55,,,NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo,NerfingMVS,ICCV,9/2/2021,2021,"@article{wei2021nerfingmvs,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Yi Wei and Shaohui Liu and Yongming Rao and Wang Zhao and Jiwen Lu and Jie Zhou},
    title = {NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo},
    year = {2021},
    publisher = {IEEE},
    month = {Sep},
    url = {http://arxiv.org/abs/2109.01129v2},
    entrytype = {article},
    id = {wei2021nerfingmvs}
}",https://arxiv.org/pdf/2109.01129.pdf,https://weiyithu.github.io/NerfingMVS/,https://github.com/weiyithu/NerfingMVS,https://drive.google.com/drive/folders/1X_w57Q_MIFlI3lzhRt7Z8C5X9tNS8cg-,https://www.youtube.com/watch?v=i-b5lPnYipA,,,"Sampling, Data-Driven Method, Local Conditioning",Fourier Feature (NeRF),Density,No,,,,,,,"Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, Jie Zhou",wei2021nerfingmvs,236,,"In this work, we present a new multi-view depth estimation method that utilizes both conventional SfM reconstruction and learning-based priors over the recently proposed neural radiance fields (NeRF). Unlike existing neural network based optimization method that relies on estimated correspondences, our method directly optimizes over implicit volumes, eliminating the challenging step of matching pixels in indoor scenes. The key to our approach is to utilize the learning-based priors to guide the optimization process of NeRF. Our system firstly adapts a monocular depth network over the target scene by finetuning on its sparse SfM reconstruction. Then, we show that the shape-radiance ambiguity of NeRF still exists in indoor environments and propose to address the issue by employing the adapted depth priors to monitor the sampling process of volume rendering. Finally, a per-pixel confidence map acquired by error computation on the rendered image can be used to further improve the depth quality. Experiments show that our proposed framework significantly outperforms state-of-the-art methods on indoor scenes, with surprising findings presented on the effectiveness of correspondence-based optimization and NeRF-based optimization over the adapted depth priors. In addition, we show that the guided optimization scheme does not sacrifice the original synthesis capability of neural radiance fields, improving the rendering quality on both seen and novel views. Code is available at https://github.com/weiyithu/NerfingMVS.",,,Direct,,,ICCV 2021,,,,,,
9/18/2021 10:03,,,IntraTomo: Self-supervised Learning-based Tomography via Sinogram Synthesis and Prediction,IntraTomo,ICCV,9/2/2021,2021,"@inproceedings{zang2021intratomo,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    year = {2021},
    author = {Guangming Zang and Ramzi Idoughi and Rui Li and Peter Wonka and Wolfgang Heidrich},
    publisher = {IEEE},
    title = {IntraTomo: Self-supervised Learning-based Tomography via Sinogram Synthesis and Prediction},
    entrytype = {inproceedings},
    id = {zang2021intratomo}
}",https://vccimaging.org/Publications/Zang2021IntraTomo/Zang2021IntraTomo.pdf,https://vccimaging.org/Publications/Zang2021IntraTomo/,https://github.com/gmzang/IntraTomo,https://github.com/gmzang/IntraTomo,,https://vccimaging.org/Publications/Zang2021IntraTomo/Zang2021IntraTomo-supp.pdf,,"Alternative Imaging, Science & Engineering",Fourier Feature (NeRF),,,,,,,,,"Guangming Zang, Ramzi Idoughi, Rui Li, Peter Wonka, Wolfgang Heidrich",zang2021intratomo,235,,"We propose IntraTomo, a powerful framework that combines the benefits of learning-based and model-based approaches for solving highly ill-posed inverse problems, in the Computed Tomography (CT) context. IntraTomo is composed of two core modules: a novel sinogram prediction module and a geometry refinement module, which are applied iteratively. In the first module, the unknown density field is represented as a continuous and differentiable function, parameterized by a deep neural network. This network is learned, in",,,Direct,,,ICCV 2021,,,,,,
9/18/2021 9:42,,,A modified physics-informed neural network with positional encoding,,IMAGE,9/1/2021,2021,"@inbook{huang2021a,
    author = {Xinquan Huang and Tariq Alkhalifah and Chao Song},
    booktitle = {First International Meeting for Applied Geoscience \& Energy},
    organization = {Society of Exploration Geophysicists},
    pages = {2480--2484},
    title = {A modified physics-informed neural network with positional encoding},
    year = {2021},
    doi = {10.1190/segam2021-3584127.1},
    url = {https://library.seg.org/doi/abs/10.1190/segam2021-3584127.1},
    entrytype = {inbook},
    id = {huang2021a}
}",https://library.seg.org/doi/pdf/10.1190/segam2021-3584127.1,,,,,,,"Fundamentals, Science & Engineering, Supervision by Gradient (PDE)",Fourier Feature (NeRF),,,,,,,,,"Xinquan Huang, Tariq Alkhalifah, Chao Song",huang2021a,234,,"Recently developed physics-informed neural network (PINN) for solving for the scattered wavefield in the Helmholtz equation showed large potential in seismic modeling because of its flexibility, low memory requirement, and no limitations on the shape of the solution space. However, the predicted solutions were somewhat smooth and the convergence of the training was slow. Thus, we propose a modified PINN using sinusoidal activation functions and positional encoding, aiming to accelerate the convergence and fit better. We transform",,,Direct,,,IMAGE 2021,,,,,,
9/18/2021 10:00,,,Seeing Implicit Neural Representations as Fourier Series,,ARXIV,9/1/2021,2021,"@article{benbarka2021seeing,
    journal = {arXiv preprint arXiv:2109.00249},
    booktitle = {ArXiv Pre-print},
    author = {Nuri Benbarka and Timon Hofer and Hamd ul-moqeet Riaz and Andreas Zell},
    title = {Seeing Implicit Neural Representations as Fourier Series},
    year = {2021},
    url = {http://arxiv.org/abs/2109.00249v1},
    entrytype = {article},
    id = {benbarka2021seeing}
}",https://arxiv.org/pdf/2109.00249.pdf,,,,,,,"2D Image Neural Fields, Fundamentals, Coarse-to-Fine, Positional Encoding",Other,,,,,,,,,"Nuri Benbarka, Timon Höfer, Hamd ul-moqeet Riaz, Andreas Zell",benbarka2021seeing,233,,"Implicit Neural Representations (INR) use multilayer perceptrons to represent high-frequency functions in low-dimensional problem domains. Recently these representations achieved state-of-the-art results on tasks related to complex 3D objects and scenes. A core problem is the representation of highly detailed signals, which is tackled using networks with periodic activation functions (SIRENs) or applying Fourier mappings to the input. This work analyzes the connection between the two methods and shows that a Fourier mapped perceptron is structurally like one hidden layer SIREN. Furthermore, we identify the relationship between the previously proposed Fourier mapping and the general d-dimensional Fourier series, leading to an integer lattice mapping. Moreover, we modify a progressive training strategy to work on arbitrary Fourier mappings and show that it improves the generalization of the interpolation task. Lastly, we compare the different mappings on the image regression and novel view synthesis tasks. We confirm the previous finding that the main contributor to the mapping performance is the size of the embedding and standard deviation of its elements.",,,,,,ARXIV 2021,,,,,,
9/28/2021 9:31,,,Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction,CO3D,CVPR,9/1/2021,2021,"@inproceedings{reizenstein2021co3d,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Jeremy Reizenstein and Roman Shapovalov and Philipp Henzler and Luca Sbordone and Patrick Labatut and David Novotny},
    title = {Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction},
    year = {2021},
    url = {http://arxiv.org/abs/2109.00512v1},
    entrytype = {inproceedings},
    id = {reizenstein2021co3d}
}",https://arxiv.org/pdf/2109.00512.pdf,,https://github.com/facebookresearch/co3d,https://ai.facebook.com/datasets/co3d-downloads/,https://www.youtube.com/watch?v=hMx9nzG50xQ,,,"Sparse Reconstruction, Generalization, Transformer, Local Conditioning",Fourier Feature (NeRF),Density,No,,,,,,,"Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, David Novotny",reizenstein2021co3d,232,,"Traditional approaches for learning 3D object categories have been predominantly trained and evaluated on synthetic datasets due to the unavailability of real 3D-annotated category-centric data. Our main goal is to facilitate advances in this field by collecting real-world data in a magnitude similar to the existing synthetic counterparts. The principal contribution of this work is thus a large-scale dataset, called Common Objects in 3D, with real multi-view images of object categories annotated with camera poses and ground truth 3D point clouds. The dataset contains a total of 1.5 million frames from nearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such, it is significantly larger than alternatives both in terms of the number of categories and objects. We exploit this new dataset to conduct one of the first large-scale ""in-the-wild"" evaluations of several new-view-synthesis and category-centric 3D reconstruction methods. Finally, we contribute NerFormer - a novel neural rendering method that leverages the powerful Transformer to reconstruct an object given a small number of its views. The CO3D dataset is available at https://github.com/facebookresearch/co3d .",,,Direct,,,CVPR 2021,,,,,,
8/30/2021 18:01,,,Self-Calibrating Neural Radiance Fields,,ICCV,8/30/2021,2021,"@inproceedings{jeong2021selfcalibrating,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    year = {2021},
    author = {Yoonwoo Jeong and Seokjun Ahn and Christopher Choy and Animashree Anandkumar and Minsu Cho and Jaesik Park},
    journal = {arXiv preprint arXiv:2108.13826},
    title = {Self-Calibrating Neural Radiance Fields},
    entrytype = {inproceedings},
    id = {jeong2021selfcalibrating}
}",http://jaesik.info/publications/data/21_iccv1.pdf,,https://github.com/POSTECH-CVLab/SCNeRF,https://github.com/POSTECH-CVLab/SCNeRF,https://www.youtube.com/watch?v=wsjx6geduvk,,,"Camera Parameter Estimation, Coarse-to-Fine",Fourier Feature (NeRF),Density,No,,,,,,,"Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Animashree Anandkumar, Minsu Cho, Jaesik Park",jeong2021selfcalibrating,231,,"In this work, we propose a camera self-calibration algorithm for generic cameras with arbitrary non-linear distortions. We jointly learn the geometry of the scene and the accurate camera parameters without any calibration objects. Our camera model consists of a pinhole model, a fourth order radial distortion, and a generic noise model that can learn arbitrary non-linear camera distortions. While traditional self-calibration algorithms mostly rely on geometric constraints, we additionally incorporate photometric consistency. This requires",,,Direct,,,ICCV 2021,,,,,,
8/30/2021 17:53,,,NeRP: Implicit Neural Representation Learning with Prior Embedding for Sparsely Sampled Image Reconstruction,NeRP,ARXIV,8/24/2021,2021,"@article{shen2021nerp,
    journal = {arXiv preprint arXiv:ers/2108/2108.10991},
    booktitle = {ArXiv Pre-print},
    author = {Liyue Shen and John Pauly and Lei Xing},
    title = {NeRP: Implicit Neural Representation Learning with Prior Embedding for Sparsely Sampled Image Reconstruction},
    year = {2021},
    url = {http://arxiv.org/abs/2108.10991v1},
    entrytype = {article},
    id = {shen2021nerp}
}",https://arxiv.org/ftp/arxiv/papers/2108/2108.10991.pdf,,,,,,,"Generalization, Alternative Imaging, Science & Engineering, Data-Driven Method",,,,,,,,,,"Liyue Shen, John Pauly, Lei Xing",shen2021nerp,230,,"Image reconstruction is an inverse problem that solves for a computational image based on sampled sensor measurement. Sparsely sampled image reconstruction poses addition challenges due to limited measurements. In this work, we propose an implicit Neural Representation learning methodology with Prior embedding (NeRP) to reconstruct a computational image from sparsely sampled measurements. The method differs fundamentally from previous deep learning-based image reconstruction approaches in that NeRP exploits the internal information in an image prior, and the physics of the sparsely sampled measurements to produce a representation of the unknown subject. No large-scale data is required to train the NeRP except for a prior image and sparsely sampled measurements. In addition, we demonstrate that NeRP is a general methodology that generalizes to different imaging modalities such as CT and MRI. We also show that NeRP can robustly capture the subtle yet significant image changes required for assessing tumor progression.",,,Direct,,,ARXIV 2021,,,,,,
9/17/2021 11:34,,,imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose,imGHUM,ICCV,8/24/2021,2021,"@inproceedings{alldieck2021imghum,
    year = {2021},
    pages = {5461--5470},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
    author = {Alldieck, Thiemo and Xu, Hongyi and Sminchisescu, Cristian},
    title = {{imGHUM}: Implicit Generative Models of 3D Human Shape and Articulated Pose},
    entrytype = {inproceedings},
    id = {alldieck2021imghum}
}",https://arxiv.org/pdf/2108.10842.pdf,https://research.google/pubs/pub50642/,,,,,,Human (Body),,SDF,No,,,,,,,"Thiemo Alldieck, Hongyi Xu, Cristian Sminchisescu",alldieck2021imghum,229,,"We present imGHUM, the first holistic generative model of 3D human shape and articulated pose, represented as a signed distance function. In contrast to prior work, we model the full human body implicitly as a function zero-level-set and without the use of an explicit template mesh. We propose a novel network architecture and a learning paradigm, which make it possible to learn a detailed implicit generative model of human pose, shape, and semantics, on par with state-of-the-art mesh-based models. Our model features desired detail for human models, such as articulated pose including hand motion and facial expressions, a broad spectrum of shape variations, and can be queried at arbitrary resolutions and spatial locations. Additionally, our model has attached spatial semantics making it straightforward to establish correspondences between different shape instances, thus enabling applications that are difficult to tackle using classical implicit representations. In extensive experiments, we demonstrate the model accuracy and its applicability to current research problems.",,,Direct,,,ICCV 2021,,,,,,
8/29/2021 16:15,,,Implicit Neural Representations for Deconvolving SAS Images,,OCEANS,8/23/2021,2021,"@article{reed2021implicit,
    author = {Albert Reed and Thomas Blanford and Daniel C Brown and Suren Jayasuriya},
    title = {Implicit Neural Representations for Deconvolving SAS Images},
    entrytype = {article},
    id = {reed2021implicit}
}",https://web.asu.edu/sites/default/files/imaging-lyceum/files/2021151090.pdf,,,,,,,"Science & Engineering, Alternative Imaging",,,,,,,,,,"Albert Reed, Thomas Blanford, Daniel C Brown, Suren Jayasuriya",reed2021implicit,228,,"Synthetic aperture sonar (SAS) image resolution is constrained by waveform bandwidth and array geometry. Specifically, the waveform bandwidth determines a point spread function (PSF) that blurs the locations of point scatterers in the scene. In theory, deconvolving the reconstructed SAS image with the scene PSF restores the original distribution of scatterers and yields sharper reconstructions. However, deconvolution is an ill-posed operation that is highly sensitive to noise. In this work, we leverage implicit neural representations (INRs)",,,Direct,,,OCEANS 2021,,,,,,
8/29/2021 16:40,,,Learning Signed Distance Field for Multi-view Surface Reconstruction,,ICCV,8/23/2021,2021,"@inproceedings{zhang2021learning,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Jingyang Zhang and Yao Yao and Long Quan},
    title = {Learning Signed Distance Field for Multi-view Surface Reconstruction},
    year = {2021},
    url = {http://arxiv.org/abs/2108.09964v1},
    entrytype = {inproceedings},
    id = {zhang2021learning}
}",https://arxiv.org/pdf/2108.09964.pdf,,,,,,,Data-Driven Method,,SDF,No,,,,,,,"Jingyang Zhang, Yao Yao, Long Quan",zhang2021learning,227,,"Recent works on implicit neural representations have shown promising results for multi-view surface reconstruction. However, most approaches are limited to relatively simple geometries and usually require clean object masks for reconstructing complex and concave objects. In this work, we introduce a novel neural surface reconstruction framework that leverages the knowledge of stereo matching and feature consistency to optimize the implicit surface representation. More specifically, we apply a signed distance field (SDF) and a surface light field to represent the scene geometry and appearance respectively. The SDF is directly supervised by geometry from stereo matching, and is refined by optimizing the multi-view feature consistency and the fidelity of rendered images. Our method is able to improve the robustness of geometry estimation and support reconstruction of complex scene topologies. Extensive experiments have been conducted on DTU, EPFL and Tanks and Temples datasets. Compared to previous state-of-the-art methods, our method achieves better mesh reconstruction in wide open scenes without masks as input.",,,Direct,,,ICCV 2021,,,,,,
8/29/2021 16:33,,,Learning Deeper Non-Monotonic Networks by Softly Transferring Solution Space,,IJCAI,8/20/2021,2021,"@inproceedings{wu2021learning,
    publisher = {International Joint Conferences on Artificial Intelligence Organization},
    booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},
    author = {Zheng-Fan Wu and Hui Xue and Weimin Bai},
    year = {2021},
    title = {Learning Deeper Non-Monotonic Networks by Softly Transferring Solution Space},
    entrytype = {inproceedings},
    id = {wu2021learning}
}",https://www.ijcai.org/proceedings/2021/0440.pdf,,,,,,,"Fundamentals, Multi-task/Continual/Transfer learning, Positional Encoding",Other,,,,,,,,,"Zheng-Fan Wu, Hui Xue, Weimin Bai",wu2021learning,226,,"Different from popular neural networks using quasiconvex activations, non-monotonic networks activated by periodic nonlinearities have emerged as a more competitive paradigm, offering revolutionary benefits: 1) compactly characterizing highfrequency patterns; 2) precisely representing highorder derivatives. Nevertheless, they are also wellknown for being hard to train, due to easily overfitting dissonant noise and only allowing for tiny architectures (shallower than 5 layers). The fundamental bottleneck is that the",,,,,,IJCAI 2021,,,,,,
8/29/2021 16:11,,,Neural-GIF: Neural Generalized Implicit Functions for Animating People in Clothing,Neural-GIF,ICCV,8/19/2021,2021,"@inproceedings{tiwari2021neuralgif,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Garvita Tiwari and Nikolaos Sarafianos and Tony Tung and Gerard Pons-Moll},
    title = {Neural-GIF: Neural Generalized Implicit Functions for Animating People in Clothing},
    year = {2021},
    url = {http://arxiv.org/abs/2108.08807v2},
    entrytype = {inproceedings},
    id = {tiwari2021neuralgif}
}",https://arxiv.org/pdf/2108.08807.pdf,,,,,,,Human (Body),,SDF,"Yes, geometry only",,,,,,,"Garvita Tiwari, Nikolaos Sarafianos, Tony Tung, Gerard Pons-Moll",tiwari2021neuralgif,225,,"We present Neural Generalized Implicit Functions(Neural-GIF), to animate people in clothing as a function of the body pose. Given a sequence of scans of a subject in various poses, we learn to animate the character for new poses. Existing methods have relied on template-based representations of the human body (or clothing). However such models usually have fixed and limited resolutions, require difficult data pre-processing steps and cannot be used with complex clothing. We draw inspiration from template-based methods, which factorize motion into articulation and non-rigid deformation, but generalize this concept for implicit shape learning to obtain a more flexible model. We learn to map every point in the space to a canonical space, where a learned deformation field is applied to model non-rigid effects, before evaluating the signed distance field. Our formulation allows the learning of complex and non-rigid deformations of clothing and soft tissue, without computing a template registration as it is common with current approaches. Neural-GIF can be trained on raw 3D scans and reconstructs detailed complex surface geometry and deformations. Moreover, the model can generalize to new poses. We evaluate our method on a variety of characters from different public datasets in diverse clothing styles and show significant improvements over baseline methods, quantitatively and qualitatively. We also extend our model to multiple shape setting. To stimulate further research, we will make the model, code and data publicly available at: https://virtualhumans.mpi-inf.mpg.de/neuralgif/",,,,,,ICCV 2021,,,,,,
8/29/2021 16:44,,,Augmenting Implicit Neural Shape Representations with Explicit Deformation Fields,,ARXIV,8/19/2021,2021,"@article{atzmon2021augmenting,
    journal = {arXiv preprint arXiv:2108.08931},
    booktitle = {ArXiv Pre-print},
    author = {Matan Atzmon and David Novotny and Andrea Vedaldi and Yaron Lipman},
    title = {Augmenting Implicit Neural Shape Representations with Explicit Deformation Fields},
    year = {2021},
    url = {http://arxiv.org/abs/2108.08931v1},
    entrytype = {article},
    id = {atzmon2021augmenting}
}",https://arxiv.org/pdf/2108.08931.pdf,,,,,,,"Dynamic/Temporal, Human (Body)",,,"Yes, geometry only",,,,,,,"Matan Atzmon, David Novotny, Andrea Vedaldi, Yaron Lipman",atzmon2021augmenting,224,,"Implicit neural representation is a recent approach to learn shape collections as zero level-sets of neural networks, where each shape is represented by a latent code. So far, the focus has been shape reconstruction, while shape generalization was mostly left to generic encoder-decoder or auto-decoder regularization. In this paper we advocate deformation-aware regularization for implicit neural representations, aiming at producing plausible deformations as latent code changes. The challenge is that implicit representations do not capture correspondences between different shapes, which makes it difficult to represent and regularize their deformations. Thus, we propose to pair the implicit representation of the shapes with an explicit, piecewise linear deformation field, learned as an auxiliary function. We demonstrate that, by regularizing these deformation fields, we can encourage the implicit neural representation to induce natural deformations in the learned shape space, such as as-rigid-as-possible deformations.",,,,,,ARXIV 2021,,,,,,
9/17/2021 14:20,,,ARCH++: Animation-Ready Clothed Human Reconstruction Revisited,ARCH++,ICCV,8/17/2021,2021,"@inproceedings{he2021arch++,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Tong He and Yuanlu Xu and Shunsuke Saito and Stefano Soatto and Tony Tung},
    title = {ARCH++: Animation-Ready Clothed Human Reconstruction Revisited},
    year = {2021},
    url = {http://arxiv.org/abs/2108.07845v1},
    entrytype = {inproceedings},
    id = {he2021arch++}
}",https://arxiv.org/pdf/2108.07845.pdf,,,,,,,"Human (Body), Sparse Reconstruction, Editable, Voxel Grid, Data-Driven Method, Local Conditioning",,,No,,,,,,,"Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, Tony Tung",he2021arch++,223,,"We present ARCH++, an image-based method to reconstruct 3D avatars with arbitrary clothing styles. Our reconstructed avatars are animation-ready and highly realistic, in both the visible regions from input views and the unseen regions. While prior work shows great promise of reconstructing animatable clothed humans with various topologies, we observe that there exist fundamental limitations resulting in sub-optimal reconstruction quality. In this paper, we revisit the major steps of image-based avatar reconstruction and address the limitations with ARCH++. First, we introduce an end-to-end point based geometry encoder to better describe the semantics of the underlying 3D human body, in replacement of previous hand-crafted features. Second, in order to address the occupancy ambiguity caused by topological changes of clothed humans in the canonical pose, we propose a co-supervising framework with cross-space consistency to jointly estimate the occupancy in both the posed and canonical spaces. Last, we use image-to-image translation networks to further refine detailed geometry and texture on the reconstructed surface, which improves the fidelity and consistency across arbitrary viewpoints. In the experiments, we demonstrate improvements over the state of the art on both public benchmarks and user studies in reconstruction quality and realism.",,,,,,ICCV 2021,,,,,,
9/18/2021 10:30,,,Unsupervised Non-Rigid Image Distortion Removal via Grid Deformation,,ICCV,8/16/2021,2021,"@inproceedings{li2021unsupervised,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    year = {2021},
    author = {Nianyi Li and Simron Thapa and Cameron Whyte and Albert Reed and Suren Jayasuriya and Jinwei Ye},
    title = {Unsupervised Non-Rigid Image Distortion Removal via Grid Deformation},
    entrytype = {inproceedings},
    id = {li2021unsupervised}
}",https://ivlab.cse.lsu.edu/pub/iccv_21_distortion_removal.pdf,,https://github.com/Nianyi-Li/unsupervised-NDIR,,,,,"2D Image Neural Fields, Alternative Imaging",,,,,,,,,,"Nianyi Li, Simron Thapa, Cameron Whyte, Albert Reed, Suren Jayasuriya, Jinwei Ye",li2021unsupervised,222,,"Many computer vision problems face difficulties when imaging through turbulent refractive media (eg, air and water) due to the refraction and scattering of light. These effects cause geometric distortion that requires either handcrafted physical priors or supervised learning methods to remove. In this paper, we present a novel unsupervised network to recover the latent distortion-free image. The key idea is to model non-rigid distortions as deformable grids. Our network consists of a grid deformer that estimates the distortion field and an image",Yes,,"Direct, Indirect",,,ICCV 2021,,,,,,
8/30/2021 13:31,,,Continual Neural Mapping: Learning An Implicit Scene Representation from Sequential Observations,,ICCV,8/12/2021,2021,"@inproceedings{yan2021continual,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Zike Yan and Yuxin Tian and Xuesong Shi and Ping Guo and Peng Wang and Hongbin Zha},
    title = {Continual Neural Mapping: Learning An Implicit Scene Representation from Sequential Observations},
    year = {2021},
    url = {http://arxiv.org/abs/2108.05851v1},
    entrytype = {inproceedings},
    id = {yan2021continual}
}",https://arxiv.org/pdf/2108.05851.pdf,,,,https://zikeyan.github.io/videos/iccv2021.mp4,,,"Robotics, Multi-task/Continual/Transfer learning, Global Conditioning",,SDF,"Yes, geometry only",,,,,,,"Zike Yan, Yuxin Tian, Xuesong Shi, Ping Guo, Peng Wang, Hongbin Zha",yan2021continual,221,,"Recent advances have enabled a single neural network to serve as an implicit scene representation, establishing the mapping function between spatial coordinates and scene properties. In this paper, we make a further step towards continual learning of the implicit scene representation directly from sequential observations, namely Continual Neural Mapping. The proposed problem setting bridges the gap between batch-trained implicit neural representations and commonly used streaming data in robotics and vision communities. We introduce an experience replay approach to tackle an exemplary task of continual neural mapping: approximating a continuous signed distance function (SDF) from sequential depth images as a scene geometry representation. We show for the first time that a single network can represent scene geometry over time continually without catastrophic forgetting, while achieving promising trade-offs between accuracy and efficiency.",,,Direct,,,ICCV 2021,,,,,,
8/30/2021 11:59,,,SIDER: Single-Image Neural Optimization for Facial Geometric Detail Recovery,SIDER,ARXIV,8/11/2021,2021,"@article{chatziagapi2021sider,
    journal = {arXiv preprint arXiv:2108.05465},
    booktitle = {ArXiv Pre-print},
    author = {Aggelina Chatziagapi and ShahRukh Athar and Francesc Moreno-Noguer and Dimitris Samaras},
    title = {SIDER: Single-Image Neural Optimization for Facial Geometric Detail Recovery},
    year = {2021},
    url = {http://arxiv.org/abs/2108.05465v1},
    entrytype = {article},
    id = {chatziagapi2021sider}
}",https://arxiv.org/pdf/2108.05465.pdf,,,,,,,"Human (Head), Sparse Reconstruction, Generalization, Data-Driven Method, Coarse-to-Fine",,SDF,No,,,,,,,"Aggelina Chatziagapi, ShahRukh Athar, Francesc Moreno-Noguer, Dimitris Samaras",chatziagapi2021sider,220,,"We present SIDER(Single-Image neural optimization for facial geometric DEtail Recovery), a novel photometric optimization method that recovers detailed facial geometry from a single image in an unsupervised manner. Inspired by classical techniques of coarse-to-fine optimization and recent advances in implicit neural representations of 3D shape, SIDER combines a geometry prior based on statistical models and Signed Distance Functions (SDFs) to recover facial details from single images. First, it estimates a coarse geometry using a morphable model represented as an SDF. Next, it reconstructs facial geometry details by optimizing a photometric loss with respect to the ground truth image. In contrast to prior work, SIDER does not rely on any dataset priors and does not require additional supervision from multiple views, lighting changes or ground truth 3D shape. Extensive qualitative and quantitative evaluation demonstrates that our method achieves state-of-the-art on facial geometric detail recovery, using only a single in-the-wild image.",,,Direct,,,ARXIV 2021,,,,,,
8/30/2021 14:56,,,FLAME-in-NeRF: Neural control of Radiance Fields for Free View Face Animation,FLAME-in-NeRF,ARXIV,8/10/2021,2021,"@article{athar2021flameinnerf,
    journal = {arXiv preprint arXiv:2108.04913},
    booktitle = {ArXiv Pre-print},
    author = {ShahRukh Athar and Zhixin Shu and Dimitris Samaras},
    title = {FLAME-in-NeRF : Neural control of Radiance Fields for Free View Face Animation},
    year = {2021},
    url = {http://arxiv.org/abs/2108.04913v1},
    entrytype = {article},
    id = {athar2021flameinnerf}
}",https://arxiv.org/pdf/2108.04913.pdf,,,,,,,"Dynamic/Temporal, Human (Head), Generalization, Editable, Global Conditioning, Data-Driven Method, Coarse-to-Fine",Fourier Feature (NeRF),Density,No,,,,,,,"ShahRukh Athar, Zhixin Shu, Dimitris Samaras",athar2021flameinnerf,219,,"This paper presents a neural rendering method for controllable portrait video synthesis. Recent advances in volumetric neural rendering, such as neural radiance fields (NeRF), has enabled the photorealistic novel view synthesis of static scenes with impressive results. However, modeling dynamic and controllable objects as part of a scene with such scene representations is still challenging. In this work, we design a system that enables both novel view synthesis for portrait video, including the human subject and the scene background, and explicit control of the facial expressions through a low-dimensional expression representation. We leverage the expression space of a 3D morphable face model (3DMM) to represent the distribution of human facial expressions, and use it to condition the NeRF volumetric function. Furthermore, we impose a spatial prior brought by 3DMM fitting to guide the network to learn disentangled control for scene appearance and facial actions. We demonstrate the effectiveness of our method on free view synthesis of portrait videos with expression controls. To train a scene, our method only requires a short video of a subject captured by a mobile device.",,,Direct,,,ARXIV 2021,,,,,,
8/30/2021 15:08,,,View Synthesis In Casually Captured Scenes Using a Cylindrical Neural Radiance Field With Exposure Compensation,,SIGGRAPH,8/9/2021,2021,"@article{khademi2021view,
    journal = {ACM Transactions on Graphics (TOG)},
    author = {Wesley Khademi and Jonathan Ventura},
    title = {View Synthesis In Casually Captured Scenes Using a Cylindrical Neural Radiance Field With Exposure Compensation},
    year = {2021},
    isbn = {9781450383714},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3450618.3469147},
    doi = {10.1145/3450618.3469147},
    booktitle = {ACM SIGGRAPH 2021 Posters},
    articleno = {28},
    numpages = {2},
    location = {Virtual Event, USA},
    series = {SIGGRAPH '21},
    entrytype = {article},
    id = {khademi2021view}
}",https://dl.acm.org/doi/pdf/10.1145/3450618.3469147,,,,,,,"Sampling, Hybrid Geometry Representation",Fourier Feature (NeRF),Density,No,,,,,,,"Wesley Khademi, Jonathan Ventura",khademi2021view,218,,"We extend Neural Radiance Fields (NeRF) with a cylindrical parameterization that
 enables rendering photorealistic novel views of 360° outward facing scenes. We further
 introduce a learned exposure compensation parameter to account for the varying exposure
 in training images that may occur from casually capturing a scene. We evaluate our
 method on a variety of 360° casually captured scenes.",,,Direct,,,SIGGRAPH 2021,,,,,,
8/29/2021 16:46,,,Neural Image Representations for Multi-Image Fusion and Layer Separation,,ARXIV,8/2/2021,2021,"@article{nam2021neural,
    journal = {arXiv preprint arXiv:2108.01199},
    booktitle = {ArXiv Pre-print},
    author = {Seonghyeon Nam and Marcus A. Brubaker and Michael S. Brown},
    title = {Neural Image Representations for Multi-Image Fusion and Layer Separation},
    year = {2021},
    url = {http://arxiv.org/abs/2108.01199v2},
    entrytype = {article},
    id = {nam2021neural}
}",https://arxiv.org/pdf/2108.01199.pdf,,,,,,,"Dynamic/Temporal, 2D Image Neural Fields",,,,,,,,,,"Seonghyeon Nam, Marcus A. Brubaker, Michael S. Brown",nam2021neural,217,,"We propose a framework for aligning and fusing multiple images into a single coordinate-based neural representations. Our framework targets burst images that have misalignment due to camera ego motion and small changes in the scene. We describe different strategies for alignment depending on the assumption of the scene motion, namely, perspective planar (i.e., homography), optical flow with minimal scene change, and optical flow with notable occlusion and disocclusion. Our framework effectively combines the multiple inputs into a single neural implicit function without the need for selecting one of the images as a reference frame. We demonstrate how to use this multi-frame fusion framework for various layer separation tasks.",,,Direct,,,ARXIV 2021,,,,,,
8/29/2021 17:44,,,H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction,H3D-Net,ARXIV,7/26/2021,2021,"@article{ramon2021h3dnet,
    journal = {arXiv preprint arXiv:2107.12512},
    booktitle = {ArXiv Pre-print},
    author = {Eduard Ramon and Gil Triginer and Janna Escur and Albert Pumarola and Jaime Garcia and Xavier Giro-i-Nieto and Francesc Moreno-Noguer},
    title = {H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction},
    year = {2021},
    url = {http://arxiv.org/abs/2107.12512v1},
    entrytype = {article},
    id = {ramon2021h3dnet}
}",https://arxiv.org/pdf/2107.12512.pdf,https://crisalixsa.github.io/h3d-net/,https://github.com/CrisalixSA/h3ds,https://github.com/CrisalixSA/h3ds,,,,"Human (Head), Sparse Reconstruction, Data-Driven Method, Global Conditioning",Fourier Feature (NeRF),SDF,No,,,,,,,"Eduard Ramon, Gil Triginer, Janna Escur, Albert Pumarola, Jaime Garcia, Xavier Giro-i-Nieto, Francesc Moreno-Noguer",ramon2021h3dnet,216,,"Recent learning approaches that implicitly represent surface geometry using coordinate-based neural representations have shown impressive results in the problem of multi-view 3D reconstruction. The effectiveness of these techniques is, however, subject to the availability of a large number (several tens) of input views of the scene, and computationally demanding optimizations. In this paper, we tackle these limitations for the specific problem of few-shot full 3D head reconstruction, by endowing coordinate-based representations with a probabilistic shape prior that enables faster convergence and better generalization when using few input images (down to three). First, we learn a shape model of 3D heads from thousands of incomplete raw scans using implicit representations. At test time, we jointly overfit two coordinate-based neural networks to the scene, one modeling the geometry and another estimating the surface radiance, using implicit differentiable rendering. We devise a two-stage optimization strategy in which the learned prior is used to initialize and constrain the geometry during an initial optimization phase. Then, the prior is unfrozen and fine-tuned to the scene. By doing this, we achieve high-fidelity head reconstructions, including hair and shoulders, and with a high level of detail that consistently outperforms both state-of-the-art 3D Morphable Models methods in the few-shot scenario, and non-parametric methods when large sets of views are available.",,,Direct,,,ARXIV 2021,,,,,,
9/17/2021 19:35,,,NeLF: Neural Light-transport Field for Portrait View Synthesis and Relighting,NeLF,EGSR,7/26/2021,2021,"@article{sun2021nelf,
    publisher = {The Eurographics Association and John Wiley & Sons Ltd.},
    journal = {Computer Graphics Forum},
    author = {Tiancheng Sun and Kai-En Lin and Sai Bi and Zexiang Xu and Ravi Ramamoorthi},
    title = {NeLF: Neural Light-transport Field for Portrait View Synthesis and Relighting},
    year = {2021},
    url = {http://arxiv.org/abs/2107.12351v1},
    entrytype = {article},
    id = {sun2021nelf}
}",https://arxiv.org/pdf/2107.12351.pdf,,,,,,,"Human (Head), Material/Lighting Estimation, Data-Driven Method, Local Conditioning",,,No,,,,,,,"Tiancheng Sun, Kai-En Lin, Sai Bi, Zexiang Xu, Ravi Ramamoorthi",sun2021nelf,215,,"Human portraits exhibit various appearances when observed from different views under different lighting conditions. We can easily imagine how the face will look like in another setup, but computer algorithms still fail on this problem given limited observations. To this end, we present a system for portrait view synthesis and relighting: given multiple portraits, we use a neural network to predict the light-transport field in 3D space, and from the predicted Neural Light-transport Field (NeLF) produce a portrait from a new camera view under a new environmental lighting. Our system is trained on a large number of synthetic models, and can generalize to different synthetic and real portraits under various lighting conditions. Our method achieves simultaneous view synthesis and relighting given multi-view portraits as the input, and achieves state-of-the-art results.",,Yes,,,,EGSR 2021,,,,,,
8/29/2021 19:19,,,A Deep Signed Directional Distance Function for Object Shape Representation,,ARXIV,7/23/2021,2021,"@article{zobeidi2021a,
    journal = {arXiv preprint arXiv:2107.11024},
    booktitle = {ArXiv Pre-print},
    author = {Ehsan Zobeidi and Nikolay Atanasov},
    title = {A Deep Signed Directional Distance Function for Object Shape Representation},
    year = {2021},
    url = {http://arxiv.org/abs/2107.11024v1},
    entrytype = {article},
    id = {zobeidi2021a}
}",https://arxiv.org/pdf/2107.11024.pdf,,,,,,,,,Signed Directional Distance Field (SDDF),"Yes, geometry only",,,,,,,"Ehsan Zobeidi, Nikolay Atanasov",zobeidi2021a,214,,"Neural networks that map 3D coordinates to signed distance function (SDF) or occupancy values have enabled high-fidelity implicit representations of object shape. This paper develops a new shape model that allows synthesizing novel distance views by optimizing a continuous signed directional distance function (SDDF). Similar to deep SDF models, our SDDF formulation can represent whole categories of shapes and complete or interpolate across shapes from partial input data. Unlike an SDF, which measures distance to the nearest surface in any direction, an SDDF measures distance in a given direction. This allows training an SDDF model without 3D shape supervision, using only distance measurements, readily available from depth camera or Lidar sensors. Our model also removes post-processing steps like surface extraction or rendering by directly predicting distance at arbitrary locations and viewing directions. Unlike deep view-synthesis techniques, such as Neural Radiance Fields, which train high-capacity black-box models, our model encodes by construction the property that SDDF values decrease linearly along the viewing direction. This structure constraint not only results in dimensionality reduction but also provides analytical confidence about the accuracy of SDDF predictions, regardless of the distance to the object surface.",,,Direct,,,ARXIV 2021,,,,,,
10/8/2021 16:41,,,Grid-Functioned Neural Networks,GFNN,PMLR,7/18/2021,2021,"@inproceedings{dehesa2021gfnn,
    title = {Grid-Functioned Neural Networks},
    author = {Javier Dehesa and Andrew Vidler and Julian Padget and Christof Lutteroth},
    booktitle = {Proceedings of the 38th International Conference on Machine Learning},
    pages = {2559--2567},
    year = {2021},
    editor = {Meila, Marina and Zhang, Tong},
    volume = {139},
    series = {Proceedings of Machine Learning Research},
    month = {18--24 Jul},
    publisher = {PMLR},
    pdf = {http://proceedings.mlr.press/v139/dehesa21a/dehesa21a.pdf},
    url = {https://proceedings.mlr.press/v139/dehesa21a.html},
    entrytype = {inproceedings},
    id = {dehesa2021gfnn}
}",http://proceedings.mlr.press/v139/dehesa21a/dehesa21a.pdf,http://proceedings.mlr.press/v139/dehesa21a.html,,,,,,"Fundamentals, Science & Engineering",,,,,,,,,,"Javier Dehesa, Andrew Vidler, Julian Padget, Christof Lutteroth",dehesa2021gfnn,213,,"We introduce a new neural network architecture that we call ""grid-functioned"" neural networks. It utilises a grid structure of network parameterisations that can be specialised for different subdomains of the problem, while maintaining smooth, continuous behaviour. The grid gives the user flexibility to prevent gross features from overshadowing important minor ones. We present a full characterisation of its computational and spatial complexity, and demonstrate its potential, compared to a traditional architecture, over a set of synthetic regression problems. We further illustrate the benefits through a real-world 3D skeletal animation case study, where it offers the same visual quality as a state-of-the-art model, but with lower computational complexity and better control accuracy.",,,,,,PMLR 2021,,,,,,
8/29/2021 20:31,,,Unsupervised Discovery of Object Radiance Fields,uORF,ARXIV,7/16/2021,2021,"@article{yu2021uorf,
    journal = {arXiv preprint arXiv:2107.07905},
    booktitle = {ArXiv Pre-print},
    author = {Hong-Xing Yu and Leonidas J. Guibas and Jiajun Wu},
    title = {Unsupervised Discovery of Object Radiance Fields},
    year = {2021},
    url = {http://arxiv.org/abs/2107.07905v1},
    entrytype = {article},
    id = {yu2021uorf}
}",https://arxiv.org/pdf/2107.07905.pdf,https://kovenyu.com/uorf/,https://github.com/KovenYu/uORF,,https://www.youtube.com/watch?v=6J9OpvT4dCA,https://kovenyu.com/uorf/static/uORF_supp.pdf,,"Generalization, Editable, Data-Driven Method, Local Conditioning, Object-Centric, Hybrid Geometry Representation",Fourier Feature (NeRF),Density,No,,,,,,,"Hong-Xing Yu, Leonidas J. Guibas, Jiajun Wu",yu2021uorf,212,,"We study the problem of inferring an object-centric scene representation from a single image, aiming to derive a representation that explains the image formation process, captures the scene's 3D nature, and is learned without supervision. Most existing methods on scene decomposition lack one or more of these characteristics, due to the fundamental challenge in integrating the complex 3D-to-2D image formation process into powerful inference schemes like deep networks. In this paper, we propose unsupervised discovery of Object Radiance Fields (uORF), integrating recent progresses in neural 3D scene representations and rendering with deep inference networks for unsupervised 3D scene decomposition. Trained on multi-view RGB images without annotations, uORF learns to decompose complex scenes with diverse, textured background from a single image. We show that uORF performs well on unsupervised 3D scene segmentation, novel view synthesis, and scene editing on three datasets.",,,Direct,,,ARXIV 2021,,,,,,
10/8/2021 16:34,,,Finite Basis Physics-Informed Neural Networks (FBPINNs): a scalable domain decomposition approach for solving differential equations,FBPINNs,ARXIV,7/16/2021,2021,"@article{moseley2021fbpinns,
    url = {http://arxiv.org/abs/2107.07871v1},
    year = {2021},
    title = {Finite Basis Physics-Informed Neural Networks (FBPINNs): a scalable domain decomposition approach for solving differential equations},
    author = {Ben Moseley and Andrew Markham and Tarje Nissen-Meyer},
    booktitle = {ArXiv Pre-print},
    journal = {arXiv preprint arXiv:2107.07871},
    entrytype = {article},
    id = {moseley2021fbpinns}
}",https://arxiv.org/pdf/2107.07871.pdf,,https://github.com/benmoseley/FBPINNs,,,,,Science & Engineering,,,,,,,,,,"Ben Moseley, Andrew Markham, Tarje Nissen-Meyer",moseley2021fbpinns,211,,"Recently, physics-informed neural networks (PINNs) have offered a powerful new paradigm for solving problems relating to differential equations. Compared to classical numerical methods PINNs have several advantages, for example their ability to provide mesh-free solutions of differential equations and their ability to carry out forward and inverse modelling within the same optimisation problem. Whilst promising, a key limitation to date is that PINNs have struggled to accurately and efficiently solve problems with large domains and/or multi-scale solutions, which is crucial for their real-world application. Multiple significant and related factors contribute to this issue, including the increasing complexity of the underlying PINN optimisation problem as the problem size grows and the spectral bias of neural networks. In this work we propose a new, scalable approach for solving large problems relating to differential equations called Finite Basis PINNs (FBPINNs). FBPINNs are inspired by classical finite element methods, where the solution of the differential equation is expressed as the sum of a finite set of basis functions with compact support. In FBPINNs neural networks are used to learn these basis functions, which are defined over small, overlapping subdomains. FBINNs are designed to address the spectral bias of neural networks by using separate input normalisation over each subdomain, and reduce the complexity of the underlying optimisation problem by using many smaller neural networks in a parallel divide-and-conquer approach. Our numerical experiments show that FBPINNs are effective in solving both small and larger, multi-scale problems, outperforming standard PINNs in both accuracy and computational resources required, potentially paving the way to the application of PINNs on large, real-world problems.",,,,,,ARXIV 2021,,,,,,
7/19/2021 21:43,,,Adaptive weight matrix and phantom intensity learning for computed tomography of chemiluminescence,,Optics Express,7/12/2021,2021,"@article{pan2021adaptive,
    author = {Hujie Pan and Di Xiao and Fuhao Zhang and Xuesong Li and Min Xu},
    journal = {Opt. Express},
    keywords = {Computational imaging; Computed tomography; Light fields; Light propagation; Neural networks; Propagation methods},
    number = {15},
    pages = {23682--23700},
    publisher = {OSA},
    title = {Adaptive weight matrix and phantom intensity learning for computed tomography of chemiluminescence},
    volume = {29},
    month = {Jul},
    year = {2021},
    url = {http://www.opticsexpress.org/abstract.cfm?URI=oe-29-15-23682},
    doi = {10.1364/OE.427459},
    entrytype = {article},
    id = {pan2021adaptive}
}",https://www.osapublishing.org/oe/fulltext.cfm?uri=oe-29-15-23682&id=453213,,,,,,,"Alternative Imaging, Science & Engineering",,,,,,,,,,"Hujie Pan, Di Xiao, Fuhao Zhang, Xuesong Li, Min Xu",pan2021adaptive,210,,"Classic algebraic reconstruction technique (ART) for computed tomography requires pre-determined weights of the voxels for the projected pixel values to build the equations. However, such weights cannot be accurately obtained in the application of chemiluminescence measurements due to the high physical complexity and computation resources required. Moreover, streaks arise in the results from ART method especially with imperfect projections. In this study, we propose a semi-case-wise learning-based method named Weight Encode Reconstruction Network (WERNet) to co-learn the target phantom intensities and the adaptive weight matrix of the case without labeling the target voxel set and thus offers a more applicable solution for computed tomography problems. Both numerical and experimental validations were conducted to evaluate the algorithm. In the numerical test, with the help of gradient normalization, the WERNet reconstructed voxel set with a high accuracy and showed a higher capability of denoising compared to the classic ART methods. In the experimental test, WERNet produces comparable results to the ART method while having a better performance in avoiding the streaks. Furthermore, with the adaptive weight matrix, WERNet is not sensitive to the ensemble intensity of the projection which shows much better robustness than ART method.",Yes,,,,,Optics Express 2021,,,,,,
7/19/2021 21:44,,,3D Neural Scene Representations for Visuomotor Control,,RSS,7/8/2021,2021,"@inproceedings{li20213d,
    booktitle = {Proceedings of Robotics: Science and Systems},
    author = {Yunzhu Li and Shuang Li and Vincent Sitzmann and Pulkit Agrawal and Antonio Torralba},
    title = {3D Neural Scene Representations for Visuomotor Control},
    year = {2021},
    url = {http://arxiv.org/abs/2107.04004v1},
    entrytype = {inproceedings},
    id = {li20213d}
}",https://arxiv.org/pdf/2107.04004.pdf,https://3d-representation-learning.github.io/nerf-dy/,,,,,"https://www.youtube.com/watch?v=boKF-q6qofQ, https://www.youtube.com/watch?v=GFkb1x6Oxgo, https://www.youtube.com/watch?v=2fSkcTOvl5M, https://www.youtube.com/watch?v=nckvx1S7-cw","Science & Engineering, Robotics",,,,,,,,,,"Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal, Antonio Torralba",li20213d,209,,"Humans have a strong intuitive understanding of the 3D environment around us. The mental model of the physics in our brain applies to objects of different materials and enables us to perform a wide range of manipulation tasks that are far beyond the reach of current robots. In this work, we desire to learn models for dynamic 3D scenes purely from 2D visual observations. Our model combines Neural Radiance Fields (NeRF) and time contrastive learning with an autoencoding framework, which learns viewpoint-invariant 3D-aware scene representations. We show that a dynamics model, constructed over the learned representation space, enables visuomotor control for challenging manipulation tasks involving both rigid bodies and fluids, where the target is specified in a viewpoint different from what the robot operates on. When coupled with an auto-decoding framework, it can even support goal specification from camera viewpoints that are outside the training distribution. We further demonstrate the richness of the learned 3D dynamics model by performing future prediction and novel view synthesis. Finally, we provide detailed ablation studies regarding different system designs and qualitative analysis of the learned representations.",,,,,,RSS 2021,,,,,,
7/19/2021 21:20,,,Rethinking positional encoding,,ARXIV,7/6/2021,2021,"@article{zheng2021rethinking,
    journal = {arXiv preprint arXiv:2107.02561},
    booktitle = {ArXiv Pre-print},
    author = {Jianqiao Zheng and Sameera Ramasinghe and Simon Lucey},
    title = {Rethinking Positional Encoding},
    year = {2021},
    url = {http://arxiv.org/abs/2107.02561v2},
    entrytype = {article},
    id = {zheng2021rethinking}
}",https://arxiv.org/pdf/2107.02561.pdf,,https://github.com/osiriszjq/Rethinking-positional-encoding,,,,,Fundamentals,,,,,,,,,,"Jianqiao Zheng, Sameera Ramasinghe, Simon Lucey",zheng2021rethinking,208,,"It is well noted that coordinate based MLPs benefit greatly -- in terms of preserving high-frequency information -- through the encoding of coordinate positions as an array of Fourier features. Hitherto, the rationale for the effectiveness of these positional encodings has been solely studied through a Fourier lens. In this paper, we strive to broaden this understanding by showing that alternative non-Fourier embedding functions can indeed be used for positional encoding. Moreover, we show that their performance is entirely determined by a trade-off between the stable rank of the embedded matrix and the distance preservation between embedded coordinates. We further establish that the now ubiquitous Fourier feature mapping of position is a special case that fulfills these conditions. Consequently, we present a more general theory to analyze positional encoding in terms of shifted basis functions. To this end, we develop the necessary theoretical formulae and empirically verify that our theoretical claims hold in practice. Codes available at https://github.com/osiriszjq/Rethinking-positional-encoding.",,,,,,ARXIV 2021,,,,,,
7/19/2021 21:49,,,Depth-supervised NeRF: Fewer Views and Faster Training for Free,DS-NeRF,ARXIV,7/6/2021,2021,"@article{deng2021dsnerf,
    journal = {arXiv preprint arXiv:2107.02791},
    booktitle = {ArXiv Pre-print},
    author = {Kangle Deng and Andrew Liu and Jun-Yan Zhu and Deva Ramanan},
    title = {Depth-supervised NeRF: Fewer Views and Faster Training for Free},
    year = {2021},
    url = {http://arxiv.org/abs/2107.02791v1},
    entrytype = {article},
    id = {deng2021dsnerf}
}",https://arxiv.org/pdf/2107.02791.pdf,https://www.cs.cmu.edu/~dsnerf/,https://github.com/dunbar12138/DSNeRF,,https://www.youtube.com/watch?v=84LFxCo7ogk,,,"Speed & Computational Efficiency, Sparse Reconstruction",Fourier Feature (NeRF),Density,No,,,,,,,"Kangle Deng, Andrew Liu, Jun-Yan Zhu, Deva Ramanan",deng2021dsnerf,207,,"One common failure mode of Neural Radiance Field (NeRF) models is fitting incorrect geometries when given an insufficient number of input views. We propose DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning neural radiance fields that takes advantage of readily-available depth supervision. Our key insight is that sparse depth supervision can be used to regularize the learned geometry, a crucial component for effectively rendering novel views using NeRF. We exploit the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as ``free"" depth supervision during training: we simply add a loss to ensure that depth rendered along rays that intersect these 3D points is close to the observed depth. We find that DS-NeRF can render more accurate images given fewer training views while training 2-6x faster. With only two training views on real-world images, DS-NeRF significantly outperforms NeRF as well as other sparse-view variants. We show that our loss is compatible with these NeRF models, demonstrating that depth is a cheap and easily digestible supervisory signal. Finally, we show that DS-NeRF supports other types of depth supervision such as scanned depth sensors and RGBD reconstruction outputs.",,,Direct,,,ARXIV 2021,,,,,,
7/19/2021 21:15,,,IREM: High-Resolution Magnetic Resonance Image Reconstruction via Implicit Neural Representation,IREM,MICCAI,6/29/2021,2021,"@article{wu2021irem,
    author = {Qing Wu and Yuwei Li and Lan Xu and Ruiming Feng and Hongjiang Wei and Qing Yang and Boliang Yu and Xiaozhao Liu and Jingyi Yu and Yuyao Zhang},
    title = {IREM: High-Resolution Magnetic Resonance (MR) Image Reconstruction via Implicit Neural Representation},
    year = {2021},
    month = {Jun},
    url = {http://arxiv.org/abs/2106.15097v1},
    entrytype = {article},
    id = {wu2021irem}
}",https://arxiv.org/pdf/2106.15097.pdf,,,,,,,,,,,,,,,,,"Qing Wu, Yuwei Li, Lan Xu, Ruiming Feng, Hongjiang Wei, Qing Yang, Boliang Yu, Xiaozhao Liu, Jingyi Yu, Yuyao Zhang",wu2021irem,206,,"For collecting high-quality high-resolution (HR) MR image, we propose a novel image reconstruction network named IREM, which is trained on multiple low-resolution (LR) MR images and achieve an arbitrary up-sampling rate for HR image reconstruction. In this work, we suppose the desired HR image as an implicit continuous function of the 3D image spatial coordinate and the thick-slice LR images as several sparse discrete samplings of this function. Then the super-resolution (SR) task is to learn the continuous volumetric function from a limited observations using an fully-connected neural network combined with Fourier feature positional encoding. By simply minimizing the error between the network prediction and the acquired LR image intensity across each imaging plane, IREM is trained to represent a continuous model of the observed tissue anatomy. Experimental results indicate that IREM succeeds in representing high frequency image feature, and in real scene data collection, IREM reduces scan time and achieves high-quality high-resolution MR imaging in terms of SNR and local image detail.",,,,,,MICCAI 2021,,,,,,
7/19/2021 21:56,,,Fast Training of Neural Lumigraph Representations using Meta Learning,MetaNLR++,ARXIV,6/28/2021,2021,"@article{bergman2021metanlr++,
    journal = {arXiv preprint arXiv:2106.14942},
    booktitle = {ArXiv Pre-print},
    author = {Alexander W. Bergman and Petr Kellnhofer and Gordon Wetzstein},
    title = {Fast Training of Neural Lumigraph Representations using Meta Learning},
    year = {2021},
    url = {http://arxiv.org/abs/2106.14942v1},
    entrytype = {article},
    id = {bergman2021metanlr++}
}",https://arxiv.org/pdf/2106.14942.pdf,http://www.computationalimaging.org/publications/metanlr/,,,https://www.youtube.com/watch?v=5pBFwyUyW6o,,,"Speed & Computational Efficiency, Generalization, Image-Based Rendering, Local Conditioning, Hypernetwork/Meta-learning",Sinusoidal Activation (SIREN),SDF,No,,,,,,,"Alexander W. Bergman, Petr Kellnhofer, Gordon Wetzstein",bergman2021metanlr++,205,,"Novel view synthesis is a long-standing problem in machine learning and computer vision. Significant progress has recently been made in developing neural scene representations and rendering techniques that synthesize photorealistic images from arbitrary views. These representations, however, are extremely slow to train and often also slow to render. Inspired by neural variants of image-based rendering, we develop a new neural rendering approach with the goal of quickly learning a high-quality representation which can also be rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a unique combination of a neural shape representation and 2D CNN-based image feature extraction, aggregation, and re-projection. To push representation convergence times down to minutes, we leverage meta learning to learn neural shape and image feature priors which accelerate training. The optimized shape and image features can then be extracted using traditional graphics techniques and rendered in real time. We show that MetaNLR++ achieves similar or better novel view synthesis results in a fraction of the time that competing methods require.",,,Direct,,,ARXIV 2021,,,,,,
7/19/2021 21:14,,,Animatable Neural Radiance Fields from Monocular RGB Video,,ICCV,6/25/2021,2021,"@inproceedings{chen2021animatable,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Jianchuan Chen and Ying Zhang and Di Kang and Xuefei Zhe and Linchao Bao and Xu Jia and Huchuan Lu},
    title = {Animatable Neural Radiance Fields from Monocular RGB Videos},
    year = {2021},
    url = {http://arxiv.org/abs/2106.13629v2},
    entrytype = {inproceedings},
    id = {chen2021animatable}
}",https://arxiv.org/pdf/2106.13629.pdf,,,,,,,"Dynamic/Temporal, Human (Body)",,,,,,,,,,"Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao Bao, Huchuan Lu",chen2021animatable,204,,"We present animatable neural radiance fields for detailed human avatar creation from monocular videos. Our approach extends neural radiance fields (NeRF) to the dynamic scenes with human movements via introducing explicit pose-guided deformation while learning the scene representation network. In particular, we estimate the human pose for each frame and learn a constant canonical space for the detailed human template, which enables natural shape deformation from the observation space to the canonical space under the explicit control of the pose parameters. To compensate for inaccurate pose estimation, we introduce the pose refinement strategy that updates the initial pose during the learning process, which not only helps to learn more accurate human reconstruction but also accelerates the convergence. In experiments we show that the proposed approach achieves 1) implicit human geometry and appearance reconstruction with high-quality details, 2) photo-realistic rendering of the human from arbitrary views, and 3) animation of the human with arbitrary poses.",,,,,,ICCV 2021,,,,,,
7/19/2021 22:00,,,HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields,HyperNeRF,ARXIV,6/24/2021,2021,"@article{park2021hypernerf,
    journal = {arXiv preprint arXiv:2106.13228},
    booktitle = {ArXiv Pre-print},
    author = {Keunhong Park and Utkarsh Sinha and Peter Hedman and Jonathan T. Barron and Sofien Bouaziz and Dan B Goldman and Ricardo Martin-Brualla and Steven M. Seitz},
    title = {HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields},
    year = {2021},
    url = {http://arxiv.org/abs/2106.13228v2},
    entrytype = {article},
    id = {park2021hypernerf}
}",https://arxiv.org/pdf/2106.13228.pdf,,,,,,,"Dynamic/Temporal, Fundamentals, Global Conditioning, Local Conditioning",Fourier Feature (NeRF),Density,No,,,,,,,"Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, Steven M. Seitz",park2021hypernerf,203,,"Neural Radiance Fields (NeRF) are able to reconstruct scenes with unprecedented fidelity, and various recent works have extended NeRF to handle dynamic scenes. A common approach to reconstruct such non-rigid scenes is through the use of a learned deformation field mapping from coordinates in each input image into a canonical template coordinate space. However, these deformation-based approaches struggle to model changes in topology, as topological changes require a discontinuity in the deformation field, but these deformation fields are necessarily continuous. We address this limitation by lifting NeRFs into a higher dimensional space, and by representing the 5D radiance field corresponding to each individual input image as a slice through this ""hyper-space"". Our method is inspired by level set methods, which model the evolution of surfaces as slices through a higher dimensional surface. We evaluate our method on two tasks: (i) interpolating smoothly between ""moments"", i.e., configurations of the scene, seen in the input images while maintaining visual plausibility, and (ii) novel-view synthesis at fixed moments. We show that our method, which we dub HyperNeRF, outperforms existing methods on both tasks by significant margins. Compared to Nerfies, HyperNeRF reduces average error rates by 8.6% for interpolation and 8.8% for novel-view synthesis, as measured by LPIPS.",,,Direct,,,ARXIV 2021,,,,,,
7/19/2021 17:55,,,Real-time Neural Radiance Caching for Path Tracing,,SIGGRAPH,6/23/2021,2021,"@article{muller2021realtime,
    publisher = {Association for Computing Machinery},
    journal = {ACM Transactions on Graphics (TOG)},
    author = {Thomas Muller and Fabrice Rousselle and Jan Novak and Alexander Keller},
    title = {Real-time Neural Radiance Caching for Path Tracing},
    doi = {10.1145/3450626.3459812},
    year = {2021},
    url = {http://arxiv.org/abs/2106.12372v2},
    entrytype = {article},
    id = {muller2021realtime}
}",https://arxiv.org/pdf/2106.12372.pdf,https://tom94.net/,,,,,,"Speed & Computational Efficiency, Material/Lighting Estimation",,,,,,,,,,"Thomas Müller, Fabrice Rousselle, Jan Novák, Alexander Keller",muller2021realtime,202,,"We present a real-time neural radiance caching method for path-traced global illumination. Our system is designed to handle fully dynamic scenes, and makes no assumptions about the lighting, geometry, and materials. The data-driven nature of our approach sidesteps many difficulties of caching algorithms, such as locating, interpolating, and updating cache points. Since pretraining neural networks to handle novel, dynamic scenes is a formidable generalization challenge, we do away with pretraining and instead achieve generalization via adaptation, i.e. we opt for training the radiance cache while rendering. We employ self-training to provide low-noise training targets and simulate infinite-bounce transport by merely iterating few-bounce training updates. The updates and cache queries incur a mild overhead -- about 2.6ms on full HD resolution -- thanks to a streaming implementation of the neural network that fully exploits modern hardware. We demonstrate significant noise reduction at the cost of little induced bias, and report state-of-the-art, real-time performance on a number of challenging scenarios.",,,,,,SIGGRAPH 2021,,,,,,
6/29/2021 17:02,,,Volume Rendering of Neural Implicit Surfaces,VolSDF,NeurIPS,6/22/2021,2021,"@inproceedings{yariv2021volsdf,
    url = {http://arxiv.org/abs/2106.12052v1},
    year = {2021},
    title = {Volume Rendering of Neural Implicit Surfaces},
    author = {Lior Yariv and Jiatao Gu and Yoni Kasten and Yaron Lipman},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    publisher = {Curran Associates, Inc.},
    entrytype = {inproceedings},
    id = {yariv2021volsdf}
}",https://arxiv.org/pdf/2106.12052.pdf,,,,,,,"Fundamentals, Hybrid Geometry Representation",None,SDF/Density Hybrid,No,,,,,,,"Lior Yariv, Jiatao Gu, Yoni Kasten, Yaron Lipman",yariv2021volsdf,201,,"Neural volume rendering became increasingly popular recently due to its success in synthesizing novel views of a scene from a sparse set of input images. So far, the geometry learned by neural volume rendering techniques was modeled using a generic density function. Furthermore, the geometry itself was extracted using an arbitrary level set of the density function leading to a noisy, often low fidelity reconstruction. The goal of this paper is to improve geometry representation and reconstruction in neural volume rendering. We achieve that by modeling the volume density as a function of the geometry. This is in contrast to previous work modeling the geometry as a function of the volume density. In more detail, we define the volume density function as Laplace's cumulative distribution function (CDF) applied to a signed distance function (SDF) representation. This simple density representation has three benefits: (i) it provides a useful inductive bias to the geometry learned in the neural volume rendering process; (ii) it facilitates a bound on the opacity approximation error, leading to an accurate sampling of the viewing ray. Accurate sampling is important to provide a precise coupling of geometry and radiance; and (iii) it allows efficient unsupervised disentanglement of shape and appearance in volume rendering. Applying this new density representation to challenging scene multiview datasets produced high quality geometry reconstructions, outperforming relevant baselines. Furthermore, switching shape and appearance between scenes is possible due to the disentanglement of the two.",,,,,,NeurIPS 2021,,,,,,
7/19/2021 21:50,,,MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images,MetaAvatar,IJCAI,6/22/2021,2021,"@inproceedings{wang2021metaavatar,
    publisher = {International Joint Conferences on Artificial Intelligence Organization},
    booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},
    author = {Shaofei Wang and Marko Mihajlovic and Qianli Ma and Andreas Geiger and Siyu Tang},
    title = {MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images},
    year = {2021},
    url = {http://arxiv.org/abs/2106.11944v1},
    entrytype = {inproceedings},
    id = {wang2021metaavatar}
}",https://arxiv.org/pdf/2106.11944.pdf,https://neuralbodies.github.io/metavatar/,,,,,"https://www.youtube.com/watch?v=SXv1sBRwm4U, https://www.youtube.com/watch?v=eLZH-h1VOm8, https://www.youtube.com/watch?v=MMQStRgWJUE","Human (Body), Hypernetwork/Meta-learning",Sinusoidal Activation (SIREN),SDF,"Yes, geometry only",,,,,,,"Shaofei Wang, Marko Mihajlovic, Qianli Ma, Andreas Geiger, Siyu Tang",wang2021metaavatar,200,,"In this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a Hypernetwork/Meta-learning that predicts the parameters of neural SDFs. The Hypernetwork/Meta-learning is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune, compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned Hypernetwork/Meta-learning is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames.",,,Direct,,,IJCAI 2021,,,,,,
10/2/2021 10:56,,,NeRF-Tex: Neural Reflectance Field Textures,NeRF-Tex,EGSR,6/22/2021,2021,"@article{baatz2021nerftex,
    journal = {Computer Graphics Forum},
    title = {NeRF-Tex: Neural Reflectance Field Textures},
    author = {Hendrik Baatz and Jonathan Granskog and Marios Papas and Fabrice Rousselle and Jan Nov{\'a}k},
    year = {2021},
    publisher = {The Eurographics Association and John Wiley & Sons Ltd.},
    entrytype = {article},
    id = {baatz2021nerftex}
}",https://d1qx31qr3h6wln.cloudfront.net/publications/NeRFTex.pdf,,,,https://d1qx31qr3h6wln.cloudfront.net/publications/NeRFTex_video.mp4,,,"Material/Lighting Estimation, Global Conditioning, Hybrid Geometry Representation",Fourier Feature (NeRF),Density,,,,,,,,"Hendrik Baatz, Jonathan Granskog, Marios Papas, Fabrice Rousselle, Jan Nov{\'a}k",baatz2021nerftex,199,,"We investigate the use of neural fields for modeling diverse mesoscale structures, such as fur, fabric, and grass. Instead of using classical graphics primitives to model the structure, we propose to employ a versatile volumetric primitive represented by a neural reflectance field (NeRF-Tex), which jointly models the geometry of the material and its response to lighting. The NeRF-Tex primitive can be instantiated over a base mesh to''texture''it with the desired meso and microscale appearance. We condition the reflectance field on user-defined",,,,,,EGSR 2021,,,,,,
7/19/2021 21:19,,,Moving in a 360 World: Synthesizing Panoramic Parallaxes from a Single Panorama,OmniNeRF,ARXIV,6/21/2021,2021,"@article{hsu2021omninerf,
    journal = {arXiv preprint arXiv:2106.10859},
    booktitle = {ArXiv Pre-print},
    author = {Ching-Yu Hsu and Cheng Sun and Hwann-Tzong Chen},
    title = {Moving in a 360 World: Synthesizing Panoramic Parallaxes from a Single Panorama},
    year = {2021},
    url = {http://arxiv.org/abs/2106.10859v1},
    entrytype = {article},
    id = {hsu2021omninerf}
}",https://arxiv.org/pdf/2106.10859.pdf,,,,,,,Hybrid Geometry Representation,Fourier Feature (NeRF),Density,No,,,,,,,"Ching-Yu Hsu, Cheng Sun, Hwann-Tzong Chen",hsu2021omninerf,198,,"We present Omnidirectional Neural Radiance Fields (OmniNeRF), the first method to the application of parallax-enabled novel panoramic view synthesis. Recent works for novel view synthesis focus on perspective images with limited field-of-view and require sufficient pictures captured in a specific condition. Conversely, OmniNeRF can generate panorama images for unknown viewpoints given a single equirectangular image as training data. To this end, we propose to augment the single RGB-D panorama by projecting back and forth between a 3D world and different 2D panoramic coordinates at different virtual camera positions. By doing so, we are able to optimize an Omnidirectional Neural Radiance Field with visible pixels collecting from omnidirectional viewing angles at a fixed center for the estimation of new viewing angles from varying camera positions. As a result, the proposed OmniNeRF achieves convincing renderings of novel panoramic views that exhibit the parallax effect. We showcase the effectiveness of each of our proposals on both synthetic and real-world datasets.",,,Direct,,,ARXIV 2021,,,,,,
7/19/2021 21:16,,,NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction,NeuS,IJCAI,6/20/2021,2021,"@inproceedings{wang2021neus,
    publisher = {International Joint Conferences on Artificial Intelligence Organization},
    booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},
    author = {Peng Wang and Lingjie Liu and Yuan Liu and Christian Theobalt and Taku Komura and Wenping Wang},
    title = {NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction},
    year = {2021},
    url = {http://arxiv.org/abs/2106.10689v1},
    entrytype = {inproceedings},
    id = {wang2021neus}
}",https://arxiv.org/pdf/2106.10689.pdf,https://lingjie0206.github.io/papers/NeuS/index.htm,https://github.com/Totoro97/NeuS,https://github.com/Totoro97/NeuS,,,,"Sampling, Hybrid Geometry Representation",,Other,,,,,,,,"Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, Wenping Wang",wang2021neus,197,,"We present a novel neural surface reconstruction method, called NeuS, for reconstructing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as DVR and IDR, require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as NeRF and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formulation that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion.",,,,,,IJCAI 2021,,,,,,
7/19/2021 22:06,,,Unsupervised Video Prediction from a Single Frame by Estimating 3D Dynamic Scene Structure,,ARXIV,6/16/2021,2021,"@article{henderson2021unsupervised,
    journal = {arXiv preprint arXiv:2106.09051},
    booktitle = {ArXiv Pre-print},
    author = {Paul Henderson and Christoph H. Lampert and Bernd Bickel},
    title = {Unsupervised Video Prediction from a Single Frame by Estimating 3D Dynamic Scene Structure},
    year = {2021},
    url = {http://arxiv.org/abs/2106.09051v1},
    entrytype = {article},
    id = {henderson2021unsupervised}
}",https://arxiv.org/pdf/2106.09051.pdf,http://pmh47.net/vipl4s/,,,,,,"Dynamic/Temporal, Global Conditioning",,,,,,,,,,"Paul Henderson, Christoph H. Lampert, Bernd Bickel",henderson2021unsupervised,196,,"Our goal in this work is to generate realistic videos given just one initial frame as input. Existing unsupervised approaches to this task do not consider the fact that a video typically shows a 3D environment, and that this should remain coherent from frame to frame even as the camera and objects move. We address this by developing a model that first estimates the latent 3D structure of the scene, including the segmentation of any moving objects. It then predicts future frames by simulating the object and camera dynamics, and rendering the resulting views. Importantly, it is trained end-to-end using only the unsupervised objective of predicting future frames, without any 3D information nor segmentation annotations. Experiments on two challenging datasets of natural videos show that our model can estimate 3D structure and motion segmentation from a single frame, and hence generate plausible and varied predictions.",,,,,,ARXIV 2021,,,,,,
9/18/2021 11:01,,,Implicit-PDF: Non-Parametric Representation of Probability Distributions on the Rotation Manifold,Implicit-PDF,ICML,6/10/2021,2021,"@inproceedings{murphy2021implicitpdf,
    publisher = {PMLR},
    booktitle = {International Conference on Machine Learning (ICML)},
    author = {Kieran Murphy and Carlos Esteves and Varun Jampani and Srikumar Ramalingam and Ameesh Makadia},
    title = {Implicit-PDF: Non-Parametric Representation of Probability Distributions on the Rotation Manifold},
    year = {2021},
    url = {http://arxiv.org/abs/2106.05965v1},
    entrytype = {inproceedings},
    id = {murphy2021implicitpdf}
}",https://arxiv.org/pdf/2106.05965.pdf,https://implicit-pdf.github.io/,https://github.com/google-research/google-research/tree/master/implicit_pdf,https://www.tensorflow.org/datasets/catalog/symmetric_solids,https://www.youtube.com/watch?v=Y-MlRRy0xJA,,,"Camera Parameter Estimation, Fundamentals",,,,,,,,,,"Kieran Murphy, Carlos Esteves, Varun Jampani, Srikumar Ramalingam, Ameesh Makadia",murphy2021implicitpdf,195,,"Single image pose estimation is a fundamental problem in many vision and robotics tasks, and existing deep learning approaches suffer by not completely modeling and handling: i) uncertainty about the predictions, and ii) symmetric objects with multiple (sometimes infinite) correct poses. To this end, we introduce a method to estimate arbitrary, non-parametric distributions on SO(3). Our key idea is to represent the distributions implicitly, with a neural network that estimates the probability given the input image and a candidate pose. Grid sampling or gradient ascent can be used to find the most likely pose, but it is also possible to evaluate the probability at any pose, enabling reasoning about symmetries and uncertainty. This is the most general way of representing distributions on manifolds, and to showcase the rich expressive power, we introduce a dataset of challenging symmetric and nearly-symmetric objects. We require no supervision on pose uncertainty -- the model trains only with a single pose per example. Nonetheless, our implicit model is highly expressive to handle complex distributions over 3D poses, while still obtaining accurate pose estimation on standard non-ambiguous environments, achieving state-of-the-art performance on Pascal3D+ and ModelNet10-SO(3) benchmarks.",,,,,,ICML 2021,,,,,,
6/29/2021 15:35,,,Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields,IDF,IJCAI,6/9/2021,2021,"@inproceedings{yifan2021idf,
    publisher = {International Joint Conferences on Artificial Intelligence Organization},
    booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},
    author = {Wang Yifan and Lukas Rahmann and Olga Sorkine-Hornung},
    title = {Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields},
    year = {2021},
    url = {http://arxiv.org/abs/2106.05187v2},
    entrytype = {inproceedings},
    id = {yifan2021idf}
}",https://arxiv.org/pdf/2106.05187.pdf,https://yifita.github.io/publication/idf/,https://github.com/yifita/idf,,https://www.youtube.com/watch?v=fl4Rje8HM3I,,,"Fundamentals, Data-Driven Method, Coarse-to-Fine, Hybrid Geometry Representation",Sinusoidal Activation (SIREN),SDF,"Yes, geometry only",,,,,,,"Wang Yifan, Lukas Rahmann, Olga Sorkine-Hornung",yifan2021idf,194,,"We present implicit displacement fields, a novel representation for detailed 3D geometry. Inspired by a classic surface deformation technique, displacement mapping, our method represents a complex surface as a smooth base surface plus a displacement along the base's normal directions, resulting in a frequency-based shape decomposition, where the high frequency signal is constrained geometrically by the low frequency signal. Importantly, this disentanglement is unsupervised thanks to a tailored architectural design that has an innate frequency hierarchy by construction. We explore implicit displacement field surface reconstruction and detail transfer and demonstrate superior representational power, training stability and generalizability.",,,,,,IJCAI 2021,,,,,,
7/19/2021 21:51,,,MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in Stationary Monocular Cameras,MoCo-Flow,ARXIV,6/8/2021,2021,"@article{chen2021mocoflow,
    journal = {arXiv preprint arXiv:2106.04477},
    booktitle = {ArXiv Pre-print},
    author = {Xuelin Chen and Weiyu Li and Daniel Cohen-Or and Niloy J. Mitra and Baoquan Chen},
    title = {MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in Stationary Monocular Cameras},
    year = {2021},
    url = {http://arxiv.org/abs/2106.04477v1},
    entrytype = {article},
    id = {chen2021mocoflow}
}",https://arxiv.org/pdf/2106.04477.pdf,https://wyysf-98.github.io/MoCo_Flow/,Coming soon,Coming soon,,,,"Human (Body), Coarse-to-Fine",Fourier Feature (NeRF),Density,No,,,,,,,"Xuelin Chen, Weiyu Li, Daniel Cohen-Or, Niloy J. Mitra, Baoquan Chen",chen2021mocoflow,193,,"Synthesizing novel views of dynamic humans from stationary monocular cameras is a popular scenario. This is particularly attractive as it does not require static scenes, controlled environments, or specialized hardware. In contrast to techniques that exploit multi-view observations to constrain the modeling, given a single fixed viewpoint only, the problem of modeling the dynamic scene is significantly more under-constrained and ill-posed. In this paper, we introduce Neural Motion Consensus Flow (MoCo-Flow), a representation that models the dynamic scene using a 4D continuous time-variant function. The proposed representation is learned by an optimization which models a dynamic scene that minimizes the error of rendering all observation images. At the heart of our work lies a novel optimization formulation, which is constrained by a motion consensus regularization on the motion flow. We extensively evaluate MoCo-Flow on several datasets that contain human motions of varying complexity, and compare, both qualitatively and quantitatively, to several baseline methods and variants of our methods. Pretrained model, code, and data will be released for research purposes upon paper acceptance.",,,Direct,,,ARXIV 2021,,,,,,
7/28/2021 23:02,,,DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Rendering,DoubleField,ARXIV,6/8/2021,2021,"@article{shao2021doublefield,
    journal = {arXiv preprint arXiv:2106.03798},
    booktitle = {ArXiv Pre-print},
    author = {Ruizhi Shao and Hongwen Zhang and He Zhang and Yanpei Cao and Tao Yu and Yebin Liu},
    title = {DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Rendering},
    year = {2021},
    url = {http://arxiv.org/abs/2106.03798v2},
    entrytype = {article},
    id = {shao2021doublefield}
}",https://arxiv.org/pdf/2106.03798.pdf,http://www.liuyebin.com/dbfield/dbfield.html,Coming soon,,,,http://www.liuyebin.com/dbfield/assets/supp2.mp4,"Speed & Computational Efficiency, Human (Body), Sparse Reconstruction, Local Conditioning",Fourier Feature (NeRF),Density,No,,,,,,,"Ruizhi Shao, Hongwen Zhang, He Zhang, Yanpei Cao, Tao Yu, Yebin Liu",shao2021doublefield,192,,"We introduce DoubleField, a novel representation combining the merits of both surface field and radiance field for high-fidelity human rendering. Within DoubleField, the surface field and radiance field are associated together by a shared feature embedding and a surface-guided sampling strategy. In this way, DoubleField has a continuous but disentangled learning space for geometry and appearance modeling, which supports fast training, inference, and finetuning. To achieve high-fidelity free-viewpoint rendering, DoubleField is further augmented to leverage ultra-high-resolution inputs, where a view-to-view transformer and a transfer learning scheme are introduced for more efficient learning and finetuning from sparse-view inputs at original resolutions. The efficacy of DoubleField is validated by the quantitative evaluations on several datasets and the qualitative results in a real-world sparse multi-view system, showing its superior capability for photo-realistic free-viewpoint human rendering. For code and demo video, please refer to our project page: http://www.liuyebin.com/dbfield/dbfield.html.",,,Direct,,,ARXIV 2021,,,,,,
6/29/2021 15:23,,,Deep Medial Fields,DMF,ARXIV,6/7/2021,2021,"@article{rebain2021dmf,
    journal = {arXiv preprint arXiv:2106.03804},
    booktitle = {ArXiv Pre-print},
    author = {Daniel Rebain and Ke Li and Vincent Sitzmann and Soroosh Yazdani and Kwang Moo Yi and Andrea Tagliasacchi},
    title = {Deep Medial Fields},
    year = {2021},
    url = {http://arxiv.org/abs/2106.03804v1},
    entrytype = {article},
    id = {rebain2021dmf}
}",https://arxiv.org/pdf/2106.03804.pdf,,,,,,,"Fundamentals, Hybrid Geometry Representation",Fourier Feature (NeRF),Medial Field,"Yes, geometry only",,,,,,,"Daniel Rebain, Ke Li, Vincent Sitzmann, Soroosh Yazdani, Kwang Moo Yi, Andrea Tagliasacchi",rebain2021dmf,191,,"Implicit representations of geometry, such as occupancy fields or signed distance fields (SDF), have recently re-gained popularity in encoding 3D solid shape in a functional form. In this work, we introduce medial fields: a field function derived from the medial axis transform (MAT) that makes available information about the underlying 3D geometry that is immediately useful for a number of downstream tasks. In particular, the medial field encodes the local thickness of a 3D shape, and enables O(1) projection of a query point onto the medial axis. To construct the medial field we require nothing but the SDF of the shape itself, thus allowing its straightforward incorporation in any application that relies on signed distance fields. Working in unison with the O(1) surface projection supported by the SDF, the medial field opens the door for an entirely new set of efficient, shape-aware operations on implicit representations. We present three such applications, including a modification to sphere tracing that renders implicit representations with better convergence properties, a fast construction method for memory-efficient rigid-body collision proxies, and an efficient approximation of ambient occlusion that remains stable with respect to viewpoint variations.",,,,,,ARXIV 2021,,,,,,
7/19/2021 21:38,,,Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering,LFNs,NeurIPS,6/4/2021,2021,"@inproceedings{sitzmann2021lfns,
    booktitle = {Advances in Neural Information Processing (NeurIPS)},
    author = {Vincent Sitzmann and Semon Rezchikov and William T. Freeman and Joshua B. Tenenbaum and Fredo Durand},
    title = {Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering},
    year = {2021},
    url = {http://arxiv.org/abs/2106.02634v1},
    entrytype = {inproceedings},
    id = {sitzmann2021lfns}
}",https://arxiv.org/pdf/2106.02634.pdf,https://vsitzmann.github.io/lfns/,Coming soon,Coming soon,https://www.youtube.com/watch?v=x3sSreTNFw4,,,"Speed & Computational Efficiency, Generalization, Sampling, Global Conditioning, Hypernetwork/Meta-learning, Hybrid Geometry Representation",,,,,,,,,,"Vincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B. Tenenbaum, Fredo Durand",sitzmann2021lfns,190,,"Inferring representations of 3D scenes from 2D observations is a fundamental problem of computer graphics, computer vision, and artificial intelligence. Emerging 3D-structured neural scene representations are a promising approach to 3D scene understanding. In this work, we propose a novel neural scene representation, Light Field Networks or LFNs, which represent both geometry and appearance of the underlying 3D scene in a 360-degree, four-dimensional light field parameterized via a neural implicit representation. Rendering a ray from an LFN requires only a *single* network evaluation, as opposed to hundreds of evaluations per ray for ray-marching or volumetric based renderers in 3D-structured neural scene representations. In the setting of simple scenes, we leverage meta-learning to learn a prior over LFNs that enables multi-view consistent light field reconstruction from as little as a single image observation. This results in dramatic reductions in time and memory complexity, and enables real-time rendering. The cost of storing a 360-degree light field via an LFN is two orders of magnitude lower than conventional methods such as the Lumigraph. Utilizing the analytical differentiability of neural implicit representations and a novel parameterization of light space, we further demonstrate the extraction of sparse depth maps from LFNs.",,,,,,IJCAI 2021,,,,,,
7/19/2021 21:33,,,NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination,NeRFactor,ARXIV,6/3/2021,2021,"@article{zhang2021nerfactor,
    author = {Zhang, Xiuming and Srinivasan, Pratul P. and Deng, Boyang and Debevec, Paul and Freeman, William T. and Barron, Jonathan T.},
    title = {NeRFactor: Neural Factorization of Shape and Reflectance under an Unknown Illumination},
    year = {2021},
    issue_date = {December 2021},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {40},
    number = {6},
    issn = {0730-0301},
    url = {https://doi.org/10.1145/3478513.3480496},
    doi = {10.1145/3478513.3480496},
    journal = {ACM Trans. Graph.},
    month = {dec},
    articleno = {237},
    numpages = {18}
}",https://arxiv.org/pdf/2106.01970.pdf,https://people.csail.mit.edu/xiuming/projects/nerfactor/,https://github.com/google/nerfactor,,https://www.youtube.com/watch?v=UUVSPJlwhPg,,,"Editable, Material/Lighting Estimation, Data-Driven Method",Fourier Feature (NeRF),Density,No,,,,,,,"Xiuming Zhang, Pratul P. Srinivasan, Boyang Deng, Paul Debevec, William T. Freeman, Jonathan T. Barron",zhang2021nerfactor,189,,"We address the problem of recovering the shape and spatially-varying reflectance of an object from posed multi-view images of the object illuminated by one unknown lighting condition. This enables the rendering of novel views of the object under arbitrary environment lighting and editing of the object's material properties. The key to our approach, which we call Neural Radiance Factorization (NeRFactor), is to distill the volumetric geometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object into a surface representation and then jointly refine the geometry while solving for the spatially-varying reflectance and the environment lighting. Specifically, NeRFactor recovers 3D neural fields of surface normals, light visibility, albedo, and Bidirectional Reflectance Distribution Functions (BRDFs) without any supervision, using only a re-rendering loss, simple smoothness priors, and a data-driven BRDF prior learned from real-world BRDF measurements. By explicitly modeling light visibility, NeRFactor is able to separate shadows from albedo and synthesize realistic soft or hard shadows under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D models for free-viewpoint relighting in this challenging and underconstrained capture setup for both synthetic and real scenes. Qualitative and quantitative experiments show that NeRFactor outperforms classic and deep learning-based state of the art across various tasks. Our code and data are available at people.csail.mit.edu/xiuming/projects/nerfactor/.",,,Direct,,,ARXIV 2021,,,,,,
7/19/2021 21:42,,,Spline Positional Encoding for Learning 3D Implicit Signed Distance Fields,,IJCAI,6/3/2021,2021,"@inproceedings{wang2021spline,
    publisher = {International Joint Conferences on Artificial Intelligence Organization},
    booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},
    author = {Peng-Shuai Wang and Yang Liu and Yu-Qi Yang and Xin Tong},
    title = {Spline Positional Encoding for Learning 3D Implicit Signed Distance Fields},
    year = {2021},
    url = {http://arxiv.org/abs/2106.01553v1},
    entrytype = {inproceedings},
    id = {wang2021spline}
}",https://arxiv.org/pdf/2106.01553.pdf,,,,,,,"Fundamentals, Positional Encoding",Other,SDF,"Yes, geometry only",,,,,,,"Peng-Shuai Wang, Yang Liu, Yu-Qi Yang, Xin Tong",wang2021spline,188,,"Multilayer perceptrons (MLPs) have been successfully used to represent 3D shapes implicitly and compactly, by mapping 3D coordinates to the corresponding signed distance values or occupancy values. In this paper, we propose a novel positional encoding scheme, called Spline Positional Encoding, to map the input coordinates to a high dimensional space before passing them to MLPs, for helping to recover 3D signed distance fields with fine-scale geometric details from unorganized 3D point clouds. We verified the superiority of our approach over other positional encoding schemes on tasks of 3D shape reconstruction from input point clouds and shape space learning. The efficacy of our approach extended to image reconstruction is also demonstrated and evaluated.",,,Direct,,,IJCAI 2021,,,,,,
7/19/2021 21:46,,,Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control,NA,SIGGRAPH,6/3/2021,2021,"@article{liu2021na,
    publisher = {Association for Computing Machinery},
    journal = {ACM Transactions on Graphics (TOG)},
    author = {Lingjie Liu and Marc Habermann and Viktor Rudnev and Kripasindhu Sarkar and Jiatao Gu and Christian Theobalt},
    title = {Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control},
    year = {2021},
    url = {http://arxiv.org/abs/2106.02019v1},
    entrytype = {article},
    id = {liu2021na}
}",https://arxiv.org/pdf/2106.02019.pdf,http://gvv.mpi-inf.mpg.de/projects/NeuralActor/,Coming soon,Coming soon,http://gvv.mpi-inf.mpg.de/projects/NeuralActor/mp4/main_video_arxiv3.mp4,,,"Human (Body), Editable, Local Conditioning",,Density,No,,,,,,,"Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, Christian Theobalt",liu2021na,187,,"We propose Neural Actor (NA), a new method for high-quality synthesis of humans from arbitrary viewpoints and under arbitrary controllable poses. Our method is built upon recent neural scene representation and rendering works which learn representations of geometry and appearance from only 2D images. While existing works demonstrated compelling rendering of static scenes and playback of dynamic scenes, photo-realistic reconstruction and rendering of humans with neural implicit methods, in particular under user-controlled novel poses, is still difficult. To address this problem, we utilize a coarse body model as the proxy to unwarp the surrounding 3D space into a canonical pose. A neural radiance field learns pose-dependent geometric deformations and pose- and view-dependent appearance effects in the canonical space from multi-view video input. To synthesize novel views of high fidelity dynamic geometry and appearance, we leverage 2D texture maps defined on the body model as latent variables for predicting residual deformations and the dynamic appearance. Experiments demonstrate that our method achieves better quality than the state-of-the-arts on playback as well as novel pose synthesis, and can even generalize well to new poses that starkly differ from the training poses. Furthermore, our method also supports body shape control of the synthesized results.",,,Direct,,,SIGGRAPH 2021,,,,,,
7/19/2021 21:34,,,Stylizing 3D Scene via Implicit Representation and Hypernetwork/Meta-learning,,ARXIV,5/27/2021,2021,"@article{chiang2021stylizing,
    journal = {arXiv preprint arXiv:2105.13016},
    booktitle = {ArXiv Pre-print},
    author = {Pei-Ze Chiang and Meng-Shiun Tsai and Hung-Yu Tseng and Wei-sheng Lai and Wei-Chen Chiu},
    title = {Stylizing 3D Scene via Implicit Representation and Hypernetwork/Meta-learning},
    year = {2021},
    url = {http://arxiv.org/abs/2105.13016v2},
    entrytype = {article},
    id = {chiang2021stylizing}
}",https://arxiv.org/pdf/2105.13016.pdf,,,,https://www.youtube.com/watch?v=MJqcI40sXhk,,,"Editable, Data-Driven Method, Hypernetwork/Meta-learning",Fourier Feature (NeRF),Density,No,,,,,,,"Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-sheng Lai, Wei-Chen Chiu",chiang2021stylizing,186,,"In this work, we aim to address the 3D scene stylization problem - generating stylized images of the scene at arbitrary novel view angles. A straightforward solution is to combine existing novel view synthesis and image/video style transfer approaches, which often leads to blurry results or inconsistent appearance. Inspired by the high quality results of the neural radiance fields (NeRF) method, we propose a joint framework to directly render novel views with the desired style. Our framework consists of two components: an implicit representation of the 3D scene with the neural radiance field model, and a Hypernetwork/Meta-learning to transfer the style information into the scene representation. In particular, our implicit representation model disentangles the scene into the geometry and appearance branches, and the Hypernetwork/Meta-learning learns to predict the parameters of the appearance branch from the reference style image. To alleviate the training difficulties and memory burden, we propose a two-stage training procedure and a patch sub-sampling approach to optimize the style and content losses with the neural radiance field model. After optimization, our model is able to render consistent novel views at arbitrary view angles with arbitrary style. Both quantitative evaluation and human subject study have demonstrated that the proposed method generates faithful stylization results with consistent appearance across different views.",,,Direct,,,ARXIV 2021,,,,,,
7/19/2021 21:41,,,Geodesy of irregular small bodies via neural density fields: geodesyNets,geodesyNets,ARXIV,5/27/2021,2021,"@article{izzo2021geodesynets,
    journal = {arXiv preprint arXiv:2105.13031},
    booktitle = {ArXiv Pre-print},
    author = {Dario Izzo and Pablo Gomez},
    title = {Geodesy of irregular small bodies via neural density fields: geodesyNets},
    year = {2021},
    url = {http://arxiv.org/abs/2105.13031v1},
    entrytype = {article},
    id = {izzo2021geodesynets}
}",https://arxiv.org/pdf/2105.13031.pdf,https://github.com/darioizzo/geodesynets,,,,,,"Alternative Imaging, Science & Engineering",,,,,,,,,,"Dario Izzo, Pablo Gómez",izzo2021geodesynets,185,,"We present a novel approach based on artificial neural networks, so-called geodesyNets, and present compelling evidence of their ability to serve as accurate geodetic models of highly irregular bodies using minimal prior information on the body. The approach does not rely on the body shape information but, if available, can harness it. GeodesyNets learn a three-dimensional, differentiable, function representing the body density, which we call neural density field. The body shape, as well as other geodetic properties, can easily be recovered. We investigate six different shapes including the bodies 101955 Bennu, 67P Churyumov-Gerasimenko, 433 Eros and 25143 Itokawa for which shape models developed during close proximity surveys are available. Both heterogeneous and homogeneous mass distributions are considered. The gravitational acceleration computed from the trained geodesyNets models, as well as the inferred body shape, show great accuracy in all cases with a relative error on the predicted acceleration smaller than 1\% even close to the asteroid surface. When the body shape information is available, geodesyNets can seamlessly exploit it and be trained to represent a high-fidelity neural density field able to give insights into the internal structure of the body. This work introduces a new unexplored approach to geodesy, adding a powerful tool to consolidated ones based on spherical harmonics, mascon models and polyhedral gravity.",,,,,,ARXIV 2021,,,,,,
7/19/2021 21:36,,,Neural Radiosity,,ARXIV,5/26/2021,2021,"@article{hadadan2021neural,
    journal = {arXiv preprint arXiv:2105.12319},
    booktitle = {ArXiv Pre-print},
    author = {Saeed Hadadan and Shuhong Chen and Matthias Zwicker},
    title = {Neural Radiosity},
    year = {2021},
    url = {http://arxiv.org/abs/2105.12319v1},
    entrytype = {article},
    id = {hadadan2021neural}
}",https://arxiv.org/pdf/2105.12319.pdf,,,,,,,"Material/Lighting Estimation, Voxel Grid, Local Conditioning",,,,,,,,,,"Saeed Hadadan, Shuhong Chen, Matthias Zwicker",hadadan2021neural,184,,"We introduce Neural Radiosity, an algorithm to solve the rendering equation by minimizing the norm of its residual similar as in traditional radiosity techniques. Traditional basis functions used in radiosity techniques, such as piecewise polynomials or meshless basis functions are typically limited to representing isotropic scattering from diffuse surfaces. Instead, we propose to leverage neural networks to represent the full four-dimensional radiance distribution, directly optimizing network parameters to minimize the norm of the residual. Our approach decouples solving the rendering equation from rendering (perspective) images similar as in traditional radiosity techniques, and allows us to efficiently synthesize arbitrary views of a scene. In addition, we propose a network architecture using geometric learnable features that improves convergence of our solver compared to previous techniques. Our approach leads to an algorithm that is simple to implement, and we demonstrate its effectiveness on a variety of scenes with non-diffuse surfaces.",,,,,,ARXIV 2021,,,,,,
5/23/2021 12:29,,,Recursive-NeRF: An Efficient and Dynamically Growing NeRF,Recursive-NeRF,ARXIV,5/19/2021,2021,"@article{yang2021recursivenerf,
    journal = {arXiv preprint arXiv:2105.09103},
    booktitle = {ArXiv Pre-print},
    author = {Guo-Wei Yang and Wen-Yang Zhou and Hao-Yang Peng and Dun Liang and Tai-Jiang Mu and Shi-Min Hu},
    title = {Recursive-NeRF: An Efficient and Dynamically Growing NeRF},
    year = {2021},
    url = {http://arxiv.org/abs/2105.09103v1},
    entrytype = {article},
    id = {yang2021recursivenerf}
}",https://arxiv.org/pdf/2105.09103.pdf,,https://github.com/Gword/Recursive-NeRF,,,,,"Sampling, Coarse-to-Fine",Fourier Feature (NeRF),Density,,,,,,,,"Guo-Wei Yang, Wen-Yang Zhou, Hao-Yang Peng, Dun Liang, Tai-Jiang Mu, Shi-Min Hu",yang2021recursivenerf,183,,"View synthesis methods using implicit continuous shape representations learned from a set of images, such as the Neural Radiance Field (NeRF) method, have gained increasing attention due to their high quality imagery and scalability to high resolution. However, the heavy computation required by its volumetric approach prevents NeRF from being useful in practice; minutes are taken to render a single image of a few megapixels. Now, an image of a scene can be rendered in a level-of-detail manner, so we posit that a complicated region of the scene should be represented by a large neural network while a small neural network is capable of encoding a simple region, enabling a balance between efficiency and quality. Recursive-NeRF is our embodiment of this idea, providing an efficient and adaptive rendering and training approach for NeRF. The core of Recursive-NeRF learns uncertainties for query coordinates, representing the quality of the predicted color and volumetric intensity at each level. Only query coordinates with high uncertainties are forwarded to the next level to a bigger neural network with a more powerful representational capability. The final rendered image is a composition of results from neural networks of all levels. Our evaluation on three public datasets shows that Recursive-NeRF is more efficient than NeRF while providing state-of-the-art quality. The code will be available at https://github.com/Gword/Recursive-NeRF.",,,,,,ARXIV 2021,,,,,,
5/23/2021 11:24,,,NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field,NeuLF,ARXIV,5/15/2021,2021,"@article{liu2021neulf,
    journal = {arXiv preprint arXiv:2105.07112},
    booktitle = {ArXiv Pre-print},
    author = {Celong Liu and Zhong Li and Junsong Yuan and Yi Xu},
    title = {NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field},
    year = {2021},
    url = {http://arxiv.org/abs/2105.07112v4},
    entrytype = {article},
    id = {liu2021neulf}
}",https://arxiv.org/pdf/2105.07112.pdf,,,,,,,"Speed & Computational Efficiency, Hybrid Geometry Representation",Fourier Feature (NeRF),Light Field,,,,,,,,"Celong Liu, Zhong Li, Junsong Yuan, Yi Xu",liu2021neulf,182,,"In this paper, we present an efficient and robust deep learning solution for novel view synthesis of complex scenes. In our approach, a 3D scene is represented as a light field, i.e., a set of rays, each of which has a corresponding color when reaching the image plane. For efficient novel view rendering, we adopt a 4D parameterization of the light field, where each ray is characterized by a 4D parameter. We then formulate the light field as a 4D function that maps 4D coordinates to corresponding color values. We train a deep fully connected network to optimize this implicit function and memorize the 3D scene. Then, the scene-specific model is used to synthesize novel views. Different from previous light field approaches which require dense view sampling to reliably render novel views, our method can render novel views by sampling rays and querying the color for each ray from the network directly, thus enabling high-quality light field rendering with a sparser set of training images. Our method achieves state-of-the-art novel view synthesis results while maintaining an interactive frame rate.",,,,,,ARXIV 2021,,,,,,
5/23/2021 18:11,,,Editing Conditional Radiance Fields,,ARXIV,5/13/2021,2021,"@article{liu2021editing,
    journal = {arXiv preprint arXiv:2105.06466},
    booktitle = {ArXiv Pre-print},
    author = {Steven Liu and Xiuming Zhang and Zhoutong Zhang and Richard Zhang and Jun-Yan Zhu and Bryan Russell},
    title = {Editing Conditional Radiance Fields},
    year = {2021},
    url = {http://arxiv.org/abs/2105.06466v2},
    entrytype = {article},
    id = {liu2021editing}
}",https://arxiv.org/pdf/2105.06466.pdf,http://editnerf.csail.mit.edu/,https://github.com/stevliu/editnerf,,https://www.youtube.com/watch?v=9qwRD4ejOpw,,,"Generalization, Editable",,,,,,,,,,"Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, Bryan Russell",liu2021editing,181,0,"A neural radiance field (NeRF) is a scene model supporting high-quality view synthesis, optimized per scene. In this paper, we explore enabling user editing of a category-level NeRF - also known as a conditional radiance field - trained on a shape category. Specifically, we introduce a method for propagating coarse 2D user scribbles to the 3D space, to modify the color or shape of a local region. First, we propose a conditional radiance field that incorporates new modular network components, including a shape branch that is shared across object instances. Observing multiple instances of the same category, our model learns underlying part semantics without any supervision, thereby allowing the propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair seat). Next, we propose a hybrid network update strategy that targets specific network components, which balances efficiency and accuracy. During user interaction, we formulate an optimization problem that both satisfies the user's constraints and preserves the original object structure. We demonstrate our approach on various editing tasks over three shape datasets and show that it outperforms prior neural editing approaches. Finally, we edit the appearance and shape of a real photograph and show that the edit propagates to extrapolated novel views.",,,,,,ARXIV 2021,,,,,,
5/23/2021 18:13,,,Dynamic View Synthesis from Dynamic Monocular Video,,ARXIV,5/13/2021,2021,"@article{gao2021dynamic,
    journal = {arXiv preprint arXiv:2105.06468},
    booktitle = {ArXiv Pre-print},
    author = {Chen Gao and Ayush Saraf and Johannes Kopf and Jia-Bin Huang},
    title = {Dynamic View Synthesis from Dynamic Monocular Video},
    year = {2021},
    url = {http://arxiv.org/abs/2105.06468v1},
    entrytype = {article},
    id = {gao2021dynamic}
}",https://arxiv.org/pdf/2105.06468.pdf,,,,,,,Dynamic/Temporal,,,,,,,,,,"Chen Gao, Ayush Saraf, Johannes Kopf, Jia-Bin Huang",gao2021dynamic,180,0,"We present an algorithm for generating novel views at arbitrary viewpoints and any input time step given a monocular video of a dynamic scene. Our work builds upon recent advances in neural implicit representation and uses continuous and differentiable functions for modeling the time-varying structure and the appearance of the scene. We jointly train a time-invariant static NeRF and a time-varying dynamic NeRF, and learn how to blend the results in an unsupervised manner. However, learning this implicit function from a single video is highly ill-posed (with infinitely many solutions that match the input video). To resolve the ambiguity, we introduce regularization losses to encourage a more physically plausible solution. We show extensive quantitative and qualitative results of dynamic view synthesis from casually captured videos.",,,,,,ARXIV 2021,,,,,,
5/19/2021 18:01,,,Electrocardio Panorama: Synthesizing New ECG Views with Self-supervision,Nef-Net,IJCAI,5/12/2021,2021,"@inproceedings{chen2021nefnet,
    publisher = {International Joint Conferences on Artificial Intelligence Organization},
    booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)},
    author = {Jintai Chen and Xiangshang Zheng and Hongyun Yu and Danny Z. Chen and Jian Wu},
    title = {Electrocardio Panorama: Synthesizing New ECG Views with Self-supervision},
    year = {2021},
    url = {http://arxiv.org/abs/2105.06293v1},
    entrytype = {inproceedings},
    id = {chen2021nefnet}
}",https://arxiv.org/pdf/2105.06293.pdf,,,,,,,Science & Engineering,,,,,,,,,,"Jintai Chen, Xiangshang Zheng, Hongyun Yu, Danny Z. Chen, Jian Wu",chen2021nefnet,179,0,"Multi-lead electrocardiogram (ECG) provides clinical information of heartbeats from several fixed viewpoints determined by the lead positioning. However, it is often not satisfactory to visualize ECG signals in these fixed and limited views, as some clinically useful information is represented only from a few specific ECG viewpoints. For the first time, we propose a new concept, Electrocardio Panorama, which allows visualizing ECG signals from any queried viewpoints. To build Electrocardio Panorama, we assume that an underlying electrocardio field exists, representing locations, magnitudes, and directions of ECG signals. We present a Neural electrocardio field Network (Nef-Net), which first predicts the electrocardio field representation by using a sparse set of one or few input ECG views and then synthesizes Electrocardio Panorama based on the predicted representations. Specially, to better disentangle electrocardio field information from viewpoint biases, a new Angular Encoding is proposed to process viewpoint angles. Also, we propose a self-supervised learning approach called Standin Learning, which helps model the electrocardio field without direct supervision. Further, with very few modifications, Nef-Net can also synthesize ECG signals from scratch. Experiments verify that our Nef-Net performs well on Electrocardio Panorama synthesis, and outperforms the previous work on the auxiliary tasks (ECG view transformation and ECG synthesis from scratch). The codes and the division labels of cardiac cycles and ECG deflections on Tianchi ECG and PTB datasets are available at https://github.com/WhatAShot/Electrocardio-Panorama.",,,,,,IJCAI 2021,,,,,,
5/23/2021 18:13,,,Neural Trajectory Fields for Dynamic Novel View Synthesis,DCT-NeRF,ARXIV,5/12/2021,2021,"@article{wang2021dctnerf,
    journal = {arXiv preprint arXiv:2105.05994},
    booktitle = {ArXiv Pre-print},
    author = {Chaoyang Wang and Ben Eckart and Simon Lucey and Orazio Gallo},
    title = {Neural Trajectory Fields for Dynamic Novel View Synthesis},
    year = {2021},
    url = {http://arxiv.org/abs/2105.05994v1},
    entrytype = {article},
    id = {wang2021dctnerf}
}",https://arxiv.org/pdf/2105.05994.pdf,,,,,,,Dynamic/Temporal,,,,,,,,,,"Chaoyang Wang, Ben Eckart, Simon Lucey, Orazio Gallo",wang2021dctnerf,178,0,"Recent approaches to render photorealistic views from a limited set of photographs have pushed the boundaries of our interactions with pictures of static scenes. The ability to recreate moments, that is, time-varying sequences, is perhaps an even more interesting scenario, but it remains largely unsolved. We introduce DCT-NeRF, a coordinatebased neural representation for dynamic scenes. DCTNeRF learns smooth and stable trajectories over the input sequence for each point in space. This allows us to enforce consistency between any two frames in the sequence, which results in high quality reconstruction, particularly in dynamic regions.",,,,,,ARXIV 2021,,,,,,
5/23/2021 18:12,,,Vision-based Neural Scene Representations for Spacecraft,,CVPR,5/11/2021,2021,"@inproceedings{mergy2021visionbased,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Anne Mergy and Gurvan Lecuyer and Dawa Derksen and Dario Izzo},
    title = {Vision-based Neural Scene Representations for Spacecraft},
    year = {2021},
    url = {http://arxiv.org/abs/2105.06405v1},
    entrytype = {inproceedings},
    id = {mergy2021visionbased}
}",https://arxiv.org/pdf/2105.06405.pdf,,,,,,,"Sparse Reconstruction, Science & Engineering, Local Conditioning",,,,,,,,,,"Anne Mergy, Gurvan Lecuyer, Dawa Derksen, Dario Izzo",mergy2021visionbased,177,0,"In advanced mission concepts with high levels of autonomy, spacecraft need to internally model the pose and shape of nearby orbiting objects. Recent works in neural scene representations show promising results for inferring generic three-dimensional scenes from optical images. Neural Radiance Fields (NeRF) have shown success in rendering highly specular surfaces using a large number of images and their pose. More recently, Generative Radiance Fields (GRAF) achieved full volumetric reconstruction of a scene from unposed images only, thanks to the use of an adversarial framework to train a NeRF. In this paper, we compare and evaluate the potential of NeRF and GRAF to render novel views and extract the 3D shape of two different spacecraft, the Soil Moisture and Ocean Salinity satellite of ESA's Living Planet Programme and a generic cube sat. Considering the best performances of both models, we observe that NeRF has the ability to render more accurate images regarding the material specularity of the spacecraft and its pose. For its part, GRAF generates precise novel views with accurate details even when parts of the satellites are shadowed while having the significant advantage of not needing any information about the relative pose.",,,,,,CVPR 2021,,,,,,
5/23/2021 18:17,,,Neural 3D Scene Compression via Model Compression,,ARXIV,5/7/2021,2021,"@article{isik2021neural,
    journal = {arXiv preprint arXiv:2105.03120},
    booktitle = {ArXiv Pre-print},
    author = {Berivan Isik},
    title = {Neural 3D Scene Compression via Model Compression},
    year = {2021},
    url = {http://arxiv.org/abs/2105.03120v1},
    entrytype = {article},
    id = {isik2021neural}
}",https://arxiv.org/pdf/2105.03120.pdf,,,,,,,Compression,,,,,,,,,,Berivan Isik,isik2021neural,176,0,"Rendering 3D scenes requires access to arbitrary viewpoints from the scene. Storage of such a 3D scene can be done in two ways; (1) storing 2D images taken from the 3D scene that can reconstruct the scene back through interpolations, or (2) storing a representation of the 3D scene itself that already encodes views from all directions. So far, traditional 3D compression methods have focused on the first type of storage and compressed the original 2D images with image compression techniques. With this approach, the user first decodes the stored 2D images and then renders the 3D scene. However, this separated procedure is inefficient since a large amount of 2D images have to be stored. In this work, we take a different approach and compress a functional representation of 3D scenes. In particular, we introduce a method to compress 3D scenes by compressing the neural networks that represent the scenes as neural radiance fields. Our method provides more efficient storage of 3D scenes since it does not store 2D images -- which are redundant when we render the scene from the neural functional representation.",,,,,,ARXIV 2021,,,,,,
5/23/2021 18:17,,,Animatable Neural Radiance Fields for Human Body Modeling,,ICCV,5/6/2021,2021,"@inproceedings{peng2021animatable,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Sida Peng and Junting Dong and Qianqian Wang and Shangzhan Zhang and Qing Shuai and Hujun Bao and Xiaowei Zhou},
    title = {Animatable Neural Radiance Fields for Human Body Modeling},
    year = {2021},
    url = {http://arxiv.org/abs/2105.02872v1},
    entrytype = {inproceedings},
    id = {peng2021animatable}
}",https://arxiv.org/pdf/2105.02872.pdf,,https://arxiv.org/pdf/2105.02872.pdf,,https://www.youtube.com/watch?v=eWOSWbmfJo4,,,"Dynamic/Temporal, Human (Body)",,,,,,,,,,"Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, Xiaowei Zhou",peng2021animatable,175,1,"This paper addresses the challenge of reconstructing an animatable human model from a multi-view video. Some recent works have proposed to decompose a dynamic scene into a canonical neural radiance field and a set of deformation fields that map observation-space points to the canonical space, thereby enabling them to learn the dynamic scene from images. However, they represent the deformation field as translational vector field or SE(3) field, which makes the optimization highly under-constrained. Moreover, these representations cannot be explicitly controlled by input motions. Instead, we introduce neural blend weight fields to produce the deformation fields. Based on the skeleton-driven deformation, blend weight fields are used with 3D human skeletons to generate observation-to-canonical and canonical-to-observation correspondences. Since 3D human skeletons are more observable, they can regularize the learning of deformation fields. Moreover, the learned blend weight fields can be combined with input skeletal motions to generate new deformation fields to animate the human model. Experiments show that our approach significantly outperforms recent human synthesis methods. The code will be available at https://zju3dv.github.io/animatable_nerf/.",,,,,,ICCV 2021,,,,,,
6/29/2021 16:36,,,acorn: Adaptive Coordinate Networks for Neural Scene Representation,ACORN,SIGGRAPH,5/6/2021,2021,"@article{martel2021acorn,
    publisher = {Association for Computing Machinery},
    journal = {ACM Transactions on Graphics (TOG)},
    author = {Julien N. P. Martel and David B. Lindell and Connor Z. Lin and Eric R. Chan and Marco Monteiro and Gordon Wetzstein},
    title = {ACORN: Adaptive Coordinate Networks for Neural Scene Representation},
    year = {2021},
    url = {http://arxiv.org/abs/2105.02788v1},
    entrytype = {article},
    id = {martel2021acorn}
}",https://arxiv.org/pdf/2105.02788.pdf,,,,,,,"Speed & Computational Efficiency, Local Conditioning, Coarse-to-Fine, Voxel Grid, Sampling, Hybrid Geometry Representation",Fourier Feature (NeRF),Occupancy,"Yes, geometry only",,,,,,,"Julien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan, Marco Monteiro, Gordon Wetzstein",martel2021acorn,174,2,"Neural representations have emerged as a new paradigm for applications in rendering, imaging, geometric modeling, and simulation. Compared to traditional representations such as meshes, point clouds, or volumes they can be flexibly incorporated into differentiable learning-based pipelines. While recent improvements to neural representations now make it possible to represent signals with fine details at moderate resolutions (e.g., for images and 3D shapes), adequately representing large-scale or complex scenes has proven a challenge. Current neural representations fail to accurately represent images at resolutions greater than a megapixel or 3D scenes with more than a few hundred thousand polygons. Here, we introduce a new hybrid implicit-explicit network architecture and training strategy that adaptively allocates resources during training and inference based on the local complexity of a signal of interest. Our approach uses a multiscale block-coordinate decomposition, similar to a quadtree or octree, that is optimized during training. The network architecture operates in two stages: using the bulk of the network parameters, a coordinate encoder generates a feature grid in a single forward pass. Then, hundreds or thousands of samples within each block can be efficiently evaluated using a lightweight feature decoder. With this hybrid implicit-explicit network architecture, we demonstrate the first experiments that fit gigapixel images to nearly 40 dB peak signal-to-noise ratio. Notably this represents an increase in scale of over 1000x compared to the resolution of previously demonstrated image-fitting experiments. Moreover, our approach is able to represent 3D shapes significantly faster and better than previous techniques; it reduces training times from days to hours or minutes and memory requirements by over an order of magnitude.",,,,,,SIGGRAPH 2021,,,,,,
5/23/2021 18:54,,,3D Scene Compression through Entropy Penalized Neural Representation Functions,cNeRF,ARXIV,5/3/2021,2021,"@article{bird2021cnerf,
    journal = {arXiv preprint arXiv:2104.12456},
    booktitle = {ArXiv Pre-print},
    author = {Thomas Bird and Johannes Balle and Saurabh Singh and Philip A. Chou},
    title = {3D Scene Compression through Entropy Penalized Neural Representation Functions},
    year = {2021},
    url = {http://arxiv.org/abs/2104.12456v1},
    entrytype = {article},
    id = {bird2021cnerf}
}",https://arxiv.org/pdf/2104.12456.pdf,,,,,,,Compression,,,,,,,,,,"Thomas Bird, Johannes Ballé, Saurabh Singh, Philip A. Chou",bird2021cnerf,173,0,"Some forms of novel visual media enable the viewer to explore a 3D scene from arbitrary viewpoints, by interpolating between a discrete set of original views. Compared to 2D imagery, these types of applications require much larger amounts of storage space, which we seek to reduce. Existing approaches for compressing 3D scenes are based on a separation of compression and rendering: each of the original views is compressed using traditional 2D image formats; the receiver decompresses the views and then performs the rendering. We unify these steps by directly compressing an implicit representation of the scene, a function that maps spatial coordinates to a radiance vector field, which can then be queried to render arbitrary viewpoints. The function is implemented as a neural network and jointly trained for reconstruction as well as compressibility, in an end-to-end manner, with the use of an entropy penalty on the parameters. Our method significantly outperforms a state-of-the-art conventional approach for scene compression, achieving simultaneously higher quality reconstructions and lower bitrates. Furthermore, we show that the performance at lower bitrates can be improved by jointly representing multiple scenes using a soft form of parameter sharing.",,,,,,ARXIV 2021,,,,,,
5/23/2021 18:18,,,Editable Free-Viewpoint Video using a Layered Neural Representation,ST-NeRF,SIGGRAPH,4/30/2021,2021,"@article{zhang2021stnerf,
    publisher = {Association for Computing Machinery},
    journal = {ACM Transactions on Graphics (TOG)},
    author = {Jiakai Zhang and Xinhang Liu and Xinyi Ye and Fuqiang Zhao and Yanshun Zhang and Minye Wu and Yingliang Zhang and Lan Xu and Jingyi Yu},
    title = {Editable Free-viewpoint Video Using a Layered Neural Representation},
    doi = {10.1145/3450626.3459756},
    year = {2021},
    url = {http://arxiv.org/abs/2104.14786v1},
    entrytype = {article},
    id = {zhang2021stnerf}
}",https://arxiv.org/pdf/2104.14786.pdf,,,,https://www.youtube.com/watch?v=Wp4HfOwFGP4&feature=emb_logo,,,"Dynamic/Temporal, Editable, Object-Centric, Hybrid Geometry Representation",,,,,,,,,,"Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang, Minye Wu, Yingliang Zhang, Lan Xu, Jingyi Yu",zhang2021stnerf,172,1,"Generating free-viewpoint videos is critical for immersive VR/AR experience but recent neural advances still lack the editing ability to manipulate the visual perception for large dynamic scenes. To fill this gap, in this paper we propose the first approach for editable photo-realistic free-viewpoint video generation for large-scale dynamic scenes using only sparse 16 cameras. The core of our approach is a new layered neural representation, where each dynamic entity including the environment itself is formulated into a space-time coherent neural layered radiance representation called ST-NeRF. Such layered representation supports fully perception and realistic manipulation of the dynamic scene whilst still supporting a free viewing experience in a wide range. In our ST-NeRF, the dynamic entity/layer is represented as continuous functions, which achieves the disentanglement of location, deformation as well as the appearance of the dynamic entity in a continuous and self-supervised manner. We propose a scene parsing 4D label map tracking to disentangle the spatial information explicitly, and a continuous deform module to disentangle the temporal motion implicitly. An object-aware volume rendering scheme is further introduced for the re-assembling of all the neural layers. We adopt a novel layered loss and motion-aware ray sampling strategy to enable efficient training for a large dynamic scene with multiple performers, Our framework further enables a variety of editing functions, i.e., manipulating the scale and location, duplicating or retiming individual neural layers to create numerous visual effects while preserving high realism. Extensive experiments demonstrate the effectiveness of our approach to achieve high-quality, photo-realistic, and editable free-viewpoint video generation for dynamic scenes.",,,,,,SIGGRAPH 2021,,,,,,
5/23/2021 18:19,,,Neural Ray-Tracing: Learning Surfaces and Reflectance for Relighting and View Synthesis,Neural Ray-Tracing,ARXIV,4/28/2021,2021,"@article{knodt2021neuralraytracing,
    journal = {arXiv preprint arXiv:2104.13562},
    booktitle = {ArXiv Pre-print},
    author = {Julian Knodt and Seung-Hwan Baek and Felix Heide},
    title = {Neural Ray-Tracing: Learning Surfaces and Reflectance for Relighting and View Synthesis},
    year = {2021},
    url = {http://arxiv.org/abs/2104.13562v1},
    entrytype = {article},
    id = {knodt2021neuralraytracing}
}",https://arxiv.org/pdf/2104.13562.pdf,,https://github.com/princeton-computational-imaging/neural_raytracing,,,,,"Editable, Material/Lighting Estimation, Hybrid Geometry Representation",,SDF,No,,,,,,,"Julian Knodt, Seung-Hwan Baek, Felix Heide",knodt2021neuralraytracing,171,0,"Recent neural rendering methods have demonstrated accurate view interpolation by predicting volumetric density and color with a neural network. Although such volumetric representations can be supervised on static and dynamic scenes, existing methods implicitly bake the complete scene light transport into a single neural network for a given scene, including surface modeling, bidirectional scattering distribution functions, and indirect lighting effects. In contrast to traditional rendering pipelines, this prohibits changing surface reflectance, illumination, or composing other objects in the scene. In this work, we explicitly model the light transport between scene surfaces and we rely on traditional integration schemes and the rendering equation to reconstruct a scene. The proposed method allows BSDF recovery with unknown light conditions and classic light transports such as pathtracing. By learning decomposed transport with surface representations established in conventional rendering methods, the method naturally facilitates editing shape, reflectance, lighting and scene composition. The method outperforms NeRV for relighting under known lighting conditions, and produces realistic reconstructions for relit and edited scenes. We validate the proposed approach for scene editing, relighting and reflectance estimation learned from synthetic and captured views on a subset of NeRV's datasets.",,,Direct,,,ARXIV 2021,,,,,,
9/30/2021 10:56,,,STORM: An Integrated Framework for Fast Joint-Space Model-Predictive Control for Reactive Manipulation,STORM,NeurIPS,4/28/2021,2021,"@inproceedings{bhardwaj2021storm,
    publisher = {Curran Associates, Inc.},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    author = {Mohak Bhardwaj and Balakumar Sundaralingam and Arsalan Mousavian and Nathan Ratliff and Dieter Fox and Fabio Ramos and Byron Boots},
    title = {STORM: An Integrated Framework for Fast Joint-Space Model-Predictive Control for Reactive Manipulation},
    year = {2021},
    url = {http://arxiv.org/abs/2104.13542v2},
    entrytype = {inproceedings},
    id = {bhardwaj2021storm}
}",https://arxiv.org/pdf/2104.13542.pdf,https://sites.google.com/view/manipulation-mpc,,,https://vimeo.com/526772348,,,Robotics,Fourier Feature (NeRF),,,,,,,,,"Mohak Bhardwaj, Balakumar Sundaralingam, Arsalan Mousavian, Nathan Ratliff, Dieter Fox, Fabio Ramos, Byron Boots",bhardwaj2021storm,170,,"Sampling-based model-predictive control (MPC) is a promising tool for feedback control of robots with complex, non-smooth dynamics, and cost functions. However, the computationally demanding nature of sampling-based MPC algorithms has been a key bottleneck in their application to high-dimensional robotic manipulation problems in the real world. Previous methods have addressed this issue by running MPC in the task space while relying on a low-level operational space controller for joint control. However, by not using the joint space of the robot in the MPC formulation, existing methods cannot directly account for non-task space related constraints such as avoiding joint limits, singular configurations, and link collisions. In this paper, we develop a system for fast, joint space sampling-based MPC for manipulators that is efficiently parallelized using GPUs. Our approach can handle task and joint space constraints while taking less than 8ms~(125Hz) to compute the next control command. Further, our method can tightly integrate perception into the control problem by utilizing learned cost functions from raw sensor data. We validate our approach by deploying it on a Franka Panda robot for a variety of dynamic manipulation tasks. We study the effect of different cost formulations and MPC parameters on the synthesized behavior and provide key insights that pave the way for the application of sampling-based MPC for manipulators in a principled manner. We also provide highly optimized, open-source code to be used by the wider robot learning and control community. Videos of experiments can be found at: https://sites.google.com/view/manipulation-mpc",,,,,,NeurIPS 2021,,,,,,
8/29/2021 16:25,,,Vector Neurons: A General Framework for SO(3)-Equivariant Networks,Vector Neurons,ARXIV,4/25/2021,2021,"@article{deng2021vectorneurons,
    journal = {arXiv preprint arXiv:2104.12229},
    booktitle = {ArXiv Pre-print},
    author = {Congyue Deng and Or Litany and Yueqi Duan and Adrien Poulenard and Andrea Tagliasacchi and Leonidas Guibas},
    title = {Vector Neurons: A General Framework for SO(3)-Equivariant Networks},
    year = {2021},
    url = {http://arxiv.org/abs/2104.12229v1},
    entrytype = {article},
    id = {deng2021vectorneurons}
}",https://arxiv.org/pdf/2104.12229.pdf,https://cs.stanford.edu/~congyue/vnn/,"https://github.com/FlyingGiraffe/vnn, https://github.com/FlyingGiraffe/vnn-neural-implicits/",,Coming soon,,,"Generalization, Fundamentals, Data-Driven Method, Symmetry",,Occupancy,"Yes, geometry only",,,,,,,"Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, Leonidas Guibas",deng2021vectorneurons,169,1,"Invariance and equivariance to the rotation group have been widely discussed in the 3D deep learning community for pointclouds. Yet most proposed methods either use complex mathematical tools that may limit their accessibility, or are tied to specific input data types and network architectures. In this paper, we introduce a general framework built on top of what we call Vector Neuron representations for creating SO(3)-equivariant neural networks for pointcloud processing. Extending neurons from 1D scalars to 3D vectors, our vector neurons enable a simple mapping of SO(3) actions to latent spaces thereby providing a framework for building equivariance in common neural operations -- including linear layers, non-linearities, pooling, and normalizations. Due to their simplicity, vector neurons are versatile and, as we demonstrate, can be incorporated into diverse network architecture backbones, allowing them to process geometry inputs in arbitrary poses. Despite its simplicity, our method performs comparably well in accuracy and generalization with other more complex and specialized state-of-the-art methods on classification and segmentation tasks. We also show for the first time a rotation equivariant reconstruction network.",,,Direct,,,ARXIV 2021,,,,,,
5/23/2021 18:21,,,Dynamic CT Reconstruction from Limited Views with Implicit Neural Representations and Parametric Motion Fields,INR,ARXIV,4/23/2021,2021,"@article{reed2021inr,
    journal = {arXiv preprint arXiv:2104.11745},
    booktitle = {ArXiv Pre-print},
    author = {Albert W. Reed and Hyojin Kim and Rushil Anirudh and K. Aditya Mohan and Kyle Champley and Jingu Kang and Suren Jayasuriya},
    title = {Dynamic CT Reconstruction from Limited Views with Implicit Neural Representations and Parametric Motion Fields},
    year = {2021},
    url = {http://arxiv.org/abs/2104.11745v1},
    entrytype = {article},
    id = {reed2021inr}
}",https://arxiv.org/pdf/2104.11745.pdf,,,,,,,"Dynamic/Temporal, Science & Engineering",,,,,,,,,,"Albert W. Reed, Hyojin Kim, Rushil Anirudh, K. Aditya Mohan, Kyle Champley, Jingu Kang, Suren Jayasuriya",reed2021inr,168,1,"Reconstructing dynamic, time-varying scenes with computed tomography (4D-CT) is a challenging and ill-posed problem common to industrial and medical settings. Existing 4D-CT reconstructions are designed for sparse sampling schemes that require fast CT scanners to capture multiple, rapid revolutions around the scene in order to generate high quality results. However, if the scene is moving too fast, then the sampling occurs along a limited view and is difficult to reconstruct due to spatiotemporal ambiguities. In this work, we design a reconstruction pipeline using implicit neural representations coupled with a novel parametric motion field warping to perform limited view 4D-CT reconstruction of rapidly deforming scenes. Importantly, we utilize a differentiable analysis-by-synthesis approach to compare with captured x-ray sinogram data in a self-supervised fashion. Thus, our resulting optimization method requires no training data to reconstruct the scene. We demonstrate that our proposed system robustly reconstructs scenes containing deformable and periodic motion and validate against state-of-the-art baselines. Further, we demonstrate an ability to reconstruct continuous spatiotemporal representations of our scenes and upsample them to arbitrary volumes and frame rates post-optimization. This research opens a new avenue for implicit neural representations in computed tomography reconstruction in general.",,,,,,ARXIV 2021,,,,,,
7/20/2021 15:00,,,Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry,S-NeRF,CVPR,4/20/2021,2021,"@inproceedings{derksen2021snerf,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Dawa Derksen and Dario Izzo},
    title = {Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry},
    year = {2021},
    url = {http://arxiv.org/abs/2104.09877v1},
    entrytype = {inproceedings},
    id = {derksen2021snerf}
}",https://arxiv.org/pdf/2104.09877.pdf,,,,,,,"Material/Lighting Estimation, Sampling",,,,,,,,,,"Dawa Derksen, Dario Izzo",derksen2021snerf,167,0,"We present a new generic method for shadow-aware multi-view satellite photogrammetry of Earth Observation scenes. Our proposed method, the Shadow Neural Radiance Field (S-NeRF) follows recent advances in implicit volumetric representation learning. For each scene, we train S-NeRF using very high spatial resolution optical images taken from known viewing angles. The learning requires no labels or shape priors: it is self-supervised by an image reconstruction loss. To accommodate for changing light source conditions both from a directional light source (the Sun) and a diffuse light source (the sky), we extend the NeRF approach in two ways. First, direct illumination from the Sun is modeled via a local light source visibility field. Second, indirect illumination from a diffuse light source is learned as a non-local color field as a function of the position of the Sun. Quantitatively, the combination of these factors reduces the altitude and color errors in shaded areas, compared to NeRF. The S-NeRF methodology not only performs novel view synthesis and full 3D shape estimation, it also enables shadow detection, albedo synthesis, and transient object filtering, without any explicit shape supervision.",,,,,,CVPR 2021,,,,,,
5/23/2021 18:32,,,UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction,UNISURF,ICCV,4/20/2021,2021,"@inproceedings{oechsle2021unisurf,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Michael Oechsle and Songyou Peng and Andreas Geiger},
    title = {UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction},
    year = {2021},
    url = {http://arxiv.org/abs/2104.10078v1},
    entrytype = {inproceedings},
    id = {oechsle2021unisurf}
}",https://arxiv.org/pdf/2104.10078.pdf,https://arxiv.org/pdf/2104.10078.pdf,,,,,,"Fundamentals, Hybrid Geometry Representation",,Occupancy,No,,,,,,,"Michael Oechsle, Songyou Peng, Andreas Geiger",oechsle2021unisurf,166,7,"Neural implicit 3D representations have emerged as a powerful paradigm for reconstructing surfaces from multi-view images and synthesizing novel views. Unfortunately, existing methods such as DVR or IDR require accurate per-pixel object masks as supervision. At the same time, neural radiance fields have revolutionized novel view synthesis. However, NeRF's estimated volume density does not admit accurate surface reconstruction. Our key insight is that implicit surface models and radiance fields can be formulated in a unified way, enabling both surface and volume rendering using the same model. This unified perspective enables novel, more efficient sampling procedures and the ability to reconstruct accurate surfaces without input masks. We compare our method on the DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments demonstrate that we outperform NeRF in terms of reconstruction quality while performing on par with IDR without requiring masks.",,,,,,ICCV 2021,,,,,,
10/8/2021 16:37,,,Parallel Physics-Informed Neural Networks via Domain Decomposition,,ARXIV,4/20/2021,2021,"@article{shukla2021parallel,
    url = {http://arxiv.org/abs/2104.10013v3},
    year = {2021},
    title = {Parallel Physics-Informed Neural Networks via Domain Decomposition},
    author = {Khemraj Shukla and Ameya D. Jagtap and George Em Karniadakis},
    booktitle = {ArXiv Pre-print},
    journal = {arXiv preprint arXiv:2104.10013},
    entrytype = {article},
    id = {shukla2021parallel}
}",https://arxiv.org/pdf/2104.10013.pdf,,,,,,,Science & Engineering,,,,,,,,,,"Khemraj Shukla, Ameya D. Jagtap, George Em Karniadakis",shukla2021parallel,165,,"We develop a distributed framework for the physics-informed neural networks (PINNs) based on two recent extensions, namely conservative PINNs (cPINNs) and extended PINNs (XPINNs), which employ domain decomposition in space and in time-space, respectively. This domain decomposition endows cPINNs and XPINNs with several advantages over the vanilla PINNs, such as parallelization capacity, large representation capacity, efficient hyperparameter tuning, and is particularly effective for multi-scale and multi-physics problems. Here, we present a parallel algorithm for cPINNs and XPINNs constructed with a hybrid programming model described by MPI $+$ X, where X $\in \{\text{CPUs},~\text{GPUs}\}$. The main advantage of cPINN and XPINN over the more classical data and model parallel approaches is the flexibility of optimizing all hyperparameters of each neural network separately in each subdomain. We compare the performance of distributed cPINNs and XPINNs for various forward problems, using both weak and strong scalings. Our results indicate that for space domain decomposition, cPINNs are more efficient in terms of communication cost but XPINNs provide greater flexibility as they can also handle time-domain decomposition for any differential equations, and can deal with any arbitrarily shaped complex subdomains. To this end, we also present an application of the parallel XPINN method for solving an inverse diffusion problem with variable conductivity on the United States map, using ten regions as subdomains.",,,,,,ARXIV 2021,,,,,,
7/19/2021 21:59,,,SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization,SAPE,ARXIV,4/19/2021,2021,"@article{hertz2021sape,
    journal = {arXiv preprint arXiv:2104.09125},
    booktitle = {ArXiv Pre-print},
    author = {Amir Hertz and Or Perel and Raja Giryes and Olga Sorkine-Hornung and Daniel Cohen-Or},
    title = {SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization},
    year = {2021},
    url = {http://arxiv.org/abs/2104.09125v2},
    entrytype = {article},
    id = {hertz2021sape}
}",https://arxiv.org/pdf/2104.09125.pdf,,,,,,,"Fundamentals, Coarse-to-Fine, Positional Encoding",Other,,,,,,,,,"Amir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung, Daniel Cohen-Or",hertz2021sape,164,1,"Multilayer-perceptrons (MLP) are known to struggle with learning functions of high-frequencies, and in particular cases with wide frequency bands. We present a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. We demonstrate the advantage of SAPE on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes.",,,,,,ARXIV 2021,,,,,,
5/23/2021 18:20,,,FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category Modelling,FiG-NeRF,ARXIV,4/17/2021,2021,"@article{xie2021fignerf,
    journal = {arXiv preprint arXiv:2104.08418},
    booktitle = {ArXiv Pre-print},
    author = {Christopher Xie and Keunhong Park and Ricardo Martin-Brualla and Matthew Brown},
    title = {FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category Modelling},
    year = {2021},
    url = {http://arxiv.org/abs/2104.08418v1},
    entrytype = {article},
    id = {xie2021fignerf}
}",https://arxiv.org/pdf/2104.08418.pdf,https://fig-nerf.github.io/,,,https://www.youtube.com/watch?v=WtZxuv_hkic,,,"Generalization, Fundamentals, Global Conditioning",,,,,,,,,,"Christopher Xie, Keunhong Park, Ricardo Martin-Brualla, Matthew Brown",xie2021fignerf,163,1,"We investigate the use of Neural Radiance Fields (NeRF) to learn high quality 3D object category models from collections of input images. In contrast to previous work, we are able to do this whilst simultaneously separating foreground objects from their varying backgrounds. We achieve this via a 2-component NeRF model, FiG-NeRF, that prefers explanation of the scene as a geometrically constant background and a deformable foreground that represents the object category. We show that this method can learn accurate 3D object category models using only photometric supervision and casually captured images of the objects. Additionally, our 2-part decomposition allows the model to perform accurate and crisp amodal segmentation. We quantitatively evaluate our method with view synthesis and image fidelity metrics, using synthetic, lab-captured, and in-the-wild data. Our results demonstrate convincing 3D object category modelling that exceed the performance of existing methods.",,,,,,ARXIV 2021,,,,,,
9/17/2021 11:58,,,Locally Aware Piecewise Transformation Fields for 3D Human Mesh Registration,,CVPR,4/16/2021,2021,"@inproceedings{wang2021locally,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Shaofei Wang and Andreas Geiger and Siyu Tang},
    title = {Locally Aware Piecewise Transformation Fields for 3D Human Mesh Registration},
    year = {2021},
    url = {http://arxiv.org/abs/2104.08160v1},
    entrytype = {inproceedings},
    id = {wang2021locally}
}",https://arxiv.org/pdf/2104.08160.pdf,https://taconite.github.io/PTF/website/PTF.html,https://github.com/taconite/PTF,,https://www.youtube.com/watch?v=TvLoGLVF70k,,,"Human (Body), Data-Driven Method, Local Conditioning, Voxel Grid",,Occupancy,"Yes, geometry only",,,,,,,"Shaofei Wang, Andreas Geiger, Siyu Tang",wang2021locally,162,,"Registering point clouds of dressed humans to parametric human models is a challenging task in computer vision. Traditional approaches often rely on heavily engineered pipelines that require accurate manual initialization of human poses and tedious post-processing. More recently, learning-based methods are proposed in hope to automate this process. We observe that pose initialization is key to accurate registration but existing methods often fail to provide accurate pose initialization. One major obstacle is that, regressing joint rotations from point clouds or images of humans is still very challenging. To this end, we propose novel piecewise transformation fields (PTF), a set of functions that learn 3D translation vectors to map any query point in posed space to its correspond position in rest-pose space. We combine PTF with multi-class occupancy networks, obtaining a novel learning-based framework that learns to simultaneously predict shape and per-point correspondences between the posed space and the canonical space for clothed human. Our key insight is that the translation vector for each query point can be effectively estimated using the point-aligned local features; consequently, rigid per bone transformations and joint rotations can be obtained efficiently via a least-square fitting given the estimated point correspondences, circumventing the challenging task of directly regressing joint rotations from neural networks. Furthermore, the proposed PTF facilitate canonicalized occupancy estimation, which greatly improves generalization capability and results in more accurate surface reconstruction with only half of the parameters compared with the state-of-the-art. Both qualitative and quantitative studies show that fitting parametric models with poses initialized by our network results in much better registration quality, especially for extreme poses.",,,Direct,,,CVPR 2021,,,,,,
5/23/2021 18:32,,,GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds,GANcraft,ICCV,4/15/2021,2021,"@inproceedings{hao2021gancraft,
    url = {http://arxiv.org/abs/2104.07659v1},
    year = {2021},
    title = {GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds},
    author = {Zekun Hao and Arun Mallya and Serge Belongie and Ming-Yu Liu},
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    entrytype = {inproceedings},
    id = {hao2021gancraft}
}",https://arxiv.org/pdf/2104.07659.pdf,https://nvlabs.github.io/GANcraft/,https://github.com/NVlabs/imaginaire,,https://www.youtube.com/watch?v=1Hky092CGFQ,,,"Generalization, Generative Models, Data-Driven Method, Local Conditioning, Voxel Grid, Large-Scale Scenes",,,,,,,,,,"Zekun Hao, Arun Mallya, Serge Belongie, Ming-Yu Liu",hao2021gancraft,161,2,"We present GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft. Our method takes a semantic block world as input, where each block is assigned a semantic label such as dirt, grass, or water. We represent the world as a continuous volumetric function and train our model to render view-consistent photorealistic images for a user-controlled camera. In the absence of paired ground truth real images for the block world, we devise a training technique based on pseudo-ground truth and adversarial training. This stands in contrast to prior work on neural rendering for view synthesis, which requires ground truth images to estimate scene geometry and view-dependent appearance. In addition to camera trajectory, GANcraft allows user control over both scene semantics and output style. Experimental results with comparison to strong baselines show the effectiveness of GANcraft on this novel task of photorealistic 3D block world synthesis. The project website is available at https://nvlabs.github.io/GANcraft/ .",,,,,,ICCV 2021,,,,,,
5/23/2021 18:33,,,A-SDF: Learning Disentangled Signed Distance Functions for Articulated Shape Representation,A-SDF,ARXIV,4/15/2021,2021,"@article{mu2021asdf,
    journal = {arXiv preprint arXiv:2104.07645},
    booktitle = {ArXiv Pre-print},
    author = {Jiteng Mu and Weichao Qiu and Adam Kortylewski and Alan Yuille and Nuno Vasconcelos and Xiaolong Wang},
    title = {A-SDF: Learning Disentangled Signed Distance Functions for Articulated Shape Representation},
    year = {2021},
    url = {http://arxiv.org/abs/2104.07645v1},
    entrytype = {article},
    id = {mu2021asdf}
}",https://arxiv.org/pdf/2104.07645.pdf,https://jitengmu.github.io/A-SDF/,https://github.com/JitengMu/A-SDF,,https://www.youtube.com/watch?v=P5WTcaXzC7A,,,Editable,,,,,,,,,,"Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille, Nuno Vasconcelos, Xiaolong Wang",mu2021asdf,160,2,"Recent work has made significant progress on using implicit functions, as a continuous representation for 3D rigid object shape reconstruction. However, much less effort has been devoted to modeling general articulated objects. Compared to rigid objects, articulated objects have higher degrees of freedom, which makes it hard to generalize to unseen shapes. To deal with the large shape variance, we introduce Articulated Signed Distance Functions (A-SDF) to represent articulated shapes with a disentangled latent space, where we have separate codes for encoding shape and articulation. We assume no prior knowledge on part geometry, articulation status, joint type, joint axis, and joint location. With this disentangled continuous representation, we demonstrate that we can control the articulation input and animate unseen instances with unseen joint angles. Furthermore, we propose a Test-Time Adaptation inference algorithm to adjust our model during inference. We demonstrate our model generalize well to out-of-distribution and unseen data, e.g., partial point clouds and real-world depth images.",,,,,,ARXIV 2021,,,,,,
5/23/2021 18:47,,,FastNeRF: High-Fidelity Neural Rendering at 200FPS,FastNeRF,ARXIV,4/15/2021,2021,"@article{garbin2021fastnerf,
    journal = {arXiv preprint arXiv:2103.10380},
    booktitle = {ArXiv Pre-print},
    author = {Stephan J. Garbin and Marek Kowalski and Matthew Johnson and Jamie Shotton and Julien Valentin},
    title = {FastNeRF: High-Fidelity Neural Rendering at 200FPS},
    year = {2021},
    url = {http://arxiv.org/abs/2103.10380v2},
    entrytype = {article},
    id = {garbin2021fastnerf}
}",https://arxiv.org/pdf/2103.10380.pdf,https://microsoft.github.io/FastNeRF/,,,,,,Speed & Computational Efficiency,,,,,,,,,,"Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, Julien Valentin",garbin2021fastnerf,159,10,"Recent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very computationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based system capable of rendering high fidelity photorealistic images at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each position in space, (ii) efficiently querying that map using ray directions to estimate the pixel values in the rendered image. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and extensibility.",,,,,,ARXIV 2021,,,,,,
5/23/2021 18:36,,,Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views of Novel Scenes,SRF,CVPR,4/14/2021,2021,"@inproceedings{chibane2021srf,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Julian Chibane and Aayush Bansal and Verica Lazova and Gerard Pons-Moll},
    title = {Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views of Novel Scenes},
    year = {2021},
    url = {http://arxiv.org/abs/2104.06935v1},
    entrytype = {inproceedings},
    id = {chibane2021srf}
}",https://arxiv.org/pdf/2104.06935.pdf,https://virtualhumans.mpi-inf.mpg.de/srf/,,,,https://arxiv.org/pdf/2104.06935.pdf,,"Speed & Computational Efficiency, Sparse Reconstruction, Generalization, Image-Based Rendering, Local Conditioning",,,,,,,,,,"Julian Chibane, Aayush Bansal, Verica Lazova, Gerard Pons-Moll",chibane2021srf,158,4,"Recent neural view synthesis methods have achieved impressive quality and realism, surpassing classical pipelines which rely on multi-view reconstruction. State-of-the-Art methods, such as NeRF, are designed to learn a single scene with a neural network and require dense multi-view inputs. Testing on a new scene requires re-training from scratch, which takes 2-3 days. In this work, we introduce Stereo Radiance Fields (SRF), a neural view synthesis approach that is trained end-to-end, generalizes to new scenes, and requires only sparse views at test time. The core idea is a neural architecture inspired by classical multi-view stereo methods, which estimates surface points by finding similar image regions in stereo images. In SRF, we predict color and density for each 3D point given an encoding of its stereo correspondence in the input images. The encoding is implicitly learned by an ensemble of pair-wise similarities -- emulating classical stereo. Experiments show that SRF learns structure instead of overfitting on a scene. We train on multiple scenes of the DTU dataset and generalize to new ones without re-training, requiring only 10 sparse and spread-out views as input. We show that 10-15 minutes of fine-tuning further improve the results, achieving significantly sharper, more detailed results than scene-specific models. The code, model, and videos are available at https://virtualhumans.mpi-inf.mpg.de/srf/.",,,,,,CVPR 2021,,,,,,
9/17/2021 11:50,,,LEAP: Learning Articulated Occupancy of People,LEAP,CVPR,4/14/2021,2021,"@inproceedings{mihajlovic2021leap,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Marko Mihajlovic and Yan Zhang and Michael J. Black and Siyu Tang},
    title = {LEAP: Learning Articulated Occupancy of People},
    year = {2021},
    url = {http://arxiv.org/abs/2104.06849v1},
    entrytype = {inproceedings},
    id = {mihajlovic2021leap}
}",https://arxiv.org/pdf/2104.06849.pdf,https://neuralbodies.github.io/LEAP/,https://github.com/neuralbodies/leap,,https://www.youtube.com/watch?v=UVB8A_T5e3c,,,"Human (Body), Editable, Data-Driven Method, Global Conditioning",,Occupancy,"Yes, geometry only",,,,,,,"Marko Mihajlovic, Yan Zhang, Michael J. Black, Siyu Tang",mihajlovic2021leap,157,,"Substantial progress has been made on modeling rigid 3D objects using deep implicit representations. Yet, extending these methods to learn neural models of human shape is still in its infancy. Human bodies are complex and the key challenge is to learn a representation that generalizes such that it can express body shape deformations for unseen subjects in unseen, highly-articulated, poses. To address this challenge, we introduce LEAP (LEarning Articulated occupancy of People), a novel neural occupancy representation of the human body. Given a set of bone transformations (i.e. joint locations and rotations) and a query point in space, LEAP first maps the query point to a canonical space via learned linear blend skinning (LBS) functions and then efficiently queries the occupancy value via an occupancy network that models accurate identity- and pose-dependent deformations in the canonical space. Experiments show that our canonicalized occupancy estimation with the learned LBS functions greatly improves the generalization capability of the learned occupancy representation across various human shapes and poses, outperforming existing solutions in all settings.",,,Direct,,,CVPR 2021,,,,,,
5/23/2021 18:34,,,BARF: Bundle-Adjusting Neural Radiance Fields,BARF,ICCV,4/13/2021,2021,"@inproceedings{lin2021barf,
    url = {http://arxiv.org/abs/2104.06405v2},
    year = {2021},
    title = {BARF: Bundle-Adjusting Neural Radiance Fields},
    author = {Chen-Hsuan Lin and Wei-Chiu Ma and Antonio Torralba and Simon Lucey},
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    entrytype = {inproceedings},
    id = {lin2021barf}
}",https://arxiv.org/pdf/2104.06405.pdf,https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF/,https://github.com/chenhsuanlin/bundle-adjusting-NeRF,,,,,"Camera Parameter Estimation, Coarse-to-Fine",,,,,,,,,,"Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, Simon Lucey",lin2021barf,156,2,"Neural Radiance Fields (NeRF) have recently gained a surge of interest within the computer vision community for its power to synthesize photorealistic novel views of real-world scenes. One limitation of NeRF, however, is its requirement of accurate camera poses to learn the scene representations. In this paper, we propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from imperfect (or even unknown) camera poses -- the joint problem of learning neural 3D representations and registering camera frames. We establish a theoretical connection to classical image alignment and show that coarse-to-fine registration is also applicable to NeRF. Furthermore, we show that na\""ively applying positional encoding in NeRF has a negative impact on registration with a synthesis-based objective. Experiments on synthetic and real-world data show that BARF can effectively optimize the neural scene representations and resolve large camera pose misalignment at the same time. This enables view synthesis and localization of video sequences from unknown camera poses, opening up new avenues for visual localization systems (e.g. SLAM) and potential applications for dense 3D mapping and reconstruction.",,,,,,ICCV 2021,,,,,,
9/17/2021 14:04,,,StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision,StereoPIFu,CVPR,4/12/2021,2021,"@inproceedings{hong2021stereopifu,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Yang Hong and Juyong Zhang and Boyi Jiang and Yudong Guo and Ligang Liu and Hujun Bao},
    title = {StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision},
    year = {2021},
    url = {http://arxiv.org/abs/2104.05289v2},
    entrytype = {inproceedings},
    id = {hong2021stereopifu}
}",https://arxiv.org/pdf/2104.05289.pdf,https://hy1995.top/StereoPIFuProject/,https://github.com/CrisHY1995/StereoPIFu_Code,,,,,"Human (Body), Data-Driven Method, Local Conditioning",,Occupancy,"Yes, geometry only",,,,,,,"Yang Hong, Juyong Zhang, Boyi Jiang, Yudong Guo, Ligang Liu, Hujun Bao",hong2021stereopifu,155,,"In this paper, we propose StereoPIFu, which integrates the geometric constraints of stereo vision with implicit function representation of PIFu, to recover the 3D shape of the clothed human from a pair of low-cost rectified images. First, we introduce the effective voxel-aligned features from a stereo vision-based network to enable depth-aware reconstruction. Moreover, the novel relative z-offset is employed to associate predicted high-fidelity human depth and occupancy inference, which helps restore fine-level surface details. Second, a network structure that fully utilizes the geometry information from the stereo images is designed to improve the human body reconstruction quality. Consequently, our StereoPIFu can naturally infer the human body's spatial location in camera space and maintain the correct relative position of different parts of the human body, which enables our method to capture human performance. Compared with previous works, our StereoPIFu significantly improves the robustness, completeness, and accuracy of the clothed human reconstruction, which is demonstrated by extensive experimental results.",,Yes,,,,CVPR 2021,,,,,,
5/23/2021 18:33,,,Compressive Neural Representations of Volumetric Scalar Fields,,EuroVis,4/11/2021,2021,"@article{lu2021compressive,
    author = {Yuzhe Lu and Kairong Jiang and Joshua A. Levine and Matthew Berger},
    title = {Compressive Neural Representations of Volumetric Scalar Fields},
    year = {2021},
    month = {Apr},
    url = {http://arxiv.org/abs/2104.04523v1},
    entrytype = {article},
    id = {lu2021compressive}
}",https://arxiv.org/pdf/2104.04523.pdf,,,,,,,"Dynamic/Temporal, Compression, Fundamentals",Sinusoidal Activation (SIREN),,,,,,,,,"Yuzhe Lu, Kairong Jiang, Joshua A. Levine, Matthew Berger",lu2021compressive,154,1,"We present an approach for compressing volumetric scalar fields using implicit neural representations. Our approach represents a scalar field as a learned function, wherein a neural network maps a point in the domain to an output scalar value. By setting the number of weights of the neural network to be smaller than the input size, we achieve compressed representations of scalar fields, thus framing compression as a type of function approximation. Combined with carefully quantizing network weights, we show that this approach yields highly compact representations that outperform state-of-the-art volume compression approaches. The conceptual simplicity of our approach enables a number of benefits, such as support for time-varying scalar fields, optimizing to preserve spatial gradients, and random-access field evaluation. We study the impact of network design choices on compression performance, highlighting how simple network architectures are effective for a broad range of volumes.",,,,,,EuroVis 2021,,,,,,
5/23/2021 18:34,,,Neural RGB-D Surface Reconstruction,,ARXIV,4/9/2021,2021,"@article{azinovic2021neural,
    journal = {arXiv preprint arXiv:2104.04532},
    booktitle = {ArXiv Pre-print},
    author = {Dejan Azinovic and Ricardo Martin-Brualla and Dan B Goldman and Matthias Niessner and Justus Thies},
    title = {Neural RGB-D Surface Reconstruction},
    year = {2021},
    url = {http://arxiv.org/abs/2104.04532v1},
    entrytype = {article},
    id = {azinovic2021neural}
}",https://arxiv.org/pdf/2104.04532.pdf,,,,,,,Camera Parameter Estimation,,,,,,,,,,"Dejan Azinović, Ricardo Martin-Brualla, Dan B Goldman, Matthias Nießner, Justus Thies",azinovic2021neural,153,1,"In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance field have shown amazing image synthesis results, but the underlying geometry representation is only a coarse approximation of the real geometry. We demonstrate how depth measurements can be incorporated into the radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone. In contrast to a density field as the underlying geometry representation, we propose to learn a deep neural network which stores a truncated signed distance field. Using this representation, we show that one can still leverage differentiable volume rendering to estimate color values of the observed images during training to compute a reconstruction loss. This is beneficial for learning the signed distance field in regions with missing depth measurements. Furthermore, we correct misalignment errors of the camera, improving the overall reconstruction quality. In several experiments, we showcase our method and compare to existing works on classical RGB-D fusion and learned representations.",,,,,,ARXIV 2021,,,,,,
10/4/2021 22:11,,,Pixel Codec Avatars,PiCA,CVPR,4/9/2021,2021,"@inproceedings{ma2021pica,
    url = {http://arxiv.org/abs/2104.04638v1},
    year = {2021},
    title = {Pixel Codec Avatars},
    author = {Shugao Ma and Tomas Simon and Jason Saragih and Dawei Wang and Yuecheng Li and Fernando De La Torre and Yaser Sheikh},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    entrytype = {inproceedings},
    id = {ma2021pica}
}",https://arxiv.org/pdf/2104.04638.pdf,,,,,,,"Human (Head), Generalization, 2D Image Neural Fields, Data-Driven Method, Hybrid Geometry Representation",,,,,,,,,,"Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang, Yuecheng Li, Fernando De La Torre, Yaser Sheikh",ma2021pica,152,,"Telecommunication with photorealistic avatars in virtual or augmented reality is a promising path for achieving authentic face-to-face communication in 3D over remote physical distances. In this work, we present the Pixel Codec Avatars (PiCA): a deep generative model of 3D human faces that achieves state of the art reconstruction performance while being computationally efficient and adaptive to the rendering conditions during execution. Our model combines two core ideas: (1) a fully convolutional architecture for decoding spatially varying features, and (2) a rendering-adaptive per-pixel decoder. Both techniques are integrated via a dense surface representation that is learned in a weakly-supervised manner from low-topology mesh tracking over training images. We demonstrate that PiCA improves reconstruction over existing techniques across testing expressions and views on persons of different gender and skin tone. Importantly, we show that the PiCA model is much smaller than the state-of-art baseline model, and makes multi-person telecommunicaiton possible: on a single Oculus Quest 2 mobile VR headset, 5 avatars are rendered in realtime in the same scene.",,,,,,CVPR 2021,,,,,,
5/23/2021 18:33,,,Direct-PoseNet: Absolute Pose Regression with Photometric Consistency,Direct-PoseNet,ARXIV,4/8/2021,2021,"@article{chen2021directposenet,
    journal = {arXiv preprint arXiv:2104.04073},
    booktitle = {ArXiv Pre-print},
    author = {Shuai Chen and Zirui Wang and Victor Prisacariu},
    title = {Direct-PoseNet: Absolute Pose Regression with Photometric Consistency},
    year = {2021},
    url = {http://arxiv.org/abs/2104.04073v1},
    entrytype = {article},
    id = {chen2021directposenet}
}",https://arxiv.org/pdf/2104.04073.pdf,,,,,,,"Camera Parameter Estimation, Coarse-to-Fine",,,,,,,,,,"Shuai Chen, Zirui Wang, Victor Prisacariu",chen2021directposenet,151,0,"We present a relocalization pipeline, which combines an absolute pose regression (APR) network with a novel view synthesis based direct matching module, offering superior accuracy while maintaining low inference time. Our contribution is twofold: i) we design a direct matching module that supplies a photometric supervision signal to refine the pose regression network via differentiable rendering; ii) we modify the rotation representation from the classical quaternion to SO(3) in pose regression, removing the need for balancing rotation and translation loss terms. As a result, our network Direct-PoseNet achieves state-of-the-art performance among all other single-image APR methods on the 7-Scenes benchmark and the LLFF dataset.",,,,,,ARXIV 2021,,,,,,
5/23/2021 18:35,,,SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes,SNARF,ICCV,4/8/2021,2021,"@inproceedings{chen2021snarf,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Xu Chen and Yufeng Zheng and Michael J. Black and Otmar Hilliges and Andreas Geiger},
    title = {SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes},
    year = {2021},
    url = {http://arxiv.org/abs/2104.03953v1},
    entrytype = {inproceedings},
    id = {chen2021snarf}
}",https://arxiv.org/pdf/2104.03953.pdf,,,,,,,Human (Body),,,,,,,,,,"Xu Chen, Yufeng Zheng, Michael J. Black, Otmar Hilliges, Andreas Geiger",chen2021snarf,150,1,"Neural implicit surface representations have emerged as a promising paradigm to capture 3D shapes in a continuous and resolution-independent manner. However, adapting them to articulated shapes is non-trivial. Existing approaches learn a backward warp field that maps deformed to canonical points. However, this is problematic since the backward warp field is pose dependent and thus requires large amounts of data to learn. To address this, we introduce SNARF, which combines the advantages of linear blend skinning (LBS) for polygonal meshes with those of neural implicit surfaces by learning a forward deformation field without direct supervision. This deformation field is defined in canonical, pose-independent space, allowing for generalization to unseen poses. Learning the deformation field from posed meshes alone is challenging since the correspondences of deformed points are defined implicitly and may not be unique under changes of topology. We propose a forward skinning model that finds all canonical correspondences of any deformed point using iterative root finding. We derive analytical gradients via implicit differentiation, enabling end-to-end training from 3D meshes with bone transformations. Compared to state-of-the-art neural implicit representations, our approach generalizes better to unseen poses while preserving accuracy. We demonstrate our method in challenging scenarios on (clothed) 3D humans in diverse and unseen poses.",,,,,,ICCV 2021,,,,,,
5/23/2021 18:35,,,Modulated Periodic Activations for Generalizable Local Functional Representations,,ICCV,4/8/2021,2021,"@inproceedings{mehta2021modulated,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Ishit Mehta and Michael Gharbi and Connelly Barnes and Eli Shechtman and Ravi Ramamoorthi and Manmohan Chandraker},
    title = {Modulated Periodic Activations for Generalizable Local Functional Representations},
    year = {2021},
    url = {http://arxiv.org/abs/2104.03960v1},
    entrytype = {inproceedings},
    id = {mehta2021modulated}
}",https://arxiv.org/pdf/2104.03960.pdf,,,,,,,"Generalization, Compression, Fundamentals, Global Conditioning, Hypernetwork/Meta-learning, Positional Encoding",Other,,,,,,,,,"Ishit Mehta, Michaël Gharbi, Connelly Barnes, Eli Shechtman, Ravi Ramamoorthi, Manmohan Chandraker",mehta2021modulated,149,1,"Multi-Layer Perceptrons (MLPs) make powerful functional representations for sampling and reconstruction problems involving low-dimensional signals like images,shapes and light fields. Recent works have significantly improved their ability to represent high-frequency content by using periodic activations or positional encodings. This often came at the expense of generalization: modern methods are typically optimized for a single signal. We present a new representation that generalizes to multiple instances and achieves state-of-the-art fidelity. We use a dual-MLP architecture to encode the signals. A synthesis network creates a functional mapping from a low-dimensional input (e.g. pixel-position) to the output domain (e.g. RGB color). A modulation network maps a latent code corresponding to the target signal to parameters that modulate the periodic activations of the synthesis network. We also propose a local-functional representation which enables generalization. The signal's domain is partitioned into a regular grid,with each tile represented by a latent code. At test time, the signal is encoded with high-fidelity by inferring (or directly optimizing) the latent code-book. Our approach produces generalizable functional representations of images, videos and shapes, and achieves higher reconstruction quality than prior works that are optimized for a single signal.",,,,,,ICCV 2021,,,,,,
5/23/2021 18:34,,,Neural Articulated Radiance Field,NARF,ICCV,4/7/2021,2021,"@inproceedings{noguchi2021narf,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Atsuhiro Noguchi and Xiao Sun and Stephen Lin and Tatsuya Harada},
    title = {Neural Articulated Radiance Field},
    year = {2021},
    url = {http://arxiv.org/abs/2104.03110v2},
    entrytype = {inproceedings},
    id = {noguchi2021narf}
}",https://arxiv.org/pdf/2104.03110.pdf,,https://arxiv.org/pdf/2104.03110.pdf,,,,,Human (Body),,,,,,,,,,"Atsuhiro Noguchi, Xiao Sun, Stephen Lin, Tatsuya Harada",noguchi2021narf,148,2,"We present Neural Articulated Radiance Field (NARF), a novel deformable 3D representation for articulated objects learned from images. While recent advances in 3D implicit representation have made it possible to learn models of complex objects, learning pose-controllable representations of articulated objects remains a challenge, as current methods require 3D shape supervision and are unable to render appearance. In formulating an implicit representation of 3D articulated objects, our method considers only the rigid transformation of the most relevant object part in solving for the radiance field at each 3D location. In this way, the proposed method represents pose-dependent changes without significantly increasing the computational complexity. NARF is fully differentiable and can be trained from images with pose annotations. Moreover, through the use of an autoencoder, it can learn appearance variations over multiple instances of an object class. Experiments show that the proposed method is efficient and can generalize well to novel poses. The code is available for research purposes at https://github.com/nogu-atsu/NARF",,,,,,ICCV 2021,,,,,,
7/19/2021 22:03,,,SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks,SCANimate,CVPR,4/7/2021,2021,"@inproceedings{saito2021scanimate,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Shunsuke Saito and Jinlong Yang and Qianli Ma and Michael J. Black},
    title = {SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks},
    year = {2021},
    url = {http://arxiv.org/abs/2104.03313v2},
    entrytype = {inproceedings},
    id = {saito2021scanimate}
}",https://arxiv.org/pdf/2104.03313.pdf,https://scanimate.is.tue.mpg.de,https://github.com/shunsukesaito/SCANimate,,https://www.youtube.com/watch?v=ohavL55Oznw,,,"Dynamic/Temporal, Human (Body)",,SDF,No,,,,,,,"Shunsuke Saito, Jinlong Yang, Qianli Ma, Michael J. Black",saito2021scanimate,147,7,"We present SCANimate, an end-to-end trainable framework that takes raw 3D scans of a clothed human and turns them into an animatable avatar. These avatars are driven by pose parameters and have realistic clothing that moves and deforms naturally. SCANimate does not rely on a customized mesh template or surface mesh registration. We observe that fitting a parametric 3D body model, like SMPL, to a clothed human scan is tractable while surface registration of the body topology to the scan is often not, because clothing can deviate significantly from the body shape. We also observe that articulated transformations are invertible, resulting in geometric cycle consistency in the posed and unposed shapes. These observations lead us to a weakly supervised learning method that aligns scans into a canonical pose by disentangling articulated deformations without template-based surface registration. Furthermore, to complete missing regions in the aligned scans while modeling pose-dependent deformations, we introduce a locally pose-aware implicit function that learns to complete and model geometry with learned pose correctives. In contrast to commonly used global pose embeddings, our local pose conditioning significantly reduces long-range spurious correlations and improves generalization to unseen poses, especially when training data is limited. Our method can be applied to pose-aware appearance modeling to generate a fully textured avatar. We demonstrate our approach on various clothing types with different amounts of training data, outperforming existing solutions and other variants in terms of fidelity and generality in every setting. The code is available at https://scanimate.is.tue.mpg.de.",,,Direct,,,CVPR 2021,,,,,,
5/23/2021 18:36,,,MirrorNeRF: One-shot Neural Portrait Radiance Field from Multi-mirror Catadioptric Imaging,MirrorNeRF,ARXIV,4/6/2021,2021,"@article{wang2021mirrornerf,
    journal = {arXiv preprint arXiv:2104.02607},
    booktitle = {ArXiv Pre-print},
    author = {Ziyu Wang and Liao Wang and Fuqiang Zhao and Minye Wu and Lan Xu and Jingyi Yu},
    title = {MirrorNeRF: One-shot Neural Portrait Radiance Field from Multi-mirror Catadioptric Imaging},
    year = {2021},
    url = {http://arxiv.org/abs/2104.02607v2},
    entrytype = {article},
    id = {wang2021mirrornerf}
}",https://arxiv.org/pdf/2104.02607.pdf,,,,,,,Sparse Reconstruction,,,,,,,,,,"Ziyu Wang, Liao Wang, Fuqiang Zhao, Minye Wu, Lan Xu, Jingyi Yu",wang2021mirrornerf,146,1,"Photo-realistic neural reconstruction and rendering of the human portrait are critical for numerous VR/AR applications. Still, existing solutions inherently rely on multi-view capture settings, and the one-shot solution to get rid of the tedious multi-view synchronization and calibration remains extremely challenging. In this paper, we propose MirrorNeRF - a one-shot neural portrait free-viewpoint rendering approach using a catadioptric imaging system with multiple sphere mirrors and a single high-resolution digital camera, which is the first to combine neural radiance field with catadioptric imaging so as to enable one-shot photo-realistic human portrait reconstruction and rendering, in a low-cost and casual capture setting. More specifically, we propose a light-weight catadioptric system design with a sphere mirror array to enable diverse ray sampling in the continuous 3D space as well as an effective online calibration for the camera and the mirror array. Our catadioptric imaging system can be easily deployed with a low budget and the casual capture ability for convenient daily usages. We introduce a novel neural warping radiance field representation to learn a continuous displacement field that implicitly compensates for the misalignment due to our flexible system setting. We further propose a density regularization scheme to leverage the inherent geometry information from the catadioptric data in a self-supervision manner, which not only improves the training efficiency but also provides more effective density supervision for higher rendering quality. Extensive experiments demonstrate the effectiveness and robustness of our scheme to achieve one-shot photo-realistic and high-quality appearance free-viewpoint rendering for human portrait scenes.",,,,,,ARXIV 2021,,,,,,
5/23/2021 18:18,,,pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis,pi-GAN,CVPR,4/5/2021,2021,"@inproceedings{chan2021pigan,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Eric R. Chan and Marco Monteiro and Petr Kellnhofer and Jiajun Wu and Gordon Wetzstein},
    title = {pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis},
    year = {2021},
    url = {http://arxiv.org/abs/2012.00926v2},
    entrytype = {inproceedings},
    id = {chan2021pigan}
}",https://arxiv.org/pdf/2012.00926.pdf,https://marcoamonteiro.github.io/pi-GAN-website/,https://github.com/marcoamonteiro/pi-GAN,Coming soon,https://www.youtube.com/watch?v=0HCdof9BGtw,,,"Generalization, Generative Models, Global Conditioning",Sinusoidal Activation (SIREN),,,,,,,,,"Eric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, Gordon Wetzstein",chan2021pigan,145,20,"We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches however fall short in two ways: first, they may lack an underlying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks ($\pi$-GAN or pi-GAN), for high-quality 3D-aware image synthesis. $\pi$-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent 3D representations with fine detail. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets.",,,,,,CVPR 2021,,,,,,
5/23/2021 18:35,,,Convolutional Neural Opacity Radiance Fields,,ICCP,4/5/2021,2021,"@article{luo2021convolutional,
    author = {Haimin Luo and Anpei Chen and Qixuan Zhang and Bai Pang and Minye Wu and Lan Xu and Jingyi Yu},
    title = {Convolutional Neural Opacity Radiance Fields},
    year = {2021},
    month = {Apr},
    url = {http://arxiv.org/abs/2104.01772v1},
    entrytype = {article},
    id = {luo2021convolutional}
}",https://arxiv.org/pdf/2104.01772.pdf,,,,,,,"Speed & Computational Efficiency, Generative Models, Sampling",,,,,,,,,,"Haimin Luo, Anpei Chen, Qixuan Zhang, Bai Pang, Minye Wu, Lan Xu, Jingyi Yu",luo2021convolutional,144,2,"Photo-realistic modeling and rendering of fuzzy objects with complex opacity are critical for numerous immersive VR/AR applications, but it suffers from strong view-dependent brightness, color. In this paper, we propose a novel scheme to generate opacity radiance fields with a convolutional neural renderer for fuzzy objects, which is the first to combine both explicit opacity supervision and convolutional mechanism into the neural radiance field framework so as to enable high-quality appearance and global consistent alpha mattes generation in arbitrary novel views. More specifically, we propose an efficient sampling strategy along with both the camera rays and image plane, which enables efficient radiance field sampling and learning in a patch-wise manner, as well as a novel volumetric feature integration scheme that generates per-patch hybrid feature embeddings to reconstruct the view-consistent fine-detailed appearance and opacity output. We further adopt a patch-wise adversarial training scheme to preserve both high-frequency appearance and opacity details in a self-supervised framework. We also introduce an effective multi-view image capture system to capture high-quality color and alpha maps for challenging fuzzy objects. Extensive experiments on existing and our new challenging fuzzy object dataset demonstrate that our method achieves photo-realistic, globally consistent, and fined detailed appearance and opacity free-viewpoint rendering for various fuzzy objects.",,,,,,ICCP 2021,,,,,,
5/23/2021 18:38,,,Synergies Between Affordance and Geometry: 6-DoF Grasp Detection via Implicit Representations,GIGA,RSS,4/4/2021,2021,"@inproceedings{jiang2021giga,
    booktitle = {Proceedings of Robotics: Science and Systems},
    author = {Zhenyu Jiang and Yifeng Zhu and Maxwell Svetlik and Kuan Fang and Yuke Zhu},
    title = {Synergies Between Affordance and Geometry: 6-DoF Grasp Detection via Implicit Representations},
    year = {2021},
    url = {http://arxiv.org/abs/2104.01542v2},
    entrytype = {inproceedings},
    id = {jiang2021giga}
}",https://arxiv.org/pdf/2104.01542.pdf,https://sites.google.com/view/rpl-giga2021,https://github.com/UT-Austin-RPL/GIGA,,,,,"Generalization, Science & Engineering, Robotics, Voxel Grid, Data-Driven Method, Local Conditioning",,,,,,,,,,"Zhenyu Jiang, Yifeng Zhu, Maxwell Svetlik, Kuan Fang, Yuke Zhu",jiang2021giga,143,1,"Grasp detection in clutter requires the robot to reason about the 3D scene from incomplete and noisy perception. In this work, we draw insight that 3D reconstruction and grasp learning are two intimately connected tasks, both of which require a fine-grained understanding of local geometry details. We thus propose to utilize the synergies between grasp affordance and 3D reconstruction through multi-task learning of a shared representation. Our model takes advantage of deep implicit functions, a continuous and memory-efficient representation, to enable differentiable training of both tasks. We train the model on self-supervised grasp trials data in simulation. Evaluation is conducted on a clutter removal task, where the robot clears cluttered objects by grasping them one at a time. The experimental results in simulation and on the real robot have demonstrated that the use of implicit neural representations and joint learning of grasp affordance and 3D reconstruction have led to state-of-the-art grasping results. Our method outperforms baselines by over 10% in terms of grasp success rate. Additional results and videos can be found at https://sites.google.com/view/rpl-giga2021",,,,,,RSS 2021,,,,,,
5/23/2021 18:37,,,Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation,OBSuRF,ARXIV,4/2/2021,2021,"@article{stelzner2021obsurf,
    journal = {arXiv preprint arXiv:2104.01148},
    booktitle = {ArXiv Pre-print},
    author = {Karl Stelzner and Kristian Kersting and Adam R. Kosiorek},
    title = {Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation},
    year = {2021},
    url = {http://arxiv.org/abs/2104.01148v1},
    entrytype = {article},
    id = {stelzner2021obsurf}
}",https://arxiv.org/pdf/2104.01148.pdf,https://stelzner.github.io/obsurf/,Coming soon,Coming soon,,,,"Object-Centric, Hybrid Geometry Representation, Global Conditioning",,,,,,,,,,"Karl Stelzner, Kristian Kersting, Adam R. Kosiorek",stelzner2021obsurf,142,1,"We present ObSuRF, a method which turns a single image of a scene into a 3D model represented as a set of Neural Radiance Fields (NeRFs), with each NeRF corresponding to a different object. A single forward pass of an encoder network outputs a set of latent vectors describing the objects in the scene. These vectors are used independently to condition a NeRF decoder, defining the geometry and appearance of each object. We make learning more computationally efficient by deriving a novel loss, which allows training NeRFs on RGB-D inputs without explicit ray marching. After confirming that the model performs equal or better than state of the art on three 2D image segmentation benchmarks, we apply it to two multi-object 3D datasets: A multiview version of CLEVR, and a novel dataset in which scenes are populated by ShapeNet models. We find that after training ObSuRF on RGB-D views of training scenes, it is capable of not only recovering the 3D geometry of a scene depicted in a single input image, but also to segment it into objects, despite receiving no supervision in that regard.",,,,,,ARXIV 2021,,,,,,
5/23/2021 18:37,,,NPMs: Neural Parametric Models for 3D Deformable Shapes,NPMs,ICCV,4/1/2021,2021,"@inproceedings{palafox2021npms,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Pablo Palafox and Aljaz Bozic and Justus Thies and Matthias Niessner and Angela Dai},
    title = {NPMs: Neural Parametric Models for 3D Deformable Shapes},
    year = {2021},
    url = {http://arxiv.org/abs/2104.00702v2},
    entrytype = {inproceedings},
    id = {palafox2021npms}
}",https://arxiv.org/pdf/2104.00702.pdf,,,,,,,"Human (Body), Global Conditioning",,SDF,,,,,,,,"Pablo Palafox, Aljaž Božič, Justus Thies, Matthias Nießner, Angela Dai",palafox2021npms,141,2,"Parametric 3D models have enabled a wide variety of tasks in computer graphics and vision, such as modeling human bodies, faces, and hands. However, the construction of these parametric models is often tedious, as it requires heavy manual tweaking, and they struggle to represent additional complexity and details such as wrinkles or clothing. To this end, we propose Neural Parametric Models (NPMs), a novel, learned alternative to traditional, parametric 3D models, which does not require hand-crafted, object-specific constraints. In particular, we learn to disentangle 4D dynamics into latent-space representations of shape and pose, leveraging the flexibility of recent developments in learned implicit functions. Crucially, once learned, our neural parametric models of shape and pose enable optimization over the learned spaces to fit to new observations, similar to the fitting of a traditional parametric model, e.g., SMPL. This enables NPMs to achieve a significantly more accurate and detailed representation of observed deformable sequences. We show that NPMs improve notably over both parametric and non-parametric state of the art in reconstruction and tracking of monocular depth sequences of clothed humans and hands. Latent-space interpolation as well as shape/pose transfer experiments further demonstrate the usefulness of NPMs. Code is publicly available at https://pablopalafox.github.io/npms.",,,,,,ICCV 2021,,,,,,
5/23/2021 18:39,,,RGB-D Local Implicit Function for Depth Completion of Transparent Objects,,CVPR,4/1/2021,2021,"@inproceedings{zhu2021rgbd,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Luyang Zhu and Arsalan Mousavian and Yu Xiang and Hammad Mazhar and Jozef van Eenbergen and Shoubhik Debnath and Dieter Fox},
    title = {RGB-D Local Implicit Function for Depth Completion of Transparent Objects},
    year = {2021},
    url = {http://arxiv.org/abs/2104.00622v1},
    entrytype = {inproceedings},
    id = {zhu2021rgbd}
}",https://arxiv.org/pdf/2104.00622.pdf,https://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit,https://github.com/NVlabs/implicit_depth,Coming soon,,,,"Voxel Grid, Local Conditioning",,,,,,,,,,"Luyang Zhu, Arsalan Mousavian, Yu Xiang, Hammad Mazhar, Jozef van Eenbergen, Shoubhik Debnath, Dieter Fox",zhu2021rgbd,140,1,"Majority of the perception methods in robotics require depth information provided by RGB-D cameras. However, standard 3D sensors fail to capture depth of transparent objects due to refraction and absorption of light. In this paper, we introduce a new approach for depth completion of transparent objects from a single RGB-D image. Key to our approach is a local implicit neural representation built on ray-voxel pairs that allows our method to generalize to unseen objects and achieve fast inference speed. Based on this representation, we present a novel framework that can complete missing depth given noisy RGB-D input. We further improve the depth estimation iteratively using a self-correcting refinement model. To train the whole pipeline, we build a large scale synthetic dataset with transparent objects. Experiments demonstrate that our method performs significantly better than the current state-of-the-art methods on both synthetic and real world data. In addition, our approach improves the inference speed by a factor of 20 compared to the previous best method, ClearGrasp. Code and dataset will be released at https://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit.",,,,,,CVPR 2021,,,,,,
5/25/2021 14:56,,,PhySG: Inverse Rendering with Spherical Gaussians for Physics-based Material Editing and Relighting,PhySG,CVPR,4/1/2021,2021,"@inproceedings{zhang2021physg,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Kai Zhang and Fujun Luan and Qianqian Wang and Kavita Bala and Noah Snavely},
    title = {PhySG: Inverse Rendering with Spherical Gaussians for Physics-based Material Editing and Relighting},
    year = {2021},
    url = {http://arxiv.org/abs/2104.00674v1},
    entrytype = {inproceedings},
    id = {zhang2021physg}
}",https://arxiv.org/pdf/2104.00674.pdf,https://kai-46.github.io/PhySG-website/,https://github.com/Kai-46/PhySG,,,,,Material/Lighting Estimation,,SDF,,,,,,,,"Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, Noah Snavely",zhang2021physg,139,2,"We present PhySG, an end-to-end inverse rendering pipeline that includes a fully differentiable renderer and can reconstruct geometry, materials, and illumination from scratch from a set of RGB input images. Our framework represents specular BRDFs and environmental illumination using mixtures of spherical Gaussians, and represents geometry as a signed distance function parameterized as a Multi-Layer Perceptron. The use of spherical Gaussians allows us to efficiently solve for approximate light transport, and our method works on scenes with challenging non-Lambertian reflectance captured under natural, static illumination. We demonstrate, with both synthetic and real data, that our reconstructions not only enable rendering of novel viewpoints, but also physics-based appearance editing of materials and illumination.",,,,,,CVPR 2021,,,,,,
5/23/2021 18:39,,,NeRF-VAE: A Geometry Aware 3D Scene Generative Model,NeRF-VAE,ARXIV,4/1/2021,2021,"@article{kosiorek2021nerfvae,
    journal = {arXiv preprint arXiv:2104.00587},
    booktitle = {ArXiv Pre-print},
    author = {Adam R. Kosiorek and Heiko Strathmann and Daniel Zoran and Pol Moreno and Rosalia Schneider and Sona Mokra and Danilo J. Rezende},
    title = {NeRF-VAE: A Geometry Aware 3D Scene Generative Model},
    year = {2021},
    url = {http://arxiv.org/abs/2104.00587v1},
    entrytype = {article},
    id = {kosiorek2021nerfvae}
}",https://arxiv.org/pdf/2104.00587.pdf,,,,https://www.youtube.com/watch?v=f-T3BLVuXkY,,,"Sparse Reconstruction, Generalization, Generative Models, Global Conditioning",,,,,,,,,,"Adam R. Kosiorek, Heiko Strathmann, Daniel Zoran, Pol Moreno, Rosalia Schneider, Soňa Mokrá, Danilo J. Rezende",kosiorek2021nerfvae,138,6,"We propose NeRF-VAE, a 3D scene generative model that incorporates geometric structure via NeRF and differentiable volume rendering. In contrast to NeRF, our model takes into account shared structure across scenes, and is able to infer the structure of a novel scene -- without the need to re-train -- using amortized inference. NeRF-VAE's explicit 3D rendering process further contrasts previous generative models with convolution-based rendering which lacks geometric structure. Our model is a VAE that learns a distribution over radiance fields by conditioning them on a latent scene representation. We show that, once trained, NeRF-VAE is able to infer and render geometrically-consistent scenes from previously unseen 3D environments using very few input images. We further demonstrate that NeRF-VAE generalizes well to out-of-distribution cameras, while convolutional models do not. Finally, we introduce and study an attention-based conditioning mechanism of NeRF-VAE's decoder, which improves model performance.",,,,,,ARXIV 2021,,,,,,
5/23/2021 18:40,,,Unconstrained Scene Generation with Locally Conditioned Radiance Fields,,ICCV,4/1/2021,2021,"@inproceedings{devries2021unconstrained,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Terrance DeVries and Miguel Angel Bautista and Nitish Srivastava and Graham W. Taylor and Joshua M. Susskind},
    title = {Unconstrained Scene Generation with Locally Conditioned Radiance Fields},
    year = {2021},
    url = {http://arxiv.org/abs/2104.00670v1},
    entrytype = {inproceedings},
    id = {devries2021unconstrained}
}",https://arxiv.org/pdf/2104.00670.pdf,https://apple.github.io/ml-gsn/,https://github.com/apple/ml-gsn,,,,,Generative Models,,,,,,,,,,"Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, Joshua M. Susskind",devries2021unconstrained,137,0,"We tackle the challenge of learning a distribution over complex, realistic, indoor scenes. In this paper, we introduce Generative Scene Networks (GSN), which learns to decompose scenes into a collection of many local radiance fields that can be rendered from a free moving camera. Our model can be used as a prior to generate new scenes, or to complete a scene given only sparse 2D observations. Recent work has shown that generative models of radiance fields can capture properties such as multi-view consistency and view-dependent lighting. However, these models are specialized for constrained viewing of single objects, such as cars or faces. Due to the size and complexity of realistic indoor environments, existing models lack the representational capacity to adequately capture them. Our decomposition scheme scales to larger and more complex scenes while preserving details and diversity, and the learned prior enables high-quality rendering from viewpoints that are significantly different from observed viewpoints. When compared to existing models, GSN produces quantitatively higher-quality scene renderings across several different scene datasets.",,,,,,ICCV 2021,,,,,,
5/23/2021 18:41,,,Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis,DietNeRF,ARXIV,4/1/2021,2021,"@article{jain2021dietnerf,
    journal = {arXiv preprint arXiv:2104.00677},
    booktitle = {ArXiv Pre-print},
    author = {Ajay Jain and Matthew Tancik and Pieter Abbeel},
    title = {Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis},
    year = {2021},
    url = {http://arxiv.org/abs/2104.00677v1},
    entrytype = {article},
    id = {jain2021dietnerf}
}",https://arxiv.org/pdf/2104.00677.pdf,https://www.ajayj.com/dietnerf,https://github.com/codestella/putting-nerf-on-a-diet,,https://www.youtube.com/watch?v=RF_3hsNizqw,,,"Sparse Reconstruction, Data-Driven Method, Local Conditioning",,,,,,,,,,"Ajay Jain, Matthew Tancik, Pieter Abbeel",jain2021dietnerf,136,0,"We present DietNeRF, a 3D neural scene representation estimated from a few images. Neural Radiance Fields (NeRF) learn a continuous volumetric representation of a scene through multi-view consistency, and can be rendered from novel viewpoints by ray casting. While NeRF has an impressive ability to reconstruct geometry and fine details given many images, up to 100 for challenging 360{\deg} scenes, it often finds a degenerate solution to its image reconstruction objective when only a few input views are available. To improve few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic consistency loss that encourages realistic renderings at novel poses. DietNeRF is trained on individual scenes to (1) correctly render given input views from the same pose, and (2) match high-level semantic attributes across different, random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary poses. We extract these semantics using a pre-trained visual encoder such as CLIP, a Vision Transformer trained on hundreds of millions of diverse single-view, 2D photographs mined from the web with natural language supervision. In experiments, DietNeRF improves the perceptual quality of few-shot view synthesis when learned from scratch, can render novel views with as few as one observed image when pre-trained on a multi-view dataset, and produces plausible completions of completely unobserved regions.",,,,,,ARXIV 2021,,,,,,
7/19/2021 21:26,,,Multi-scene Representation Learning with Neural Radiance Fields,,Journal of Physics: Conference Series,4/1/2021,2021,"@article{fu2021multiscene,
    doi = {10.1088/1742-6596/1880/1/012034},
    url = {https://doi.org/10.1088/1742-6596/1880/1/012034},
    year = {2021},
    publisher = {{IOP} Publishing},
    volume = {1880},
    number = {1},
    pages = {012034},
    author = {Bofeng Fu and Zheng Wang},
    title = {Multi-scene Representation Learning with Neural Radiance Fields},
    journal = {Journal of Physics: Conference Series},
    entrytype = {article},
    id = {fu2021multiscene}
}",https://iopscience.iop.org/article/10.1088/1742-6596/1880/1/012034,,,,,,,Generalization,Fourier Feature (NeRF),Density,No,,,,,,,"Bofeng Fu, Zheng Wang",fu2021multiscene,135,0,"Getting representations of multiple objects or scenes is a raising research topic in Machine Learning (ML) community. Here, we propose a multi-scene representation model that can learn the representation of complex scenes and reconstruct them in high resolution given novel viewing directions. Our method represents a single scene with fully-connected layers. Each set of fully-connected layers are controlled by hyper-networks for multiple scenes modeling. For each scene, we take 3D coordinates (x, y, z) and 2D view-point orientations (I,, E,) as inputs. A set of fully-connected layers output volume density and RGB values at given 3D spatial positions. Then, we render the output volume density and RGB values along the camera rays into images using volume density rendering techniques. During training process, we optimize a continuous volume scene function with a small amount of input viewing directions. By designing versatile embedding module and multi-scene representation networks, our model can render photographic images with novel viewing directions for different complex scenes. Experiment results demonstrate the neural rendering and multi-scene representation abilities of our model. Several thorough experiments show that our method outperforms previous model on both reconstruction precision and scenes generation ability from novel viewing directions.",,,Direct,,,Journal of Physics: Conference Series 2021,,,,,,
10/4/2021 19:39,,,NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video,NeuralRecon,CVPR,4/1/2021,2021,"@inproceedings{sun2021neuralrecon,
    url = {http://arxiv.org/abs/2104.00681v1},
    year = {2021},
    title = {NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video},
    author = {Jiaming Sun and Yiming Xie and Linghao Chen and Xiaowei Zhou and Hujun Bao},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    entrytype = {inproceedings},
    id = {sun2021neuralrecon}
}",https://arxiv.org/pdf/2104.00681.pdf,https://zju3dv.github.io/neuralrecon/,https://github.com/zju3dv/NeuralRecon,,https://www.youtube.com/watch?v=wuMPaUTJuO0,https://zju3dv.github.io/neuralrecon/files/NeuralRecon-suppmat.pdf,,"Robotics, SLAM, Hybrid Geometry Representation, Voxel Grid, Local Conditioning",,TSDF,"Yes, geometry only",,,,,,,"Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, Hujun Bao",sun2021neuralrecon,134,,"We present a novel framework named NeuralRecon for real-time 3D scene reconstruction from a monocular video. Unlike previous methods that estimate single-view depth maps separately on each key-frame and fuse them later, we propose to directly reconstruct local surfaces represented as sparse TSDF volumes for each video fragment sequentially by a neural network. A learning-based TSDF fusion module based on gated recurrent units is used to guide the network to fuse features from previous fragments. This design allows the network to capture local smoothness prior and global shape prior of 3D surfaces when sequentially reconstructing the surfaces, resulting in accurate, coherent, and real-time surface reconstruction. The experiments on ScanNet and 7-Scenes datasets show that our system outperforms state-of-the-art methods in terms of both accuracy and speed. To the best of our knowledge, this is the first learning-based system that is able to reconstruct dense coherent 3D geometry in real-time.",No,Yes,Direct,,,CVPR 2021,,,,,,
5/23/2021 18:43,,,CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields,CAMPARI,ARXIV,3/31/2021,2021,"@article{niemeyer2021campari,
    journal = {arXiv preprint arXiv:2103.17269},
    booktitle = {ArXiv Pre-print},
    author = {Michael Niemeyer and Andreas Geiger},
    title = {CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields},
    year = {2021},
    url = {http://arxiv.org/abs/2103.17269v1},
    entrytype = {article},
    id = {niemeyer2021campari}
}",https://arxiv.org/pdf/2103.17269.pdf,,,,,,,"Generative Models, Data-Driven Method, Local Conditioning, Global Conditioning",,,,,,,,,,"Michael Niemeyer, Andreas Geiger",niemeyer2021campari,133,2,"Tremendous progress in deep generative models has led to photorealistic image synthesis. While achieving compelling results, most approaches operate in the two-dimensional image domain, ignoring the three-dimensional nature of our world. Several recent works therefore propose generative models which are 3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably to the image plane. This leads to impressive 3D consistency, but incorporating such a bias comes at a price: the camera needs to be modeled as well. Current approaches assume fixed intrinsics and a predefined prior over camera pose ranges. As a result, parameter tuning is typically required for real-world data, and results degrade if the data distribution is not matched. Our key hypothesis is that learning a camera generator jointly with the image generator leads to a more principled approach to 3D-aware image synthesis. Further, we propose to decompose the scene into a background and foreground model, leading to more efficient and disentangled scene representations. While training from raw, unposed image collections, we learn a 3D- and camera-aware generative model which faithfully recovers not only the image but also the camera data distribution. At test time, our model generates images with explicit control over the camera as well as the shape and appearance of the scene.",,,,,,ARXIV 2021,,,,,,
10/8/2021 16:49,,,Neural Surface Maps,,CVPR,3/31/2021,2021,"@inproceedings{morreale2021neural,
    url = {http://arxiv.org/abs/2103.16942v1},
    year = {2021},
    title = {Neural Surface Maps},
    author = {Luca Morreale and Noam Aigerman and Vladimir Kim and Niloy J. Mitra},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    entrytype = {inproceedings},
    id = {morreale2021neural}
}",https://arxiv.org/pdf/2103.16942.pdf,http://geometry.cs.ucl.ac.uk/projects/2021/neuralmaps/,https://github.com/luca-morreale/neural_surface_maps,,https://www.youtube.com/watch?v=DHkDCwapxc4,,,,,,,,,,,,,"Luca Morreale, Noam Aigerman, Vladimir Kim, Niloy J. Mitra",morreale2021neural,132,,"Maps are arguably one of the most fundamental concepts used to define and operate on manifold surfaces in differentiable geometry. Accordingly, in geometry processing, maps are ubiquitous and are used in many core applications, such as paramterization, shape analysis, remeshing, and deformation. Unfortunately, most computational representations of surface maps do not lend themselves to manipulation and optimization, usually entailing hard, discrete problems. While algorithms exist to solve these problems, they are problem-specific, and a general framework for surface maps is still in need. In this paper, we advocate considering neural networks as encoding surface maps. Since neural networks can be composed on one another and are differentiable, we show it is easy to use them to define surfaces via atlases, compose them for surface-to-surface mappings, and optimize differentiable objectives relating to them, such as any notion of distortion, in a trivial manner. In our experiments, we represent surfaces by generating a neural map that approximates a UV parameterization of a 3D model. Then, we compose this map with other neural maps which we optimize with respect to distortion measures. We show that our formulation enables trivial optimization of rather elusive mapping tasks, such as maps between a collection of surfaces.",,,,,,CVPR 2021,,,,,,
5/23/2021 18:41,,,Unsupervised Learning of 3D Object Categories from Videos in the Wild,,CVPR,3/30/2021,2021,"@inproceedings{henzler2021unsupervised,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Philipp Henzler and Jeremy Reizenstein and Patrick Labatut and Roman Shapovalov and Tobias Ritschel and Andrea Vedaldi and David Novotny},
    title = {Unsupervised Learning of 3D Object Categories from Videos in the Wild},
    year = {2021},
    url = {http://arxiv.org/abs/2103.16552v1},
    entrytype = {inproceedings},
    id = {henzler2021unsupervised}
}",https://arxiv.org/pdf/2103.16552.pdf,https://henzler.github.io/publication/unsupervised_videos/,,,,,,"Generalization, Data-Driven Method, Local Conditioning",,,,,,,,,,"Philipp Henzler, Jeremy Reizenstein, Patrick Labatut, Roman Shapovalov, Tobias Ritschel, Andrea Vedaldi, David Novotny",henzler2021unsupervised,131,2,"Our goal is to learn a deep network that, given a small number of images of an object of a given category, reconstructs it in 3D. While several recent works have obtained analogous results using synthetic data or assuming the availability of 2D primitives such as keypoints, we are interested in working with challenging real data and with no manual annotations. We thus focus on learning a model from multiple views of a large collection of object instances. We contribute with a new large dataset of object centric videos suitable for training and benchmarking this class of models. We show that existing techniques leveraging meshes, voxels, or implicit surfaces, which work well for reconstructing isolated objects, fail on this challenging data. Finally, we propose a new neural network design, called warp-conditioned ray embedding (WCR), which significantly improves reconstruction while obtaining a detailed implicit representation of the object surface and texture, also compensating for the noise in the initial SfM reconstruction that bootstrapped the learning process. Our evaluation demonstrates performance improvements over several deep monocular reconstruction baselines on existing benchmarks and on our novel dataset.",,,,,,CVPR 2021,,,,,,
5/23/2021 18:42,,,Foveated Neural Radiance Fields for Real-Time and Egocentric Virtual Reality,,ARXIV,3/30/2021,2021,"@article{deng2021foveated,
    journal = {arXiv preprint arXiv:2103.16365},
    booktitle = {ArXiv Pre-print},
    author = {Nianchen Deng and Zhenyi He and Jiannan Ye and Praneeth Chakravarthula and Xubo Yang and Qi Sun},
    title = {Foveated Neural Radiance Fields for Real-Time and Egocentric Virtual Reality},
    year = {2021},
    url = {http://arxiv.org/abs/2103.16365v1},
    entrytype = {article},
    id = {deng2021foveated}
}",https://arxiv.org/pdf/2103.16365.pdf,,,,,,,"Speed & Computational Efficiency, Compression, Sampling, Hybrid Geometry Representation",,,,,,,,,,"Nianchen Deng, Zhenyi He, Jiannan Ye, Praneeth Chakravarthula, Xubo Yang, Qi Sun",deng2021foveated,130,0,"Traditional high-quality 3D graphics requires large volumes of fine-detailed scene data for rendering. This demand compromises computational efficiency and local storage resources. Specifically, it becomes more concerning for future wearable and portable virtual and augmented reality (VR/AR) displays. Recent approaches to combat this problem include remote rendering/streaming and neural representations of 3D assets. These approaches have redefined the traditional local storage-rendering pipeline by distributed computing or compression of large data. However, these methods typically suffer from high latency or low quality for practical visualization of large immersive virtual scenes, notably with extra high resolution and refresh rate requirements for VR applications such as gaming and design. Tailored for the future portable, low-storage, and energy-efficient VR platforms, we present the first gaze-contingent 3D neural representation and view synthesis method. We incorporate the human psychophysics of visual- and stereo-acuity into an egocentric neural representation of 3D scenery. Furthermore, we jointly optimize the latency/performance and visual quality, while mutually bridging human perception and neural scene synthesis, to achieve perceptually high-quality immersive interaction. Both objective analysis and subjective study demonstrate the effectiveness of our approach in significantly reducing local storage volume and synthesis latency (up to 99% reduction in both data size and computational time), while simultaneously presenting high-fidelity rendering, with perceptual quality identical to that of fully locally stored and rendered high-quality imagery.",,,,,,ARXIV 2021,,,,,,
6/15/2021 16:07,,,In-Place Scene Labelling and Understanding with Implicit Scene Representation,Semantic-NeRF,ICCV,3/29/2021,2021,"@inproceedings{zhi2021semanticnerf,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Shuaifeng Zhi and Tristan Laidlow and Stefan Leutenegger and Andrew J. Davison},
    title = {In-Place Scene Labelling and Understanding with Implicit Scene Representation},
    year = {2021},
    url = {http://arxiv.org/abs/2103.15875v2},
    entrytype = {inproceedings},
    id = {zhi2021semanticnerf}
}",https://arxiv.org/pdf/2103.15875.pdf,,,,,,,Beyond Visual Computing,,,,,,,,,,"Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, Andrew J. Davison",zhi2021semanticnerf,129,1,"Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, but the same fully self-supervised approach is not possible for semantics because labels are human-defined properties. We extend neural radiance fields (NeRF) to jointly encode semantics with appearance and geometry, so that complete and accurate 2D semantic labels can be achieved using a small amount of in-place annotations specific to the scene. The intrinsic multi-view consistency and smoothness of NeRF benefit semantics by enabling sparse labels to efficiently propagate. We show the benefit of this approach when labels are either sparse or very noisy in room-scale scenes. We demonstrate its advantageous properties in various interesting applications such as an efficient scene labelling tool, novel semantic view synthesis, label denoising, super-resolution, label interpolation and multi-view semantic label fusion in visual semantic mapping systems.",,,,,,ICCV 2021,,,,,,
5/23/2021 18:42,,,MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo,MVSNeRF,ICCV,3/29/2021,2021,"@inproceedings{chen2021mvsnerf,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Anpei Chen and Zexiang Xu and Fuqiang Zhao and Xiaoshuai Zhang and Fanbo Xiang and Jingyi Yu and Hao Su},
    title = {MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo},
    year = {2021},
    url = {http://arxiv.org/abs/2103.15595v2},
    entrytype = {inproceedings},
    id = {chen2021mvsnerf}
}",https://arxiv.org/pdf/2103.15595.pdf,https://apchenstu.github.io/mvsnerf/,https://github.com/apchenstu/mvsnerf,,https://www.youtube.com/watch?v=68N21TacPxw,,,"Speed & Computational Efficiency, Sparse Reconstruction, Generalization, Data-Driven Method, Local Conditioning",,,,,,,,,,"Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, Hao Su",chen2021mvsnerf,128,4,"We present MVSNeRF, a novel neural rendering approach that can efficiently reconstruct neural radiance fields for view synthesis. Unlike prior works on neural radiance fields that consider per-scene optimization on densely captured images, we propose a generic deep neural network that can reconstruct radiance fields from only three nearby input views via fast network inference. Our approach leverages plane-swept cost volumes (widely used in multi-view stereo) for geometry-aware scene reasoning, and combines this with physically based volume rendering for neural radiance field reconstruction. We train our network on real objects in the DTU dataset, and test it on three different datasets to evaluate its effectiveness and generalizability. Our approach can generalize across scenes (even indoor scenes, completely different from our training scenes of objects) and generate realistic view synthesis results using only three input images, significantly outperforming concurrent works on generalizable radiance field reconstruction. Moreover, if dense images are captured, our estimated radiance field representation can be easily fine-tuned; this leads to fast per-scene reconstruction with higher rendering quality and substantially less optimization time than NeRF.",,,,,,ICCV 2021,,,,,,
5/23/2021 18:43,,,GNeRF: GAN-based Neural Radiance Field without Posed Camera,GNeRF,ICCV,3/29/2021,2021,"@article{meng2021gnerf,
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    author = {Quan Meng and Anpei Chen and Haimin Luo and Minye Wu and Hao Su and Lan Xu and Xuming He and Jingyi Yu},
    title = {GNeRF: GAN-based Neural Radiance Field without Posed Camera},
    year = {2021},
    url = {http://arxiv.org/abs/2103.15606v3},
    entrytype = {article},
    id = {meng2021gnerf}
}",https://arxiv.org/pdf/2103.15606.pdf,,https://github.com/MQ66/gnerf,,,,,"Camera Parameter Estimation, Generative Models",,,,,,,,,,"Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, Jingyi Yu",meng2021gnerf,127,1,"We introduce GNeRF, a framework to marry Generative Adversarial Networks (GAN) with Neural Radiance Field (NeRF) reconstruction for the complex scenarios with unknown and even randomly initialized camera poses. Recent NeRF-based advances have gained popularity for remarkable realistic novel view synthesis. However, most of them heavily rely on accurate camera poses estimation, while few recent methods can only optimize the unknown camera poses in roughly forward-facing scenes with relatively short camera trajectories and require rough camera poses initialization. Differently, our GNeRF only utilizes randomly initialized poses for complex outside-in scenarios. We propose a novel two-phases end-to-end framework. The first phase takes the use of GANs into the new realm for optimizing coarse camera poses and radiance fields jointly, while the second phase refines them with additional photometric loss. We overcome local minima using a hybrid and iterative optimization scheme. Extensive experiments on a variety of synthetic and natural scenes demonstrate the effectiveness of GNeRF. More impressively, our approach outperforms the baselines favorably in those scenes with repeated patterns or even low textures that are regarded as extremely challenging before.",,,,,,ICCV 2021,,,,,,
8/31/2021 16:19,,,MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis,MINE,ICCV,3/27/2021,2021,"@inproceedings{li2021mine,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Jiaxin Li and Zijian Feng and Qi She and Henghui Ding and Changhu Wang and Gim Hee Lee},
    title = {MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis},
    year = {2021},
    url = {http://arxiv.org/abs/2103.14910v3},
    entrytype = {inproceedings},
    id = {li2021mine}
}",https://arxiv.org/pdf/2103.14910.pdf,https://vincentfung13.github.io/projects/mine/,https://github.com/vincentfung13/MINE,,,,,"Sparse Reconstruction, Generalization, Image-Based Rendering, Data-Driven Method, Hybrid Geometry Representation",Fourier Feature (NeRF),MPI,,,,,,,,"Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu Wang, Gim Hee Lee",li2021mine,126,0,"In this paper, we propose MINE to perform novel view synthesis and depth estimation via dense 3D reconstruction from a single image. Our approach is a continuous depth generalization of the Multiplane Images (MPI) by introducing the NEural radiance fields (NeRF). Given a single image as input, MINE predicts a 4-channel image (RGB and volume density) at arbitrary depth values to jointly reconstruct the camera frustum and fill in occluded contents. The reconstructed and inpainted frustum can then be easily rendered into novel RGB or depth views using differentiable rendering. Extensive experiments on RealEstate10K, KITTI and Flowers Light Fields show that our MINE outperforms state-of-the-art by a large margin in novel view synthesis. We also achieve competitive results in depth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our source code is available at https://github.com/vincentfung13/MINE",No,,"Direct, Indirect",,,ICCV 2021,,,,,,
5/23/2021 18:46,,,Baking Neural Radiance Fields for Real-Time View Synthesis,SNeRG,ARXIV,3/26/2021,2021,"@article{hedman2021snerg,
    journal = {arXiv preprint arXiv:2103.14645},
    booktitle = {ArXiv Pre-print},
    author = {Peter Hedman and Pratul P. Srinivasan and Ben Mildenhall and Jonathan T. Barron and Paul Debevec},
    title = {Baking Neural Radiance Fields for Real-Time View Synthesis},
    year = {2021},
    url = {http://arxiv.org/abs/2103.14645v1},
    entrytype = {article},
    id = {hedman2021snerg}
}",https://arxiv.org/pdf/2103.14645.pdf,https://phog.github.io/snerg/,,,https://www.youtube.com/watch?v=5jKry8n5YO8,,,"Speed & Computational Efficiency, Compression, Local Conditioning, Voxel Grid",,,,,,,,,,"Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. Barron, Paul Debevec",hedman2021snerg,125,2,"Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF's computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. ""bake"") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF's ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.",,,,,,ARXIV 2021,,,,,,
5/25/2021 15:13,,,PlenOctrees for Real-time Rendering of Neural Radiance Fields,"NeRF-SH, PlenOctrees",ICCV,3/25/2021,2021,"@inproceedings{yu2021nerfsh,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Alex Yu and Ruilong Li and Matthew Tancik and Hao Li and Ren Ng and Angjoo Kanazawa},
    title = {PlenOctrees for Real-time Rendering of Neural Radiance Fields},
    year = {2021},
    url = {http://arxiv.org/abs/2103.14024v2},
    entrytype = {inproceedings},
    id = {yu2021nerfsh}
}",https://arxiv.org/pdf/2103.14024.pdf,https://alexyu.net/plenoctrees/,"https://github.com/sxyu/plenoctree, https://github.com/sxyu/volrend",,,,,"Speed & Computational Efficiency, Material/Lighting Estimation, Sampling, Hybrid Geometry Representation",,,,,,,,,,"Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, Angjoo Kanazawa",yu2021nerfsh,124,12,"We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800x800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: https://alexyu.net/plenoctrees",,,,,,ICCV 2021,,,,,,
8/5/2021 15:51,,,KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs,KiloNeRF,ICCV,3/25/2021,2021,"@inproceedings{reiser2021kilonerf,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Christian Reiser and Songyou Peng and Yiyi Liao and Andreas Geiger},
    title = {KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs},
    year = {2021},
    url = {http://arxiv.org/abs/2103.13744v2},
    entrytype = {inproceedings},
    id = {reiser2021kilonerf}
}",https://arxiv.org/pdf/2103.13744.pdf,,,,,,,"Speed & Computational Efficiency, Sampling, Voxel Grid, Hybrid Geometry Representation",Fourier Feature (NeRF),Density,No,,,,,,,"Christian Reiser, Songyou Peng, Yiyi Liao, Andreas Geiger",reiser2021kilonerf,123,6,"NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that real-time rendering is possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by three orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.",,,Direct,,,ICCV 2021,,,,,,
5/23/2021 18:46,,,Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields,Mip-NeRF,ICCV,3/24/2021,2021,"@inproceedings{barron2021mipnerf,
    url = {http://arxiv.org/abs/2103.13415v3},
    year = {2021},
    title = {Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields},
    author = {Jonathan T. Barron and Ben Mildenhall and Matthew Tancik and Peter Hedman and Ricardo Martin-Brualla and Pratul P. Srinivasan},
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    entrytype = {inproceedings},
    id = {barron2021mipnerf}
}",https://arxiv.org/pdf/2103.13415.pdf,https://jonbarron.info/mipnerf/,,,https://www.youtube.com/watch?v=EpH175PY1A0,,,"Fundamentals, Sampling",,,,,,,,,,"Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan",barron2021mipnerf,122,3,"The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call ""mip-NeRF"" (a la ""mipmap""), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22x faster.",,,,,,ICCV 2021,,,,,,
5/23/2021 18:47,,,iMAP: Implicit Mapping and Positioning in Real-Time,iMAP,ICCV,3/23/2021,2021,"@inproceedings{sucar2021imap,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Edgar Sucar and Shikun Liu and Joseph Ortiz and Andrew J. Davison},
    title = {iMAP: Implicit Mapping and Positioning in Real-Time},
    year = {2021},
    url = {http://arxiv.org/abs/2103.12352v2},
    entrytype = {inproceedings},
    id = {sucar2021imap}
}",https://arxiv.org/pdf/2103.12352.pdf,https://edgarsucar.github.io/iMAP/,,,https://www.youtube.com/watch?v=c-zkKGArl5Y,,,"Camera Parameter Estimation, Robotics, Multi-task/Continual/Transfer learning, Sampling",,,,,,,,"RGB, Depth",,"Edgar Sucar, Shikun Liu, Joseph Ortiz, Andrew J. Davison",sucar2021imap,121,3,"We show for the first time that a multilayer perceptron (MLP) can serve as the only scene representation in a real-time SLAM system for a handheld RGB-D camera. Our network is trained in live operation without prior data, building a dense, scene-specific implicit 3D model of occupancy and colour which is also immediately used for tracking. Achieving real-time SLAM via continual training of a neural network against a live image stream requires significant innovation. Our iMAP algorithm uses a keyframe structure and multi-processing computation flow, with dynamic information-guided pixel sampling for speed, with tracking at 10 Hz and global map updating at 2 Hz. The advantages of an implicit MLP over standard dense SLAM techniques include efficient geometry representation with automatic detail control and smooth, plausible filling-in of unobserved regions such as the back surfaces of objects.",,,,,,ICCV 2021,,,,,,
6/21/2021 16:43,,,Neural Lumigraph Rendering,NLR,CVPR,3/22/2021,2021,"@inproceedings{kellnhofer2021nlr,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Petr Kellnhofer and Lars Jebe and Andrew Jones and Ryan Spicer and Kari Pulli and Gordon Wetzstein},
    title = {Neural Lumigraph Rendering},
    year = {2021},
    url = {http://arxiv.org/abs/2103.11571v1},
    entrytype = {inproceedings},
    id = {kellnhofer2021nlr}
}",https://arxiv.org/pdf/2103.11571.pdf,http://www.computationalimaging.org/publications/nlr/,,,https://www.youtube.com/watch?v=maVF-7x9644,https://openaccess.thecvf.com/content/CVPR2021/supplemental/Kellnhofer_Neural_Lumigraph_Rendering_CVPR_2021_supplemental.pdf,,"Speed & Computational Efficiency, Image-Based Rendering",,SDF,,,,,,,,"Petr Kellnhofer, Lars Jebe, Andrew Jones, Ryan Spicer, Kari Pulli, Gordon Wetzstein",kellnhofer2021nlr,120,8,"Novel view synthesis is a challenging and ill-posed inverse rendering problem. Neural rendering techniques have recently achieved photorealistic image quality for this task. State-of-the-art (SOTA) neural volume rendering approaches, however, are slow to train and require minutes of inference (i.e., rendering) time for high image resolutions. We adopt high-capacity neural scene representations with periodic activations for jointly optimizing an implicit surface and a radiance field of a scene supervised exclusively with posed 2D images. Our neural rendering pipeline accelerates SOTA neural volume rendering by about two orders of magnitude and our implicit surface representation is unique in allowing us to export a mesh with view-dependent texture information. Thus, like other implicit surface representations, ours is compatible with traditional graphics pipelines, enabling real-time rendering rates, while achieving unprecedented image quality compared to other surface methods. We assess the quality of our approach using existing datasets as well as high-quality 3D face data captured with a custom multi-camera rig.",,,,,,CVPR 2021,,,,,,
5/23/2021 18:44,,,AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis,AD-NeRF,ICCV,3/20/2021,2021,"@inproceedings{guo2021adnerf,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Yudong Guo and Keyu Chen and Sen Liang and Yong-Jin Liu and Hujun Bao and Juyong Zhang},
    title = {AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis},
    year = {2021},
    url = {http://arxiv.org/abs/2103.11078v3},
    entrytype = {inproceedings},
    id = {guo2021adnerf}
}",https://arxiv.org/pdf/2103.11078.pdf,,,,https://www.youtube.com/watch?v=TQO2EBYXLyU,,,"Dynamic/Temporal, Audio, Global Conditioning, Local Conditioning",,,,,,,,,,"Yudong Guo, Keyu Chen, Sen Liang, Yongjin Liu, Hujun Bao, Juyong Zhang",guo2021adnerf,119,0,"Generating high-fidelity talking head video by fitting with the input audio sequence is a challenging problem that receives considerable attentions recently. In this paper, we address this problem with the aid of neural scene representation networks. Our method is completely different from existing methods that rely on intermediate representations like 2D landmarks or 3D face models to bridge the gap between audio input and video output. Specifically, the feature of input audio signal is directly fed into a conditional implicit function to generate a dynamic neural radiance field, from which a high-fidelity talking-head video corresponding to the audio signal is synthesized using volume rendering. Another advantage of our framework is that not only the head (with hair) region is synthesized as previous methods did, but also the upper body is generated via two individual neural radiance fields. Experimental results demonstrate that our novel framework can (1) produce high-fidelity and natural results, and (2) support free adjustment of audio signals, viewing directions, and background images. Code is available at https://github.com/YudongGuo/AD-NeRF.",,,,,,ICCV 2021,,,,,,
9/17/2021 11:42,,,SMPLicit: Topology-aware Generative Model for Clothed People,SMPLicit,CVPR,3/11/2021,2021,"@inproceedings{corona2021smplicit,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Enric Corona and Albert Pumarola and Guillem Alenya and Gerard Pons-Moll and Francesc Moreno-Noguer},
    title = {SMPLicit: Topology-aware Generative Model for Clothed People},
    year = {2021},
    url = {http://arxiv.org/abs/2103.06871v2},
    entrytype = {inproceedings},
    id = {corona2021smplicit}
}",https://arxiv.org/pdf/2103.06871.pdf,http://www.iri.upc.edu/people/ecorona/smplicit/,https://github.com/enriccorona/SMPLicit,,,,,"Human (Body), Editable, Generative Models, Global Conditioning",Fourier Feature (NeRF),UDF,No,,,,,,,"Enric Corona, Albert Pumarola, Guillem Alenyà, Gerard Pons-Moll, Francesc Moreno-Noguer",corona2021smplicit,118,,"In this paper we introduce SMPLicit, a novel generative model to jointly represent body pose, shape and clothing geometry. In contrast to existing learning-based approaches that require training specific models for each type of garment, SMPLicit can represent in a unified manner different garment topologies (e.g. from sleeveless tops to hoodies and to open jackets), while controlling other properties like the garment size or tightness/looseness. We show our model to be applicable to a large variety of garments including T-shirts, hoodies, jackets, shorts, pants, skirts, shoes and even hair. The representation flexibility of SMPLicit builds upon an implicit model conditioned with the SMPL human body parameters and a learnable latent space which is semantically interpretable and aligned with the clothing attributes. The proposed model is fully differentiable, allowing for its use into larger end-to-end trainable systems. In the experimental section, we demonstrate SMPLicit can be readily used for fitting 3D scans and for 3D reconstruction in images of dressed people. In both cases we are able to go beyond state of the art, by retrieving complex garment geometries, handling situations with multiple clothing layers and providing a tool for easy outfit editing. To stimulate further research in this direction, we will make our code and model publicly available at http://www.iri.upc.edu/people/ecorona/smplicit/.",,,Direct,,,CVPR 2021,,,,,,
5/23/2021 18:48,,,NeX: Real-time View Synthesis with Neural Basis Expansion,NeX,CVPR,3/9/2021,2021,"@inproceedings{wizadwongsa2021nex,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Suttisak Wizadwongsa and Pakkapon Phongthawee and Jiraphon Yenphraphai and Supasorn Suwajanakorn},
    title = {NeX: Real-time View Synthesis with Neural Basis Expansion},
    year = {2021},
    url = {http://arxiv.org/abs/2103.05606v2},
    entrytype = {inproceedings},
    id = {wizadwongsa2021nex}
}",https://arxiv.org/pdf/2103.05606.pdf,https://nex-mpi.github.io/,https://github.com/nex-mpi/nex-code/,https://vistec-my.sharepoint.com/personal/pakkapon_p_s19_vistec_ac_th/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fpakkapon%5Fp%5Fs19%5Fvistec%5Fac%5Fth%2FDocuments%2Fpublic%2FVLL%2FNeX%2Fshiny%5Fdatasets&originalPath=aHR0cHM6Ly92aXN0ZWMtbXkuc2hhcmVwb2ludC5jb20vOmY6L2cvcGVyc29uYWwvcGFra2Fwb25fcF9zMTlfdmlzdGVjX2FjX3RoL0VuSVVoc1JWSk9kTnNaXzRzbWRoeWUwQjh6MFZseHFPUjM1SVIzYnAwdUd1cFE%5FcnRpbWU9c1hYTTNEd2UyVWc,,,,"Speed & Computational Efficiency, Hybrid Geometry Representation",,,,,,,,,,"Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, Supasorn Suwajanakorn",wizadwongsa2021nex,117,9,"We present NeX, a new approach to novel view synthesis based on enhancements of multiplane image (MPI) that can reproduce next-level view-dependent effects -- in real time. Unlike traditional MPI that uses a set of simple RGB$\alpha$ planes, our technique models view-dependent effects by instead parameterizing each pixel as a linear combination of basis functions learned from a neural network. Moreover, we propose a hybrid implicit-explicit modeling strategy that improves upon fine detail and produces state-of-the-art results. Our method is evaluated on benchmark forward-facing datasets as well as our newly-introduced dataset designed to test the limit of view-dependent modeling with significantly more challenging effects such as rainbow reflections on a CD. Our method achieves the best overall scores across all major metrics on these datasets with more than 1000$\times$ faster rendering time than the state of the art. For real-time demos, visit https://nex-mpi.github.io/",,,,,,CVPR 2021,,,,,,
5/25/2021 0:11,,,DONeRF: Towards Real-Time Rendering of Neural Radiance Fields using Depth Oracle Networks,DONeRF,EGSR,3/4/2021,2021,"@article{neff2021donerf,
    publisher = {The Eurographics Association and John Wiley & Sons Ltd.},
    journal = {Computer Graphics Forum},
    author = {Thomas Neff and Pascal Stadlbauer and Mathias Parger and Andreas Kurz and Joerg H. Mueller and Chakravarty R. Alla Chaitanya and Anton Kaplanyan and Markus Steinberger},
    title = {DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks},
    doi = {10.1111/cgf.14340},
    year = {2021},
    url = {http://arxiv.org/abs/2103.03231v4},
    entrytype = {article},
    id = {neff2021donerf}
}",https://arxiv.org/pdf/2103.03231.pdf,,,,,,,"Speed & Computational Efficiency, Sampling, Data-Driven Method",,,,,,,,,,"Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, Anton Kaplanyan, Markus Steinberger",neff2021donerf,116,6,"The recent research explosion around implicit neural representations, such as NeRF, shows that there is immense potential for implicitly storing high-quality scene and lighting information in compact neural networks. However, one major limitation preventing the use of NeRF in real-time rendering applications is the prohibitive computational cost of excessive network evaluations along each view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural representations closer to practical rendering of synthetic content in real-time applications, such as games and virtual reality. We show that the number of samples required for each view ray can be significantly reduced when samples are placed around surfaces in the scene without compromising image quality. To this end, we propose a depth oracle network that predicts ray sample locations for each view ray with a single network evaluation. We show that using a classification network around logarithmically discretized and spherically warped depth values is essential to encode surface locations rather than directly estimating depth. The combination of these techniques leads to DONeRF, our compact dual network design with a depth oracle network as its first step and a locally sampled shading network for ray accumulation. With DONeRF, we reduce the inference costs by up to 48x compared to NeRF when conditioning on available ground truth depth information. Compared to concurrent acceleration methods for raymarching-based neural representations, DONeRF does not require additional memory for explicit caching or acceleration structures, and can render interactively (20 frames per second) on a single GPU.",,,,,,EGSR 2021,,,,,,
5/23/2021 18:50,,,COIN: COmpression with Implicit Neural representations,COIN,ICLR,3/3/2021,2021,"@inproceedings{dupont2021coin,
    booktitle = {International Conference on Learning Representations},
    author = {Emilien Dupont and Adam Golinski and Milad Alizadeh and Yee Whye Teh and Arnaud Doucet},
    title = {COIN: COmpression with Implicit Neural representations},
    year = {2021},
    url = {http://arxiv.org/abs/2103.03123v2},
    entrytype = {inproceedings},
    id = {dupont2021coin}
}",https://arxiv.org/pdf/2103.03123.pdf,,,,https://www.youtube.com/watch?v=FjPurtmqgmw,,,Compression,Sinusoidal Activation (SIREN),,,,,,,,,"Emilien Dupont, Adam Goliński, Milad Alizadeh, Yee Whye Teh, Arnaud Doucet",dupont2021coin,115,0,"We propose a new simple approach for image compression: instead of storing the RGB values for each pixel of an image, we store the weights of a neural network overfitted to the image. Specifically, to encode an image, we fit it with an MLP which maps pixel locations to RGB values. We then quantize and store the weights of this MLP as a code for the image. To decode the image, we simply evaluate the MLP at every pixel location. We found that this simple approach outperforms JPEG at low bit-rates, even without entropy coding or learning a distribution over weights. While our framework is not yet competitive with state of the art compression methods, we show that it has various attractive properties which could make it a viable alternative to other neural data compression approaches.",,,,,,ICLR 2021,,,,,,
5/23/2021 18:50,,,Neural 3D Video Synthesis,DyNeRF,ARXIV,3/3/2021,2021,"@article{li2021dynerf,
    journal = {arXiv preprint arXiv:2103.02597},
    booktitle = {ArXiv Pre-print},
    author = {Tianye Li and Mira Slavcheva and Michael Zollhoefer and Simon Green and Christoph Lassner and Changil Kim and Tanner Schmidt and Steven Lovegrove and Michael Goesele and Zhaoyang Lv},
    title = {Neural 3D Video Synthesis},
    year = {2021},
    url = {http://arxiv.org/abs/2103.02597v1},
    entrytype = {article},
    id = {li2021dynerf}
}",https://arxiv.org/pdf/2103.02597.pdf,https://neural-3d-video.github.io/,,Coming soon,https://neural-3d-video.github.io/resources/video.mp4,,,"Dynamic/Temporal, Global Conditioning, Local Conditioning",,,,,,,,,,"Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Zhaoyang Lv",li2021dynerf,114,3,"We propose a novel approach for 3D video synthesis that is able to represent multi-view video recordings of a dynamic real-world scene in a compact, yet expressive representation that enables high-quality view synthesis and motion interpolation. Our approach takes the high quality and compactness of static neural radiance fields in a new direction: to a model-free, dynamic setting. At the core of our approach is a novel time-conditioned neural radiance fields that represents scene dynamics using a set of compact latent codes. To exploit the fact that changes between adjacent frames of a video are typically small and locally consistent, we propose two novel strategies for efficient training of our neural network: 1) An efficient hierarchical training scheme, and 2) an importance sampling strategy that selects the next rays for training based on the temporal variation of the input videos. In combination, these two strategies significantly boost the training speed, lead to fast convergence of the training process, and enable high quality results. Our learned representation is highly compact and able to represent a 10 second 30 FPS multi-view video recording by 18 cameras with a model size of just 28MB. We demonstrate that our method can render high-fidelity wide-angle novel views at over 1K resolution, even for highly complex and dynamic scenes. We perform an extensive qualitative and quantitative evaluation that shows that our approach outperforms the current state of the art. We include additional video and information at: https://neural-3d-video.github.io/",,,,,,ARXIV 2021,,,,,,
5/23/2021 18:56,,,NeuTex: Neural Texture Mapping for Volumetric Neural Rendering,NeuTex,CVPR,3/1/2021,2021,"@inproceedings{xiang2021neutex,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Fanbo Xiang and Zexiang Xu and Milos Hasan and Yannick Hold-Geoffroy and Kalyan Sunkavalli and Hao Su},
    title = {NeuTex: Neural Texture Mapping for Volumetric Neural Rendering},
    year = {2021},
    url = {http://arxiv.org/abs/2103.00762v1},
    entrytype = {inproceedings},
    id = {xiang2021neutex}
}",https://arxiv.org/pdf/2103.00762.pdf,,,,,,,Fundamentals,,,,,,,,,,"Fanbo Xiang, Zexiang Xu, Miloš Hašan, Yannick Hold-Geoffroy, Kalyan Sunkavalli, Hao Su",xiang2021neutex,113,2,"Recent work has demonstrated that volumetric scene representations combined with differentiable volume rendering can enable photo-realistic rendering for challenging scenes that mesh reconstruction fails on. However, these methods entangle geometry and appearance in a ""black-box"" volume that cannot be edited. Instead, we present an approach that explicitly disentangles geometry--represented as a continuous 3D volume--from appearance--represented as a continuous 2D texture map. We achieve this by introducing a 3D-to-2D texture mapping (or surface parameterization) network into volumetric representations. We constrain this texture mapping network using an additional 2D-to-3D inverse mapping network and a novel cycle consistency loss to make 3D surface points map to 2D texture points that map back to the original 3D points. We demonstrate that this representation can be reconstructed using only multi-view image supervision and generates high-quality rendering results. More importantly, by separating geometry and texture, we allow users to edit appearance by simply editing 2D texture maps.",,,,,,CVPR 2021,,,,,,
5/23/2021 18:52,,,IBRNet: Learning Multi-View Image-Based Rendering,IBRNet,CVPR,2/25/2021,2021,"@inproceedings{wang2021ibrnet,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Qianqian Wang and Zhicheng Wang and Kyle Genova and Pratul Srinivasan and Howard Zhou and Jonathan T. Barron and Ricardo Martin-Brualla and Noah Snavely and Thomas Funkhouser},
    title = {IBRNet: Learning Multi-View Image-Based Rendering},
    year = {2021},
    url = {http://arxiv.org/abs/2102.13090v2},
    entrytype = {inproceedings},
    id = {wang2021ibrnet}
}",https://arxiv.org/pdf/2102.13090.pdf,https://ibrnet.github.io/,,,,,,"Speed & Computational Efficiency, Generalization, Image-Based Rendering, Data-Driven Method, Local Conditioning",,,,,,,,,,"Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, Thomas Funkhouser",wang2021ibrnet,112,20,"We present a method that synthesizes novel views of complex scenes by interpolating a sparse set of nearby views. The core of our method is a network architecture that includes a multilayer perceptron and a ray transformer that estimates radiance and volume density at continuous 5D locations (3D spatial locations and 2D viewing directions), drawing appearance information on the fly from multiple source views. By drawing on source views at render time, our method hearkens back to classic work on image-based rendering (IBR), and allows us to render high-resolution imagery. Unlike neural scene representation work that optimizes per-scene functions for rendering, we learn a generic view interpolation function that generalizes to novel scenes. We render images using classic volume rendering, which is fully differentiable and allows us to train using only multi-view posed images as supervision. Experiments show that our method outperforms recent novel view synthesis methods that also seek to generalize to novel scenes. Further, if fine-tuned on each scene, our method is competitive with state-of-the-art single-scene neural rendering methods. Project page: https://ibrnet.github.io/",,,,,,CVPR 2021,,,,,,
5/23/2021 18:53,,,NTopo: Mesh-free Topolibogy Optimization using Implicit Neural Representations,NTopo,ARXIV,2/22/2021,2021,"@article{zehnder2021ntopo,
    journal = {arXiv preprint arXiv:2102.10782},
    booktitle = {ArXiv Pre-print},
    author = {Jonas Zehnder and Yue Li and Stelian Coros and Bernhard Thomaszewski},
    title = {NTopo: Mesh-free Topology Optimization using Implicit Neural Representations},
    year = {2021},
    url = {http://arxiv.org/abs/2102.10782v1},
    entrytype = {article},
    id = {zehnder2021ntopo}
}",https://arxiv.org/pdf/2102.10782.pdf,,,,,,,"Science & Engineering, Supervision by Gradient (PDE)",,,,,,,,,,"Jonas Zehnder, Yue Li, Stelian Coros, Bernhard Thomaszewski",zehnder2021ntopo,111,0,"Recent advances in implicit neural representations show great promise when it comes to generating numerical solutions to partial differential equations (PDEs). Compared to conventional alternatives, such representations employ parameterized neural networks to define, in a mesh-free manner, signals that are highly-detailed, continuous, and fully differentiable. Most prior works aim to exploit these benefits in order to solve PDE-governed forward problems, or associated inverse problems that are defined by a small number of parameters. In this work, we present a novel machine learning approach to tackle topology optimization (TO) problems. Topology optimization refers to an important class of inverse problems that typically feature very high-dimensional parameter spaces and objective landscapes which are highly non-linear. To effectively leverage neural representations in the context of TO problems, we use multilayer perceptrons (MLPs) to parameterize both density and displacement fields. Using sensitivity analysis with a moving mean squared error, we show that our formulation can be used to efficiently minimize traditional structural compliance objectives. As we show through our experiments, a major benefit of our approach is that it enables self-supervised learning of continuous solution spaces to topology optimization problems.",,,,,,ARXIV 2021,,,,,,
7/26/2021 14:37,,,ShaRF: Shape-conditioned Radiance Fields from a Single View,ShaRF,ICML,2/17/2021,2021,"@inproceedings{rematas2021sharf,
    publisher = {PMLR},
    booktitle = {International Conference on Machine Learning (ICML)},
    author = {Konstantinos Rematas and Ricardo Martin-Brualla and Vittorio Ferrari},
    title = {ShaRF: Shape-conditioned Radiance Fields from a Single View},
    year = {2021},
    url = {http://arxiv.org/abs/2102.08860v2},
    entrytype = {inproceedings},
    id = {rematas2021sharf}
}",https://arxiv.org/pdf/2102.08860.pdf,http://www.krematas.com/sharf/,,,,,,"Sparse Reconstruction, Generalization, Generative Models, Data-Driven Method, Local Conditioning, Voxel Grid, Sampling",Fourier Feature (NeRF),Density,No,,,,,,,"Konstantinos Rematas, Ricardo Martin-Brualla, Vittorio Ferrari",rematas2021sharf,110,5,"We present a method for estimating neural scenes representations of objects given only a single image. The core of our method is the estimation of a geometric scaffold for the object and its use as a guide for the reconstruction of the underlying radiance field. Our formulation is based on a generative process that first maps a latent code to a voxelized shape, and then renders it to an image, with the object appearance being controlled by a second latent code. During inference, we optimize both the latent codes and the networks to fit a test image of a new object. The explicit disentanglement of shape and appearance allows our model to be fine-tuned given a single image. We can then render new views in a geometrically consistent manner and they represent faithfully the input object. Additionally, our method is able to generalize to images outside of the training domain (more realistic renderings and even real photographs). Finally, the inferred geometric scaffold is itself an accurate estimate of the object's 3D shape. We demonstrate in several experiments the effectiveness of our approach in both synthetic and real images.",,,Direct,,,ICML 2021,,,,,,
5/23/2021 18:54,,,NeRF−−: Neural Radiance Fields Without Known Camera Parameters,NeRF−−,ARXIV,2/14/2021,2021,"@article{wang2021nerf--,
    journal = {arXiv preprint arXiv:2102.07064},
    booktitle = {ArXiv Pre-print},
    author = {Zirui Wang and Shangzhe Wu and Weidi Xie and Min Chen and Victor Adrian Prisacariu},
    title = {NeRF--: Neural Radiance Fields Without Known Camera Parameters},
    year = {2021},
    url = {http://arxiv.org/abs/2102.07064v3},
    entrytype = {article},
    id = {wang2021nerf--}
}",https://arxiv.org/pdf/2102.07064.pdf,,https://github.com/ActiveVisionLab/nerfmm,,,,,Camera Parameter Estimation,,,,,,,,,,"Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, Victor Adrian Prisacariu",wang2021nerf--,109,9,"This paper tackles the problem of novel view synthesis (NVS) from 2D images without known camera poses and intrinsics. Among various NVS techniques, Neural Radiance Field (NeRF) has recently gained popularity due to its remarkable synthesis quality. Existing NeRF-based approaches assume that the camera parameters associated with each input image are either directly accessible at training, or can be accurately estimated with conventional techniques based on correspondences, such as Structure-from-Motion. In this work, we propose an end-to-end framework, termed NeRF--, for training NeRF models given only RGB images, without pre-computed camera parameters. Specifically, we show that the camera parameters, including both intrinsics and extrinsics, can be automatically discovered via joint optimisation during the training of the NeRF model. On the standard LLFF benchmark, our model achieves comparable novel view synthesis results compared to the baseline trained with COLMAP pre-computed camera parameters. We also conduct extensive analyses to understand the model behaviour under different camera trajectories, and show that in scenarios where COLMAP fails, our model still produces robust results.",,,,,,ARXIV 2021,,,,,,
5/23/2021 18:54,,,A-NeRF: Surface-free Human 3D Pose Refinement via Neural Rendering,A-NeRF,ARXIV,2/11/2021,2021,"@article{su2021anerf,
    journal = {arXiv preprint arXiv:2102.06199},
    booktitle = {ArXiv Pre-print},
    author = {Shih-Yang Su and Frank Yu and Michael Zollhoefer and Helge Rhodin},
    title = {A-NeRF: Surface-free Human 3D Pose Refinement via Neural Rendering},
    year = {2021},
    url = {http://arxiv.org/abs/2102.06199v1},
    entrytype = {article},
    id = {su2021anerf}
}",https://arxiv.org/pdf/2102.06199.pdf,,,,,,,"Dynamic/Temporal, Human (Body)",,,,,,,,,,"Shih-Yang Su, Frank Yu, Michael Zollhoefer, Helge Rhodin",su2021anerf,108,1,"While deep learning has reshaped the classical motion capture pipeline, generative, analysis-by-synthesis elements are still in use to recover fine details if a high-quality 3D model of the user is available. Unfortunately, obtaining such a model for every user a priori is challenging, time-consuming, and limits the application scenarios. We propose a novel test-time optimization approach for monocular motion capture that learns a volumetric body model of the user in a self-supervised manner. To this end, our approach combines the advantages of neural radiance fields with an articulated skeleton representation. Our proposed skeleton embedding serves as a common reference that links constraints across time, thereby reducing the number of required camera views from traditionally dozens of calibrated cameras, down to a single uncalibrated one. As a starting point, we employ the output of an off-the-shelf model that predicts the 3D skeleton pose. The volumetric body shape and appearance is then learned from scratch, while jointly refining the initial pose estimate. Our approach is self-supervised and does not require any additional ground truth labels for appearance, pose, or 3D shape. We demonstrate that our novel combination of a discriminative pose estimation technique with surface-free analysis-by-synthesis outperforms purely discriminative monocular pose estimation approaches and generalizes well to multiple views.",,,,,,ARXIV 2021,,,,,,
7/19/2021 21:41,,,CoIL: Coordinate-based Internal Learning for Imaging Inverse Problems,CoIL,"IEEE Transactions on Computational Imaging, vol. 7, pp. 1400-1412",2/9/2021,2021,"@article{9606601,
    doi = {10.1109/TCI.2021.3125564},
    pages = {1400-1412},
    number = {},
    volume = {7},
    year = {2021},
    title = {CoIL: Coordinate-Based Internal Learning for Tomographic Imaging},
    journal = {IEEE Transactions on Computational Imaging},
    author = {Sun, Yu and Liu, Jiaming and Xie, Mingyang and Wohlberg, Brendt and Kamilov, Ulugbek S.},
    entrytype = {article},
    id = {9606601}
}",https://ieeexplore.ieee.org/document/9606601,,https://github.com/wustl-cig/Cooridnate-based-Internal-Learning,https://www.youtube.com/watch?v=7LXagKec31U,,,,"Alternative Imaging, tomographic imaging, Positional Encoding",Linear encoding,,,,,,,,,"Yu Sun, Jiaming Liu, Mingyang Xie, Brendt Wohlberg, Ulugbek S. Kamilov",sun2021coil,107,3,"We propose Coordinate-based Internal Learning (CoIL) as a new deep-learning (DL) methodology for continuous representation of measurements. Unlike traditional DL methods that learn a mapping from the measurements to the desired image, CoIL trains a multilayer perceptron (MLP) to encode the complete measurement field by mapping the coordinates of the measurements to their responses. CoIL is a self-supervised method that requires no training examples besides the measurements of the test object itself. Once the MLP is trained, CoIL generates new measurements that can be used within most image reconstruction methods. We validate CoIL on sparse-view computed tomography using several widely-used reconstruction methods, including purely model-based methods and those based on DL. Our results demonstrate the ability of CoIL to consistently improve the performance of all the considered methods by providing high-fidelity measurement fields.",,,,,,"IEEE Transactions on Computational Imaging, vol. 7, pp. 1400-1412 2021",,,,,,
5/23/2021 18:53,,,Towards Generalising Neural Implicit Representations,,ARXIV,1/29/2021,2021,"@article{costain2021towards,
    journal = {arXiv preprint arXiv:2101.12690},
    booktitle = {ArXiv Pre-print},
    author = {Theo W. Costain and Victor Adrian Prisacariu},
    title = {Towards Generalising Neural Implicit Representations},
    year = {2021},
    url = {http://arxiv.org/abs/2101.12690v2},
    entrytype = {article},
    id = {costain2021towards}
}",https://arxiv.org/pdf/2101.12690.pdf,,,,,,,"Generalization, Global Conditioning",,,,,,,,,,"Theo W. Costain, Victor Adrian Prisacariu",costain2021towards,106,0,"Neural implicit representations have shown substantial improvements in efficiently storing 3D data, when compared to conventional formats. However, the focus of existing work has mainly been on storage and subsequent reconstruction. In this work, we show that training neural representations for reconstruction tasks alongside conventional tasks can produce more general encodings that admit equal quality reconstructions to single task training, whilst improving results on conventional tasks when compared to single task encodings. We reformulate the semantic segmentation task, creating a more representative task for implicit representation contexts, and through multi-task experiments on reconstruction, classification, and segmentation, show our approach learns feature rich encodings that admit equal performance for each task.",,,,,,ARXIV 2021,,,,,,
6/29/2021 15:40,,,Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes,NGLOD,CVPR,1/26/2021,2021,"@inproceedings{takikawa2021nglod,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Towaki Takikawa and Joey Litalien and Kangxue Yin and Karsten Kreis and Charles Loop and Derek Nowrouzezahrai and Alec Jacobson and Morgan McGuire and Sanja Fidler},
    title = {Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes},
    year = {2021},
    url = {http://arxiv.org/abs/2101.10994v1},
    entrytype = {inproceedings},
    id = {takikawa2021nglod}
}",https://arxiv.org/pdf/2101.10994.pdf,nv-tlabs.github.io/nglod,https://github.com/nv-tlabs/nglod,,https://www.youtube.com/watch?v=Pi7W6XrFtMs,,,"Speed & Computational Efficiency, Generalization, Coarse-to-Fine, Voxel Grid, Sampling, Hybrid Geometry Representation",None,SDF,"Yes, geometry only",,,,,,,"Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, Sanja Fidler",takikawa2021nglod,105,12,"Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2-3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.",,,,,,CVPR 2021,,,,,,
9/17/2021 11:46,,,"S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling",S3,CVPR,1/17/2021,2021,"@inproceedings{yang2021s3,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Ze Yang and Shenlong Wang and Sivabalan Manivasagam and Zeng Huang and Wei-Chiu Ma and Xinchen Yan and Ersin Yumer and Raquel Urtasun},
    title = {S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling},
    year = {2021},
    url = {http://arxiv.org/abs/2101.06571v1},
    entrytype = {inproceedings},
    id = {yang2021s3}
}",https://arxiv.org/pdf/2101.06571.pdf,,,,,,,"Human (Body), Editable, Voxel Grid, Local Conditioning",,Occupancy,"Yes, geometry only",,,,,,,"Ze Yang, Shenlong Wang, Sivabalan Manivasagam, Zeng Huang, Wei-Chiu Ma, Xinchen Yan, Ersin Yumer, Raquel Urtasun",yang2021s3,104,,"Constructing and animating humans is an important component for building virtual worlds in a wide variety of applications such as virtual reality or robotics testing in simulation. As there are exponentially many variations of humans with different shape, pose and clothing, it is critical to develop methods that can automatically reconstruct and animate humans at scale from real world data. Towards this goal, we represent the pedestrian's shape, pose and skinning weights as neural implicit functions that are directly learned from data. This representation enables us to handle a wide variety of different pedestrian shapes and poses without explicitly fitting a human parametric body model, allowing us to handle a wider range of human geometries and topologies. We demonstrate the effectiveness of our approach on various datasets and show that our reconstructions outperform existing state-of-the-art methods. Furthermore, our re-animation experiments show that we can generate 3D human animations at scale from a single RGB image (and/or an optional LiDAR sweep) as input.",,No,Direct,,,CVPR 2021,,,,,,
5/23/2021 18:56,,,Pixel-aligned Volumetric Avatars,PVA,CVPR,1/7/2021,2021,"@inproceedings{raj2021pva,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Amit Raj and Michael Zollhoefer and Tomas Simon and Jason Saragih and Shunsuke Saito and James Hays and Stephen Lombardi},
    title = {PVA: Pixel-aligned Volumetric Avatars},
    year = {2021},
    url = {http://arxiv.org/abs/2101.02697v1},
    entrytype = {inproceedings},
    id = {raj2021pva}
}",https://arxiv.org/pdf/2101.02697.pdf,,,,,,,"Sparse Reconstruction, Generalization, Image-Based Rendering, Data-Driven Method, Local Conditioning",,,,,,,,,,"Amit Raj, Michael Zollhoefer, Tomas Simon, Jason Saragih, Shunsuke Saito, James Hays, Stephen Lombardi",raj2021pva,103,6,"Acquisition and rendering of photo-realistic human heads is a highly challenging research problem of particular importance for virtual telepresence. Currently, the highest quality is achieved by volumetric approaches trained in a person specific manner on multi-view data. These models better represent fine structure, such as hair, compared to simpler mesh-based models. Volumetric models typically employ a global code to represent facial expressions, such that they can be driven by a small set of animation parameters. While such architectures achieve impressive rendering quality, they can not easily be extended to the multi-identity setting. In this paper, we devise a novel approach for predicting volumetric avatars of the human head given just a small number of inputs. We enable generalization across identities by a novel parameterization that combines neural radiance fields with local, pixel-aligned features extracted directly from the inputs, thus sidestepping the need for very deep or complex networks. Our approach is trained in an end-to-end manner solely based on a photometric re-rendering loss without requiring explicit 3D supervision.We demonstrate that our approach outperforms the existing state of the art in terms of quality and is able to generate faithful facial expressions in a multi-identity setting.",,,,,,CVPR 2021,,,,,,
5/23/2021 18:43,,,Non-line-of-Sight Imaging via Neural Transient Fields,,TPAMI,1/2/2021,2021,"@article{shen2021nonlineofsight,
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    author = {Siyuan Shen and Zi Wang and Ping Liu and Zhengqing Pan and Ruiqian Li and Tian Gao and Shiying Li and Jingyi Yu},
    title = {Non-line-of-Sight Imaging via Neural Transient Fields},
    year = {2021},
    url = {http://arxiv.org/abs/2101.00373v3},
    entrytype = {article},
    id = {shen2021nonlineofsight}
}",https://arxiv.org/pdf/2101.00373.pdf,https://sci2020.github.io/paper/2021/01/05/Non-line-of-Sight-Imaging-via-Neural-Transient-Fields.html,https://github.com/zeromakerplus/NeTF_public,,,,,Beyond Visual Computing,,,,,,,,,,"Siyuan Shen, Zi Wang, Ping Liu, Zhengqing Pan, Ruiqian Li, Tian Gao, Shiying Li, Jingyi Yu",shen2021nonlineofsight,102,0,"We present a neural modeling framework for Non-Line-of-Sight (NLOS) imaging. Previous solutions have sought to explicitly recover the 3D geometry (e.g., as point clouds) or voxel density (e.g., within a pre-defined volume) of the hidden scene. In contrast, inspired by the recent Neural Radiance Field (NeRF) approach, we use a multi-layer perceptron (MLP) to represent the neural transient field or NeTF. However, NeTF measures the transient over spherical wavefronts rather than the radiance along lines. We therefore formulate a spherical volume NeTF reconstruction pipeline, applicable to both confocal and non-confocal setups. Compared with NeRF, NeTF samples a much sparser set of viewpoints (scanning spots) and the sampling is highly uneven. We thus introduce a Monte Carlo technique to improve the robustness in the reconstruction. Comprehensive experiments on synthetic and real datasets demonstrate NeTF provides higher quality reconstruction and preserves fine details largely missing in the state-of-the-art.",,,,,,TPAMI 2021,,,,,,
5/23/2021 18:58,,,Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans,Neural Body,CVPR,12/31/2020,2021,"@inproceedings{peng2021neuralbody,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Sida Peng and Yuanqing Zhang and Yinghao Xu and Qianqian Wang and Qing Shuai and Hujun Bao and Xiaowei Zhou},
    title = {Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans},
    year = {2021},
    url = {http://arxiv.org/abs/2012.15838v2},
    entrytype = {inproceedings},
    id = {peng2021neuralbody}
}",https://arxiv.org/pdf/2012.15838.pdf,https://zju3dv.github.io/neuralbody/,https://github.com/zju3dv/neuralbody,https://zjueducn-my.sharepoint.com/:f:/g/personal/pengsida_zju_edu_cn/Eo9zn4x_xcZKmYHZNjzel7gBdWf_d4m-pISHhPWB-GZBYw?e=Hf4mz7,,https://github.com/zju3dv/neuralbody/blob/master/supplementary_material.md,,"Dynamic/Temporal, Human (Body)",,,,,,,,,,"Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, Xiaowei Zhou",peng2021neuralbody,101,23,"This paper addresses the challenge of novel view synthesis for a human performer from a very sparse set of camera views. Some recent works have shown that learning implicit neural representations of 3D scenes achieves remarkable view synthesis quality given dense input views. However, the representation learning will be ill-posed if the views are highly sparse. To solve this ill-posed problem, our key idea is to integrate observations over video frames. To this end, we propose Neural Body, a new human body representation which assumes that the learned neural representations at different frames share the same set of latent codes anchored to a deformable mesh, so that the observations across frames can be naturally integrated. The deformable mesh also provides geometric guidance for the network to learn 3D representations more efficiently. To evaluate our approach, we create a multi-view dataset named ZJU-MoCap that captures performers with complex motions. Experiments on ZJU-MoCap show that our approach outperforms prior works by a large margin in terms of novel view synthesis quality. We also demonstrate the capability of our approach to reconstruct a moving person from a monocular video on the People-Snapshot dataset. The code and dataset are available at https://zju3dv.github.io/neuralbody/.",,,,,,CVPR 2021,,,,,,
5/23/2021 18:16,,,Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video,NR-NeRF,ICCV,12/22/2020,2021,"@inproceedings{tretschk2021nrnerf,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Edgar Tretschk and Ayush Tewari and Vladislav Golyanik and Michael Zollhofer and Christoph Lassner and Christian Theobalt},
    title = {Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video},
    year = {2021},
    url = {http://arxiv.org/abs/2012.12247v4},
    entrytype = {inproceedings},
    id = {tretschk2021nrnerf}
}",https://arxiv.org/pdf/2012.12247.pdf,,,,,,,"Dynamic/Temporal, Global Conditioning",,,,,,,,,,"Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Christoph Lassner, Christian Theobalt",tretschk2021nrnerf,100,18,"We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and novel view synthesis approach for general non-rigid dynamic scenes. Our approach takes RGB images of a dynamic scene as input, e.g., from a monocular video recording, and creates a high-quality space-time geometry and appearance representation. In particular, we show that even a single handheld consumer-grade camera is sufficient to synthesize sophisticated renderings of a dynamic scene from novel virtual camera views, for example a `bullet-time' video effect. Our method disentangles the dynamic scene into a canonical volume and its deformation. Scene deformation is implemented as ray bending, where straight rays are deformed non-rigidly to represent scene motion. We also propose a novel rigidity regression network that enables us to better constrain rigid regions of the scene, which leads to more stable results. The ray bending and rigidity network are trained without any explicit supervision. In addition to novel view synthesis, our formulation enables dense correspondence estimation across views and time, as well as compelling video editing applications such as motion exaggeration. We demonstrate the effectiveness of our method using extensive evaluations, including ablation studies and comparisons to the state of the art. We urge the reader to watch the supplemental video for qualitative results. Our code will be open sourced.",,,,,,ICCV 2021,,,,,,
5/23/2021 18:56,,,STaR: Self-supervised Tracking and Reconstruction of Rigid Objects in Motion with Neural Rendering,STaR,CVPR,12/22/2020,2021,"@inproceedings{yuan2021star,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Wentao Yuan and Zhaoyang Lv and Tanner Schmidt and Steven Lovegrove},
    title = {STaR: Self-supervised Tracking and Reconstruction of Rigid Objects in Motion with Neural Rendering},
    year = {2021},
    url = {http://arxiv.org/abs/2101.01602v1},
    entrytype = {inproceedings},
    id = {yuan2021star}
}",https://arxiv.org/pdf/2101.01602.pdf,https://wentaoyuan.github.io/star/,Coming soon,,,,,Dynamic/Temporal,,,,,,,,,,"Wentao Yuan, Zhaoyang Lv, Tanner Schmidt, Steven Lovegrove",yuan2021star,99,3,"We present STaR, a novel method that performs Self-supervised Tracking and Reconstruction of dynamic scenes with rigid motion from multi-view RGB videos without any manual annotation. Recent work has shown that neural networks are surprisingly effective at the task of compressing many views of a scene into a learned function which maps from a viewing ray to an observed radiance value via volume rendering. Unfortunately, these methods lose all their predictive power once any object in the scene has moved. In this work, we explicitly model rigid motion of objects in the context of neural representations of radiance fields. We show that without any additional human specified supervision, we can reconstruct a dynamic scene with a single rigid object in motion by simultaneously decomposing it into its two constituent parts and encoding each with its own neural representation. We achieve this by jointly optimizing the parameters of two neural radiance fields and a set of rigid poses which align the two fields at each frame. On both synthetic and real world datasets, we demonstrate that our method can render photorealistic novel views, where novelty is measured on both spatial and temporal axes. Our factored representation furthermore enables animation of unseen object motion.",,,,,,CVPR 2021,,,,,,
5/23/2021 18:14,,,Neural Radiance Flow for 4D View Synthesis and Video Processing,NeRFlow,ICCV,12/17/2020,2021,"@inproceedings{du2021nerflow,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Yilun Du and Yinan Zhang and Hong-Xing Yu and Joshua B. Tenenbaum and Jiajun Wu},
    title = {Neural Radiance Flow for 4D View Synthesis and Video Processing},
    year = {2021},
    url = {http://arxiv.org/abs/2012.09790v2},
    entrytype = {inproceedings},
    id = {du2021nerflow}
}",https://arxiv.org/pdf/2012.09790.pdf,https://yilundu.github.io/nerflow/,https://github.com/yilundu/nerflow,,,,,Dynamic/Temporal,,,,,,,,,,"Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B. Tenenbaum, Jiajun Wu",du2021nerflow,97,5,"We present a method, Neural Radiance Flow (NeRFlow),to learn a 4D spatial-temporal representation of a dynamic scene from a set of RGB images. Key to our approach is the use of a neural implicit representation that learns to capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing consistency across different modalities, our representation enables multi-view rendering in diverse dynamic scenes, including water pouring, robotic interaction, and real images, outperforming state-of-the-art methods for spatial-temporal view synthesis. Our approach works even when inputs images are captured with only one camera. We further demonstrate that the learned representation can serve as an implicit scene prior, enabling video processing tasks such as image super-resolution and de-noising without any additional supervision.",,,,,,ICCV 2021,,,,,,
7/19/2021 16:53,,,Learning Compositional Radiance Fields of Dynamic Human Heads,HybridNeRF,CVPR,12/17/2020,2021,"@inproceedings{wang2021hybridnerf,
    url = {http://arxiv.org/abs/2012.09955v1},
    year = {2021},
    title = {Learning Compositional Radiance Fields of Dynamic Human Heads},
    author = {Ziyan Wang and Timur Bagautdinov and Stephen Lombardi and Tomas Simon and Jason Saragih and Jessica Hodgins and Michael Zollhofer},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    entrytype = {inproceedings},
    id = {wang2021hybridnerf}
}",https://arxiv.org/pdf/2012.09955.pdf,https://ziyanw1.github.io/hybrid_nerf/,,,,https://openaccess.thecvf.com/content/CVPR2021/supplemental/Wang_Learning_Compositional_Radiance_CVPR_2021_supplemental.pdf,,"Dynamic/Temporal, Human (Head), Generalization, Sampling, Voxel Grid, Local Conditioning",,,No,,,,,,,"Ziyan Wang, Timur Bagautdinov, Stephen Lombardi, Tomas Simon, Jason Saragih, Jessica Hodgins, Michael Zollhöfer",wang2021hybridnerf,96,5,"Photorealistic rendering of dynamic humans is an important ability for telepresence systems, virtual shopping, synthetic data generation, and more. Recently, neural rendering methods, which combine techniques from computer graphics and machine learning, have created high-fidelity models of humans and objects. Some of these methods do not produce results with high-enough fidelity for driveable human models (Neural Volumes) whereas others have extremely long rendering times (NeRF). We propose a novel compositional 3D representation that combines the best of previous methods to produce both higher-resolution and faster results. Our representation bridges the gap between discrete and continuous volumetric representations by combining a coarse 3D-structure-aware grid of animation codes with a continuous learned scene function that maps every position and its corresponding local animation code to its view-dependent emitted radiance and local volume density. Differentiable volume rendering is employed to compute photo-realistic novel views of the human head and upper body as well as to train our novel representation end-to-end using only 2D supervision. In addition, we show that the learned dynamic radiance field can be used to synthesize novel unseen expressions based on a global animation code. Our approach achieves state-of-the-art results for synthesizing novel views of dynamic human heads and the upper body.",,,,,,CVPR 2021,,,,,,
8/29/2021 17:03,,,Learning Continuous Image Representation with Local Implicit Image Function,LIIF,CVPR,12/16/2020,2021,"@inproceedings{chen2021liif,
    url = {http://arxiv.org/abs/2012.09161v2},
    year = {2021},
    title = {Learning Continuous Image Representation with Local Implicit Image Function},
    author = {Yinbo Chen and Sifei Liu and Xiaolong Wang},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    entrytype = {inproceedings},
    id = {chen2021liif}
}",https://arxiv.org/pdf/2012.09161.pdf,https://yinboc.github.io/liif/,https://github.com/yinboc/liif,,https://www.youtube.com/watch?v=6f2roieSY_8,,,"2D Image Neural Fields, Fundamentals, Data-Driven Method, Hybrid Geometry Representation",Sinusoidal Activation (SIREN),,,,,,,,,"Yinbo Chen, Sifei Liu, Xiaolong Wang",chenoralliif,95,10,"How to represent an image? While the visual world is presented in a continuous manner, machines store and see the images in a discrete way with 2D arrays of pixels. In this paper, we seek to learn a continuous representation for images. Inspired by the recent progress in 3D reconstruction with implicit neural representation, we propose Local Implicit Image Function (LIIF), which takes an image coordinate and the 2D deep features around the coordinate as inputs, predicts the RGB value at a given coordinate as an output. Since the coordinates are continuous, LIIF can be presented in arbitrary resolution. To generate the continuous representation for images, we train an encoder with LIIF representation via a self-supervised task with super-resolution. The learned continuous representation can be presented in arbitrary resolution even extrapolate to x30 higher resolution, where the training tasks are not provided. We further show that LIIF representation builds a bridge between discrete and continuous representation in 2D, it naturally supports the learning tasks with size-varied image ground-truths and significantly outperforms the method with resizing the ground-truths.",,,Direct,,,CVPR 2021,,,,,,
5/23/2021 18:37,,,Object-Centric Neural Scene Rendering,OSFs,ARXIV,12/15/2020,2020,"@article{guo2020osfs,
    journal = {arXiv preprint arXiv:2012.08503},
    booktitle = {ArXiv Pre-print},
    author = {Michelle Guo and Alireza Fathi and Jiajun Wu and Thomas Funkhouser},
    title = {Object-Centric Neural Scene Rendering},
    year = {2020},
    url = {http://arxiv.org/abs/2012.08503v1},
    entrytype = {article},
    id = {guo2020osfs}
}",https://arxiv.org/pdf/2012.08503.pdf,https://www.shellguo.com/osf/,,,https://www.youtube.com/watch?v=NtR7xgxSL1U,,,"Editable, Material/Lighting Estimation, Object-Centric, Hybrid Geometry Representation",,,,,,,,,,"Michelle Guo, Alireza Fathi, Jiajun Wu, Thomas Funkhouser",guo2020osfs,94,6,"We present a method for composing photorealistic scenes from captured images of objects. Our work builds upon neural radiance fields (NeRFs), which implicitly model the volumetric density and directionally-emitted radiance of a scene. While NeRFs synthesize realistic pictures, they only model static scenes and are closely tied to specific imaging conditions. This property makes NeRFs hard to generalize to new scenarios, including new lighting or new arrangements of objects. Instead of learning a scene radiance field as a NeRF does, we propose to learn object-centric neural scattering functions (OSFs), a representation that models per-object light transport implicitly using a lighting- and view-dependent neural network. This enables rendering scenes even when objects or lights move, without retraining. Combined with a volumetric path tracing procedure, our framework is capable of rendering both intra- and inter-object light transport effects including occlusions, specularities, shadows, and indirect illumination. We evaluate our approach on scene composition and show that it generalizes to novel illumination conditions, producing photorealistic, physically accurate renderings of multi-object scenes.",,,,,,ARXIV 2020,,,,,,
7/19/2021 21:48,,,Deep Optimized Priors for 3D Shape Modeling and Reconstruction,,CVPR,12/14/2020,2021,"@inproceedings{yang2021deep,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Mingyue Yang and Yuxin Wen and Weikai Chen and Yongwei Chen and Kui Jia},
    title = {Deep Optimized Priors for 3D Shape Modeling and Reconstruction},
    year = {2021},
    url = {http://arxiv.org/abs/2012.07241v1},
    entrytype = {inproceedings},
    id = {yang2021deep}
}",https://arxiv.org/pdf/2012.07241.pdf,,,,,,,"Generalization, Data-Driven Method, Global Conditioning, Coarse-to-Fine",,,,,,,,,,"Mingyue Yang, Yuxin Wen, Weikai Chen, Yongwei Chen, Kui Jia",yang2021deep,93,3,"Many learning-based approaches have difficulty scaling to unseen data, as the generality of its learned prior is limited to the scale and variations of the training samples. This holds particularly true with 3D learning tasks, given the sparsity of 3D datasets available. We introduce a new learning framework for 3D modeling and reconstruction that greatly improves the generalization ability of a deep generator. Our approach strives to connect the good ends of both learning-based and optimization-based methods. In particular, unlike the common practice that fixes the pre-trained priors at test time, we propose to further optimize the learned prior and latent code according to the input physical measurements after the training. We show that the proposed strategy effectively breaks the barriers constrained by the pre-trained priors and could lead to high-quality adaptation to unseen data. We realize our framework using the implicit surface representation and validate the efficacy of our approach in a variety of challenging tasks that take highly sparse or collapsed observations as input. Experimental results show that our approach compares favorably with the state-of-the-art methods in terms of both generality and accuracy.",,,,,,CVPR 2021,,,,,,
5/23/2021 18:59,,,Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid Representations,Iso-Points,CVPR,12/11/2020,2021,"@inproceedings{yifan2021isopoints,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Wang Yifan and Shihao Wu and Cengiz Oztireli and Olga Sorkine-Hornung},
    title = {Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid Representations},
    year = {2021},
    url = {http://arxiv.org/abs/2012.06434v2},
    entrytype = {inproceedings},
    id = {yifan2021isopoints}
}",https://arxiv.org/pdf/2012.06434.pdf,,,,,,,"Sampling, Hybrid Geometry Representation",,,,,,,,,,"Wang Yifan, Shihao Wu, Cengiz Oztireli, Olga Sorkine-Hornung",yifan2021isopoints,92,1,"Neural implicit functions have emerged as a powerful representation for surfaces in 3D. Such a function can encode a high quality surface with intricate details into the parameters of a deep neural network. However, optimizing for the parameters for accurate and robust reconstructions remains a challenge, especially when the input data is noisy or incomplete. In this work, we develop a hybrid neural surface representation that allows us to impose geometry-aware sampling and regularization, which significantly improves the fidelity of reconstructions. We propose to use \emph{iso-points} as an explicit representation for a neural implicit function. These points are computed and updated on-the-fly during training to capture important geometric features and impose geometric constraints on the optimization. We demonstrate that our method can be adopted to improve state-of-the-art techniques for reconstructing neural implicit surfaces from multi-view images or point clouds. Quantitative and qualitative evaluations show that, compared with existing sampling and optimization methods, our approach allows faster convergence, better generalization, and accurate recovery of details and topology.",,,,,,CVPR 2021,,,,,,
5/23/2021 18:55,,,iNeRF: Inverting Neural Radiance Fields for Pose Estimation,iNeRF,IROS,12/10/2020,2021,"@article{yen-chen2021inerf,
    author = {Lin Yen-Chen and Pete Florence and Jonathan T. Barron and Alberto Rodriguez and Phillip Isola and Tsung-Yi Lin},
    title = {INeRF: Inverting Neural Radiance Fields for Pose Estimation},
    year = {2020},
    month = {Dec},
    url = {http://arxiv.org/abs/2012.05877v3},
    entrytype = {article},
    id = {yen-chen2021inerf}
}",https://arxiv.org/pdf/2012.05877.pdf,https://yenchenlin.me/inerf/,Coming soon,,https://www.youtube.com/watch?v=eQuCZaQN0tI,,,"Camera Parameter Estimation, Sampling",,,,,,,,,,"Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto Rodriguez, Phillip Isola, Tsung-Yi Lin",yen-chen2021inerf,91,9,"We present iNeRF, a framework that performs mesh-free pose estimation by ""inverting"" a Neural RadianceField (NeRF). NeRFs have been shown to be remarkably effective for the task of view synthesis - synthesizing photorealistic novel views of real-world scenes or objects. In this work, we investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free, RGB-only 6DoF pose estimation - given an image, find the translation and rotation of a camera relative to a 3D object or scene. Our method assumes that no object mesh models are available during either training or test time. Starting from an initial pose estimate, we use gradient descent to minimize the residual between pixels rendered from a NeRF and pixels in an observed image. In our experiments, we first study 1) how to sample rays during pose refinement for iNeRF to collect informative gradients and 2) how different batch sizes of rays affect iNeRF on a synthetic dataset. We then show that for complex real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating the camera poses of novel images and using these images as additional training data for NeRF. Finally, we show iNeRF can perform category-level object pose estimation, including object instances not seen during training, with RGB images by inverting a NeRF model inferred from a single view.",,,,,,IROS 2021,,,,,,
7/19/2021 21:25,,,Portrait Neural Radiance Fields from a Single Image,PortraitNeRF,ARXIV,12/10/2020,2020,"@article{gao2020portraitnerf,
    journal = {arXiv preprint arXiv:2012.05903},
    booktitle = {ArXiv Pre-print},
    author = {Chen Gao and Yichang Shih and Wei-Sheng Lai and Chia-Kai Liang and Jia-Bin Huang},
    title = {Portrait Neural Radiance Fields from a Single Image},
    year = {2020},
    url = {http://arxiv.org/abs/2012.05903v2},
    entrytype = {article},
    id = {gao2020portraitnerf}
}",https://arxiv.org/pdf/2012.05903.pdf,https://portrait-nerf.github.io/,Coming soon,https://portrait-nerf.github.io/#,,,,"Human (Head), Sparse Reconstruction, Generalization, Data-Driven Method",Fourier Feature (NeRF),Density,No,,,,,,,"Chen Gao, Yichang Shih, Wei-Sheng Lai, Chia-Kai Liang, Jia-Bin Huang",gao2020portraitnerf,90,6,"We present a method for estimating Neural Radiance Fields (NeRF) from a single headshot portrait. While NeRF has demonstrated high-quality view synthesis, it requires multiple images of static scenes and thus impractical for casual captures and moving subjects. In this work, we propose to pretrain the weights of a multilayer perceptron (MLP), which implicitly models the volumetric density and colors, with a meta-learning framework using a light stage portrait dataset. To improve the generalization to unseen faces, we train the MLP in the canonical coordinate space approximated by 3D face morphable models. We quantitatively evaluate the method using controlled captures and demonstrate the generalization to real portrait images, showing favorable results against state-of-the-arts.",,,Direct,,,ARXIV 2020,,,,,,
5/23/2021 19:07,,,NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis,NeRV,CVPR,12/7/2020,2021,"@inproceedings{srinivasan2021nerv,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Pratul P. Srinivasan and Boyang Deng and Xiuming Zhang and Matthew Tancik and Ben Mildenhall and Jonathan T. Barron},
    title = {NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis},
    year = {2021},
    url = {http://arxiv.org/abs/2012.03927v1},
    entrytype = {inproceedings},
    id = {srinivasan2021nerv}
}",https://arxiv.org/pdf/2012.03927.pdf,https://pratulsrinivasan.github.io/nerv/,Coming soon,,https://www.youtube.com/watch?v=4XyDdvhhjVo,,,Material/Lighting Estimation,,,,,,,,,,"Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, Jonathan T. Barron",srinivasan2021nerv,89,21,"We present a method that takes as input a set of images of a scene illuminated by unconstrained known lighting, and produces as output a 3D representation that can be rendered from novel viewpoints under arbitrary lighting conditions. Our method represents the scene as a continuous volumetric function parameterized as MLPs whose inputs are a 3D location and whose outputs are the following scene properties at that input location: volume density, surface normal, material parameters, distance to the first surface intersection in any direction, and visibility of the external environment in any direction. Together, these allow us to render novel views of the object under arbitrary lighting, including indirect illumination effects. The predicted visibility and surface intersection fields are critical to our model's ability to simulate direct and indirect illumination during training, because the brute-force techniques used by prior work are intractable for lighting conditions outside of controlled setups with a single light. Our method outperforms alternative approaches for recovering relightable 3D scene representations, and performs well in complex lighting settings that have posed a significant challenge to prior work.",,,,,,CVPR 2021,,,,,,
5/23/2021 19:01,,,NeRD: Neural Reflectance Decomposition from Image Collections,NeRD,ICCV,12/7/2020,2021,"@inproceedings{boss2021nerd,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Mark Boss and Raphael Braun and Varun Jampani and Jonathan T. Barron and Ce Liu and Hendrik P. A. Lensch},
    title = {NeRD: Neural Reflectance Decomposition from Image Collections},
    year = {2021},
    url = {http://arxiv.org/abs/2012.03918v4},
    entrytype = {inproceedings},
    id = {boss2021nerd}
}",https://arxiv.org/pdf/2012.03918.pdf,https://markboss.me/publication/2021-nerd/?s=09,https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition,https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition/blob/master/download_datasets.py,"https://www.youtube.com/watch?v=JL-qMTXw9VU, https://www.youtube.com/watch?v=IM9OgMwHNTI",,,"Material/Lighting Estimation, Sampling",,,,,,,,,,"Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu, Hendrik P. A. Lensch",boss2021nerd,88,3,"Decomposing a scene into its shape, reflectance, and illumination is a challenging but essential problem in computer vision and graphics. This problem is inherently more challenging when the illumination is not a single light source under laboratory conditions but is instead an unconstrained environmental illumination. Though recent work has shown that implicit representations can be used to model the radiance field of an object, these techniques only enable view synthesis and not relighting. Additionally, evaluating these radiance fields is resource and time-intensive. By decomposing a scene into explicit representations, any rendering framework can be leveraged to generate novel views under any illumination in real-time. NeRD is a method that achieves this decomposition by introducing physically-based rendering to neural radiance fields. Even challenging non-Lambertian reflectances, complex geometry, and unknown illumination can be decomposed into high-quality models. The datasets and code is available on the project page: https://markboss.me/publication/2021-nerd/",,,,,,ICCV 2021,,,,,,
7/19/2021 21:30,,,Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction,NerFACE,CVPR,12/5/2020,2021,"@inproceedings{gafni2021nerface,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Guy Gafni and Justus Thies and Michael Zollhofer and Matthias Niessner},
    title = {Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction},
    year = {2021},
    url = {http://arxiv.org/abs/2012.03065v1},
    entrytype = {inproceedings},
    id = {gafni2021nerface}
}",https://arxiv.org/pdf/2012.03065.pdf,https://gafniguy.github.io/4D-Facial-Avatars/,https://github.com/gafniguy/4D-Facial-Avatars,Available upon request,"https://www.youtube.com/watch?v=XihxC65tmyA, https://www.youtube.com/watch?v=m7oROLdQnjk",,,"Human (Head), Data-Driven Method, Global Conditioning",Fourier Feature (NeRF),Density,No,,,,,,,"Guy Gafni, Justus Thies, Michael Zollhöfer, Matthias Nießner",gafni2021nerface,87,18,"We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications. Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoints or head-poses is required. In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks. To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions. We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup. In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.",,,Direct,,,CVPR 2021,,,,,,
8/29/2021 20:39,,,Spatially-Adaptive Pixelwise Networks for Fast Image Translation,ASAPNet,CVPR,12/5/2020,2021,"@inproceedings{shaham2021asapnet,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Tamar Rott Shaham and Michael Gharbi and Richard Zhang and Eli Shechtman and Tomer Michaeli},
    title = {Spatially-Adaptive Pixelwise Networks for Fast Image Translation},
    year = {2021},
    url = {http://arxiv.org/abs/2012.02992v1},
    entrytype = {inproceedings},
    id = {shaham2021asapnet}
}",https://arxiv.org/pdf/2012.02992.pdf,https://tamarott.github.io/ASAPNet_web/,https://github.com/tamarott/ASAPNet,,https://www.youtube.com/watch?v=6-OfZ32CoBE,,,"Speed & Computational Efficiency, 2D Image Neural Fields, Data-Driven Method",,,,,,,,,,"Tamar Rott Shaham, Michael Gharbi, Richard Zhang, Eli Shechtman, Tomer Michaeli",shaham2021asapnet,86,2,"We introduce a new generator architecture, aimed at fast and efficient high-resolution image-to-image translation. We design the generator to be an extremely lightweight function of the full-resolution image. In fact, we use pixel-wise networks; that is, each pixel is processed independently of others, through a composition of simple affine transformations and nonlinearities. We take three important steps to equip such a seemingly simple function with adequate expressivity. First, the parameters of the pixel-wise networks are spatially varying so they can represent a broader function class than simple 1x1 convolutions. Second, these parameters are predicted by a fast convolutional network that processes an aggressively low-resolution representation of the input; Third, we augment the input image with a sinusoidal encoding of spatial coordinates, which provides an effective inductive bias for generating realistic novel high-frequency image content. As a result, our model is up to 18x faster than state-of-the-art baselines. We achieve this speedup while generating comparable visual quality across different image resolutions and translation domains.",Yes,,"Direct, Indirect",,,CVPR 2021,,,,,,
5/23/2021 18:50,,,pixelNeRF: Neural Radiance Fields from One or Few Images,pixelNeRF,CVPR,12/3/2020,2021,"@inproceedings{yu2021pixelnerf,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Alex Yu and Vickie Ye and Matthew Tancik and Angjoo Kanazawa},
    title = {pixelNeRF: Neural Radiance Fields from One or Few Images},
    year = {2021},
    url = {http://arxiv.org/abs/2012.02190v3},
    entrytype = {inproceedings},
    id = {yu2021pixelnerf}
}",https://arxiv.org/pdf/2012.02190.pdf,https://alexyu.net/pixelnerf/,https://github.com/sxyu/pixel-nerf,https://drive.google.com/drive/folders/1PsT3uKwqHHD2bEEHkIXB99AlIjtmrEiR,https://www.youtube.com/watch?v=voebZx7f32g,,,"Sparse Reconstruction, Generalization, Data-Driven Method, Local Conditioning",,,,,,,,,,"Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa",yu2021pixelnerf,85,37,"We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixelNeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pixelNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website: https://alexyu.net/pixelnerf",,,,,,CVPR 2021,,,,,,
5/23/2021 18:53,,,Learned Initializations for Optimizing Coordinate-Based Neural Representations,,CVPR,12/3/2020,2021,"@inproceedings{tancik2021learned,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Matthew Tancik and Ben Mildenhall and Terrance Wang and Divi Schmidt and Pratul P. Srinivasan and Jonathan T. Barron and Ren Ng},
    title = {Learned Initializations for Optimizing Coordinate-Based Neural Representations},
    year = {2021},
    url = {http://arxiv.org/abs/2012.02189v2},
    entrytype = {inproceedings},
    id = {tancik2021learned}
}",https://arxiv.org/pdf/2012.02189.pdf,https://www.matthewtancik.com/learnit,,,,,,"Generalization, Fundamentals, Data-Driven Method, Hypernetwork/Meta-learning",,,,,,,,,,"Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P. Srinivasan, Jonathan T. Barron, Ren Ng",tancik2021learned,84,12,"Coordinate-based neural representations have shown significant promise as an alternative to discrete, array-based representations for complex low dimensional signals. However, optimizing a coordinate-based network from randomly initialized weights for each new signal is inefficient. We propose applying standard meta-learning algorithms to learn the initial weight parameters for these fully-connected networks based on the underlying class of signals being represented (e.g., images of faces or 3D models of chairs). Despite requiring only a minor change in implementation, using these learned initial weights enables faster convergence during optimization and can serve as a strong prior over the signal class being modeled, resulting in better generalization when only partial observations of a given signal are available. We explore these benefits across a variety of tasks, including representing 2D images, reconstructing CT scans, and recovering 3D shapes and scenes from 2D image observations.",,,,,,CVPR 2021,,,,,,
5/23/2021 18:59,,,AutoInt: Automatic Integration for Fast Neural Volume Rendering,AutoInt,CVPR,12/3/2020,2021,"@inproceedings{lindell2021autoint,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {David B. Lindell and Julien N. P. Martel and Gordon Wetzstein},
    title = {AutoInt: Automatic Integration for Fast Neural Volume Rendering},
    year = {2021},
    url = {http://arxiv.org/abs/2012.01714v2},
    entrytype = {inproceedings},
    id = {lindell2021autoint}
}",https://arxiv.org/pdf/2012.01714.pdf,http://www.computationalimaging.org/publications/automatic-integration/,,,,,,"Speed & Computational Efficiency, Fundamentals, Sampling",,,,,,,,,,"David B. Lindell, Julien N. P. Martel, Gordon Wetzstein",lindell2021autoint,83,16,"Numerical integration is a foundational technique in scientific computing and is at the core of many computer vision applications. Among these applications, neural volume rendering has recently been proposed as a new paradigm for view synthesis, achieving photorealistic image quality. However, a fundamental obstacle to making these methods practical is the extreme computational and memory requirements caused by the required volume integrations along the rendered rays during training and inference. Millions of rays, each requiring hundreds of forward passes through a neural network are needed to approximate those integrations with Monte Carlo sampling. Here, we propose automatic integration, a new framework for learning efficient, closed-form solutions to integrals using coordinate-based neural networks. For training, we instantiate the computational graph corresponding to the derivative of the network. The graph is fitted to the signal to integrate. After optimization, we reassemble the graph to obtain a network that represents the antiderivative. By the fundamental theorem of calculus, this enables the calculation of any definite integral in two evaluations of the network. Applying this approach to neural rendering, we improve a tradeoff between rendering speed and image quality: improving render times by greater than 10 times with a tradeoff of slightly reduced image quality.",,,,,,CVPR 2021,,,,,,
7/19/2021 22:05,,,Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction,Neural Deformation Graphs,CVPR,12/2/2020,2021,"@inproceedings{bozic2021neuraldeformationgraphs,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Aljaz Bozic and Pablo Palafox and Michael Zollhofer and Justus Thies and Angela Dai and Matthias Niessner},
    title = {Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction},
    year = {2021},
    url = {http://arxiv.org/abs/2012.01451v1},
    entrytype = {inproceedings},
    id = {bozic2021neuraldeformationgraphs}
}",https://arxiv.org/pdf/2012.01451.pdf,,,,,,,"Dynamic/Temporal, Human (Body)",,,,,,,,,,"Aljaž Božič, Pablo Palafox, Michael Zollhöfer, Justus Thies, Angela Dai, Matthias Nießner",bozic2021neuraldeformationgraphs,82,1,"We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D reconstruction of non-rigid objects. Specifically, we implicitly model a deformation graph via a deep neural network. This neural deformation graph does not rely on any object-specific structure and, thus, can be applied to general non-rigid deformation tracking. Our method globally optimizes this neural graph on a given sequence of depth camera observations of a non-rigidly moving object. Based on explicit viewpoint consistency as well as inter-frame graph and surface consistency constraints, the underlying network is trained in a self-supervised fashion. We additionally optimize for the geometry of the object with an implicit deformable multi-MLP shape representation. Our approach does not assume sequential input data, thus enabling robust tracking of fast motions or even temporally disconnected recordings. Our experiments demonstrate that our Neural Deformation Graphs outperform state-of-the-art non-rigid reconstruction approaches both qualitatively and quantitatively, with 64% improved reconstruction and 62% improved deformation tracking performance.",,,,,,CVPR 2021,,,,,,
7/19/2021 21:22,,,i3DMM: Deep Implicit 3D Morphable Model of Human Heads,i3DMM,CVPR,11/28/2020,2021,"@inproceedings{yenamandra2021i3dmm,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Tarun Yenamandra and Ayush Tewari and Florian Bernard and Hans-Peter Seidel and Mohamed Elgharib and Daniel Cremers and Christian Theobalt},
    title = {i3DMM: Deep Implicit 3D Morphable Model of Human Heads},
    year = {2021},
    url = {http://arxiv.org/abs/2011.14143v1},
    entrytype = {inproceedings},
    id = {yenamandra2021i3dmm}
}",https://arxiv.org/pdf/2011.14143.pdf,https://vcai.mpi-inf.mpg.de/projects/i3DMM/,https://github.com/tarun738/i3DMM,,https://www.youtube.com/watch?v=4pYzV3ButPY,,,"Dynamic/Temporal, Human (Head), Editable, Global Conditioning",,SDF,,,,,,,,"Tarun Yenamandra, Ayush Tewari, Florian Bernard, Hans-Peter Seidel, Mohamed Elgharib, Daniel Cremers, Christian Theobalt",yenamandra2021i3dmm,81,3,"We present the first deep implicit 3D morphable model (i3DMM) of full heads. Unlike earlier morphable face models it not only captures identity-specific geometry, texture, and expressions of the frontal face, but also models the entire head, including hair. We collect a new dataset consisting of 64 people with different expressions and hairstyles to train i3DMM. Our approach has the following favorable properties: (i) It is the first full head morphable model that includes hair. (ii) In contrast to mesh-based models it can be trained on merely rigidly aligned scans, without requiring difficult non-rigid registration. (iii) We design a novel architecture to decouple the shape model into an implicit reference shape and a deformation of this reference shape. With that, dense correspondences between shapes can be learned implicitly. (iv) This architecture allows us to semantically disentangle the geometry and color components, as color is learned in the reference space. Geometry is further disentangled as identity, expressions, and hairstyle, while color is disentangled as identity and hairstyle components. We show the merits of i3DMM using ablation studies, comparisons to state-of-the-art models, and applications such as semantic head editing and texture transfer. We will make our model publicly available.",,,,,,CVPR 2021,,,,,,
5/23/2021 18:16,,,D-NeRF: Neural Radiance Fields for Dynamic Scenes,D-NeRF,CVPR,11/27/2020,2021,"@inproceedings{pumarola2021dnerf,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Albert Pumarola and Enric Corona and Gerard Pons-Moll and Francesc Moreno-Noguer},
    title = {D-NeRF: Neural Radiance Fields for Dynamic Scenes},
    year = {2021},
    url = {http://arxiv.org/abs/2011.13961v1},
    entrytype = {inproceedings},
    id = {pumarola2021dnerf}
}",https://arxiv.org/pdf/2011.13961.pdf,https://www.albertpumarola.com/research/D-NeRF/index.html,https://github.com/albertpumarola/D-NeRF,https://www.dropbox.com/s/0bf6fl0ye2vz3vr/data.zip?dl=0,https://www.youtube.com/watch?v=lSgzmgi2JPw,,,Dynamic/Temporal,,,,,,,,,,"Albert Pumarola, Enric Corona, Gerard Pons-Moll, Francesc Moreno-Noguer",pumarola2021dnerf,80,41,"Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF), which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions from a \emph{single} camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be released.",,,,,,CVPR 2021,,,,,,
8/29/2021 20:48,,,Image Generators with Conditionally-Independent Pixel Synthesis,CIPS,CVPR,11/27/2020,2021,"@inproceedings{anokhin2021cips,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Ivan Anokhin and Kirill Demochkin and Taras Khakhulin and Gleb Sterkin and Victor Lempitsky and Denis Korzhenkov},
    title = {Image Generators with Conditionally-Independent Pixel Synthesis},
    year = {2021},
    url = {http://arxiv.org/abs/2011.13775v1},
    entrytype = {inproceedings},
    id = {anokhin2021cips}
}",https://arxiv.org/pdf/2011.13775.pdf,,https://github.com/saic-mdal/CIPS,,,,,2D Image Neural Fields,,,,,,,,,,"Ivan Anokhin, Kirill Demochkin, Taras Khakhulin, Gleb Sterkin, Victor Lempitsky, Denis Korzhenkov",anokhin2021cips,79,11,"Existing image generator networks rely heavily on spatial convolutions and, optionally, self-attention blocks in order to gradually synthesize images in a coarse-to-fine manner. Here, we present a new architecture for image generators, where the color value at each pixel is computed independently given the value of a random latent vector and the coordinate of that pixel. No spatial convolutions or similar operations that propagate information across pixels are involved during the synthesis. We analyze the modeling capabilities of such generators when trained in an adversarial fashion, and observe the new generators to achieve similar generation quality to state-of-the-art convolutional generators. We also investigate several interesting properties unique to the new architecture.",,,,,,CVPR 2021,,,,,,
5/23/2021 19:03,,,Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes,NSFF,CVPR,11/26/2020,2021,"@inproceedings{li2021nsff,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Zhengqi Li and Simon Niklaus and Noah Snavely and Oliver Wang},
    title = {Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes},
    year = {2021},
    url = {http://arxiv.org/abs/2011.13084v3},
    entrytype = {inproceedings},
    id = {li2021nsff}
}",https://arxiv.org/pdf/2011.13084.pdf,http://www.cs.cornell.edu/~zl548/NSFF/,https://github.com/zhengqili/Neural-Scene-Flow-Fields,,,https://www.cs.cornell.edu/~zl548/NSFF/NSFF_supp.pdf,,Dynamic/Temporal,,,,,,,,,,"Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang",li2021nsff,78,36,"We present a method to perform novel view and time synthesis of dynamic scenes, requiring only a monocular video with known camera poses as input. To do this, we introduce Neural Scene Flow Fields, a new representation that models the dynamic scene as a time-variant continuous function of appearance, geometry, and 3D scene motion. Our representation is optimized through a neural network to fit the observed input views. We show that our representation can be used for complex dynamic scenes, including thin structures, view-dependent effects, and natural degrees of motion. We conduct a number of experiments that demonstrate our approach significantly outperforms recent monocular view synthesis methods, and show qualitative results of space-time view synthesis on a variety of real-world videos.",,,,,,CVPR 2021,,,,,,
5/23/2021 19:01,,,Nerfies: Deformable Neural Radiance Fields,Nerfies,ICCV,11/25/2020,2021,"@inproceedings{park2021nerfies,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Keunhong Park and Utkarsh Sinha and Jonathan T. Barron and Sofien Bouaziz and Dan B Goldman and Steven M. Seitz and Ricardo Martin-Brualla},
    title = {Nerfies: Deformable Neural Radiance Fields},
    year = {2021},
    url = {http://arxiv.org/abs/2011.12948v5},
    entrytype = {inproceedings},
    id = {park2021nerfies}
}",https://arxiv.org/pdf/2011.12948.pdf,https://nerfies.github.io/,,,https://www.youtube.com/watch?v=MrKrnHhk8IA,,,"Dynamic/Temporal, Global Conditioning, Coarse-to-Fine",,,,,,,,,,"Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, Ricardo Martin-Brualla",park2021nerfies,77,54,"We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub ""nerfies."" We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity.",,,,,,ICCV 2021,,,,,,
5/23/2021 19:01,,,DeRF: Decomposed Radiance Fields,DeRF,CVPR,11/25/2020,2021,"@inproceedings{rebain2021derf,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Daniel Rebain and Wei Jiang and Soroosh Yazdani and Ke Li and Kwang Moo Yi and Andrea Tagliasacchi},
    title = {DeRF: Decomposed Radiance Fields},
    year = {2021},
    url = {http://arxiv.org/abs/2011.12490v1},
    entrytype = {inproceedings},
    id = {rebain2021derf}
}",https://arxiv.org/pdf/2011.12490.pdf,,,,,,,Speed & Computational Efficiency,,,,,,,,,,"Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, Andrea Tagliasacchi",rebain2021derf,76,17,"With the advent of Neural Radiance Fields (NeRF), neural networks can now render novel views of a 3D scene with quality that fools the human eye. Yet, generating these images is very computationally intensive, limiting their applicability in practical scenarios. In this paper, we propose a technique based on spatial decomposition capable of mitigating this issue. Our key observation is that there are diminishing returns in employing larger (deeper and/or wider) networks. Hence, we propose to spatially decompose a scene and dedicate smaller networks for each decomposed part. When working together, these networks can render the whole scene. This allows us near-constant inference time regardless of the number of decomposed parts. Moreover, we show that a Voronoi spatial decomposition is preferable for this purpose, as it is provably compatible with the Painter's Algorithm for efficient and GPU-friendly rendering. Our experiments show that for real-world scenes, our method provides up to 3x more efficient inference than NeRF (with the same rendering quality), or an improvement of up to 1.0~dB in PSNR (for the same inference cost).",,,,,,CVPR 2021,,,,,,
5/23/2021 19:02,,,Space-time Neural Irradiance Fields for Free-Viewpoint Video,,CVPR,11/25/2020,2021,"@inproceedings{xian2021spacetime,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Wenqi Xian and Jia-Bin Huang and Johannes Kopf and Changil Kim},
    title = {Space-time Neural Irradiance Fields for Free-Viewpoint Video},
    year = {2021},
    url = {http://arxiv.org/abs/2011.12950v2},
    entrytype = {inproceedings},
    id = {xian2021spacetime}
}",https://arxiv.org/pdf/2011.12950.pdf,https://video-nerf.github.io/,,,https://www.youtube.com/watch?v=2tN8ghNu2sI,,,Dynamic/Temporal,,,,,,,,,,"Wenqi Xian, Jia-Bin Huang, Johannes Kopf, Changil Kim",xian2021spacetime,75,28,"We present a method that learns a spatiotemporal neural irradiance field for dynamic scenes from a single video. Our learned representation enables free-viewpoint rendering of the input video. Our method builds upon recent advances in implicit representations. Learning a spatiotemporal irradiance field from a single video poses significant challenges because the video contains only one observation of the scene at any point in time. The 3D geometry of a scene can be legitimately represented in numerous ways since varying geometry (motion) can be explained with varying appearance and vice versa. We address this ambiguity by constraining the time-varying geometry of our dynamic scene representation using the scene depth estimated from video depth estimation methods, aggregating contents from individual frames into a single global representation. We provide an extensive quantitative evaluation and demonstrate compelling free-viewpoint rendering results.",,,,,,CVPR 2021,,,,,,
5/23/2021 19:03,,,GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields,GIRAFFE,CVPR,11/24/2020,2021,"@inproceedings{niemeyer2021giraffe,
    url = {http://arxiv.org/abs/2011.12100v2},
    year = {2021},
    title = {GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields},
    author = {Michael Niemeyer and Andreas Geiger},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    entrytype = {inproceedings},
    id = {niemeyer2021giraffe}
}",https://arxiv.org/pdf/2011.12100.pdf,,,,,,,"Generative Models, Global Conditioning",,,,,,,,,,"Michael Niemeyer, Andreas Geiger",niemeyer2021giraffe,74,21,"Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects' shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.",,,,,,CVPR 2021,,,,,,
8/29/2021 20:43,,,Adversarial Generation of Continuous Images,INR-GAN,CVPR,11/24/2020,2021,"@inproceedings{skorokhodov2021inrgan,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Ivan Skorokhodov and Savva Ignatyev and Mohamed Elhoseiny},
    title = {Adversarial Generation of Continuous Images},
    year = {2021},
    url = {http://arxiv.org/abs/2011.12026v2},
    entrytype = {inproceedings},
    id = {skorokhodov2021inrgan}
}",https://arxiv.org/pdf/2011.12026.pdf,https://universome.github.io/inr-gan,https://github.com/universome/inr-gan,,,,,"Generalization, 2D Image Neural Fields, Generative Models, Global Conditioning, Hypernetwork/Meta-learning",,,,,,,,,,"Ivan Skorokhodov, Savva Ignatyev, Mohamed Elhoseiny",skorokhodov2021inrgan,73,7,"In most existing learning systems, images are typically viewed as 2D pixel arrays. However, in another paradigm gaining popularity, a 2D image is represented as an implicit neural representation (INR) - an MLP that predicts an RGB pixel value given its (x,y) coordinate. In this paper, we propose two novel architectural techniques for building INR-based image decoders: factorized multiplicative modulation and multi-scale INRs, and use them to build a state-of-the-art continuous image GAN. Previous attempts to adapt INRs for image generation were limited to MNIST-like datasets and do not scale to complex real-world data. Our proposed INR-GAN architecture improves the performance of continuous image generators by several times, greatly reducing the gap between continuous image GANs and pixel-based ones. Apart from that, we explore several exciting properties of the INR-based decoders, like out-of-the-box superresolution, meaningful image-space interpolation, accelerated inference of low-resolution images, an ability to extrapolate outside of image boundaries, and strong geometric prior. The project page is located at https://universome.github.io/inr-gan.",,,Direct,,,CVPR 2021,,,,,,
6/21/2021 16:40,,,Neural Scene Graphs for Dynamic Scenes,,CVPR,11/20/2020,2021,"@inproceedings{ost2021neural,
    url = {http://arxiv.org/abs/2011.10379v3},
    year = {2021},
    title = {Neural Scene Graphs for Dynamic Scenes},
    author = {Julian Ost and Fahim Mannan and Nils Thuerey and Julian Knodt and Felix Heide},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    entrytype = {inproceedings},
    id = {ost2021neural}
}",https://arxiv.org/pdf/2011.10379.pdf,https://light.princeton.edu/publication/neural-scene-graphs/,,,https://www.youtube.com/watch?v=ea4Y6P0Hk3o,https://light.cs.princeton.edu/wp-content/uploads/2021/02/NeuralSceneGraphs_Supplement.pdf,,"Dynamic/Temporal, Object-Centric, Global Conditioning, Hybrid Geometry Representation",,,,,,,,,,"Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, Felix Heide",ost2021neural,72,12,"Recent implicit neural rendering methods have demonstrated that it is possible to learn accurate view synthesis for complex scenes by predicting their volumetric density and color supervised solely by a set of RGB images. However, existing methods are restricted to learning efficient representations of static scenes that encode all scene objects into a single neural network, and lack the ability to represent dynamic scenes and decompositions into individual scene objects. In this work, we present the first neural rendering method that decomposes dynamic scenes into scene graphs. We propose a learned scene graph representation, which encodes object transformation and radiance, to efficiently render novel arrangements and views of the scene. To this end, we learn implicitly encoded scenes, combined with a jointly learned latent representation to describe objects with a single implicit function. We assess the proposed method on synthetic and real automotive data, validating that our approach learns dynamic scenes -- only by observing a video of this scene -- and allows for rendering novel photo-realistic views of novel scene compositions with unseen sets of objects at unseen poses.",,,,,,CVPR 2021,,,,,,
9/30/2021 8:45,,,Learning Barrier Functions with Memory for Robust Safe Navigation,,RAL,11/3/2020,2021,"@article{long2021learning,
    author = {Kehan Long and Cheng Qian and Jorge Cortes and Nikolay Atanasov},
    title = {Learning Barrier Functions with Memory for Robust Safe Navigation},
    year = {2020},
    month = {Nov},
    url = {http://arxiv.org/abs/2011.01899v2},
    entrytype = {article},
    id = {long2021learning}
}",https://arxiv.org/pdf/2011.01899.pdf,,,,,,,"Robotics, Multi-task/Continual/Transfer learning",,SDF,,,,,,,,"Kehan Long, Cheng Qian, Jorge Cortés, Nikolay Atanasov",long2021learning,71,,"Control barrier functions are widely used to enforce safety properties in robot motion planning and control. However, the problem of constructing barrier functions online and synthesizing safe controllers that can deal with the associated uncertainty has received little attention. This paper investigates safe navigation in unknown environments, using onboard range sensing to construct control barrier functions online. To represent different objects in the environment, we use the distance measurements to train neural network approximations of the signed distance functions incrementally with replay memory. This allows us to formulate a novel robust control barrier safety constraint which takes into account the error in the estimated distance fields and its gradient. Our formulation leads to a second-order cone program, enabling safe and stable control synthesis in a priori unknown environments.",,,,,,RAL 2021,,,,,,
9/17/2021 11:30,,,Neural Light Field 3D Printing,,SIGGRAPH,11/1/2020,2020,"@article{zheng2020neural,
    year = {2020},
    author = {Quan Zheng and Vahid Babaei and Gordon Wetzstein and Hans-Peter Seidel and Matthias Zwicker and Gurprit Singh},
    journal = {ACM Transactions on Graphics (TOG)},
    number = {6},
    pages = {1--12},
    publisher = {Association for Computing Machinery},
    title = {Neural light field 3D printing},
    volume = {39},
    entrytype = {article},
    id = {zheng2020neural}
}",https://dl.acm.org/doi/pdf/10.1145/3414685.3417879,https://quan-zheng.github.io/publication/neuralLF3Dprinting20/,Coming soon,https://drive.google.com/uc?id=1EGwwQsAlw4C1IS6jgK_4P9uP0Rf4ICug&export=download,,https://quan-zheng.github.io/publication/NeuralLightField3DPrinting-supp.pdf,,"Alternative Imaging, Science & Engineering",Fourier Feature (NeRF),,,,,,,,,"Quan Zheng, Vahid Babaei, Gordon Wetzstein, Hans-Peter Seidel, Matthias Zwicker, Gurprit Singh",zheng2020neural,70,,"Modern 3D printers are capable of printing large-size light-field displays at high-resolutions. However, optimizing such displays in full 3D volume for a given light-field imagery is still a challenging task. Existing light field displays optimize over relatively small resolutions using",,,Direct,,,SIGGRAPH 2020,,,,,,
10/8/2021 16:31,,,Extended Physics-InformedNeuralNetworks (XPINNs): A Generalized Space-Time Domain Decomposition Based Deep Learning Framework for Nonlinear Partial Differential Equations,XPINNs,Computer Physics Communications,11/1/2020,2020,"@article{jagtap2020extended,
    title = {Extended physics-informed neural networks (xpinns): A generalized space-time domain decomposition based deep learning framework for nonlinear partial differential equations},
    author = {Ameya D Jagtap and George Em Karniadakis},
    journal = {Communications in Computational Physics},
    volume = {28},
    number = {5},
    pages = {2002--2041},
    year = {2020},
    entrytype = {article},
    id = {jagtap2020extended}
}",https://doc.global-sci.org/uploads/Issue/CiCP/shortpdf/v28n5/285_2002.pdf,https://github.com/AmeyaJagtap/XPINNs,,,,,,Beyond Visual Computing,,,,,,,,,,"Ameya D Jagtap, George Em Karniadakis",jagtap2020extended,69,,"We propose a generalized space-time domain decomposition approach for the physics-informed neural networks (PINNs) to solve nonlinear partial differential equations (PDEs) on arbitrary complex-geometry domains. The proposed framework, named eXtended PINNs (XPINNs), further pushes the boundaries of both PINNs as well as conservative PINNs (cPINNs), which is a recently proposed domain decomposition approach in the PINN framework tailored to conservation laws. Compared to PINN, the XPINN method has large representation and parallelization capacity due to the inherent property of deployment of multiple neural networks in the smaller subdomains. Unlike cPINN, XPINN can be extended to any type of PDEs. Moreover, the domain can be decomposed in any arbitrary way (in space and time), which is not possible in cPINN. Thus, XPINN offers both space and time parallelization, thereby reducing the training cost more effectively. In each subdomain, a separate neural network is employed with optimally selected hyperparameters, e.g., depth/width of the network, number and location of residual points, activation function, optimization method, etc. A deep network can be employed in a subdomain with complex solution, whereas a shallow neural network can be used in a subdomain with relatively simple and smooth solutions. We demonstrate the versatility of XPINN by solving both forward and inverse PDE problems, ranging from one-dimensional to three-dimensional problems, from time-dependent to time-independent problems, and from continuous to discontinuous problems, which clearly shows that the XPINN method is promising in many practical problems. The proposed XPINN method is the generalization of PINN and cPINN methods, both in terms of applicability as well as domain decomposition approach, which efficiently lends itself to parallelized computation. The XPINN code is available on https://github.com/AmeyaJagtap/XPINNs.",,,,,,Computer Physics Communications 2020,,,,,,
6/29/2021 15:14,,,Neural Unsigned Distance Fields for Implicit Function Learning,NDF,NeurIPS,10/26/2020,2020,"@inproceedings{chibane2020ndf,
    publisher = {Curran Associates, Inc.},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    author = {Julian Chibane and Aymen Mir and Gerard Pons-Moll},
    title = {Neural Unsigned Distance Fields for Implicit Function Learning},
    year = {2020},
    url = {http://arxiv.org/abs/2010.13938v1},
    entrytype = {inproceedings},
    id = {chibane2020ndf}
}",https://arxiv.org/pdf/2010.13938.pdf,http://virtualhumans.mpi-inf.mpg.de/ndf/,https://github.com/jchibane/ndf/,,https://www.youtube.com/watch?v=_xsLdVzX8DY,http://virtualhumans.mpi-inf.mpg.de/papers/chibane2020ndf/chibane2020ndf-supp.pdf,,"Generalization, Data-Driven Method",,UDF,"Yes, geometry only",,,,,,,"Julian Chibane, Aymen Mir, Gerard Pons-Moll",chibane2020ndf,68,17,"In this work we target a learnable output representation that allows continuous, high resolution outputs of arbitrary shape. Recent works represent 3D surfaces implicitly with a Neural Network, thereby breaking previous barriers in resolution, and ability to represent diverse topologies. However, neural implicit representations are limited to closed surfaces, which divide the space into inside and outside. Many real world objects such as walls of a scene scanned by a sensor, clothing, or a car with inner structures are not closed. This constitutes a significant barrier, in terms of data pre-processing (objects need to be artificially closed creating artifacts), and the ability to output open surfaces. In this work, we propose Neural Distance Fields (NDF), a neural network based model which predicts the unsigned distance field for arbitrary 3D shapes given sparse point clouds. NDF represent surfaces at high resolutions as prior implicit models, but do not require closed surface data, and significantly broaden the class of representable shapes in the output. NDF allow to extract the surface as very dense point clouds and as meshes. We also show that NDF allow for surface normal calculation and can be rendered using a slight modification of sphere tracing. We find NDF can be used for multi-target regression (multiple outputs for one input) with techniques that have been exclusively used for rendering in graphics. Experiments on ShapeNet show that NDF, while simple, is the state-of-the art, and allows to reconstruct shapes with inner structures, such as the chairs inside a bus. Notably, we show that NDF are not restricted to 3D shapes, and can approximate more general open surfaces such as curves, manifolds, and functions. Code is available for research at https://virtualhumans.mpi-inf.mpg.de/ndf/.",,,,,,NeurIPS 2020,,,,,,
9/17/2021 13:19,,,"LoopReg: Self-supervised Learning of Implicit Surface Correspondences, Pose and Shape for 3D Human Mesh Registration",LoopReg,NeurIPS,10/23/2020,2020,"@inproceedings{bhatnagar2020loopreg,
    publisher = {Curran Associates, Inc.},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    author = {Bharat Lal Bhatnagar and Cristian Sminchisescu and Christian Theobalt and Gerard Pons-Moll},
    title = {LoopReg: Self-supervised Learning of Implicit Surface Correspondences, Pose and Shape for 3D Human Mesh Registration},
    year = {2020},
    url = {http://arxiv.org/abs/2010.12447v1},
    entrytype = {inproceedings},
    id = {bhatnagar2020loopreg}
}",https://arxiv.org/pdf/2010.12447.pdf,,https://github.com/bharat-b7/LoopReg,,https://www.youtube.com/watch?v=fIhm_tWG_X8,,,Human (Body),,SDF,"Yes, geometry only",,,,,,,"Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian Theobalt, Gerard Pons-Moll",bhatnagar2020loopreg,67,,"We address the problem of fitting 3D human models to 3D scans of dressed humans. Classical methods optimize both the data-to-model correspondences and the human model parameters (pose and shape), but are reliable only when initialized close to the solution. Some methods initialize the optimization based on fully supervised correspondence predictors, which is not differentiable end-to-end, and can only process a single scan at a time. Our main contribution is LoopReg, an end-to-end learning framework to register a corpus of scans to a common 3D human model. The key idea is to create a self-supervised loop. A backward map, parameterized by a Neural Network, predicts the correspondence from every scan point to the surface of the human model. A forward map, parameterized by a human model, transforms the corresponding points back to the scan based on the model parameters (pose and shape), thus closing the loop. Formulating this closed loop is not straightforward because it is not trivial to force the output of the NN to be on the surface of the human model - outside this surface the human model is not even defined. To this end, we propose two key innovations. First, we define the canonical surface implicitly as the zero level set of a distance field in R3, which in contrast to morecommon UV parameterizations, does not require cutting the surface, does not have discontinuities, and does not induce distortion. Second, we diffuse the human model to the 3D domain R3. This allows to map the NN predictions forward,even when they slightly deviate from the zero level set. Results demonstrate that we can train LoopRegmainly self-supervised - following a supervised warm-start, the model becomes increasingly more accurate as additional unlabelled raw scans are processed. Our code and pre-trained models can be downloaded for research.",,,Direct,,,NeurIPS 2020,,,,,,
7/19/2021 21:13,,,SDF-SRN: Learning Signed Distance 3D Object Reconstruction from Static Images,SDF-SRN,NeurIPS,10/20/2020,2020,"@inproceedings{lin2020sdfsrn,
    publisher = {Curran Associates, Inc.},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    author = {Chen-Hsuan Lin and Chaoyang Wang and Simon Lucey},
    title = {SDF-SRN: Learning Signed Distance 3D Object Reconstruction from Static Images},
    year = {2020},
    url = {http://arxiv.org/abs/2010.10505v1},
    entrytype = {inproceedings},
    id = {lin2020sdfsrn}
}",https://arxiv.org/pdf/2010.10505.pdf,https://chenhsuanlin.bitbucket.io/signed-distance-SRN/,https://github.com/chenhsuanlin/signed-distance-SRN,,"https://chenhsuanlin.bitbucket.io/signed-distance-SRN/video.mp4, https://chenhsuanlin.bitbucket.io/signed-distance-SRN/shorttalk.mp4",https://chenhsuanlin.bitbucket.io/signed-distance-SRN/supplementary.pdf,,"Generalization, Data-Driven Method, Hypernetwork/Meta-learning",,,"Yes, geometry only",,,,,,,"Chen-Hsuan Lin, Chaoyang Wang, Simon Lucey",lin2020sdfsrn,66,7,"Dense 3D object reconstruction from a single image has recently witnessed remarkable advances, but supervising neural networks with ground-truth 3D shapes is impractical due to the laborious process of creating paired image-shape datasets. Recent efforts have turned to learning 3D reconstruction without 3D supervision from RGB images with annotated 2D silhouettes, dramatically reducing the cost and effort of annotation. These techniques, however, remain impractical as they still require multi-view annotations of the same object instance during training. As a result, most experimental efforts to date have been limited to synthetic datasets. In this paper, we address this issue and propose SDF-SRN, an approach that requires only a single view of objects at training time, offering greater utility for real-world scenarios. SDF-SRN learns implicit 3D shape representations to handle arbitrary shape topologies that may exist in the datasets. To this end, we derive a novel differentiable rendering formulation for learning signed distance functions (SDF) from 2D silhouettes. Our method outperforms the state of the art under challenging single-view supervision settings on both synthetic and real-world datasets.",,,,,,NeurIPS 2020,,,,,,
5/23/2021 19:04,,,NeRF++: Analyzing and Improving Neural Radiance Fields,NeRF++,ARXIV,10/15/2020,2020,"@article{zhang2020nerf++,
    journal = {arXiv preprint arXiv:2010.07492},
    booktitle = {ArXiv Pre-print},
    author = {Kai Zhang and Gernot Riegler and Noah Snavely and Vladlen Koltun},
    title = {NeRF++: Analyzing and Improving Neural Radiance Fields},
    year = {2020},
    url = {http://arxiv.org/abs/2010.07492v2},
    entrytype = {article},
    id = {zhang2020nerf++}
}",https://arxiv.org/pdf/2010.07492.pdf,https://github.com/Kai-46/nerfplusplus,,,https://www.youtube.com/watch?v=Rd0nBO6--bM&feature=youtu.be&t=1992,,,"Fundamentals, Sampling",,,,,,,,,,"Kai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun",zhang2020nerf++,65,48,"Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF's success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at https://github.com/Kai-46/nerfplusplus.",,,,,,ARXIV 2020,,,,,,
7/20/2021 10:58,,,General Radiance Field,GRF,ICCV,10/9/2020,2021,"@inproceedings{trevithick2021grf,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Alex Trevithick and Bo Yang},
    title = {GRF: Learning a General Radiance Field for 3D Representation and Rendering},
    year = {2021},
    url = {http://arxiv.org/abs/2010.04595v3},
    entrytype = {inproceedings},
    id = {trevithick2021grf}
}",https://arxiv.org/pdf/2010.04595.pdf,,https://github.com/alextrevithick/GRF,,,,https://drive.google.com/file/d/1H2FNeAsKoQqCsO0n7PiA1HcT1ingnwJd/view,"Speed & Computational Efficiency, Generalization, Image-Based Rendering, Local Conditioning",,,,,,,,,,"Alex Trevithick, Bo Yang",trevithick2021grf,64,25,"We present a simple yet powerful neural network that implicitly represents and renders 3D objects and scenes only from 2D observations. The network models 3D geometries as a general radiance field, which takes a set of 2D images with camera poses and intrinsics as input, constructs an internal representation for each point of the 3D space, and then renders the corresponding appearance and geometry of that point viewed from an arbitrary position. The key to our approach is to learn local features for each pixel in 2D images and to then project these features to 3D points, thus yielding general and rich point representations. We additionally integrate an attention mechanism to aggregate pixel features from multiple 2D views, such that visual occlusions are implicitly taken into account. Extensive experiments demonstrate that our method can generate high-quality and realistic novel views for novel objects, unseen categories and challenging real-world scenes.",,,,,,ICCV 2021,,,,,,
8/29/2021 21:03,,,"X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation",X-Fields,SIGGRAPH,10/1/2020,2020,"@article{bemana2020xfields,
    publisher = {Association for Computing Machinery},
    journal = {ACM Transactions on Graphics (TOG)},
    author = {Mojtaba Bemana and Karol Myszkowski and Hans-Peter Seidel and Tobias Ritschel},
    title = {X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation},
    year = {2020},
    url = {http://arxiv.org/abs/2010.00450v1},
    entrytype = {article},
    id = {bemana2020xfields}
}",https://arxiv.org/pdf/2010.00450.pdf,https://xfields.mpi-inf.mpg.de/,https://github.com/m-bemana/xfields,https://xfields.mpi-inf.mpg.de/dataset/view_light_time.zip,https://www.youtube.com/watch?v=0tsw7yJGfFI,,,"Dynamic/Temporal, 2D Image Neural Fields, Editable, Material/Lighting Estimation",,,,,,,,,,"Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, Tobias Ritschel",bemana2020xfields,63,17,"We suggest to represent an X-Field -a set of 2D images taken across different view, time or illumination conditions, i.e., video, light field, reflectance fields or combinations thereof-by learning a neural network (NN) to map their view, time or light coordinates to 2D images. Executing this NN at new coordinates results in joint view, time or light interpolation. The key idea to make this workable is a NN that already knows the ""basic tricks"" of graphics (lighting, 3D projection, occlusion) in a hard-coded and differentiable form. The NN represents the input to that rendering as an implicit map, that for any view, time, or light coordinate and for any pixel can quantify how it will move if view, time or light coordinates change (Jacobian of pixel position with respect to view, time, illumination, etc.). Our X-Field representation is trained for one scene within minutes, leading to a compact set of trainable parameters and hence real-time navigation in view, time and illumination.",Yes,,"Direct, Indirect",,,SIGGRAPH 2020,,,,,,
9/30/2021 11:06,,,Learning Equality Constraints for Motion Planning on Manifolds,,CoRL,9/24/2020,2020,"@inproceedings{sutanto2020learning,
    booktitle = {Proceedings of the Conference on Robot Learning (CoRL)},
    author = {Giovanni Sutanto and Isabel M. Rayas Fernandez and Peter Englert and Ragesh K. Ramachandran and Gaurav S. Sukhatme},
    title = {Learning Equality Constraints for Motion Planning on Manifolds},
    year = {2020},
    url = {http://arxiv.org/abs/2009.11852v1},
    entrytype = {inproceedings},
    id = {sutanto2020learning}
}",https://arxiv.org/pdf/2009.11852.pdf,,https://github.com/gsutanto/smp_manifold_learning,,https://www.youtube.com/watch?v=WoC7nqp4XNk,,,"Fundamentals, Science & Engineering, Robotics",,,,,,,,,,"Giovanni Sutanto, Isabel M. Rayas Fernández, Peter Englert, Ragesh K. Ramachandran, Gaurav S. Sukhatme",sutanto2020learning,62,,"Constrained robot motion planning is a widely used technique to solve complex robot tasks. We consider the problem of learning representations of constraints from demonstrations with a deep neural network, which we call Equality Constraint Manifold Neural Network (ECoMaNN). The key idea is to learn a level-set function of the constraint suitable for integration into a constrained sampling-based motion planner. Learning proceeds by aligning subspaces in the network with subspaces of the data. We combine both learned constraints and analytically described constraints into the planner and use a projection-based strategy to find valid points. We evaluate ECoMaNN on its representation capabilities of constraint manifolds, the impact of its individual loss terms, and the motions produced when incorporated into a planner.",,,,,,CoRL 2020,,,,,,
9/1/2021 14:36,,,"ContactNets: Learning Discontinuous Contact Dynamics with Smooth, Implicit Representations",ContactNets,CoRL,9/23/2020,2020,"@inproceedings{pfrommer2020contactnets,
    booktitle = {Proceedings of the Conference on Robot Learning (CoRL)},
    author = {Samuel Pfrommer and Mathew Halm and Michael Posa},
    title = {ContactNets: Learning Discontinuous Contact Dynamics with Smooth, Implicit Representations},
    year = {2020},
    url = {http://arxiv.org/abs/2009.11193v2},
    entrytype = {inproceedings},
    id = {pfrommer2020contactnets}
}",https://arxiv.org/pdf/2009.11193.pdf,,https://github.com/DAIRLab/contact-nets,,https://www.youtube.com/watch?v=I6p8JrIp1Es,,,"Science & Engineering, Robotics, Data-Driven Method, Hybrid Geometry Representation",,,,,,,,,,"Samuel Pfrommer, Mathew Halm, Michael Posa",pfrommer2020contactnets,61,0,"Common methods for learning robot dynamics assume motion is continuous, causing unrealistic model predictions for systems undergoing discontinuous impact and stiction behavior. In this work, we resolve this conflict with a smooth, implicit encoding of the structure inherent to contact-induced discontinuities. Our method, ContactNets, learns parameterizations of inter-body signed distance and contact-frame Jacobians, a representation that is compatible with many simulation, control, and planning environments for robotics. We furthermore circumvent the need to differentiate through stiff or non-smooth dynamics with a novel loss function inspired by the principles of complementarity and maximum dissipation. Our method can predict realistic impact, non-penetration, and stiction when trained on 60 seconds of real-world data.",,,Direct,,,CoRL 2020,,,,,,
7/19/2021 21:09,,,Implicit Feature Networks for Texture Completion from Partial 3D Data,IF-Net-Texture,ECCV,9/20/2020,2020,"@inproceedings{chibane2020ifnettexture,
    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
    author = {Julian Chibane and Gerard Pons-Moll},
    title = {Implicit Feature Networks for Texture Completion from Partial 3D Data},
    year = {2020},
    url = {http://arxiv.org/abs/2009.09458v1},
    entrytype = {inproceedings},
    id = {chibane2020ifnettexture}
}",https://arxiv.org/pdf/2009.09458.pdf,https://virtualhumans.mpi-inf.mpg.de/ifnets/,https://github.com/jchibane/if-net_texture,,,,,Data-Driven Method,None,Occupancy,No,,,,,,,"Julian Chibane, Gerard Pons-Moll",chibane2020ifnettexture,60,6,"Prior work to infer 3D texture use either texture atlases, which require uv-mappings and hence have discontinuities, or colored voxels, which are memory inefficient and limited in resolution. Recent work, predicts RGB color at every XYZ coordinate forming a texture field, but focus on completing texture given a single 2D image. Instead, we focus on 3D texture and geometry completion from partial and incomplete 3D scans. IF-Nets have recently achieved state-of-the-art results on 3D geometry completion using a multi-scale deep feature encoding, but the outputs lack texture. In this work, we generalize IF-Nets to texture completion from partial textured scans of humans and arbitrary objects. Our key insight is that 3D texture completion benefits from incorporating local and global deep features extracted from both the 3D partial texture and completed geometry. Specifically, given the partial 3D texture and the 3D geometry completed with IF-Nets, our model successfully in-paints the missing texture parts in consistence with the completed geometry. Our model won the SHARP ECCV'20 challenge, achieving highest performance on all challenges.",,Yes,,,,ECCV 2020,,,,,,
6/29/2021 15:53,,,On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes,,ICML,9/17/2020,2021,"@inproceedings{davies2021on,
    publisher = {PMLR},
    booktitle = {International Conference on Machine Learning (ICML)},
    author = {Thomas Davies and Derek Nowrouzezahrai and Alec Jacobson},
    title = {On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes},
    year = {2021},
    url = {http://arxiv.org/abs/2009.09808v3},
    entrytype = {inproceedings},
    id = {davies2021on}
}",https://arxiv.org/pdf/2009.09808.pdf,,https://github.com/u2ni/ICML2021,https://ten-thousand-models.appspot.com/,,,,"Speed & Computational Efficiency, Compression, Fundamentals, Sampling",,,"Yes, geometry only",,,,,,,"Thomas Davies, Derek Nowrouzezahrai, Alec Jacobson",davies2021on,59,5,"A neural implicit outputs a number indicating whether the given query point in space is inside, outside, or on a surface. Many prior works have focused on _latent-encoded_ neural implicits, where a latent vector encoding of a specific shape is also fed as input. While affording latent-space interpolation, this comes at the cost of reconstruction accuracy for any _single_ shape. Training a specific network for each 3D shape, a _weight-encoded_ neural implicit may forgo the latent vector and focus reconstruction accuracy on the details of a single shape. While previously considered as an intermediary representation for 3D scanning tasks or as a toy-problem leading up to latent-encoding tasks, weight-encoded neural implicits have not yet been taken seriously as a 3D shape representation. In this paper, we establish that weight-encoded neural implicits meet the criteria of a first-class 3D shape representation. We introduce a suite of technical contributions to improve reconstruction accuracy, convergence, and robustness when learning the signed distance field induced by a polygonal mesh -- the _de facto_ standard representation. Viewed as a lossy compression, our conversion outperforms standard techniques from geometry processing. Compared to previous latent- and weight-encoded neural implicits we demonstrate superior robustness, scalability, and performance.",,,,,,ICML 2021,,,,,,
9/18/2021 10:22,,,Pix2Surf: Learning Parametric 3D Surface Models of Objects from Images,Pix2Surf,ECCV,8/18/2020,2020,"@inproceedings{lei2020pix2surf,
    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
    author = {Jiahui Lei and Srinath Sridhar and Paul Guerrero and Minhyuk Sung and Niloy Mitra and Leonidas J. Guibas},
    title = {Pix2Surf: Learning Parametric 3D Surface Models of Objects from Images},
    year = {2020},
    url = {http://arxiv.org/abs/2008.07760v1},
    entrytype = {inproceedings},
    id = {lei2020pix2surf}
}",https://arxiv.org/pdf/2008.07760.pdf,https://geometry.stanford.edu/projects/pix2surf/,https://github.com/JiahuiLei/Pix2Surf,,https://www.youtube.com/watch?v=jaxB0VSuvms,https://geometry.stanford.edu/projects/pix2surf/pub/pix2surf_supp.pdf,,"Sparse Reconstruction, Generalization, Generative Models, Global Conditioning, Local Conditioning",None,"Atlas, Explicit","Yes, geometry only",,,,,,,"Jiahui Lei, Srinath Sridhar, Paul Guerrero, Minhyuk Sung, Niloy Mitra, Leonidas J. Guibas",lei2020pix2surf,58,,"We investigate the problem of learning to generate 3D parametric surface representations for novel object instances, as seen from one or more views. Previous work on learning shape reconstruction from multiple views uses discrete representations such as point clouds or voxels, while continuous surface generation approaches lack multi-view consistency. We address these issues by designing neural networks capable of generating high-quality parametric 3D surfaces which are also consistent between views. Furthermore, the generated 3D surfaces preserve accurate image pixel to 3D surface point correspondences, allowing us to lift texture information to reconstruct shapes with rich geometry and appearance. Our method is supervised and trained on a public dataset of shapes from common object categories. Quantitative results indicate that our method significantly outperforms previous work, while qualitative results demonstrate the high quality of our reconstructions.",No,,Direct,,,ECCV 2020,,,,,,
9/17/2021 11:38,,,Grasping Field: Learning Implicit Representations for Human Grasps,Grasping Field,3DV,8/10/2020,2020,"@inproceedings{karunratanakul2020graspingfield,
    url = {http://arxiv.org/abs/2008.04451v3},
    year = {2020},
    title = {Grasping Field: Learning Implicit Representations for Human Grasps},
    author = {Korrawe Karunratanakul and Jinlong Yang and Yan Zhang and Michael Black and Krikamol Muandet and Siyu Tang},
    booktitle = {International Conference on 3D Vision (3DV)},
    organization = {IEEE},
    entrytype = {inproceedings},
    id = {karunratanakul2020graspingfield}
}",https://arxiv.org/pdf/2008.04451.pdf,https://mano.is.tue.mpg.de/,https://github.com/korrawe/grasping_field,,https://www.youtube.com/watch?v=_1o21xc3TD0,,,Robotics,,SDF,"Yes, geometry only",,,,,,,"Korrawe Karunratanakul, Jinlong Yang, Yan Zhang, Michael Black, Krikamol Muandet, Siyu Tang",karunratanakul2020graspingfield,57,,"Robotic grasping of house-hold objects has made remarkable progress in recent years. Yet, human grasps are still difficult to synthesize realistically. There are several key reasons: (1) the human hand has many degrees of freedom (more than robotic manipulators); (2) the synthesized hand should conform to the surface of the object; and (3) it should interact with the object in a semantically and physically plausible manner. To make progress in this direction, we draw inspiration from the recent progress on learning-based implicit representations for 3D object reconstruction. Specifically, we propose an expressive representation for human grasp modelling that is efficient and easy to integrate with deep neural networks. Our insight is that every point in a three-dimensional space can be characterized by the signed distances to the surface of the hand and the object, respectively. Consequently, the hand, the object, and the contact area can be represented by implicit surfaces in a common space, in which the proximity between the hand and the object can be modelled explicitly. We name this 3D to 2D mapping as Grasping Field, parameterize it with a deep neural network, and learn it from data. We demonstrate that the proposed grasping field is an effective and expressive representation for human grasp generation. Specifically, our generative model is able to synthesize high-quality human grasps, given only on a 3D object point cloud. The extensive experiments demonstrate that our generative model compares favorably with a strong baseline and approaches the level of natural human grasps. Our method improves the physical plausibility of the hand-object contact reconstruction and achieves comparable performance for 3D hand reconstruction compared to state-of-the-art methods.",No,,"Direct, Indirect",,,3DV 2020,,,,,,
5/23/2021 19:10,,,Neural Reflectance Fields for Appearance Acquisition,,ARXIV,8/9/2020,2020,"@article{bi2020neural,
    journal = {arXiv preprint arXiv:2008.03824},
    booktitle = {ArXiv Pre-print},
    author = {Sai Bi and Zexiang Xu and Pratul Srinivasan and Ben Mildenhall and Kalyan Sunkavalli and Milos Hasan and Yannick Hold-Geoffroy and David Kriegman and Ravi Ramamoorthi},
    title = {Neural Reflectance Fields for Appearance Acquisition},
    year = {2020},
    url = {http://arxiv.org/abs/2008.03824v2},
    entrytype = {article},
    id = {bi2020neural}
}",https://arxiv.org/pdf/2008.03824.pdf,,,,https://www.youtube.com/watch?v=tQZk5OoFgsc,,,Material/Lighting Estimation,,,,,,,,,,"Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall, Kalyan Sunkavalli, Miloš Hašan, Yannick Hold-Geoffroy, David Kriegman, Ravi Ramamoorthi",bi2020neural,56,21,"We present Neural Reflectance Fields, a novel deep scene representation that encodes volume density, normal and reflectance properties at any 3D point in a scene using a fully-connected neural network. We combine this representation with a physically-based differentiable ray marching framework that can render images from a neural reflectance field under any viewpoint and light. We demonstrate that neural reflectance fields can be estimated from images captured with a simple collocated camera-light setup, and accurately model the appearance of real-world scenes with complex geometry and reflectance. Once estimated, they can be used to render photo-realistic images under novel viewpoint and (non-collocated) lighting conditions and accurately reproduce challenging effects like specularities, shadows and occlusions. This allows us to perform high-quality view synthesis and relighting that is significantly better than previous methods. We also demonstrate that we can compose the estimated neural reflectance field of a real scene with traditional scene models and render them using standard Monte Carlo rendering engines. Our work thus enables a complete pipeline from high-quality and practical appearance acquisition to 3D scene composition and rendering.",,,,,,ARXIV 2020,,,,,,
5/23/2021 19:07,,,NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections,NeRF-W,CVPR,8/5/2020,2021,"@inproceedings{martin-brualla2021nerfw,
    url = {http://arxiv.org/abs/2008.02268v3},
    year = {2021},
    title = {NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections},
    author = {Ricardo Martin-Brualla and Noha Radwan and Mehdi S. M. Sajjadi and Jonathan T. Barron and Alexey Dosovitskiy and Daniel Duckworth},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    entrytype = {inproceedings},
    id = {martin-brualla2021nerfw}
}",https://arxiv.org/pdf/2008.02268.pdf,https://nerf-w.github.io/,,,,,,"Dynamic/Temporal, Material/Lighting Estimation",,,,,,,,,,"Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, Daniel Duckworth",martin-brualla2021nerfw,55,78,"We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multilayer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art.",,,,,,CVPR 2021,,,,,,
8/29/2021 21:13,,,PatchNets: Patch-Based Generalizable Deep Implicit 3D Shape Representations,PatchNets,ECCV,8/4/2020,2020,"@inproceedings{tretschk2020patchnets,
    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
    author = {Edgar Tretschk and Ayush Tewari and Vladislav Golyanik and Michael Zollhofer and Carsten Stoll and Christian Theobalt},
    title = {PatchNets: Patch-Based Generalizable Deep Implicit 3D Shape Representations},
    year = {2020},
    url = {http://arxiv.org/abs/2008.01639v2},
    entrytype = {inproceedings},
    id = {tretschk2020patchnets}
}",https://arxiv.org/pdf/2008.01639.pdf,http://gvv.mpi-inf.mpg.de/projects/PatchNets/,https://github.com/edgar-tr/patchnets,,"http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_short.mp4, http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_supplemental.mp4, http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_talk.mp4",http://gvv.mpi-inf.mpg.de/projects/PatchNets/data/patchnets_supplemental.pdf,,"Human (Body), Generalization, Data-Driven Method, Global Conditioning, Local Conditioning",,SDF,"Yes, geometry only",,,,,,,"Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Carsten Stoll, Christian Theobalt",tretschk2020patchnets,54,16,"Implicit surface representations, such as signed-distance functions, combined with deep learning have led to impressive models which can represent detailed shapes of objects with arbitrary topology. Since a continuous function is learned, the reconstructions can also be extracted at any arbitrary resolution. However, large datasets such as ShapeNet are required to train such models. In this paper, we present a new mid-level patch-based surface representation. At the level of patches, objects across different categories share similarities, which leads to more generalizable models. We then introduce a novel method to learn this patch-based representation in a canonical space, such that it is as object-agnostic as possible. We show that our representation trained on one category of objects from ShapeNet can also well represent detailed shapes from any other category. In addition, it can be trained using much fewer shapes, compared to existing approaches. We show several applications of our new representation, including shape interpolation and partial point cloud completion. Due to explicit control over positions, orientations and scales of patches, our representation is also more controllable compared to object-level representations, which enables us to deform encoded shapes non-rigidly.",,,Direct,,,ECCV 2020,,,,,,
7/19/2021 21:28,,,Continuous Object Representation Networks: Novel View Synthesis without Target View Supervision,CORN,NeurIPS,7/30/2020,2020,"@inproceedings{hani2020corn,
    publisher = {Curran Associates, Inc.},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    author = {Nicolai Hani and Selim Engin and Jun-Jee Chao and Volkan Isler},
    title = {Continuous Object Representation Networks: Novel View Synthesis without Target View Supervision},
    year = {2020},
    url = {http://arxiv.org/abs/2007.15627v2},
    entrytype = {inproceedings},
    id = {hani2020corn}
}",https://arxiv.org/pdf/2007.15627.pdf,,,,,,,"Sparse Reconstruction, Generalization, Image-Based Rendering, Data-Driven Method, Global Conditioning, Local Conditioning",,,,,,,,,,"Nicolai Häni, Selim Engin, Jun-Jee Chao, Volkan Isler",hani2020corn,53,5,"Novel View Synthesis (NVS) is concerned with synthesizing views under camera viewpoint transformations from one or multiple input images. NVS requires explicit reasoning about 3D object structure and unseen parts of the scene to synthesize convincing results. As a result, current approaches typically rely on supervised training with either ground truth 3D models or multiple target images. We propose Continuous Object Representation Networks (CORN), a conditional architecture that encodes an input image's geometry and appearance that map to a 3D consistent scene representation. We can train CORN with only two source images per object by combining our model with a neural renderer. A key feature of CORN is that it requires no ground truth 3D models or target view supervision. Regardless, CORN performs well on challenging tasks such as novel view synthesis and single-view 3D reconstruction and achieves performance comparable to state-of-the-art approaches that use direct supervision. For up-to-date information, data, and code, please see our project page: https://nicolaihaeni.github.io/corn/.",,,,,,NeurIPS 2020,,,,,,
9/17/2021 14:11,,,Monocular Real-Time Volumetric Performance Capture,Monoport,ECCV,7/28/2020,2020,"@inproceedings{li2020monoport,
    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
    author = {Ruilong Li and Yuliang Xiu and Shunsuke Saito and Zeng Huang and Kyle Olszewski and Hao Li},
    title = {Monocular Real-Time Volumetric Performance Capture},
    year = {2020},
    url = {http://arxiv.org/abs/2007.13988v1},
    entrytype = {inproceedings},
    id = {li2020monoport}
}",https://arxiv.org/pdf/2007.13988.pdf,https://project-splinter.github.io/monoport/,https://github.com/Project-Splinter/MonoPort,,https://github.com/Project-Splinter/MonoPort,,,"Human (Body), Local Conditioning",,,"Yes, geometry only",,,,,,,"Ruilong Li, Yuliang Xiu, Shunsuke Saito, Zeng Huang, Kyle Olszewski, Hao Li",li2020monoport,52,,"We present the first approach to volumetric performance capture and novel-view rendering at real-time speed from monocular video, eliminating the need for expensive multi-view systems or cumbersome pre-acquisition of a personalized template model. Our system reconstructs a fully textured 3D human from each frame by leveraging Pixel-Aligned Implicit Function (PIFu). While PIFu achieves high-resolution reconstruction in a memory-efficient manner, its computationally expensive inference prevents us from deploying such a system for real-time applications. To this end, we propose a novel hierarchical surface localization algorithm and a direct rendering method without explicitly extracting surface meshes. By culling unnecessary regions for evaluation in a coarse-to-fine manner, we successfully accelerate the reconstruction by two orders of magnitude from the baseline without compromising the quality. Furthermore, we introduce an Online Hard Example Mining (OHEM) technique that effectively suppresses failure modes due to the rare occurrence of challenging examples. We adaptively update the sampling probability of the training data based on the current reconstruction accuracy, which effectively alleviates reconstruction artifacts. Our experiments and evaluations demonstrate the robustness of our system to various challenging angles, illuminations, poses, and clothing styles. We also show that our approach compares favorably with the state-of-the-art monocular performance capture. Our proposed approach removes the need for multi-view studio settings and enables a consumer-accessible solution for volumetric capture.",,,,,,ECCV 2020,,,,,,
8/29/2021 16:29,,,Ladybird: Quasi-Monte Carlo Sampling for Deep Implicit Field Based 3D Reconstruction with Symmetry,Ladybird,ECCV,7/27/2020,2020,"@inproceedings{xu2020ladybird,
    url = {http://arxiv.org/abs/2007.13393v1},
    year = {2020},
    title = {Ladybird: Quasi-Monte Carlo Sampling for Deep Implicit Field Based 3D Reconstruction with Symmetry},
    author = {Yifan Xu and Tianqi Fan and Yi Yuan and Gurprit Singh},
    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
    entrytype = {inproceedings},
    id = {xu2020ladybird}
}",https://arxiv.org/pdf/2007.13393.pdf,,https://github.com/FuxiCV/Ladybird,,,,,"Sparse Reconstruction, Fundamentals, Sampling, Symmetry",,,,,,,,,,"Yifan Xu, Tianqi Fan, Yi Yuan, Gurprit Singh",xuoralladybird,51,6,"Deep implicit field regression methods are effective for 3D reconstruction from single-view images. However, the impact of different sampling patterns on the reconstruction quality is not well-understood. In this work, we first study the effect of point set discrepancy on the network training. Based on Farthest Point Sampling algorithm, we propose a sampling scheme that theoretically encourages better generalization performance, and results in fast convergence for SGD-based optimization algorithms. Secondly, based on the reflective symmetry of an object, we propose a feature fusion method that alleviates issues due to self-occlusions which makes it difficult to utilize local image features. Our proposed system Ladybird is able to create high quality 3D object reconstructions from a single input image. We evaluate Ladybird on a large scale 3D dataset (ShapeNet) demonstrating highly competitive results in terms of Chamfer distance, Earth Mover's distance and Intersection Over Union (IoU).",,,,,,ECCV 2020,,,,,,
5/23/2021 19:11,,,Neural Sparse Voxel Fields,NSVF,ECCV,7/22/2020,2020,"@inproceedings{liu2020nsvf,
    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
    author = {Lingjie Liu and Jiatao Gu and Kyaw Zaw Lin and Tat-Seng Chua and Christian Theobalt},
    title = {Neural Sparse Voxel Fields},
    year = {2020},
    url = {http://arxiv.org/abs/2007.11571v2},
    entrytype = {inproceedings},
    id = {liu2020nsvf}
}",https://arxiv.org/pdf/2007.11571.pdf,https://lingjie0206.github.io/papers/NSVF/,https://github.com/facebookresearch/NSVF,,https://www.youtube.com/watch?v=RFqPwH7QFEI,,,"Speed & Computational Efficiency, Local Conditioning, Coarse-to-Fine, Voxel Grid, Sampling, Hybrid Geometry Representation",,,,,,,,,,"Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, Christian Theobalt",liu2020nsvf,50,90,"Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a differentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is typically over 10 times faster than the state-of-the-art (namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering. Code and data are available at our website: https://github.com/facebookresearch/NSVF.",,Yes,,,,ECCV 2020,,,,,,
9/17/2021 13:11,,,Combining Implicit Function Learning and Parametric Models for 3D Human Reconstruction,IP-Net,ECCV,7/22/2020,2020,"@inproceedings{bhatnagar2020ipnet,
    url = {http://arxiv.org/abs/2007.11432v1},
    year = {2020},
    title = {Combining Implicit Function Learning and Parametric Models for 3D Human Reconstruction},
    author = {Bharat Lal Bhatnagar and Cristian Sminchisescu and Christian Theobalt and Gerard Pons-Moll},
    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
    entrytype = {inproceedings},
    id = {bhatnagar2020ipnet}
}",https://arxiv.org/pdf/2007.11432.pdf,https://virtualhumans.mpi-inf.mpg.de/ipnet/,https://github.com/bharat-b7/IPNet,,https://virtualhumans.mpi-inf.mpg.de/ipnet/ECCV_short.mp4,,,"Human (Body), Local Conditioning, Voxel Grid, Hybrid Geometry Representation",,Occupancy,"Yes, geometry only",,,,,,,"Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian Theobalt, Gerard Pons-Moll",bhatnagar2020ipnet,49,,"Implicit functions represented as deep learning approximations are powerful for reconstructing 3D surfaces. However, they can only produce static surfaces that are not controllable, which provides limited ability to modify the resulting model by editing its pose or shape parameters. Nevertheless, such features are essential in building flexible models for both computer graphics and computer vision. In this work, we present methodology that combines detail-rich implicit functions and parametric representations in order to reconstruct 3D models of people that remain controllable and accurate even in the presence of clothing. Given sparse 3D point clouds sampled on the surface of a dressed person, we use an Implicit Part Network (IP-Net)to jointly predict the outer 3D surface of the dressed person, the and inner body surface, and the semantic correspondences to a parametric body model. We subsequently use correspondences to fit the body model to our inner surface and then non-rigidly deform it (under a parametric body + displacement model) to the outer surface in order to capture garment, face and hair detail. In quantitative and qualitative experiments with both full body data and hand scans we show that the proposed methodology generalizes, and is effective even given incomplete point clouds collected from single-view depth images. Our models and code can be downloaded from http://virtualhumans.mpi-inf.mpg.de/ipnet.",,Yes,Direct,,,ECCV 2020,,,,,,
10/2/2021 20:25,,,Coupling Explicit and Implicit Surface Representations for Generative 3D Modeling,HybridNet,ECCV,7/20/2020,2020,"@inproceedings{poursaeed2020hybridnet,
    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
    author = {Omid Poursaeed and Matthew Fisher and Noam Aigerman and Vladimir G. Kim},
    title = {Coupling Explicit and Implicit Surface Representations for Generative 3D Modeling},
    year = {2020},
    url = {http://arxiv.org/abs/2007.10294v2},
    entrytype = {inproceedings},
    id = {poursaeed2020hybridnet}
}",https://arxiv.org/pdf/2007.10294.pdf,https://omidpoursaeed.github.io/publication/hybrid/,,,https://drive.google.com/file/d/1wwnp6HlDdfYw19__ESxWdTQfmc8Bf--v/view,https://omidpoursaeed.github.io/pdf/HybridNet_Supp.pdf,,"Surface Reconstruction, Global Conditioning, Hybrid Geometry Representation",,"Occupancy, Atlas",No,,,,,,,"Omid Poursaeed, Matthew Fisher, Noam Aigerman, Vladimir G. Kim",poursaeed2020hybridnet,48,,"We propose a novel neural architecture for representing 3D surfaces, which harnesses two complementary shape representations: (i) an explicit representation via an atlas, i.e., embeddings of 2D domains into 3D; (ii) an implicit-function representation, i.e., a scalar function over the 3D volume, with its levels denoting surfaces. We make these two representations synergistic by introducing novel consistency losses that ensure that the surface created from the atlas aligns with the level-set of the implicit function. Our hybrid architecture outputs results which are superior to the output of the two equivalent single-representation networks, yielding smoother explicit surfaces with more accurate normals, and a more accurate implicit occupancy function. Additionally, our surface reconstruction step can directly leverage the explicit atlas-based representation. This process is computationally efficient, and can be directly used by differentiable rasterizers, enabling training our hybrid representation with image-based losses.",No,,Direct,,,ECCV 2020,,,,,,
5/23/2021 19:12,,,GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis,GRAF,NeurIPS,7/5/2020,2020,"@inproceedings{schwarz2020graf,
    publisher = {Curran Associates, Inc.},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    author = {Katja Schwarz and Yiyi Liao and Michael Niemeyer and Andreas Geiger},
    title = {GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis},
    year = {2020},
    url = {http://arxiv.org/abs/2007.02442v4},
    entrytype = {inproceedings},
    id = {schwarz2020graf}
}",https://arxiv.org/pdf/2007.02442.pdf,,https://github.com/autonomousvision/graf,,https://www.youtube.com/watch?v=akQf7WaCOHo,,,"Generalization, Generative Models, Data-Driven Method, Global Conditioning, Local Conditioning",,,,,,,,,,"Katja Schwarz, Yiyi Liao, Michael Niemeyer, Andreas Geiger",schwarz2020graf,47,61,"While 2D generative adversarial networks have enabled high-resolution image synthesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper, we propose a generative model for radiance fields which have recently proven successful for novel view synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high fidelity.",,,,,,NeurIPS 2020,,,,,,
6/29/2021 16:22,,,Neural Splines: Fitting 3D Surfaces with Infinitely-Wide Neural Networks,Neural Splines,CVPR,6/24/2020,2021,"@inproceedings{williams2021neuralsplines,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Francis Williams and Matthew Trager and Joan Bruna and Denis Zorin},
    title = {Neural Splines: Fitting 3D Surfaces with Infinitely-Wide Neural Networks},
    year = {2021},
    url = {http://arxiv.org/abs/2006.13782v3},
    entrytype = {inproceedings},
    id = {williams2021neuralsplines}
}",https://arxiv.org/pdf/2006.13782.pdf,,https://github.com/fwilliams/neural-splines,,,,,Fundamentals,None,SDF,"Yes, geometry only",,,,,,,"Francis Williams, Matthew Trager, Joan Bruna, Denis Zorin",williams2021neuralsplines,46,2,"We present Neural Splines, a technique for 3D surface reconstruction that is based on random feature kernels arising from infinitely-wide shallow ReLU networks. Our method achieves state-of-the-art results, outperforming recent neural network-based techniques and widely used Poisson Surface Reconstruction (which, as we demonstrate, can also be viewed as a type of kernel method). Because our approach is based on a simple kernel formulation, it is easy to analyze and can be accelerated by general techniques designed for kernel-based learning. We provide explicit analytical expressions for our kernel and argue that our formulation can be seen as a generalization of cubic spline interpolation to higher dimensions. In particular, the RKHS norm associated with Neural Splines biases toward smooth interpolants.",,,,,,CVPR 2021,,,,,,
5/23/2021 19:11,,,Deep Reflectance Volumes,,ECCV,6/20/2020,2020,"@inproceedings{bi2020deep,
    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
    author = {Sai Bi and Zexiang Xu and Kalyan Sunkavalli and Milos Hasan and Yannick Hold-Geoffroy and David Kriegman and Ravi Ramamoorthi},
    title = {Deep Reflectance Volumes: Relightable Reconstructions from Multi-View Photometric Images},
    year = {2020},
    url = {http://arxiv.org/abs/2007.09892v1},
    entrytype = {inproceedings},
    id = {bi2020deep}
}",https://arxiv.org/pdf/2007.09892.pdf,,,,https://drive.google.com/file/d/1JEbeIrIttznaowJJcBGZD56KR22j635q/view,https://drive.google.com/file/d/12IAg73kWtGtvKp2RNeaJ8WrmlsTehV4U/view,,Material/Lighting Estimation,,,,,,,,,,"Sai Bi, Zexiang Xu, Kalyan Sunkavalli, Miloš Hašan, Yannick Hold-Geoffroy, David Kriegman, Ravi Ramamoorthi",bi2020deep,45,18,"We present a deep learning approach to reconstruct scene appearance from unstructured images captured under collocated point lighting. At the heart of Deep Reflectance Volumes is a novel volumetric scene representation consisting of opacity, surface normal and reflectance voxel grids. We present a novel physically-based differentiable volume ray marching framework to render these scene volumes under arbitrary viewpoint and lighting. This allows us to optimize the scene volumes to minimize the error between their rendered images and the captured images. Our method is able to reconstruct real scenes with challenging non-Lambertian reflectance and complex geometry with occlusions and shadowing. Moreover, it accurately generalizes to novel viewpoints and lighting, including non-collocated lighting, rendering photorealistic images that are significantly better than state-of-the-art mesh-based methods. We also show that our learned reflectance volumes are editable, allowing for modifying the materials of the captured scenes.",,,,,,ECCV 2020,,,,,,
6/23/2021 13:23,,,Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains,FFN,NeurIPS,6/18/2020,2020,"@inproceedings{tancik2020ffn,
    publisher = {Curran Associates, Inc.},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    author = {Matthew Tancik and Pratul P. Srinivasan and Ben Mildenhall and Sara Fridovich-Keil and Nithin Raghavan and Utkarsh Singhal and Ravi Ramamoorthi and Jonathan T. Barron and Ren Ng},
    title = {Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains},
    year = {2020},
    url = {http://arxiv.org/abs/2006.10739v1},
    entrytype = {inproceedings},
    id = {tancik2020ffn}
}",https://arxiv.org/pdf/2006.10739.pdf,https://bmild.github.io/fourfeat/,https://github.com/tancik/fourier-feature-networks,,"https://www.youtube.com/watch?v=iKyIJ_EtSkw, https://www.youtube.com/watch?v=h0SXP6lJxak",,,Fundamentals,Fourier Feature (NeRF),,,,,,,,,"Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, Ren Ng",tancik2020ffn,44,135,"We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.",,,,,,NeurIPS 2020,,,,,,
6/23/2021 13:23,,,Implicit Neural Representations with Periodic Activation Functions,SIREN,NeurIPS,6/17/2020,2020,"@inproceedings{sitzmann2020siren,
    publisher = {Curran Associates, Inc.},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    author = {Vincent Sitzmann and Julien N. P. Martel and Alexander W. Bergman and David B. Lindell and Gordon Wetzstein},
    title = {Implicit Neural Representations with Periodic Activation Functions},
    year = {2020},
    url = {http://arxiv.org/abs/2006.09661v1},
    entrytype = {inproceedings},
    id = {sitzmann2020siren}
}",https://arxiv.org/pdf/2006.09661.pdf,https://vsitzmann.github.io/siren/,https://github.com/vsitzmann/siren,https://drive.google.com/drive/u/1/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K,,,,"Generalization, Fundamentals, Audio, Supervision by Gradient (PDE), Hypernetwork/Meta-learning",Sinusoidal Activation (SIREN),,,,,,,,,"Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, Gordon Wetzstein",sitzmann2020siren,43,185,"Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with Hypernetwork/Meta-learnings to learn priors over the space of Siren functions.",,,,,,NeurIPS 2020,,,,,,
7/19/2021 17:57,,,MetaSDF: Meta-learning Signed Distance Functions,MetaSDF,NeurIPS,6/17/2020,2020,"@inproceedings{sitzmann2020metasdf,
    publisher = {Curran Associates, Inc.},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    author = {Vincent Sitzmann and Eric R. Chan and Richard Tucker and Noah Snavely and Gordon Wetzstein},
    title = {MetaSDF: Meta-learning Signed Distance Functions},
    year = {2020},
    url = {http://arxiv.org/abs/2006.09662v1},
    entrytype = {inproceedings},
    id = {sitzmann2020metasdf}
}",https://arxiv.org/pdf/2006.09662.pdf,https://vsitzmann.github.io/metasdf/,https://github.com/vsitzmann/metasdf,,,,,"Generalization, Data-Driven Method, Hypernetwork/Meta-learning",,,,,,,,,,"Vincent Sitzmann, Eric R. Chan, Richard Tucker, Noah Snavely, Gordon Wetzstein",sitzmann2020metasdf,42,25,"Neural implicit shape representations are an emerging paradigm that offers many potential benefits over conventional discrete representations, including memory efficiency at a high spatial resolution. Generalizing across shapes with such neural implicit representations amounts to learning priors over the respective function space and enables geometry reconstruction from partial or noisy observations. Existing generalization methods rely on conditioning a neural network on a low-dimensional latent code that is either regressed by an encoder or jointly optimized in the auto-decoder framework. Here, we formalize learning of a shape space as a meta-learning problem and leverage gradient-based meta-learning algorithms to solve this task. We demonstrate that this approach performs on par with auto-decoder based approaches while being an order of magnitude faster at test-time inference. We further demonstrate that the proposed gradient-based method outperforms encoder-decoder based methods that leverage pooling-based set encoders.",,,,,,NeurIPS 2020,,,,,,
9/17/2021 14:03,,,Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction,Geo-PIFu,NeurIPS,6/15/2020,2020,"@inproceedings{he2020geopifu,
    publisher = {Curran Associates, Inc.},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    author = {Tong He and John Collomosse and Hailin Jin and Stefano Soatto},
    title = {Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view Human Reconstruction},
    year = {2020},
    url = {http://arxiv.org/abs/2006.08072v2},
    entrytype = {inproceedings},
    id = {he2020geopifu}
}",https://arxiv.org/pdf/2006.08072.pdf,,https://github.com/simpleig/Geo-PIFu,,,,,"Human (Body), Voxel Grid, Local Conditioning",,Occupancy,"Yes, geometry only",,,,,,,"Tong He, John Collomosse, Hailin Jin, Stefano Soatto",he2020geopifu,41,,"We propose Geo-PIFu, a method to recover a 3D mesh from a monocular color image of a clothed person. Our method is based on a deep implicit function-based representation to learn latent voxel features using a structure-aware 3D U-Net, to constrain the model in two ways: first, to resolve feature ambiguities in query point encoding, second, to serve as a coarse human shape proxy to regularize the high-resolution mesh and encourage global shape regularity. We show that, by both encoding query points and constraining global shape using latent voxel features, the reconstruction we obtain for clothed human meshes exhibits less shape distortion and improved surface details compared to competing methods. We evaluate Geo-PIFu on a recent human mesh public dataset that is $10 \times$ larger than the private commercial dataset used in PIFu and previous derivative work. On average, we exceed the state of the art by $42.7\%$ reduction in Chamfer and Point-to-Surface Distances, and $19.4\%$ reduction in normal estimation errors.",,Yes,,,,NeurIPS 2020,,,,,,
8/29/2021 20:52,,,MeshfreeFlowNet: A Physics-Constrained Deep Continuous Space-Time Super-Resolution Framework,MeshfreeFlowNet,"SC20: International Conference for High Performance Computing, Networking, Storage and Analysis",5/1/2020,2020,"@article{jiang2020meshfreeflownet,
    author = {Chiyu Max Jiang and Soheil Esmaeilzadeh and Kamyar Azizzadenesheli and Karthik Kashinath and Mustafa Mustafa and Hamdi A. Tchelepi and Philip Marcus and Prabhat and Anima Anandkumar},
    title = {MeshfreeFlowNet: A Physics-Constrained Deep Continuous Space-Time Super-Resolution Framework},
    year = {2020},
    month = {May},
    url = {http://arxiv.org/abs/2005.01463v2},
    entrytype = {article},
    id = {jiang2020meshfreeflownet}
}",https://arxiv.org/pdf/2005.01463.pdf,http://www.maxjiang.ml/proj/meshfreeflownet,https://github.com/maxjiang93/space_time_pde,,"https://www.youtube.com/watch?v=mjqwPch9gDo, https://www.youtube.com/watch?v=anZ_gLrvnYs&t=538s",,,"Science & Engineering, Supervision by Gradient (PDE)",,,,,,,,,,"Chiyu Max Jiang, Soheil Esmaeilzadeh, Kamyar Azizzadenesheli, Karthik Kashinath, Mustafa Mustafa, Hamdi A. Tchelepi, Philip Marcus, Prabhat, Anima Anandkumar",jiang2020meshfreeflownet,40,11,"We propose MeshfreeFlowNet, a novel deep learning-based super-resolution framework to generate continuous (grid-free) spatio-temporal solutions from the low-resolution inputs. While being computationally efficient, MeshfreeFlowNet accurately recovers the fine-scale quantities of interest. MeshfreeFlowNet allows for: (i) the output to be sampled at all spatio-temporal resolutions, (ii) a set of Partial Differential Equation (PDE) constraints to be imposed, and (iii) training on fixed-size inputs on arbitrarily sized spatio-temporal domains owing to its fully convolutional encoder. We empirically study the performance of MeshfreeFlowNet on the task of super-resolution of turbulent flows in the Rayleigh-Benard convection problem. Across a diverse set of evaluation metrics, we show that MeshfreeFlowNet significantly outperforms existing baselines. Furthermore, we provide a large scale implementation of MeshfreeFlowNet and show that it efficiently scales across large clusters, achieving 96.80% scaling efficiency on up to 128 GPUs and a training time of less than 4 minutes.",,,,,,"SC20: International Conference for High Performance Computing, Networking, Storage and Analysis 2020",,,,,,
9/17/2021 14:27,,,ARCH: Animatable Reconstruction of Clothed Humans,ARCH,CVPR,4/8/2020,2020,"@inproceedings{huang2020arch,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Zeng Huang and Yuanlu Xu and Christoph Lassner and Hao Li and Tony Tung},
    title = {ARCH: Animatable Reconstruction of Clothed Humans},
    year = {2020},
    url = {http://arxiv.org/abs/2004.04572v2},
    entrytype = {inproceedings},
    id = {huang2020arch}
}",https://arxiv.org/pdf/2004.04572.pdf,https://vgl.ict.usc.edu/Research/ARCH/,,,https://www.youtube.com/watch?v=DG3QNMcmTvo,,,"Human (Body), Data-Driven Method, Local Conditioning, Voxel Grid",None,Occupancy,No,,,,,,,"Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, Tony Tung",huang2020arch,39,,"In this paper, we propose ARCH (Animatable Reconstruction of Clothed Humans), a novel end-to-end framework for accurate reconstruction of animation-ready 3D clothed humans from a monocular image. Existing approaches to digitize 3D humans struggle to handle pose variations and recover details. Also, they do not produce models that are animation ready. In contrast, ARCH is a learned pose-aware model that produces detailed 3D rigged full-body human avatars from a single unconstrained RGB image. A Semantic Space and a Semantic Deformation Field are created using a parametric 3D body estimator. They allow the transformation of 2D/3D clothed humans into a canonical space, reducing ambiguities in geometry caused by pose variations and occlusions in training data. Detailed surface geometry and appearance are learned using an implicit function representation with spatial local features. Furthermore, we propose additional per-pixel supervision on the 3D reconstruction using opacity-aware differentiable rendering. Our experiments indicate that ARCH increases the fidelity of the reconstructed humans. We obtain more than 50% lower reconstruction errors for standard metrics compared to state-of-the-art methods on public datasets. We also show numerous qualitative examples of animated, high-quality reconstructed avatars unseen in the literature so far.",,,Direct,,,CVPR 2020,,,,,,
8/29/2021 16:20,,,DualSDF: Semantic Shape Manipulation using a Two-Level Representation,DualSDF,CVPR,4/6/2020,2020,"@inproceedings{hao2020dualsdf,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Zekun Hao and Hadar Averbuch-Elor and Noah Snavely and Serge Belongie},
    title = {DualSDF: Semantic Shape Manipulation using a Two-Level Representation},
    year = {2020},
    url = {http://arxiv.org/abs/2004.02869v1},
    entrytype = {inproceedings},
    id = {hao2020dualsdf}
}",https://arxiv.org/pdf/2004.02869.pdf,https://www.cs.cornell.edu/~hadarelor/dualsdf/,https://github.com/zekunhao1995/DualSDF,,https://www.youtube.com/watch?v=pAszEMLd5Xk,,https://www.youtube.com/watch?v=u40ZwDINz0A,"Editable, Data-Driven Method, Global Conditioning, Hybrid Geometry Representation",,SDF,"Yes, geometry only",,,,,,,"Zekun Hao, Hadar Averbuch-Elor, Noah Snavely, Serge Belongie",hao2020dualsdf,38,21,"We are seeing a Cambrian explosion of 3D shape representations for use in machine learning. Some representations seek high expressive power in capturing high-resolution detail. Other approaches seek to represent shapes as compositions of simple parts, which are intuitive for people to understand and easy to edit and manipulate. However, it is difficult to achieve both fidelity and interpretability in the same representation. We propose DualSDF, a representation expressing shapes at two levels of granularity, one capturing fine details and the other representing an abstracted proxy shape using simple and semantically consistent shape primitives. To achieve a tight coupling between the two representations, we use a variational objective over a shared latent space. Our two-level model gives rise to a new shape manipulation technique in which a user can interactively manipulate the coarse proxy shape and see the changes instantly mirrored in the high-resolution shape. Moreover, our model actively augments and guides the manipulation towards producing semantically meaningful shapes, making complex manipulations possible with minimal user input.",,,Direct,,,CVPR 2020,,,,,,
9/17/2021 13:51,,,Robust 3D Self-portraits in Seconds,PIFusion,CVPR,4/6/2020,2020,"@inproceedings{li2020pifusion,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Zhe Li and Tao Yu and Chuanyu Pan and Zerong Zheng and Yebin Liu},
    title = {Robust 3D Self-portraits in Seconds},
    year = {2020},
    url = {http://arxiv.org/abs/2004.02460v1},
    entrytype = {inproceedings},
    id = {li2020pifusion}
}",https://arxiv.org/pdf/2004.02460.pdf,http://www.liuyebin.com/portrait/portrait.html,,,https://www.youtube.com/watch?v=tayZT0exfVA,,http://www.liuyebin.com/portrait/assets/portrait.mp4,"Human (Body), Local Conditioning",,,No,,,,,,,"Zhe Li, Tao Yu, Chuanyu Pan, Zerong Zheng, Yebin Liu",li2020pifusion,37,,"In this paper, we propose an efficient method for robust 3D self-portraits using a single RGBD camera. Benefiting from the proposed PIFusion and lightweight bundle adjustment algorithm, our method can generate detailed 3D self-portraits in seconds and shows the ability to handle subjects wearing extremely loose clothes. To achieve highly efficient and robust reconstruction, we propose PIFusion, which combines learning-based 3D recovery with volumetric non-rigid fusion to generate accurate sparse partial scans of the subject. Moreover, a non-rigid volumetric deformation method is proposed to continuously refine the learned shape prior. Finally, a lightweight bundle adjustment algorithm is proposed to guarantee that all the partial scans can not only ""loop"" with each other but also remain consistent with the selected live key observations. The results and experiments show that the proposed method achieves more robust and efficient 3D self-portraits compared with state-of-the-art methods.",,Yes,,,,CVPR 2020,,,,,,
6/29/2021 16:42,,,PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization,PIFuHD,CVPR,4/1/2020,2020,"@inproceedings{saito2020pifuhd,
    url = {http://arxiv.org/abs/2004.00452v1},
    year = {2020},
    title = {PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization},
    author = {Shunsuke Saito and Tomas Simon and Jason Saragih and Hanbyul Joo},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    entrytype = {inproceedings},
    id = {saito2020pifuhd}
}",https://arxiv.org/pdf/2004.00452.pdf,https://shunsukesaito.github.io/PIFuHD/,https://github.com/facebookresearch/pifuhd,,"https://www.youtube.com/watch?v=uEDqCxvF5yc, https://www.youtube.com/watch?v=-1XYTmm8HhE",,,"Human (Body), Sparse Reconstruction, Generalization, Image-Based Rendering, Data-Driven Method, Coarse-to-Fine, Local Conditioning",None,Occupancy,No,,,,,,,"Shunsuke Saito, Tomas Simon, Jason Saragih, Hanbyul Joo",saito2020pifuhd,36,115,"Recent advances in image-based 3D human shape estimation have been driven by the significant improvement in representation power afforded by deep neural networks. Although current approaches have demonstrated the potential in real world settings, they still fail to produce reconstructions with the level of detail often present in the input images. We argue that this limitation stems primarily form two conflicting requirements; accurate predictions require large context, but precise predictions require high resolution. Due to memory limitations in current hardware, previous approaches tend to take low resolution images as input to cover large spatial context, and produce less precise (or low resolution) 3D estimates as a result. We address this limitation by formulating a multi-level architecture that is end-to-end trainable. A coarse level observes the whole image at lower resolution and focuses on holistic reasoning. This provides context to an fine level which estimates highly detailed geometry by observing higher-resolution images. We demonstrate that our approach significantly outperforms existing state-of-the-art techniques on single image human shape reconstruction by fully leveraging 1k-resolution input images.",,,,,,CVPR 2020,,,,,,
5/23/2021 19:16,,,Semantic Implicit Neural Scene Representations With Semi-Supervised Training,,3DV,3/28/2020,2020,"@inproceedings{kohli2020semantic,
    url = {http://arxiv.org/abs/2003.12673v2},
    year = {2020},
    title = {Semantic Implicit Neural Scene Representations With Semi-Supervised Training},
    author = {Amit Kohli and Vincent Sitzmann and Gordon Wetzstein},
    booktitle = {International Conference on 3D Vision (3DV)},
    organization = {IEEE},
    entrytype = {inproceedings},
    id = {kohli2020semantic}
}",https://arxiv.org/pdf/2003.12673.pdf,http://www.computationalimaging.org/publications/semantic-srn/,,,https://www.youtube.com/watch?v=iVubC_ymE5w,,,"Generative Models, Global Conditioning",,,No,,,,,,,"Amit Kohli, Vincent Sitzmann, Gordon Wetzstein",kohli2020semantic,35,7,"The recent success of implicit neural scene representations has presented a viable new method for how we capture and store 3D scenes. Unlike conventional 3D representations, such as point clouds, which explicitly store scene properties in discrete, localized units, these implicit representations encode a scene in the weights of a neural network which can be queried at any coordinate to produce these same scene properties. Thus far, implicit representations have primarily been optimized to estimate only the appearance and/or 3D geometry information in a scene. We take the next step and demonstrate that an existing implicit representation (SRNs) is actually multi-modal; it can be further leveraged to perform per-point semantic segmentation while retaining its ability to represent appearance and geometry. To achieve this multi-modal behavior, we utilize a semi-supervised learning strategy atop the existing pre-trained scene representation. Our method is simple, general, and only requires a few tens of labeled 2D segmentation masks in order to achieve dense 3D semantic segmentation. We explore two novel applications for this semantically aware implicit neural scene representation: 3D novel view and semantic label synthesis given only a single input RGB image or 2D label mask, as well as 3D interpolation of appearance and semantics.",,,,,,3DV 2020,,,,,,
9/18/2021 9:39,,,EikoNet: Solving the Eikonal equation with Deep Neural Networks,EikoNet,IEEE Transactions on Geoscience and Remote Sensing,3/25/2020,2020,"@article{smith2020eikonet,
    author = {Jonathan D. Smith and Kamyar Azizzadenesheli and Zachary E. Ross},
    title = {EikoNet: Solving the Eikonal equation with Deep Neural Networks},
    year = {2020},
    month = {Mar},
    url = {http://arxiv.org/abs/2004.00361v3},
    entrytype = {article},
    id = {smith2020eikonet}
}",https://arxiv.org/pdf/2004.00361.pdf,,https://github.com/Ulvetanna/EikoNet,,,,,"Science & Engineering, Supervision by Gradient (PDE)",,,,,,,,,,"Jonathan D. Smith, Kamyar Azizzadenesheli, Zachary E. Ross",smith2020eikonet,34,,"The recent deep learning revolution has created an enormous opportunity for accelerating compute capabilities in the context of physics-based simulations. Here, we propose EikoNet, a deep learning approach to solving the Eikonal equation, which characterizes the first-arrival-time field in heterogeneous 3D velocity structures. Our grid-free approach allows for rapid determination of the travel time between any two points within a continuous 3D domain. These travel time solutions are allowed to violate the differential equation - which casts the problem as one of optimization - with the goal of finding network parameters that minimize the degree to which the equation is violated. In doing so, the method exploits the differentiability of neural networks to calculate the spatial gradients analytically, meaning the network can be trained on its own without ever needing solutions from a finite difference algorithm. EikoNet is rigorously tested on several velocity models and sampling methods to demonstrate robustness and versatility. Training and inference are highly parallelized, making the approach well-suited for GPUs. EikoNet has low memory overhead, and further avoids the need for travel-time lookup tables. The developed approach has important applications to earthquake hypocenter inversion, ray multi-pathing, and tomographic modeling, as well as to other fields beyond seismology where ray tracing is essential.",,,Direct,,,IEEE Transactions on Geoscience and Remote Sensing 2020,,,,,,
6/29/2021 16:58,,,Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction,DeepLS,ECCV,3/24/2020,2020,"@inproceedings{chabra2020deepls,
    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
    author = {Rohan Chabra and Jan Eric Lenssen and Eddy Ilg and Tanner Schmidt and Julian Straub and Steven Lovegrove and Richard Newcombe},
    title = {Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction},
    year = {2020},
    url = {http://arxiv.org/abs/2003.10983v3},
    entrytype = {inproceedings},
    id = {chabra2020deepls}
}",https://arxiv.org/pdf/2003.10983.pdf,,,,,,,"Generalization, Voxel Grid, Local Conditioning",None,SDF,"Yes, geometry only",,,,,,,"Rohan Chabra, Jan Eric Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove, Richard Newcombe",chabra2020deepls,33,61,"Efficiently reconstructing complex and intricate surfaces at scale is a long-standing goal in machine perception. To address this problem we introduce Deep Local Shapes (DeepLS), a deep shape representation that enables encoding and reconstruction of high-quality 3D shapes without prohibitive memory requirements. DeepLS replaces the dense volumetric signed distance function (SDF) representation used in traditional surface reconstruction systems with a set of locally learned continuous SDFs defined by a neural network, inspired by recent work such as DeepSDF. Unlike DeepSDF, which represents an object-level SDF with a neural network and a single latent code, we store a grid of independent latent codes, each responsible for storing information about surfaces in a small local neighborhood. This decomposition of scenes into local shapes simplifies the prior distribution that the network must learn, and also enables efficient inference. We demonstrate the effectiveness and generalization power of DeepLS by showing object shape encoding and reconstructions of full scenes, where DeepLS delivers high compression, accuracy, and local shape completion.",,,,,,ECCV 2020,,,,,,
5/23/2021 19:12,,,Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance,IDR,NeurIPS,3/22/2020,2020,"@inproceedings{yariv2020idr,
    url = {http://arxiv.org/abs/2003.09852v3},
    year = {2020},
    title = {Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance},
    author = {Lior Yariv and Yoni Kasten and Dror Moran and Meirav Galun and Matan Atzmon and Ronen Basri and Yaron Lipman},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    publisher = {Curran Associates, Inc.},
    entrytype = {inproceedings},
    id = {yariv2020idr}
}",https://arxiv.org/pdf/2003.09852.pdf,,https://github.com/lioryariv/idr,,,,,Material/Lighting Estimation,,SDF,No,,,,,,,"Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Ronen Basri, Yaron Lipman",yariv2020idr,32,52,"In this work we address the challenging problem of multiview 3D surface reconstruction. We introduce a neural network architecture that simultaneously learns the unknown geometry, camera parameters, and a neural renderer that approximates the light reflected from the surface towards the camera. The geometry is represented as a zero level-set of a neural network, while the neural renderer, derived from the rendering equation, is capable of (implicitly) modeling a wide set of lighting conditions and materials. We trained our network on real world 2D images of objects with different material properties, lighting conditions, and noisy camera initializations from the DTU MVS dataset. We found our model to produce state of the art 3D surface reconstructions with high fidelity, resolution and detail.",,,,,,NeurIPS 2020,,,,,,
5/23/2021 19:13,,,NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis,NeRF,ECCV,3/19/2020,2020,"@inproceedings{mildenhall2020nerf,
    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
    author = {Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
    title = {NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
    year = {2020},
    url = {http://arxiv.org/abs/2003.08934v2},
    entrytype = {inproceedings},
    id = {mildenhall2020nerf}
}",https://arxiv.org/pdf/2003.08934.pdf,https://www.matthewtancik.com/nerf,https://github.com/bmild/nerf,https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1,https://www.youtube.com/watch?v=JuH79E8rdKc,,,Sampling,,,No,,,,,,,"Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng",mildenhall2020nerf,31,366,"We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\theta, \phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.",,,,,,ECCV 2020,,,,,,
5/23/2021 19:21,,,Convolutional Occupancy Networks,,ECCV,3/10/2020,2020,"@inproceedings{peng2020convolutional,
    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
    author = {Songyou Peng and Michael Niemeyer and Lars Mescheder and Marc Pollefeys and Andreas Geiger},
    title = {Convolutional Occupancy Networks},
    year = {2020},
    url = {http://arxiv.org/abs/2003.04618v2},
    entrytype = {inproceedings},
    id = {peng2020convolutional}
}",https://arxiv.org/pdf/2003.04618.pdf,https://pengsongyou.github.io/conv_onet,https://github.com/autonomousvision/convolutional_occupancy_networks,,"https://www.youtube.com/watch?v=k0monzIcjUo, https://www.youtube.com/watch?v=EmauovgrDSM",http://www.cvlibs.net/publications/Peng2020ECCV_supplementary.pdf,,,,Occupancy,"Yes, geometry only",,,,,,,"Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, Andreas Geiger",peng2020convolutional,30,99,"Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fully-connected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases, enabling structured reasoning in 3D space. We investigate the effectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically find that our method enables the fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data.",,,,,,ECCV 2020,,,,,,
7/19/2021 21:08,,,Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion,IF-Net,CVPR,3/3/2020,2020,"@inproceedings{chibane2020ifnet,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Julian Chibane and Thiemo Alldieck and Gerard Pons-Moll},
    title = {Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion},
    year = {2020},
    url = {http://arxiv.org/abs/2003.01456v2},
    entrytype = {inproceedings},
    id = {chibane2020ifnet}
}",https://arxiv.org/pdf/2003.01456.pdf,https://virtualhumans.mpi-inf.mpg.de/ifnets/,https://github.com/jchibane/if-net,,https://www.youtube.com/watch?v=cko07jINRZg,http://virtualhumans.mpi-inf.mpg.de/papers/chibane20ifnet/chibane20ifnet_supp.pdf,,"Generalization, Data-Driven Method",None,Occupancy,"Yes, geometry only",,,,,,,"Julian Chibane, Thiemo Alldieck, Gerard Pons-Moll",chibane2020ifnet,29,86,"While many works focus on 3D reconstruction from images, in this paper, we focus on 3D shape reconstruction and completion from a variety of 3D inputs, which are deficient in some respect: low and high resolution voxels, sparse and dense point clouds, complete or incomplete. Processing of such 3D inputs is an increasingly important problem as they are the output of 3D scanners, which are becoming more accessible, and are the intermediate output of 3D computer vision algorithms. Recently, learned implicit functions have shown great promise as they produce continuous reconstructions. However, we identified two limitations in reconstruction from 3D inputs: 1) details present in the input data are not retained, and 2) poor reconstruction of articulated humans. To solve this, we propose Implicit Feature Networks (IF-Nets), which deliver continuous outputs, can handle multiple topologies, and complete shapes for missing or sparse input data retaining the nice properties of recent learned implicit functions, but critically they can also retain detail when it is present in the input data, and can reconstruct articulated humans. Our work differs from prior work in two crucial aspects. First, instead of using a single vector to encode a 3D shape, we extract a learnable 3-dimensional multi-scale tensor of deep features, which is aligned with the original Euclidean space embedding the shape. Second, instead of classifying x-y-z point coordinates directly, we classify deep features extracted from the tensor at a continuous query point. We show that this forces our model to make decisions based on global and local shape structure, as opposed to point coordinates, which are arbitrary under Euclidean transformations. Experiments demonstrate that IF-Nets clearly outperform prior work in 3D object reconstruction in ShapeNet, and obtain significantly more accurate 3D human reconstructions.",,Yes,,,,CVPR 2020,,,,,,
6/29/2021 16:32,,,Implicit Geometric Regularization for Learning Shapes,IGR,CVPR,2/24/2020,2020,"@inproceedings{gropp2020igr,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Amos Gropp and Lior Yariv and Niv Haim and Matan Atzmon and Yaron Lipman},
    title = {Implicit Geometric Regularization for Learning Shapes},
    year = {2020},
    url = {http://arxiv.org/abs/2002.10099v2},
    entrytype = {inproceedings},
    id = {gropp2020igr}
}",https://arxiv.org/pdf/2002.10099.pdf,,https://github.com/amosgropp/IGR,,https://www.youtube.com/watch?v=6cOvBGBQF9g,,,"Human (Body), Generalization, Fundamentals",None,SDF,"Yes, geometry only",,,,,,,"Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, Yaron Lipman",gropp2020igr,28,65,"Representing shapes as level sets of neural networks has been recently proved to be useful for different shape analysis and reconstruction tasks. So far, such representations were computed using either: (i) pre-computed implicit shape representations; or (ii) loss functions explicitly defined over the neural level sets. In this paper we offer a new paradigm for computing high fidelity implicit neural representations directly from raw data (i.e., point clouds, with or without normal information). We observe that a rather simple loss function, encouraging the neural network to vanish on the input point cloud and to have a unit norm gradient, possesses an implicit geometric regularization property that favors smooth and natural zero level set surfaces, avoiding bad zero-loss solutions. We provide a theoretical analysis of this property for the linear case, and show that, in practice, our method leads to state of the art implicit neural representations with higher level-of-details and fidelity compared to previous methods.",,,,,,CVPR 2020,,,,,,
7/7/2021 17:49,,,Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision,DVR,CVPR,12/16/2019,2020,"@inproceedings{niemeyer2020dvr,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Michael Niemeyer and Lars Mescheder and Michael Oechsle and Andreas Geiger},
    title = {Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision},
    year = {2020},
    url = {http://arxiv.org/abs/1912.07372v2},
    entrytype = {inproceedings},
    id = {niemeyer2020dvr}
}",https://arxiv.org/pdf/1912.07372.pdf,https://www.youtube.com/watch?v=U_jIN3qWVEw,https://github.com/autonomousvision/differentiable_volumetric_rendering,,https://www.youtube.com/watch?v=U_jIN3qWVEw,http://www.cvlibs.net/publications/Niemeyer2020CVPR_supplementary.pdf,https://www.youtube.com/watch?v=lcub1KH-mmk,Global Conditioning,,Occupancy,No,,,,,,,"Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger",niemeyer2020dvr,27,142,"Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.",,,,,,CVPR 2020,,,,,,
6/29/2021 16:55,,,Local Deep Implicit Functions for 3D Shape,LDIF,CVPR,12/12/2019,2020,"@inproceedings{genova2020ldif,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Kyle Genova and Forrester Cole and Avneesh Sud and Aaron Sarna and Thomas Funkhouser},
    title = {Local Deep Implicit Functions for 3D Shape},
    year = {2020},
    url = {http://arxiv.org/abs/1912.06126v2},
    entrytype = {inproceedings},
    id = {genova2020ldif}
}",https://arxiv.org/pdf/1912.06126.pdf,https://ldif.cs.princeton.edu/,https://github.com/google/ldif,,https://www.youtube.com/watch?v=3RAITzNWVJs,,,"Human (Body), Data-Driven Method, Hybrid Geometry Representation",None,Occupancy,"Yes, geometry only",,,,,,,"Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, Thomas Funkhouser",genova2020ldif,26,70,"The goal of this project is to learn a 3D shape representation that enables accurate surface reconstruction, compact storage, efficient computation, consistency for similar shapes, generalization across diverse shape categories, and inference from depth camera observations. Towards this end, we introduce Local Deep Implicit Functions (LDIF), a 3D shape representation that decomposes space into a structured set of learned implicit functions. We provide networks that infer the space decomposition and local deep implicit functions from a 3D mesh or posed depth image. During experiments, we find that it provides 10.3 points higher surface reconstruction accuracy (F-Score) than the state-of-the-art (OccNet), while requiring fewer than 1 percent of the network parameters. Experiments on posed depth image completion and generalization to unseen classes show 15.8 and 17.8 point improvements over the state-of-the-art, while producing a structured 3D representation for each input with consistency across diverse shape collections.",,,,,,CVPR 2020,,,,,,
10/8/2021 16:47,,,Learning a Neural 3D Texture Space from 2D Exemplars,Neural Texture,CVPR,12/9/2019,2020,"@inproceedings{henzler2020neuraltexture,
    url = {http://arxiv.org/abs/1912.04158v2},
    year = {2020},
    title = {Learning a Neural 3D Texture Space from 2D Exemplars},
    author = {Philipp Henzler and Niloy J. Mitra and Tobias Ritschel},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    entrytype = {inproceedings},
    id = {henzler2020neuraltexture}
}",https://arxiv.org/pdf/1912.04158.pdf,https://geometry.cs.ucl.ac.uk/group_website/projects/2020/neuraltexture/,https://github.com/henzler/neuraltexture,,https://www.youtube.com/watch?v=it5y2qaONBE,,,2D Image Neural Fields,,,,,,,,,,"Philipp Henzler, Niloy J. Mitra, Tobias Ritschel",henzler2020neuraltexture,25,,"We propose a generative model of 2D and 3D natural textures with diversity, visual fidelity and at high computational efficiency. This is enabled by a family of methods that extend ideas from classic stochastic procedural texturing (Perlin noise) to learned, deep, non-linearities. The key idea is a hard-coded, tunable and differentiable step that feeds multiple transformed random 2D or 3D fields into an MLP that can be sampled over infinite domains. Our model encodes all exemplars from a diverse set of textures without a need to be re-trained for each exemplar. Applications include texture interpolation, and learning 3D textures from 2D exemplars.",,,,,,CVPR 2020,,,,,,
9/17/2021 11:55,,,NASA: Neural Articulated Shape Approximation,NASA,ECCV,12/6/2019,2020,"@inproceedings{deng2020nasa,
    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
    author = {Boyang Deng and JP Lewis and Timothy Jeruzalski and Gerard Pons-Moll and Geoffrey Hinton and Mohammad Norouzi and Andrea Tagliasacchi},
    title = {NASA: Neural Articulated Shape Approximation},
    year = {2020},
    url = {http://arxiv.org/abs/1912.03207v4},
    entrytype = {inproceedings},
    id = {deng2020nasa}
}",https://arxiv.org/pdf/1912.03207.pdf,,,,,,,Human (Body),,Occupancy,"Yes, geometry only",,,,,,,"Boyang Deng, JP Lewis, Timothy Jeruzalski, Gerard Pons-Moll, Geoffrey Hinton, Mohammad Norouzi, Andrea Tagliasacchi",deng2020nasa,24,,"Efficient representation of articulated objects such as human bodies is an important problem in computer vision and graphics. To efficiently simulate deformation, existing approaches represent 3D objects using polygonal meshes and deform them using skinning techniques. This paper introduces neural articulated shape approximation (NASA), an alternative framework that enables efficient representation of articulated deformable objects using neural indicator functions that are conditioned on pose. Occupancy testing using NASA is straightforward, circumventing the complexity of meshes and the issue of water-tightness. We demonstrate the effectiveness of NASA for 3D tracking applications, and discuss other potential extensions.",,,Direct,,,ECCV 2020,,,,,,
8/29/2021 20:20,,,DIST: Rendering Deep Implicit Signed Distance Function with Differentiable Sphere Tracing,DIST,CVPR,11/29/2019,2020,"@inproceedings{liu2020dist,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Shaohui Liu and Yinda Zhang and Songyou Peng and Boxin Shi and Marc Pollefeys and Zhaopeng Cui},
    title = {DIST: Rendering Deep Implicit Signed Distance Function with Differentiable Sphere Tracing},
    year = {2020},
    url = {http://arxiv.org/abs/1911.13225v2},
    entrytype = {inproceedings},
    id = {liu2020dist}
}",https://arxiv.org/pdf/1911.13225.pdf,http://b1ueber2y.me/projects/DIST-Renderer/,https://github.com/B1ueber2y/DIST-Renderer,,https://www.youtube.com/watch?v=KjfNS1mnqoM,http://b1ueber2y.me/projects/DIST-Renderer/dist-supp.pdf,,"Fundamentals, Data-Driven Method",None,SDF,"Yes, geometry only",,,,,,,"Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc Pollefeys, Zhaopeng Cui",liu2020dist,23,68,"We propose a differentiable sphere tracing algorithm to bridge the gap between inverse graphics methods and the recently proposed deep learning based implicit signed distance function. Due to the nature of the implicit function, the rendering process requires tremendous function queries, which is particularly problematic when the function is represented as a neural network. We optimize both the forward and backward passes of our rendering layer to make it run efficiently with affordable memory consumption on a commodity graphics card. Our rendering method is fully differentiable such that losses can be directly computed on the rendered 2D observations, and the gradients can be propagated backwards to optimize the 3D geometry. We show that our rendering method can effectively reconstruct accurate 3D shapes from various inputs, such as sparse depth and multi-view images, through inverse optimization. With the geometry based reasoning, our 3D shape prediction methods show excellent generalization capability and robustness against various noises.",,,Direct,,,CVPR 2020,,,,,,
6/29/2021 16:19,,,SAL: Sign Agnostic Learning of Shapes from Raw Data,SAL,CVPR,11/23/2019,2020,"@inproceedings{atzmon2020sal,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Matan Atzmon and Yaron Lipman},
    title = {SAL: Sign Agnostic Learning of Shapes from Raw Data},
    year = {2020},
    url = {http://arxiv.org/abs/1911.10414v2},
    entrytype = {inproceedings},
    id = {atzmon2020sal}
}",https://arxiv.org/pdf/1911.10414.pdf,,https://github.com/matanatz/SAL,,,,,,,SDF,"Yes, geometry only",,,,,,,"Matan Atzmon, Yaron Lipman",atzmon2020sal,22,68,"Recently, neural networks have been used as implicit representations for surface reconstruction, modelling, learning, and generation. So far, training neural networks to be implicit representations of surfaces required training data sampled from a ground-truth signed implicit functions such as signed distance or occupancy functions, which are notoriously hard to compute. In this paper we introduce Sign Agnostic Learning (SAL), a deep learning approach for learning implicit shape representations directly from raw, unsigned geometric data, such as point clouds and triangle soups. We have tested SAL on the challenging problem of surface reconstruction from an un-oriented point cloud, as well as end-to-end human shape space learning directly from raw scans dataset, and achieved state of the art reconstructions compared to current approaches. We believe SAL opens the door to many geometric deep learning applications with real-world data, alleviating the usual painstaking, often manual pre-process.",,,,,,CVPR 2020,,,,,,
7/19/2021 21:31,,,Learning to Infer Implicit Surfaces without 3D Supervision,,NeurIPS,11/2/2019,2019,"@inproceedings{liu2019learning,
    publisher = {Curran Associates, Inc.},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    author = {Shichen Liu and Shunsuke Saito and Weikai Chen and Hao Li},
    title = {Learning to Infer Implicit Surfaces without 3D Supervision},
    year = {2019},
    url = {http://arxiv.org/abs/1911.00767v1},
    entrytype = {inproceedings},
    id = {liu2019learning}
}",https://arxiv.org/pdf/1911.00767.pdf,,,,,,,"Fundamentals, Sampling, Hybrid Geometry Representation",None,Occupancy,"Yes, geometry only",,,,,,,"Shichen Liu, Shunsuke Saito, Weikai Chen, Hao Li",liu2019learning,21,69,"Recent advances in 3D deep learning have shown that it is possible to train highly effective deep models for 3D shape generation, directly from 2D images. This is particularly interesting since the availability of 3D models is still limited compared to the massive amount of accessible 2D images, which is invaluable for training. The representation of 3D surfaces itself is a key factor for the quality and resolution of the 3D output. While explicit representations, such as point clouds and voxels, can span a wide range of shape variations, their resolutions are often limited. Mesh-based representations are more efficient but are limited by their ability to handle varying topologies. Implicit surfaces, however, can robustly handle complex shapes, topologies, and also provide flexible resolution control. We address the fundamental problem of learning implicit surfaces for shape inference without the need of 3D supervision. Despite their advantages, it remains nontrivial to (1) formulate a differentiable connection between implicit surfaces and their 2D renderings, which is needed for image-based supervision; and (2) ensure precise geometric properties and control, such as local smoothness. In particular, sampling implicit surfaces densely is also known to be a computationally demanding and very slow operation. To this end, we propose a novel ray-based field probing technique for efficient image-to-field supervision, as well as a general geometric regularizer for implicit surfaces, which provides natural shape priors in unconstrained regions. We demonstrate the effectiveness of our framework on the task of single-view image-based 3D shape digitization and show how we outperform state-of-the-art techniques both quantitatively and qualitatively.",,,Direct,,,NeurIPS 2019,,,,,,
8/29/2021 21:39,,,Occupancy Flow: 4D Reconstruction by Learning Particle Dynamics,Occupancy Flow,ICCV,10/1/2019,2019,"@inproceedings{niemeyer2019occupancyflow,
    title = {Occupancy flow: 4d reconstruction by learning particle dynamics},
    author = {Michael Niemeyer and Lars Mescheder and Michael Oechsle and Andreas Geiger},
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    pages = {5379--5389},
    year = {2019},
    entrytype = {inproceedings},
    id = {niemeyer2019occupancyflow}
}",https://openaccess.thecvf.com/content_ICCV_2019/papers/Niemeyer_Occupancy_Flow_4D_Reconstruction_by_Learning_Particle_Dynamics_ICCV_2019_paper.pdf,https://avg.is.tuebingen.mpg.de/publications/niemeyer2019iccv,https://github.com/autonomousvision/occupancy_flow,,https://www.youtube.com/watch?v=c0yOugTgrWc,http://www.cvlibs.net/publications/Niemeyer2019ICCV_supplementary.pdf,,"Dynamic/Temporal, Human (Body)",None,Occupancy,"Yes, geometry only",,,,,,,"Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger",niemeyer2019occupancyflow,20,65,"Deep learning based 3D reconstruction techniques have recently achieved impressive results. However, while stateof-the-art methods are able to output complex 3D geometry, it is not clear how to extend these results to time-varying topologies. Approaches treating each time step individually lack continuity and exhibit slow inference, while traditional 4D reconstruction methods often utilize a template model or discretize the 4D space at fixed resolution. In this work, we present Occupancy Flow, a novel spatio-temporal representation of time-varying 3D geometry with implicit correspondences. Towards this goal, we learn a temporally and spatially continuous vector field which assigns a motion vector to every point in space and time. In order to perform dense 4D reconstruction from images or sparse point clouds, we combine our method with a continuous 3D representation. Implicitly, our model yields correspondences over time, thus enabling fast inference while providing a sound physical description of the temporal dynamics. We show that our method can be used for interpolation and reconstruction tasks, and demonstrate the accuracy of the learned correspondences. We believe that Occupancy Flow is a promising new 4D representation which will be useful for a variety of spatio-temporal reconstruction tasks.",,,Direct,,,ICCV 2019,,,,,,
9/19/2021 15:43,,,Reconstructing continuous distributions of 3D protein structure from cryo-EM images,cryoDRGN,ICLR,9/11/2019,2020,"@inproceedings{zhong2020cryodrgn,
    booktitle = {International Conference on Learning Representations},
    author = {Ellen D. Zhong and Tristan Bepler and Joseph H. Davis and Bonnie Berger},
    title = {Reconstructing continuous distributions of 3D protein structure from cryo-EM images},
    year = {2020},
    url = {http://arxiv.org/abs/1909.05215v3},
    entrytype = {inproceedings},
    id = {zhong2020cryodrgn}
}",https://arxiv.org/pdf/1909.05215.pdf,http://cb.csail.mit.edu/cb/cryodrgn/,https://github.com/zhonge/cryodrgn,,https://www.youtube.com/watch?v=zd6YcUyDhPE,,,"Alternative Imaging, Science & Engineering, Data-Driven Method, Global Conditioning, Local Conditioning",Fourier Feature (NeRF),Electron density,,,,,,,,"Ellen D. Zhong, Tristan Bepler, Joseph H. Davis, Bonnie Berger",zhong2020cryodrgn,19,,"Cryo-electron microscopy (cryo-EM) is a powerful technique for determining the structure of proteins and other macromolecular complexes at near-atomic resolution. In single particle cryo-EM, the central problem is to reconstruct the three-dimensional structure of a macromolecule from $10^{4-7}$ noisy and randomly oriented two-dimensional projections. However, the imaged protein complexes may exhibit structural variability, which complicates reconstruction and is typically addressed using discrete clustering approaches that fail to capture the full range of protein dynamics. Here, we introduce a novel method for cryo-EM reconstruction that extends naturally to modeling continuous generative factors of structural heterogeneity. This method encodes structures in Fourier space using coordinate-based deep neural networks, and trains these networks from unlabeled 2D cryo-EM images by combining exact inference over image orientation with variational inference for structural heterogeneity. We demonstrate that the proposed method, termed cryoDRGN, can perform ab initio reconstruction of 3D protein complexes from simulated and real 2D cryo-EM image data. To our knowledge, cryoDRGN is the first neural network-based approach for cryo-EM reconstruction and the first end-to-end method for directly reconstructing continuous ensembles of protein structures from cryo-EM images.",No,,,,,ICLR 2020,,,,,,
10/8/2021 14:17,,,Deep Meta Functionals for Shape Representation,,ICCV,8/17/2019,2019,"@inproceedings{littwin2019deep,
    url = {http://arxiv.org/abs/1908.06277v1},
    year = {2019},
    title = {Deep Meta Functionals for Shape Representation},
    author = {Gidi Littwin and Lior Wolf},
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    entrytype = {inproceedings},
    id = {littwin2019deep}
}",https://arxiv.org/pdf/1908.06277.pdf,,https://github.com/gidilittwin/Deep-Meta,,,,,Hypernetwork/Meta-learning,,Occupancy,"Yes, geometry only",,,,,,,"Gidi Littwin, Lior Wolf",littwin2019deep,18,,"We present a new method for 3D shape reconstruction from a single image, in which a deep neural network directly maps an image to a vector of network weights. The network \textcolor{black}{parametrized by} these weights represents a 3D shape by classifying every point in the volume as either within or outside the shape. The new representation has virtually unlimited capacity and resolution, and can have an arbitrary topology. Our experiments show that it leads to more accurate shape inference from a 2D projection than the existing methods, including voxel-, silhouette-, and mesh-based methods. The code is available at: https://github.com/gidilittwin/Deep-Meta",No,No,,,,ICCV 2019,,,,,,
7/19/2021 21:58,,,Learning elementary structures for 3D shape generation and matching,,ICCV,8/13/2019,2019,"@inproceedings{deprelle2019learning,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Theo Deprelle and Thibault Groueix and Matthew Fisher and Vladimir G. Kim and Bryan C. Russell and Mathieu Aubry},
    title = {Learning elementary structures for 3D shape generation and matching},
    year = {2019},
    url = {http://arxiv.org/abs/1908.04725v2},
    entrytype = {inproceedings},
    id = {deprelle2019learning}
}",https://arxiv.org/pdf/1908.04725.pdf,,,,,,,"Surface Reconstruction, Global Conditioning",None,Atlas,"Yes, geometry only",,,,,,,"Theo Deprelle, Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry",deprelle2019learning,17,61,"We propose to represent shapes as the deformation and combination of learnable elementary 3D structures, which are primitives resulting from training over a collection of shape. We demonstrate that the learned elementary 3D structures lead to clear improvements in 3D shape generation and matching. More precisely, we present two complementary approaches for learning elementary structures: (i) patch deformation learning and (ii) point translation learning. Both approaches can be extended to abstract structures of higher dimensions for improved results. We evaluate our method on two tasks: reconstructing ShapeNet objects and estimating dense correspondences between human scans (FAUST inter challenge). We show 16% improvement over surface deformation approaches for shape reconstruction and outperform FAUST inter challenge state of the art by 6%.",,,Direct,,,ICCV 2019,,,,,,
5/23/2021 19:19,,,Neural Volumes: Learning Dynamic Renderable Volumes from Images,NV,SIGGRAPH,6/18/2019,2019,"@article{lombardi2019nv,
    publisher = {Association for Computing Machinery},
    journal = {ACM Transactions on Graphics (TOG)},
    author = {Stephen Lombardi and Tomas Simon and Jason Saragih and Gabriel Schwartz and Andreas Lehrmann and Yaser Sheikh},
    title = {Neural Volumes: Learning Dynamic Renderable Volumes from Images},
    doi = {10.1145/3306346.3323020},
    year = {2019},
    url = {http://arxiv.org/abs/1906.07751v1},
    entrytype = {article},
    id = {lombardi2019nv}
}",https://arxiv.org/pdf/1906.07751.pdf,https://stephenlombardi.github.io/projects/neuralvolumes/,https://github.com/facebookresearch/neuralvolumes,,"https://youtu.be/JlyGNvbGKB8?t=5347, https://crossminds.ai/video/neural-volumes-learning-dynamic-renderable-volumes-from-images-606f94d175292b321dd0906f/",,,"Dynamic/Temporal, Local Conditioning, Voxel Grid, Global Conditioning, Hybrid Geometry Representation",,,No,,,,,,,"Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, Yaser Sheikh",lombardi2019nv,16,161,"Modeling and rendering of dynamic scenes is challenging, as natural scenes often contain complex phenomena such as thin structures, evolving topology, translucency, scattering, occlusion, and biological motion. Mesh-based reconstruction and tracking often fail in these cases, and other approaches (e.g., light field video) typically rely on constrained viewing conditions, which limit interactivity. We circumvent these difficulties by presenting a learning-based approach to representing dynamic objects inspired by the integral projection model used in tomographic imaging. The approach is supervised directly from 2D images in a multi-view capture setting and does not require explicit reconstruction or tracking of the object. Our method has two primary components: an encoder-decoder network that transforms input images into a 3D volume representation, and a differentiable ray-marching operation that enables end-to-end training. By virtue of its 3D representation, our construction extrapolates better to novel viewpoints compared to screen-space rendering techniques. The encoder-decoder architecture learns a latent representation of a dynamic scene that enables us to produce novel content sequences not seen during training. To overcome memory limitations of voxel-based representations, we learn a dynamic irregular grid structure implemented with a warp field during ray-marching. This structure greatly improves the apparent resolution and reduces grid-like artifacts and jagged motion. Finally, we demonstrate how to incorporate surface-based representations into our volumetric-learning framework for applications where the highest resolution is required, using facial performance capture as a case in point.",,,,,,SIGGRAPH 2019,,,,,,
5/23/2021 19:15,,,Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations,SRN,NeurIPS,6/4/2019,2019,"@inproceedings{sitzmann2019srn,
    url = {http://arxiv.org/abs/1906.01618v2},
    year = {2019},
    title = {Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations},
    author = {Vincent Sitzmann and Michael Zollhofer and Gordon Wetzstein},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    publisher = {Curran Associates, Inc.},
    entrytype = {inproceedings},
    id = {sitzmann2019srn}
}",https://arxiv.org/pdf/1906.01618.pdf,https://vsitzmann.github.io/srns/,https://github.com/vsitzmann/scene-representation-networks,https://drive.google.com/drive/folders/1OkYgeRcIcLOFu1ft5mRODWNQaPJ0ps90,"https://www.youtube.com/watch?v=6vMEBWD8O20, https://slideslive.com/38922305/scene-representation-networks-continuous-3dstructureaware-neural-scene-representations",,,"Generalization, Global Conditioning, Hypernetwork/Meta-learning",,,No,,,,,,,"Vincent Sitzmann, Michael Zollhöfer, Gordon Wetzstein",sitzmann2020srn,15,262,"Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3D-structure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model.",,,,,,NeurIPS 2019,,,,,,
8/29/2021 17:38,,,Controlling Neural Level Sets,,NeurIPS,5/28/2019,2019,"@inproceedings{atzmon2019controlling,
    publisher = {Curran Associates, Inc.},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    author = {Matan Atzmon and Niv Haim and Lior Yariv and Ofer Israelov and Haggai Maron and Yaron Lipman},
    title = {Controlling Neural Level Sets},
    year = {2019},
    url = {http://arxiv.org/abs/1905.11911v2},
    entrytype = {inproceedings},
    id = {atzmon2019controlling}
}",https://arxiv.org/pdf/1905.11911.pdf,https://github.com/matanatz/ControllingNeuralLevelsets,,http://faust.is.tue.mpg.de/,,,,"Generalization, Fundamentals, Sampling",,,"Yes, geometry only",,,,,,,"Matan Atzmon, Niv Haim, Lior Yariv, Ofer Israelov, Haggai Maron, Yaron Lipman",atzmon2019controlling,14,30,"The level sets of neural networks represent fundamental properties such as decision boundaries of classifiers and are used to model non-linear manifold data such as curves and surfaces. Thus, methods for controlling the neural level sets could find many applications in machine learning. In this paper we present a simple and scalable approach to directly control level sets of a deep neural network. Our method consists of two parts: (i) sampling of the neural level sets, and (ii) relating the samples' positions to the network parameters. The latter is achieved by a sample network that is constructed by adding a single fixed linear layer to the original network. In turn, the sample network can be used to incorporate the level set samples into a loss function of interest. We have tested our method on three different learning tasks: improving generalization to unseen data, training networks robust to adversarial attacks, and curve and surface reconstruction from point clouds. For surface reconstruction, we produce high fidelity surfaces directly from raw 3D point clouds. When training small to medium networks to be robust to adversarial attacks we obtain robust accuracy comparable to state-of-the-art methods.",,,Direct,,,NeurIPS 2019,,,,,,
10/8/2021 14:20,,,DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction,DISN,NeurIPS,5/26/2019,2019,"@inproceedings{xu2019disn,
    url = {http://arxiv.org/abs/1905.10711v2},
    year = {2019},
    title = {DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction},
    author = {Qiangeng Xu and Weiyue Wang and Duygu Ceylan and Radomir Mech and Ulrich Neumann},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    publisher = {Curran Associates, Inc.},
    entrytype = {inproceedings},
    id = {xu2019disn}
}",https://arxiv.org/pdf/1905.10711.pdf,,https://github.com/laughtervv/DISN,,,,,Local Conditioning,,SDF,"Yes, geometry only",,,,,,,"Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, Ulrich Neumann",xu2019disn,13,,"Reconstructing 3D shapes from single-view images has been a long-standing research problem. In this paper, we present DISN, a Deep Implicit Surface Network which can generate a high-quality detail-rich 3D mesh from an 2D image by predicting the underlying signed distance fields. In addition to utilizing global image features, DISN predicts the projected location for each 3D point on the 2D image, and extracts local features from the image feature maps. Combining global and local features significantly improves the accuracy of the signed distance field prediction, especially for the detail-rich areas. To the best of our knowledge, DISN is the first method that constantly captures details such as holes and thin structures present in 3D shapes from single-view images. DISN achieves the state-of-the-art single-view reconstruction performance on a variety of shape categories reconstructed from both synthetic and real images. Code is available at https://github.com/xharlie/DISN The supplementary can be found at https://xharlie.github.io/images/neurips_2019_supp.pdf",,,,,,NeurIPS 2019,,,,,,
8/29/2021 21:45,,,Texture Fields: Learning Texture Representations in Function Space,Texture Fields,ICCV,5/17/2019,2019,"@inproceedings{oechsle2019texturefields,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Michael Oechsle and Lars Mescheder and Michael Niemeyer and Thilo Strauss and Andreas Geiger},
    title = {Texture Fields: Learning Texture Representations in Function Space},
    year = {2019},
    url = {http://arxiv.org/abs/1905.07259v1},
    entrytype = {inproceedings},
    id = {oechsle2019texturefields}
}",https://arxiv.org/pdf/1905.07259.pdf,https://autonomousvision.github.io/texture-fields/,https://github.com/autonomousvision/texture_fields,,https://www.youtube.com/watch?v=pbfeE0qmD2E,http://www.cvlibs.net/publications/Oechsle2019ICCV_supplementary.pdf,,"Generalization, Generative Models, Data-Driven Method, Global Conditioning",None,,,,,,,,,"Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, Andreas Geiger",oechsle2019texturefields,12,77,"In recent years, substantial progress has been achieved in learning-based reconstruction of 3D objects. At the same time, generative models were proposed that can generate highly realistic images. However, despite this success in these closely related tasks, texture reconstruction of 3D objects has received little attention from the research community and state-of-the-art methods are either limited to comparably low resolution or constrained experimental setups. A major reason for these limitations is that common representations of texture are inefficient or hard to interface for modern deep learning techniques. In this paper, we propose Texture Fields, a novel texture representation which is based on regressing a continuous 3D function parameterized with a neural network. Our approach circumvents limiting factors like shape discretization and parameterization, as the proposed texture representation is independent of the shape representation of the 3D object. We show that Texture Fields are able to represent high frequency texture and naturally blend with modern deep learning techniques. Experimentally, we find that Texture Fields compare favorably to state-of-the-art methods for conditional texture reconstruction of 3D objects and enable learning of probabilistic generative models for texturing unseen 3D models. We believe that Texture Fields will become an important building block for the next generation of generative 3D models.",,,Direct,,,ICCV 2019,,,,,,
6/29/2021 16:44,,,PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization,PIFu,ICCV,5/13/2019,2019,"@inproceedings{saito2019pifu,
    booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
    author = {Shunsuke Saito and Zeng Huang and Ryota Natsume and Shigeo Morishima and Angjoo Kanazawa and Hao Li},
    title = {PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization},
    year = {2019},
    url = {http://arxiv.org/abs/1905.05172v3},
    entrytype = {inproceedings},
    id = {saito2019pifu}
}",https://arxiv.org/pdf/1905.05172.pdf,https://shunsukesaito.github.io/PIFu/,https://github.com/shunsukesaito/PIFu,,https://www.youtube.com/watch?v=S1FpjwKqtPs,,,"Human (Body), Sparse Reconstruction, Generalization, Image-Based Rendering, Data-Driven Method, Local Conditioning",None,Occupancy,No,,,,,,,"Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, Hao Li",saito2019pifu,11,295,"We introduce Pixel-aligned Implicit Function (PIFu), a highly effective implicit representation that locally aligns pixels of 2D images with the global context of their corresponding 3D object. Using PIFu, we propose an end-to-end deep learning method for digitizing highly detailed clothed humans that can infer both 3D surface and texture from a single image, and optionally, multiple input images. Highly intricate shapes, such as hairstyles, clothing, as well as their variations and deformations can be digitized in a unified way. Compared to existing representations used for 3D deep learning, PIFu can produce high-resolution surfaces including largely unseen regions such as the back of a person. In particular, it is memory efficient unlike the voxel representation, can handle arbitrary topology, and the resulting surface is spatially aligned with the input image. Furthermore, while previous techniques are designed to process either a single image or multiple views, PIFu extends naturally to arbitrary number of views. We demonstrate high-resolution and robust reconstructions on real world images from the DeepFashion dataset, which contains a variety of challenging clothing types. Our method achieves state-of-the-art performance on a public benchmark and outperforms the prior work for clothed human digitization from a single image.",,,,,,ICCV 2019,,,,,,
10/4/2021 22:06,,,SurRF: Unsupervised Multi-view Stereopsis by Learning Surface Radiance Field,SurRF,ARXIV,5/7/2019,2021,"@article{khot2021surrf,
    url = {http://arxiv.org/abs/1905.02706v2},
    year = {2021},
    title = {Learning Unsupervised Multi-View Stereopsis via Robust Photometric Consistency},
    author = {Tejas Khot and Shubham Agrawal and Shubham Tulsiani and Christoph Mertz and Simon Lucey and Martial Hebert},
    booktitle = {ArXiv Pre-print},
    journal = {arXiv preprint arXiv:1905.02706},
    entrytype = {article},
    id = {khot2021surrf}
}",https://arxiv.org/pdf/1905.02706.pdf,https://tejaskhot.github.io/unsup_mvs/,https://github.com/tejaskhot/unsup_mvs,,,,,Hybrid Geometry Representation,,Other,No,,,,,,,"Tejas Khot, Shubham Agrawal, Shubham Tulsiani, Christoph Mertz, Simon Lucey, Martial Hebert",khot2021surrf,10,,"We present a learning based approach for multi-view stereopsis (MVS). While current deep MVS methods achieve impressive results, they crucially rely on ground-truth 3D training data, and acquisition of such precise 3D geometry for supervision is a major hurdle. Our framework instead leverages photometric consistency between multiple views as supervisory signal for learning depth prediction in a wide baseline MVS setup. However, naively applying photo consistency constraints is undesirable due to occlusion and lighting changes across views. To overcome this, we propose a robust loss formulation that: a) enforces first order consistency and b) for each point, selectively enforces consistency with some views, thus implicitly handling occlusions. We demonstrate our ability to learn MVS without 3D supervision using a real dataset, and show that each component of our proposed robust loss results in a significant improvement. We qualitatively observe that our reconstructions are often more complete than the acquired ground truth, further showing the merits of this approach. Lastly, our learned model generalizes to novel settings, and our approach allows adaptation of existing CNNs to datasets without ground-truth 3D by unsupervised finetuning. Project webpage: https://tejaskhot.github.io/unsup_mvs",No,No,Direct,,,ARXIV 2021,,,,,,
5/23/2021 19:20,,,DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation,DeepSDF,CVPR,1/16/2019,2019,"@inproceedings{park2019deepsdf,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Jeong Joon Park and Peter Florence and Julian Straub and Richard Newcombe and Steven Lovegrove},
    title = {DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation},
    year = {2019},
    url = {http://arxiv.org/abs/1901.05103v1},
    entrytype = {inproceedings},
    id = {park2019deepsdf}
}",https://arxiv.org/pdf/1901.05103.pdf,,https://github.com/facebookresearch/DeepSDF,,,,,"Generalization, Generative Models",,SDF,"Yes, geometry only",,,,,,,"Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove",park2019deepsdf,9,593,"Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.",,,,,,CVPR 2019,,,,,,
5/23/2021 19:22,,,Occupancy Networks: Learning 3D Reconstruction in Function Space,Occupancy Networks,CVPR,12/10/2018,2019,"@inproceedings{mescheder2019occupancynetworks,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Lars Mescheder and Michael Oechsle and Michael Niemeyer and Sebastian Nowozin and Andreas Geiger},
    title = {Occupancy Networks: Learning 3D Reconstruction in Function Space},
    year = {2019},
    url = {http://arxiv.org/abs/1812.03828v2},
    entrytype = {inproceedings},
    id = {mescheder2019occupancynetworks}
}",https://arxiv.org/pdf/1812.03828.pdf,https://avg.is.tuebingen.mpg.de/publications/occupancy-networks,https://github.com/autonomousvision/occupancy_networks,,https://www.youtube.com/watch?v=w1Qo3bOiPaE,,,"Generalization, Sampling, Global Conditioning",,Occupancy,"Yes, geometry only",,,,,,,"Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, Andreas Geiger",mescheder2019occupancynetworks,8,540,"With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.",,,,,,CVPR 2019,,,,,,
6/29/2021 15:38,,,Learning Implicit Fields for Generative Shape Modeling,IM-NET,CVPR,12/6/2018,2019,"@inproceedings{chen2019imnet,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Zhiqin Chen and Hao Zhang},
    title = {Learning Implicit Fields for Generative Shape Modeling},
    year = {2019},
    url = {http://arxiv.org/abs/1812.02822v5},
    entrytype = {inproceedings},
    id = {chen2019imnet}
}",https://arxiv.org/pdf/1812.02822.pdf,https://www.sfu.ca/~zhiqinc/imgan/Readme.html,https://github.com/czq142857/implicit-decoder,,,,,"Generalization, Generative Models, Data-Driven Method, Global Conditioning",None,Occupancy,"Yes, geometry only",,,,,,,"Zhiqin Chen, Hao Zhang",chen2019imnet,7,324,"We advocate the use of implicit fields for learning generative models of shapes and introduce an implicit field decoder, called IM-NET, for shape generation, aimed at improving the visual quality of the generated shapes. An implicit field assigns a value to each point in 3D space, so that a shape can be extracted as an iso-surface. IM-NET is trained to perform this assignment by means of a binary classifier. Specifically, it takes a point coordinate, along with a feature vector encoding a shape, and outputs a value which indicates whether the point is outside the shape or not. By replacing conventional decoders by our implicit decoder for representation learning (via IM-AE) and shape generation (via IM-GAN), we demonstrate superior results for tasks such as generative shape modeling, interpolation, and single-view 3D reconstruction, particularly in terms of visual quality. Code and supplementary material are available at https://github.com/czq142857/implicit-decoder.",,,,,,CVPR 2019,,,,,,
6/29/2021 15:56,,,Deep Geometric Prior for Surface Reconstruction,,CVPR,11/27/2018,2019,"@inproceedings{williams2019deep,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Francis Williams and Teseo Schneider and Claudio Silva and Denis Zorin and Joan Bruna and Daniele Panozzo},
    title = {Deep Geometric Prior for Surface Reconstruction},
    year = {2019},
    url = {http://arxiv.org/abs/1811.10943v2},
    entrytype = {inproceedings},
    id = {williams2019deep}
}",https://arxiv.org/pdf/1811.10943.pdf,,https://github.com/fwilliams/deep-geometric-prior,,,,,,None,Atlas,"Yes, geometry only",,,,,,,"Francis Williams, Teseo Schneider, Claudio Silva, Denis Zorin, Joan Bruna, Daniele Panozzo",williams2019deep,6,64,"The reconstruction of a discrete surface from a point cloud is a fundamental geometry processing problem that has been studied for decades, with many methods developed. We propose the use of a deep neural network as a geometric prior for surface reconstruction. Specifically, we overfit a neural network representing a local chart parameterization to part of an input point cloud using the Wasserstein distance as a measure of approximation. By jointly fitting many such networks to overlapping parts of the point cloud, while enforcing a consistency condition, we compute a manifold atlas. By sampling this atlas, we can produce a dense reconstruction of the surface approximating the input cloud. The entire procedure does not require any training data or explicit regularization, yet, we show that it is able to perform remarkably well: not introducing typical overfitting artifacts, and approximating sharp features closely at the same time. We experimentally show that this geometric prior produces good results for both man-made objects containing sharp features and smoother organic objects, as well as noisy inputs. We compare our method with a number of well-known reconstruction methods on a standard surface reconstruction benchmark.",,,,,,CVPR 2019,,,,,,
9/17/2021 22:42,,,Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations,,Journal of Computational Physics,6/13/2018,2019,"@article{raissi2019physicsinformed,
    author = {Maziar Raissi and Paris Perdikaris and George E Karniadakis},
    journal = {Journal of Computational Physics},
    pages = {686--707},
    year = {2019},
    publisher = {Elsevier},
    title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
    volume = {378},
    entrytype = {article},
    id = {raissi2019physicsinformed}
}",https://www.sciencedirect.com/science/article/pii/S0021999118307125,https://maziarraissi.github.io/PINNs/,https://github.com/maziarraissi/PINNs,,,,,"Fundamentals, Science & Engineering, Supervision by Gradient (PDE)",,,,,,,,,,"Maziar Raissi, Paris Perdikaris, George E Karniadakis",raissi2019physicsinformed,5,,"We introduce physics-informed neural networks-neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and",,,,,,Journal of Computational Physics 2019,,,,,,
5/23/2021 19:23,,,AtlasNet: A Papier-Mâché Approach to Learning 3D Surface Generation,AtlasNet,CVPR,2/15/2018,2019,"@inproceedings{groueix2018atlasnet,
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Thibault Groueix and Matthew Fisher and Vladimir G. Kim and Bryan C. Russell and Mathieu Aubry},
    title = {AtlasNet: A Papier-Mache Approach to Learning 3D Surface Generation},
    year = {2018},
    url = {http://arxiv.org/abs/1802.05384v3},
    entrytype = {inproceedings},
    id = {groueix2018atlasnet}
}",https://arxiv.org/pdf/1802.05384.pdf,http://imagine.enpc.fr/~groueixt/atlasnet/,https://github.com/ThibaultGROUEIX/AtlasNet,,"http://imagine.enpc.fr/~groueixt/atlasnet/atlasnet_slides_spotlight_CVPR.pptx, http://imagine.enpc.fr/~groueixt/atlasnet/atlasnet_poster.pdf",,,"Sampling, Data-Driven Method, Global Conditioning",,,"Yes, geometry only",,,,,,,"Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry",groueix2018atlasnet,4,6,"We introduce a method for learning to generate the surface of 3D shapes. Our approach represents a 3D shape as a collection of parametric surface elements and, in contrast to methods generating voxel grids or point clouds, naturally infers a surface representation of the shape. Beyond its novelty, our new shape generation framework, AtlasNet, comes with significant advantages, such as improved precision and generalization capabilities, and the possibility to generate a shape of arbitrary resolution without memory issues. We demonstrate these benefits and compare to strong baselines on the ShapeNet benchmark for two applications: (i) auto-encoding shapes, and (ii) single-view reconstruction from a still image. We also provide results showing its potential for other applications, such as morphing, parametrization, super-resolution, matching, and co-segmentation.",,,,,,CVPR 2019,,,,,,
10/8/2021 16:45,,,Global Illumination with Radiance Regression Functions,,SIGGRAPH,7/1/2013,2018,"@article{ren2013global,
    author = {Peiran Ren and Jiaping Wang and Minmin Gong and Stephen Lin and Xin Tong and Baining Guo},
    title = {Global Illumination with Radiance Regression Functions},
    year = {2013},
    date = {July 2013},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {32},
    number = {4},
    issn = {0730-0301},
    url = {https://doi.org/10.1145/2461912.2462009},
    doi = {10.1145/2461912.2462009},
    journal = {ACM Trans. Graph.},
    month = {jul},
    articleno = {130},
    numpages = {12},
    keywords = {real time rendering, global illumination, neural network, non-linear regression},
    entrytype = {article},
    id = {ren2013global}
}",https://cseweb.ucsd.edu//~ravir/274/15/papers/a130-ren.pdf,,,,,,,Material/Lighting Estimation,,,,,,,,,,"Peiran Ren, Jiaping Wang, Minmin Gong, Stephen Lin, Xin Tong, Baining Guo",ren2013global,2,,"We present radiance regression functions for fast rendering of global illumination in scenes with dynamic local light sources. A radiance regression function (RRF) represents a non-linear mapping from local and contextual attributes of surface points, such as position, viewing direction, and lighting condition, to their indirect illumination values. The RRF is obtained from precomputed shading samples through regression analysis, which determines a function that best fits the shading data. For a given scene, the shading samples are precomputed by an offline renderer.
 The key idea behind our approach is to exploit the nonlinear coherence of the indirect illumination data to make the RRF both compact and fast to evaluate. We model the RRF as a multilayer acyclic feed-forward neural network, which provides a close functional approximation of the indirect illumination and can be efficiently evaluated at run time. To effectively model scenes with spatially variant material properties, we utilize an augmented set of attributes as input to the neural network RRF to reduce the amount of inference that the network needs to perform. To handle scenes with greater geometric complexity, we partition the input space of the RRF model and represent the subspaces with separate, smaller RRFs that can be evaluated more rapidly. As a result, the RRF model scales well to increasingly complex scene geometry and material variation. Because of its compactness and ease of evaluation, the RRF model enables real-time rendering with full global illumination effects, including changing caustics and multiple-bounce high-frequency glossy interreflections.",,,,,,SIGGRAPH 2018,,,,,,
6/29/2021 15:19,,,3D Object Reconstruction and Representation Using Neural Networks,,GRAPHITE,6/15/2004,2004,"@book{lim20043d,
    title = {3D Object Reconstruction and Representation Using Neural Networks},
    author = {Wen Peng Lim and Siti Mariyam Shamsuddin},
    year = {2004},
    publisher = {Universiti Teknologi Malaysia},
    booktitle = {International Conference on Computer Graphics and Interactive Techniques in Australasia and South East Asia},
    entrytype = {book},
    id = {lim20043d}
}",http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.6810&rep=rep1&type=pdf,,,,,,,Fundamentals,None,Other,"Yes, geometry only",,,,,,,"Lim Wen Peng, Siti Mariyam Shamsuddin",lim20043d,1,23,"3D object reconstruction is frequent used in various fields such as product design, engineering, medical and artistic applications. Numerous reconstruction techniques and software were introduced and developed. However, the purpose of this paper is to fully integrate an adaptive artificial neural network (ANN) based method in reconstructing and representing 3D objects. This study explores the ability of neural networks in learning through experience when reconstructing an object by estimating it’s z-coordinate. Neural networks’ capability in representing most classes of 3D objects used in computer graphics is also proven. Simple affined transformation is applied on different objects using this approach and compared with the real objects. The results show that neural network is a promising approach for reconstruction and representation of 3D objects.",,,,,,GRAPHITE 2004,,,,,,
8/29/2021 20:09,,,Approximating Reflectance Functions using Neural Networks,,EGSR,6/29/1998,1998,"@article{gargan1998approximating,
    publisher = {The Eurographics Association and John Wiley & Sons Ltd.},
    journal = {Computer Graphics Forum},
    title = {Approximating reflectance functions using neural networks},
    author = {David Gargan and Francis Neelamkavil},
    booktitle = {Eurographics Workshop on Rendering Techniques},
    pages = {23--34},
    year = {1998},
    organization = {Springer},
    entrytype = {article},
    id = {gargan1998approximating}
}",https://link.springer.com/chapter/10.1007/978-3-7091-6453-2_3,,,,,,,Material/Lighting Estimation,None,,,,,,,,,"David Gargan, Francis Neelamkavil",gargan1998approximating,0,11,"We present a new representation for the storage and reconstruction of arbitrary reflectance functions. This non-linear representation, based on a neural network model, accurately captures the spectral and spatial variation of these functions. It is both computationally efficient and concise, yet expressive. We reconstruct the subtle reflection characteristics of an analytic reflection model as well as measured and simulated reflection data",,,,,,EGSR 1998,,,,,,
