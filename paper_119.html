

<!DOCTYPE html>
<html lang="en">

  <head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />

    <!-- External Javascript libs_ext  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
      integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://code.jquery.com/jquery-3.6.0.min.js"
      integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
      integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
      integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
      integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>


    <!-- Library libs_ext -->
    <script src="static/js/libs_ext/typeahead.bundle.js"></script>

    <script src="static/js/tagEditor/jquery.caret.min.js"></script>
    <script src="static/js/tagEditor/jquery.tag-editor.min.js"></script>


    <!-- Internal Libs -->
    <script src="static/js/data/api.js"></script>

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
      integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link href="static/css/Lato.css" rel="stylesheet" />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link href="static/css/Cuprum.css" rel="stylesheet" />
    <link rel="stylesheet" href="static/css/main.css" />
    <!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
    <link rel="stylesheet" href="static/css/fa_solid.css" />
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />

    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/daterangepicker/daterangepicker.min.js"></script>
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/daterangepicker/daterangepicker.css" />

    <title>Neural Fields: Neural 3D Video Synthesis</title>

    <link rel="icon" type="image/png" href="favicon.ico">

    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-GT7LEVW06Z"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());

      gtag('config', 'G-GT7LEVW06Z');
    </script>
    <!-- head block ends here-->
    
<meta name="citation_title" content="Neural 3D Video Synthesis" />


<meta name="citation_author" content="Tianye Li" />

<meta name="citation_author" content="Mira Slavcheva" />

<meta name="citation_author" content="Michael Zollhoefer" />

<meta name="citation_author" content="Simon Green" />

<meta name="citation_author" content="Christoph Lassner" />

<meta name="citation_author" content="Changil Kim" />

<meta name="citation_author" content="Tanner Schmidt" />

<meta name="citation_author" content="Steven Lovegrove" />

<meta name="citation_author" content="Michael Goesele" />

<meta name="citation_author" content="Zhaoyang Lv" />


<meta name="citation_abstract" content="We propose a novel approach for 3D video synthesis that is able to represent multi-view video recordings of a dynamic real-world scene in a compact, yet expressive representation that enables high-quality view synthesis and motion interpolation. Our approach takes the high quality and compactness of static neural radiance fields in a new direction: to a model-free, dynamic setting. At the core of our approach is a novel time-conditioned neural radiance fields that represents scene dynamics using a set of compact latent codes. To exploit the fact that changes between adjacent frames of a video are typically small and locally consistent, we propose two novel strategies for efficient training of our neural network: 1) An efficient hierarchical training scheme, and 2) an importance sampling strategy that selects the next rays for training based on the temporal variation of the input videos. In combination, these two strategies significantly boost the training speed, lead to fast convergence of the training process, and enable high quality results. Our learned representation is highly compact and able to represent a 10 second 30 FPS multi-view video recording by 18 cameras with a model size of just 28MB. We demonstrate that our method can render high-fidelity wide-angle novel views at over 1K resolution, even for highly complex and dynamic scenes. We perform an extensive qualitative and quantitative evaluation that shows that our approach outperforms the current state of the art. We include additional video and information at: https://neural-3d-video.github.io/" />


<meta name="citation_keywords" content="Dynamic/Temporal" />

<meta name="citation_keywords" content="Global Conditioning" />

<meta name="citation_keywords" content="Local Conditioning" />

<meta name="citation_pdf_url" content="https://arxiv.org/pdf/2103.02597.pdf" />

<script type="text/javascript" src="static/js/vis/vis-network.min.js"></script>
<style>
    .red-hyper-link {
        color: #ED1C24 !important;
    }
</style>

  </head>

  <body>
    <!-- NAV -->
    
    <nav class="navbar sticky-top navbar-expand-lg navbar-light" style="background-color: #EFECE5;" id="main-nav">
      <div class="container">
        <a class="navbar-brand" href="index.html">
          <img class="logo" src="static/images/long_logo.png" style="width: 300px;"/>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
          aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="index.html" style="font-family: 'Ubuntu', sans-serif;">Home</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html" style="font-family: 'Ubuntu', sans-serif;">Advanced Search</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="add_paper.html" style="font-family: 'Ubuntu', sans-serif;">Add Paper</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="join_us.html" style="font-family: 'Ubuntu', sans-serif;">Join Us</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="faq.html" style="font-family: 'Ubuntu', sans-serif;">FAQ</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->

    
    <!-- top block ends here-->
    

    <div class="container">
      <!-- Tabs -->
      <div class="tabs">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3">
    <div class="card-header">
        <h2 class="card-title main-title text-center" style="color: black;">
            Neural 3D Video Synthesis
        </h2>
        <h3 class="card-subtitle mb-2 text-muted text-center">
            
            <a href="papers.html?author=Tianye Li" target="_blank"
                data-tippy-content="See all papers authored by Tianye Li"
                class="text-muted filterByAuthorLink">Tianye Li</a>,
            
            <a href="papers.html?author=Mira Slavcheva" target="_blank"
                data-tippy-content="See all papers authored by Mira Slavcheva"
                class="text-muted filterByAuthorLink">Mira Slavcheva</a>,
            
            <a href="papers.html?author=Michael Zollhoefer" target="_blank"
                data-tippy-content="See all papers authored by Michael Zollhoefer"
                class="text-muted filterByAuthorLink">Michael Zollhoefer</a>,
            
            <a href="papers.html?author=Simon Green" target="_blank"
                data-tippy-content="See all papers authored by Simon Green"
                class="text-muted filterByAuthorLink">Simon Green</a>,
            
            <a href="papers.html?author=Christoph Lassner" target="_blank"
                data-tippy-content="See all papers authored by Christoph Lassner"
                class="text-muted filterByAuthorLink">Christoph Lassner</a>,
            
            <a href="papers.html?author=Changil Kim" target="_blank"
                data-tippy-content="See all papers authored by Changil Kim"
                class="text-muted filterByAuthorLink">Changil Kim</a>,
            
            <a href="papers.html?author=Tanner Schmidt" target="_blank"
                data-tippy-content="See all papers authored by Tanner Schmidt"
                class="text-muted filterByAuthorLink">Tanner Schmidt</a>,
            
            <a href="papers.html?author=Steven Lovegrove" target="_blank"
                data-tippy-content="See all papers authored by Steven Lovegrove"
                class="text-muted filterByAuthorLink">Steven Lovegrove</a>,
            
            <a href="papers.html?author=Michael Goesele" target="_blank"
                data-tippy-content="See all papers authored by Michael Goesele"
                class="text-muted filterByAuthorLink">Michael Goesele</a>,
            
            <a href="papers.html?author=Zhaoyang Lv" target="_blank"
                data-tippy-content="See all papers authored by Zhaoyang Lv"
                class="text-muted filterByAuthorLink">Zhaoyang Lv</a>
            
        </h3>
        <h3 class="card-subtitle mb-2 text-muted text-center">
            3/3/2021
        </h3>
        <div id="citation-code-wrapper" class="text-muted text-center"
        >
        </div>
        <p class="text-center" style="margin-bottom: 0px;">
            <span>Keywords:</span>
            
            <a href="papers.html?keyword=Dynamic/Temporal" target="_blank"
                data-tippy-content="See all papers with keyword 'Dynamic/Temporal'"
                class="text-secondary text-decoration-none filterByKeywordLink">Dynamic/Temporal</a>,
            
            <a href="papers.html?keyword=Global Conditioning" target="_blank"
                data-tippy-content="See all papers with keyword 'Global Conditioning'"
                class="text-secondary text-decoration-none filterByKeywordLink">Global Conditioning</a>,
            
            <a href="papers.html?keyword=Local Conditioning" target="_blank"
                data-tippy-content="See all papers with keyword 'Local Conditioning'"
                class="text-secondary text-decoration-none filterByKeywordLink">Local Conditioning</a>
            
        </p>

        
        <p class="text-center" style="margin-bottom: 0px;">
            <span>Venue: </span>
            <a href="papers.html?venue=ARXIV" target="_blank" class="text-secondary text-decoration-none">ARXIV 2021</a>
        </p>
        

        <div class="text-center p-3">
            <a class="card-link red-hyper-link" target="_blank" href="https://arxiv.org/pdf/2103.02597.pdf">
                Paper
            </a>
            
            <a class="card-link red-hyper-link" data-toggle="collapse" role="button" href="#citation">
                Citation
            </a>
            
            
            
            <span class="card-link">Data coming soon...</span>
            
        </div>
    </div>
    <span id="invisible-paper-id" style="display: none;">119</span>
</div>
<div id="citation" class="pp-card m-3 collapse">
    <div class="card-body">
        <div class="card-text">
            <span style="font-size: large; font-weight: bold;">Bibtex:</span>
            <span style="white-space: pre-line; position: relative; left: 20px;">
                @article{li2021dynerf,
    journal = {arXiv preprint arXiv:2103.02597},
    booktitle = {ArXiv Pre-print},
    author = {Tianye Li and Mira Slavcheva and Michael Zollhoefer and Simon Green and Christoph Lassner and Changil Kim and Tanner Schmidt and Steven Lovegrove and Michael Goesele and Zhaoyang Lv},
    title = {Neural 3D Video Synthesis},
    year = {2021},
    url = {http://arxiv.org/abs/2103.02597v1},
    entrytype = {article},
    id = {li2021dynerf}
}
            </span>
        </div>
        <p></p>
    </div>
</div>
<div id="details" class="pp-card m-3">
    <div class="card-body">
        <div class="card-text">
            <div id="abstractExample">
                <p style="font-weight: bolder; font-size: 25px; text-align: center;">Abstract</p>
                We propose a novel approach for 3D video synthesis that is able to represent multi-view video recordings of a dynamic real-world scene in a compact, yet expressive representation that enables high-quality view synthesis and motion interpolation. Our approach takes the high quality and compactness of static neural radiance fields in a new direction: to a model-free, dynamic setting. At the core of our approach is a novel time-conditioned neural radiance fields that represents scene dynamics using a set of compact latent codes. To exploit the fact that changes between adjacent frames of a video are typically small and locally consistent, we propose two novel strategies for efficient training of our neural network: 1) An efficient hierarchical training scheme, and 2) an importance sampling strategy that selects the next rays for training based on the temporal variation of the input videos. In combination, these two strategies significantly boost the training speed, lead to fast convergence of the training process, and enable high quality results. Our learned representation is highly compact and able to represent a 10 second 30 FPS multi-view video recording by 18 cameras with a model size of just 28MB. We demonstrate that our method can render high-fidelity wide-angle novel views at over 1K resolution, even for highly complex and dynamic scenes. We perform an extensive qualitative and quantitative evaluation that shows that our approach outperforms the current state of the art. We include additional video and information at: https://neural-3d-video.github.io/
            </div>
        </div>
        <p></p>
    </div>
</div>

<!-- Youtube Video -->


<!-- Project Webpage -->

<div class="border-top my-3"></div>
<div class="row p-4">
  <div class="col-12" style="text-align: center;">
    <a style="font-size: 25px; color: #ED1C24;" target="_blank" href="https://neural-3d-video.github.io/" >Project Webpage</a>
  </div>
</div>
<div class="col-md-12 col-xs-12 p-2">
    <iframe style="width: 100%; height: 60vh;" src="https://neural-3d-video.github.io/">
    </iframe>
</div>



<!-- Citation Graph -->
<div class="border-top my-3"></div>
<div class="row p-4">
  <div class="col-12" style="text-align: center;">
    <span style="font-size: 25px;">Citation Graph</span> <br>
    <span>(Double click on nodes to open corresponding papers' pages)</span>
  </div>
</div>

<div id="citationGraph" style="width: 100%;
  height: 600px;
  border: 1px solid lightgray;" ondblclick="openPaperLink()"></div>
<p><span style="font-size: 10px;">*</span> Showing citation graph for papers within our database. Data retrieved from <a href="https://www.semanticscholar.org/search?q=Neural 3D Video Synthesis&sort=relevance">Semantic Scholar</a>. For full citation graphs, visit <a href="https://www.connectedpapers.com/search?q=Neural 3D Video Synthesis">ConnectedPapers</a>.</p>
<script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.4.0/dist/umd/popper.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/tippy.js@6/dist/tippy-bundle.umd.min.js"></script>
<script src="static/js/views/paper_detail.js"></script>
<script>
    tippy(".filterByAuthorLink");
    tippy(".filterByKeywordLink");
    const paperID = document.getElementById("invisible-paper-id").innerHTML;
    drawCitationGraphAndGenerateCitationCode(paperID);
</script>


      </div>
    </div>

    <!-- body block ends here-->
    


    


    <!-- Footer -->
    <footer class="footer"
      style="background-color: #353535; width: 100%; height: 8vh; display: flex; justify-content: center; align-items: center; ">
      <a class="fas" style="cursor: pointer; margin-right: auto; padding-left: 10px;"
        href="https://twitter.com/neural_fields" target="_blank">
        <img src="static/images/twitter_logo.png" style="width: 30px; height: 24px;">
      </a>
      <a href="https://visual.cs.brown.edu/" target="_blank"><img src="static/images/Brown_logo.png"
    style="height: 6vh; padding-right: 1vw;"></a>
      <a href="http://www.scenerepresentations.org/" target="_blank"><img src="static/images/MIT_logo.png" style="height: 2.7vh"></a>
      <a style="color: white; margin-left: auto; padding-right: 10px;" href="mailto:neuralfields@brown.edu"
        target="_blank">Contact Us</a>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (window.location.hash !== "") {
          $(`a[href="${window.location.hash}"]`).tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          const hash = $(e.target).attr("href");
          if (hash.substr(0, 1) === "#") {
            const position = $(window).scrollTop();
            window.location.replace(`#${hash.substr(1)}`);
            $(window).scrollTop(position);
          }
        });
      });
    </script>

    <!-- footer block ends here -->
    
  </body>

</html>