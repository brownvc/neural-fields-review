@book{lim20043d,
    TITLE = {3D Object Reconstruction and Representation Using Neural Networks},
    AUTHOR = {Lim, Wen Peng and Shamsuddin, Siti Mariyam},
    YEAR = {2004},
    PUBLISHER = {Universiti Teknologi Malaysia}
}

@article{yang2017foldingnet,
    AUTHOR = {Yaoqing Yang and Chen Feng and Yiru Shen and Dong Tian},
    TITLE = {FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation},
    EPRINT = {1712.07262v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Recent deep networks that directly handle points in a point set, e.g.,
    PointNet, have been state-of-the-art for supervised learning tasks on point
    clouds such as classification and segmentation. In this work, a novel
    end-to-end deep auto-encoder is proposed to address unsupervised learning
    challenges on point clouds. On the encoder side, a graph-based enhancement is
    enforced to promote local structures on top of PointNet. Then, a novel
    folding-based decoder deforms a canonical 2D grid onto the underlying 3D object
    surface of a point cloud, achieving low reconstruction errors even for objects
    with delicate structures. The proposed decoder only uses about 7% parameters of
    a decoder with fully-connected neural networks, yet leads to a more
    discriminative representation that achieves higher linear SVM classification
    accuracy than the benchmark. In addition, the proposed decoder structure is
    shown, in theory, to be a generic architecture that is able to reconstruct an
    arbitrary point cloud from a 2D grid. Our code is available at
    http://www.merl.com/research/license#FoldingNet},
    YEAR = {2017},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/1712.07262v2},
    FILE = {1712.07262v2.pdf}
}

@article{groueix2018atlasnet,
    AUTHOR = {Thibault Groueix and Matthew Fisher and Vladimir G. Kim and Bryan C. Russell and Mathieu Aubry},
    TITLE = {AtlasNet: A Papier-Mache Approach to Learning 3D Surface Generation},
    EPRINT = {1802.05384v3},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We introduce a method for learning to generate the surface of 3D shapes. Our
    approach represents a 3D shape as a collection of parametric surface elements
    and, in contrast to methods generating voxel grids or point clouds, naturally
    infers a surface representation of the shape. Beyond its novelty, our new shape
    generation framework, AtlasNet, comes with significant advantages, such as
    improved precision and generalization capabilities, and the possibility to
    generate a shape of arbitrary resolution without memory issues. We demonstrate
    these benefits and compare to strong baselines on the ShapeNet benchmark for
    two applications: (i) auto-encoding shapes, and (ii) single-view reconstruction
    from a still image. We also provide results showing its potential for other
    applications, such as morphing, parametrization, super-resolution, matching,
    and co-segmentation.},
    YEAR = {2018},
    MONTH = {Feb},
    URL = {http://arxiv.org/abs/1802.05384v3},
    FILE = {1802.05384v3.pdf}
}

@article{williams2018deep,
    AUTHOR = {Francis Williams and Teseo Schneider and Claudio Silva and Denis Zorin and Joan Bruna and Daniele Panozzo},
    TITLE = {Deep Geometric Prior for Surface Reconstruction},
    EPRINT = {1811.10943v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {The reconstruction of a discrete surface from a point cloud is a fundamental
    geometry processing problem that has been studied for decades, with many
    methods developed. We propose the use of a deep neural network as a geometric
    prior for surface reconstruction. Specifically, we overfit a neural network
    representing a local chart parameterization to part of an input point cloud
    using the Wasserstein distance as a measure of approximation. By jointly
    fitting many such networks to overlapping parts of the point cloud, while
    enforcing a consistency condition, we compute a manifold atlas. By sampling
    this atlas, we can produce a dense reconstruction of the surface approximating
    the input cloud. The entire procedure does not require any training data or
    explicit regularization, yet, we show that it is able to perform remarkably
    well: not introducing typical overfitting artifacts, and approximating sharp
    features closely at the same time. We experimentally show that this geometric
    prior produces good results for both man-made objects containing sharp features
    and smoother organic objects, as well as noisy inputs. We compare our method
    with a number of well-known reconstruction methods on a standard surface
    reconstruction benchmark.},
    YEAR = {2018},
    MONTH = {Nov},
    URL = {http://arxiv.org/abs/1811.10943v2},
    FILE = {1811.10943v2.pdf}
}

@article{chen2018imnet,
    AUTHOR = {Zhiqin Chen and Hao Zhang},
    TITLE = {Learning Implicit Fields for Generative Shape Modeling},
    EPRINT = {1812.02822v5},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.GR},
    ABSTRACT = {We advocate the use of implicit fields for learning generative models of
    shapes and introduce an implicit field decoder, called IM-NET, for shape
    generation, aimed at improving the visual quality of the generated shapes. An
    implicit field assigns a value to each point in 3D space, so that a shape can
    be extracted as an iso-surface. IM-NET is trained to perform this assignment by
    means of a binary classifier. Specifically, it takes a point coordinate, along
    with a feature vector encoding a shape, and outputs a value which indicates
    whether the point is outside the shape or not. By replacing conventional
    decoders by our implicit decoder for representation learning (via IM-AE) and
    shape generation (via IM-GAN), we demonstrate superior results for tasks such
    as generative shape modeling, interpolation, and single-view 3D reconstruction,
    particularly in terms of visual quality. Code and supplementary material are
    available at https://github.com/czq142857/implicit-decoder.},
    YEAR = {2018},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/1812.02822v5},
    FILE = {1812.02822v5.pdf}
}

@article{mescheder2018occupancy networks,
    AUTHOR = {Lars Mescheder and Michael Oechsle and Michael Niemeyer and Sebastian Nowozin and Andreas Geiger},
    TITLE = {Occupancy Networks: Learning 3D Reconstruction in Function Space},
    EPRINT = {1812.03828v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {With the advent of deep neural networks, learning-based approaches for 3D
    reconstruction have gained popularity. However, unlike for images, in 3D there
    is no canonical representation which is both computationally and memory
    efficient yet allows for representing high-resolution geometry of arbitrary
    topology. Many of the state-of-the-art learning-based 3D reconstruction
    approaches can hence only represent very coarse 3D geometry or are limited to a
    restricted domain. In this paper, we propose Occupancy Networks, a new
    representation for learning-based 3D reconstruction methods. Occupancy networks
    implicitly represent the 3D surface as the continuous decision boundary of a
    deep neural network classifier. In contrast to existing approaches, our
    representation encodes a description of the 3D output at infinite resolution
    without excessive memory footprint. We validate that our representation can
    efficiently encode 3D structure and can be inferred from various kinds of
    input. Our experiments demonstrate competitive results, both qualitatively and
    quantitatively, for the challenging tasks of 3D reconstruction from single
    images, noisy point clouds and coarse discrete voxel grids. We believe that
    occupancy networks will become a useful tool in a wide variety of
    learning-based 3D tasks.},
    YEAR = {2018},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/1812.03828v2},
    FILE = {1812.03828v2.pdf}
}

@article{park2019deepsdf,
    AUTHOR = {Jeong Joon Park and Peter Florence and Julian Straub and Richard Newcombe and Steven Lovegrove},
    TITLE = {DeepSDF: Learning Continuous Signed Distance Functions for Shape
    Representation},
    EPRINT = {1901.05103v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Computer graphics, 3D computer vision and robotics communities have produced
    multiple approaches to representing 3D geometry for rendering and
    reconstruction. These provide trade-offs across fidelity, efficiency and
    compression capabilities. In this work, we introduce DeepSDF, a learned
    continuous Signed Distance Function (SDF) representation of a class of shapes
    that enables high quality shape representation, interpolation and completion
    from partial and noisy 3D input data. DeepSDF, like its classical counterpart,
    represents a shape's surface by a continuous volumetric field: the magnitude of
    a point in the field represents the distance to the surface boundary and the
    sign indicates whether the region is inside (-) or outside (+) of the shape,
    hence our representation implicitly encodes a shape's boundary as the
    zero-level-set of the learned function while explicitly representing the
    classification of space as being part of the shapes interior or not. While
    classical SDF's both in analytical or discretized voxel form typically
    represent the surface of a single shape, DeepSDF can represent an entire class
    of shapes. Furthermore, we show state-of-the-art performance for learned 3D
    shape representation and completion while reducing the model size by an order
    of magnitude compared with previous work.},
    YEAR = {2019},
    MONTH = {Jan},
    URL = {http://arxiv.org/abs/1901.05103v1},
    FILE = {1901.05103v1.pdf}
}

@article{saito2019pifu,
    AUTHOR = {Shunsuke Saito and Zeng Huang and Ryota Natsume and Shigeo Morishima and Angjoo Kanazawa and Hao Li},
    TITLE = {PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human
    Digitization},
    EPRINT = {1905.05172v3},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We introduce Pixel-aligned Implicit Function (PIFu), a highly effective
    implicit representation that locally aligns pixels of 2D images with the global
    context of their corresponding 3D object. Using PIFu, we propose an end-to-end
    deep learning method for digitizing highly detailed clothed humans that can
    infer both 3D surface and texture from a single image, and optionally, multiple
    input images. Highly intricate shapes, such as hairstyles, clothing, as well as
    their variations and deformations can be digitized in a unified way. Compared
    to existing representations used for 3D deep learning, PIFu can produce
    high-resolution surfaces including largely unseen regions such as the back of a
    person. In particular, it is memory efficient unlike the voxel representation,
    can handle arbitrary topology, and the resulting surface is spatially aligned
    with the input image. Furthermore, while previous techniques are designed to
    process either a single image or multiple views, PIFu extends naturally to
    arbitrary number of views. We demonstrate high-resolution and robust
    reconstructions on real world images from the DeepFashion dataset, which
    contains a variety of challenging clothing types. Our method achieves
    state-of-the-art performance on a public benchmark and outperforms the prior
    work for clothed human digitization from a single image.},
    YEAR = {2019},
    MONTH = {May},
    NOTE = {The IEEE International Conference on Computer Vision (ICCV), 2019,
    pp. 2304-2314},
    URL = {http://arxiv.org/abs/1905.05172v3},
    FILE = {1905.05172v3.pdf}
}

@article{sitzmann2019srn,
    AUTHOR = {Vincent Sitzmann and Michael Zollhofer and Gordon Wetzstein},
    TITLE = {Scene Representation Networks: Continuous 3D-Structure-Aware Neural
    Scene Representations},
    EPRINT = {1906.01618v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Unsupervised learning with generative models has the potential of discovering
    rich representations of 3D scenes. While geometric deep learning has explored
    3D-structure-aware representations of scene geometry, these models typically
    require explicit 3D supervision. Emerging neural scene representations can be
    trained only with posed 2D images, but existing methods ignore the
    three-dimensional structure of scenes. We propose Scene Representation Networks
    (SRNs), a continuous, 3D-structure-aware scene representation that encodes both
    geometry and appearance. SRNs represent scenes as continuous functions that map
    world coordinates to a feature representation of local scene properties. By
    formulating the image formation as a differentiable ray-marching algorithm,
    SRNs can be trained end-to-end from only 2D images and their camera poses,
    without access to depth or shape. This formulation naturally generalizes across
    scenes, learning powerful geometry and appearance priors in the process. We
    demonstrate the potential of SRNs by evaluating them for novel view synthesis,
    few-shot reconstruction, joint shape and appearance interpolation, and
    unsupervised discovery of a non-rigid face model.},
    YEAR = {2019},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/1906.01618v2},
    FILE = {1906.01618v2.pdf}
}

@article{lombardi2019nv,
    AUTHOR = {Stephen Lombardi and Tomas Simon and Jason Saragih and Gabriel Schwartz and Andreas Lehrmann and Yaser Sheikh},
    TITLE = {Neural Volumes: Learning Dynamic Renderable Volumes from Images},
    EPRINT = {1906.07751v1},
    DOI = {10.1145/3306346.3323020},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.GR},
    ABSTRACT = {Modeling and rendering of dynamic scenes is challenging, as natural scenes
    often contain complex phenomena such as thin structures, evolving topology,
    translucency, scattering, occlusion, and biological motion. Mesh-based
    reconstruction and tracking often fail in these cases, and other approaches
    (e.g., light field video) typically rely on constrained viewing conditions,
    which limit interactivity. We circumvent these difficulties by presenting a
    learning-based approach to representing dynamic objects inspired by the
    integral projection model used in tomographic imaging. The approach is
    supervised directly from 2D images in a multi-view capture setting and does not
    require explicit reconstruction or tracking of the object. Our method has two
    primary components: an encoder-decoder network that transforms input images
    into a 3D volume representation, and a differentiable ray-marching operation
    that enables end-to-end training. By virtue of its 3D representation, our
    construction extrapolates better to novel viewpoints compared to screen-space
    rendering techniques. The encoder-decoder architecture learns a latent
    representation of a dynamic scene that enables us to produce novel content
    sequences not seen during training. To overcome memory limitations of
    voxel-based representations, we learn a dynamic irregular grid structure
    implemented with a warp field during ray-marching. This structure greatly
    improves the apparent resolution and reduces grid-like artifacts and jagged
    motion. Finally, we demonstrate how to incorporate surface-based
    representations into our volumetric-learning framework for applications where
    the highest resolution is required, using facial performance capture as a case
    in point.},
    YEAR = {2019},
    MONTH = {Jun},
    NOTE = {ACM Transactions on Graphics (SIGGRAPH 2019) 38, 4, Article 65},
    URL = {http://arxiv.org/abs/1906.07751v1},
    FILE = {1906.07751v1.pdf}
}

@article{deprelle2019learning,
    AUTHOR = {Theo Deprelle and Thibault Groueix and Matthew Fisher and Vladimir G. Kim and Bryan C. Russell and Mathieu Aubry},
    TITLE = {Learning elementary structures for 3D shape generation and matching},
    EPRINT = {1908.04725v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We propose to represent shapes as the deformation and combination of
    learnable elementary 3D structures, which are primitives resulting from
    training over a collection of shape. We demonstrate that the learned elementary
    3D structures lead to clear improvements in 3D shape generation and matching.
    More precisely, we present two complementary approaches for learning elementary
    structures: (i) patch deformation learning and (ii) point translation learning.
    Both approaches can be extended to abstract structures of higher dimensions for
    improved results. We evaluate our method on two tasks: reconstructing ShapeNet
    objects and estimating dense correspondences between human scans (FAUST inter
    challenge). We show 16% improvement over surface deformation approaches for
    shape reconstruction and outperform FAUST inter challenge state of the art by
    6%.},
    YEAR = {2019},
    MONTH = {Aug},
    URL = {http://arxiv.org/abs/1908.04725v2},
    FILE = {1908.04725v2.pdf}
}

@article{liu2019learning,
    AUTHOR = {Shichen Liu and Shunsuke Saito and Weikai Chen and Hao Li},
    TITLE = {Learning to Infer Implicit Surfaces without 3D Supervision},
    EPRINT = {1911.00767v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Recent advances in 3D deep learning have shown that it is possible to train
    highly effective deep models for 3D shape generation, directly from 2D images.
    This is particularly interesting since the availability of 3D models is still
    limited compared to the massive amount of accessible 2D images, which is
    invaluable for training. The representation of 3D surfaces itself is a key
    factor for the quality and resolution of the 3D output. While explicit
    representations, such as point clouds and voxels, can span a wide range of
    shape variations, their resolutions are often limited. Mesh-based
    representations are more efficient but are limited by their ability to handle
    varying topologies. Implicit surfaces, however, can robustly handle complex
    shapes, topologies, and also provide flexible resolution control. We address
    the fundamental problem of learning implicit surfaces for shape inference
    without the need of 3D supervision. Despite their advantages, it remains
    nontrivial to (1) formulate a differentiable connection between implicit
    surfaces and their 2D renderings, which is needed for image-based supervision;
    and (2) ensure precise geometric properties and control, such as local
    smoothness. In particular, sampling implicit surfaces densely is also known to
    be a computationally demanding and very slow operation. To this end, we propose
    a novel ray-based field probing technique for efficient image-to-field
    supervision, as well as a general geometric regularizer for implicit surfaces,
    which provides natural shape priors in unconstrained regions. We demonstrate
    the effectiveness of our framework on the task of single-view image-based 3D
    shape digitization and show how we outperform state-of-the-art techniques both
    quantitatively and qualitatively.},
    YEAR = {2019},
    MONTH = {Nov},
    URL = {http://arxiv.org/abs/1911.00767v1},
    FILE = {1911.00767v1.pdf}
}

@article{atzmon2019sal,
    AUTHOR = {Matan Atzmon and Yaron Lipman},
    TITLE = {SAL: Sign Agnostic Learning of Shapes from Raw Data},
    EPRINT = {1911.10414v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Recently, neural networks have been used as implicit representations for
    surface reconstruction, modelling, learning, and generation. So far, training
    neural networks to be implicit representations of surfaces required training
    data sampled from a ground-truth signed implicit functions such as signed
    distance or occupancy functions, which are notoriously hard to compute.
    In this paper we introduce Sign Agnostic Learning (SAL), a deep learning
    approach for learning implicit shape representations directly from raw,
    unsigned geometric data, such as point clouds and triangle soups.
    We have tested SAL on the challenging problem of surface reconstruction from
    an un-oriented point cloud, as well as end-to-end human shape space learning
    directly from raw scans dataset, and achieved state of the art reconstructions
    compared to current approaches. We believe SAL opens the door to many geometric
    deep learning applications with real-world data, alleviating the usual
    painstaking, often manual pre-process.},
    YEAR = {2019},
    MONTH = {Nov},
    URL = {http://arxiv.org/abs/1911.10414v2},
    FILE = {1911.10414v2.pdf}
}

@article{genova2019ldif,
    AUTHOR = {Kyle Genova and Forrester Cole and Avneesh Sud and Aaron Sarna and Thomas Funkhouser},
    TITLE = {Local Deep Implicit Functions for 3D Shape},
    EPRINT = {1912.06126v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {The goal of this project is to learn a 3D shape representation that enables
    accurate surface reconstruction, compact storage, efficient computation,
    consistency for similar shapes, generalization across diverse shape categories,
    and inference from depth camera observations. Towards this end, we introduce
    Local Deep Implicit Functions (LDIF), a 3D shape representation that decomposes
    space into a structured set of learned implicit functions. We provide networks
    that infer the space decomposition and local deep implicit functions from a 3D
    mesh or posed depth image. During experiments, we find that it provides 10.3
    points higher surface reconstruction accuracy (F-Score) than the
    state-of-the-art (OccNet), while requiring fewer than 1 percent of the network
    parameters. Experiments on posed depth image completion and generalization to
    unseen classes show 15.8 and 17.8 point improvements over the state-of-the-art,
    while producing a structured 3D representation for each input with consistency
    across diverse shape collections.},
    YEAR = {2019},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/1912.06126v2},
    FILE = {1912.06126v2.pdf}
}

@article{niemeyer2019dvr,
    AUTHOR = {Michael Niemeyer and Lars Mescheder and Michael Oechsle and Andreas Geiger},
    TITLE = {Differentiable Volumetric Rendering: Learning Implicit 3D
    Representations without 3D Supervision},
    EPRINT = {1912.07372v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Learning-based 3D reconstruction methods have shown impressive results.
    However, most methods require 3D supervision which is often hard to obtain for
    real-world datasets. Recently, several works have proposed differentiable
    rendering techniques to train reconstruction models from RGB images.
    Unfortunately, these approaches are currently restricted to voxel- and
    mesh-based representations, suffering from discretization or low resolution. In
    this work, we propose a differentiable rendering formulation for implicit shape
    and texture representations. Implicit representations have recently gained
    popularity as they represent shape and texture continuously. Our key insight is
    that depth gradients can be derived analytically using the concept of implicit
    differentiation. This allows us to learn implicit shape and texture
    representations directly from RGB images. We experimentally show that our
    single-view reconstructions rival those learned with full 3D supervision.
    Moreover, we find that our method can be used for multi-view 3D reconstruction,
    directly resulting in watertight meshes.},
    YEAR = {2019},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/1912.07372v2},
    FILE = {1912.07372v2.pdf}
}

@article{gropp2020igr,
    AUTHOR = {Amos Gropp and Lior Yariv and Niv Haim and Matan Atzmon and Yaron Lipman},
    TITLE = {Implicit Geometric Regularization for Learning Shapes},
    EPRINT = {2002.10099v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.LG},
    ABSTRACT = {Representing shapes as level sets of neural networks has been recently proved
    to be useful for different shape analysis and reconstruction tasks. So far,
    such representations were computed using either: (i) pre-computed implicit
    shape representations; or (ii) loss functions explicitly defined over the
    neural level sets. In this paper we offer a new paradigm for computing high
    fidelity implicit neural representations directly from raw data (i.e., point
    clouds, with or without normal information). We observe that a rather simple
    loss function, encouraging the neural network to vanish on the input point
    cloud and to have a unit norm gradient, possesses an implicit geometric
    regularization property that favors smooth and natural zero level set surfaces,
    avoiding bad zero-loss solutions. We provide a theoretical analysis of this
    property for the linear case, and show that, in practice, our method leads to
    state of the art implicit neural representations with higher level-of-details
    and fidelity compared to previous methods.},
    YEAR = {2020},
    MONTH = {Feb},
    URL = {http://arxiv.org/abs/2002.10099v2},
    FILE = {2002.10099v2.pdf}
}

@article{chibane2020ifnets,
    AUTHOR = {Julian Chibane and Thiemo Alldieck and Gerard Pons-Moll},
    TITLE = {Implicit Functions in Feature Space for 3D Shape Reconstruction and
    Completion},
    EPRINT = {2003.01456v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {While many works focus on 3D reconstruction from images, in this paper, we
    focus on 3D shape reconstruction and completion from a variety of 3D inputs,
    which are deficient in some respect: low and high resolution voxels, sparse and
    dense point clouds, complete or incomplete. Processing of such 3D inputs is an
    increasingly important problem as they are the output of 3D scanners, which are
    becoming more accessible, and are the intermediate output of 3D computer vision
    algorithms. Recently, learned implicit functions have shown great promise as
    they produce continuous reconstructions. However, we identified two limitations
    in reconstruction from 3D inputs: 1) details present in the input data are not
    retained, and 2) poor reconstruction of articulated humans. To solve this, we
    propose Implicit Feature Networks (IF-Nets), which deliver continuous outputs,
    can handle multiple topologies, and complete shapes for missing or sparse input
    data retaining the nice properties of recent learned implicit functions, but
    critically they can also retain detail when it is present in the input data,
    and can reconstruct articulated humans. Our work differs from prior work in two
    crucial aspects. First, instead of using a single vector to encode a 3D shape,
    we extract a learnable 3-dimensional multi-scale tensor of deep features, which
    is aligned with the original Euclidean space embedding the shape. Second,
    instead of classifying x-y-z point coordinates directly, we classify deep
    features extracted from the tensor at a continuous query point. We show that
    this forces our model to make decisions based on global and local shape
    structure, as opposed to point coordinates, which are arbitrary under Euclidean
    transformations. Experiments demonstrate that IF-Nets clearly outperform prior
    work in 3D object reconstruction in ShapeNet, and obtain significantly more
    accurate 3D human reconstructions.},
    YEAR = {2020},
    MONTH = {Mar},
    NOTE = {{IEEE} Conference on Computer Vision and Pattern Recognition
    (CVPR) 2020},
    URL = {http://arxiv.org/abs/2003.01456v2},
    FILE = {2003.01456v2.pdf}
}

@article{peng2020convolutional,
    AUTHOR = {Songyou Peng and Michael Niemeyer and Lars Mescheder and Marc Pollefeys and Andreas Geiger},
    TITLE = {Convolutional Occupancy Networks},
    EPRINT = {2003.04618v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Recently, implicit neural representations have gained popularity for
    learning-based 3D reconstruction. While demonstrating promising results, most
    implicit approaches are limited to comparably simple geometry of single objects
    and do not scale to more complicated or large-scale scenes. The key limiting
    factor of implicit methods is their simple fully-connected network architecture
    which does not allow for integrating local information in the observations or
    incorporating inductive biases such as translational equivariance. In this
    paper, we propose Convolutional Occupancy Networks, a more flexible implicit
    representation for detailed reconstruction of objects and 3D scenes. By
    combining convolutional encoders with implicit occupancy decoders, our model
    incorporates inductive biases, enabling structured reasoning in 3D space. We
    investigate the effectiveness of the proposed representation by reconstructing
    complex geometry from noisy point clouds and low-resolution voxel
    representations. We empirically find that our method enables the fine-grained
    implicit 3D reconstruction of single objects, scales to large indoor scenes,
    and generalizes well from synthetic to real data.},
    YEAR = {2020},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2003.04618v2},
    FILE = {2003.04618v2.pdf}
}

@article{mildenhall2020nerf,
    AUTHOR = {Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
    TITLE = {NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
    EPRINT = {2003.08934v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present a method that achieves state-of-the-art results for synthesizing
    novel views of complex scenes by optimizing an underlying continuous volumetric
    scene function using a sparse set of input views. Our algorithm represents a
    scene using a fully-connected (non-convolutional) deep network, whose input is
    a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing
    direction $(\theta, \phi)$) and whose output is the volume density and
    view-dependent emitted radiance at that spatial location. We synthesize views
    by querying 5D coordinates along camera rays and use classic volume rendering
    techniques to project the output colors and densities into an image. Because
    volume rendering is naturally differentiable, the only input required to
    optimize our representation is a set of images with known camera poses. We
    describe how to effectively optimize neural radiance fields to render
    photorealistic novel views of scenes with complicated geometry and appearance,
    and demonstrate results that outperform prior work on neural rendering and view
    synthesis. View synthesis results are best viewed as videos, so we urge readers
    to view our supplementary video for convincing comparisons.},
    YEAR = {2020},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2003.08934v2},
    FILE = {2003.08934v2.pdf}
}

@article{yariv2020idr,
    AUTHOR = {Lior Yariv and Yoni Kasten and Dror Moran and Meirav Galun and Matan Atzmon and Ronen Basri and Yaron Lipman},
    TITLE = {Multiview Neural Surface Reconstruction by Disentangling Geometry and
    Appearance},
    EPRINT = {2003.09852v3},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {In this work we address the challenging problem of multiview 3D surface
    reconstruction. We introduce a neural network architecture that simultaneously
    learns the unknown geometry, camera parameters, and a neural renderer that
    approximates the light reflected from the surface towards the camera. The
    geometry is represented as a zero level-set of a neural network, while the
    neural renderer, derived from the rendering equation, is capable of
    (implicitly) modeling a wide set of lighting conditions and materials. We
    trained our network on real world 2D images of objects with different material
    properties, lighting conditions, and noisy camera initializations from the DTU
    MVS dataset. We found our model to produce state of the art 3D surface
    reconstructions with high fidelity, resolution and detail.},
    YEAR = {2020},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2003.09852v3},
    FILE = {2003.09852v3.pdf}
}

@article{chabra2020deepls,
    AUTHOR = {Rohan Chabra and Jan Eric Lenssen and Eddy Ilg and Tanner Schmidt and Julian Straub and Steven Lovegrove and Richard Newcombe},
    TITLE = {Deep Local Shapes: Learning Local SDF Priors for Detailed 3D
    Reconstruction},
    EPRINT = {2003.10983v3},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Efficiently reconstructing complex and intricate surfaces at scale is a
    long-standing goal in machine perception. To address this problem we introduce
    Deep Local Shapes (DeepLS), a deep shape representation that enables encoding
    and reconstruction of high-quality 3D shapes without prohibitive memory
    requirements. DeepLS replaces the dense volumetric signed distance function
    (SDF) representation used in traditional surface reconstruction systems with a
    set of locally learned continuous SDFs defined by a neural network, inspired by
    recent work such as DeepSDF. Unlike DeepSDF, which represents an object-level
    SDF with a neural network and a single latent code, we store a grid of
    independent latent codes, each responsible for storing information about
    surfaces in a small local neighborhood. This decomposition of scenes into local
    shapes simplifies the prior distribution that the network must learn, and also
    enables efficient inference. We demonstrate the effectiveness and
    generalization power of DeepLS by showing object shape encoding and
    reconstructions of full scenes, where DeepLS delivers high compression,
    accuracy, and local shape completion.},
    YEAR = {2020},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2003.10983v3},
    FILE = {2003.10983v3.pdf}
}

@article{kohli2020semantic,
    AUTHOR = {Amit Kohli and Vincent Sitzmann and Gordon Wetzstein},
    TITLE = {Semantic Implicit Neural Scene Representations With Semi-Supervised
    Training},
    EPRINT = {2003.12673v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {The recent success of implicit neural scene representations has presented a
    viable new method for how we capture and store 3D scenes. Unlike conventional
    3D representations, such as point clouds, which explicitly store scene
    properties in discrete, localized units, these implicit representations encode
    a scene in the weights of a neural network which can be queried at any
    coordinate to produce these same scene properties. Thus far, implicit
    representations have primarily been optimized to estimate only the appearance
    and/or 3D geometry information in a scene. We take the next step and
    demonstrate that an existing implicit representation (SRNs) is actually
    multi-modal; it can be further leveraged to perform per-point semantic
    segmentation while retaining its ability to represent appearance and geometry.
    To achieve this multi-modal behavior, we utilize a semi-supervised learning
    strategy atop the existing pre-trained scene representation. Our method is
    simple, general, and only requires a few tens of labeled 2D segmentation masks
    in order to achieve dense 3D semantic segmentation. We explore two novel
    applications for this semantically aware implicit neural scene representation:
    3D novel view and semantic label synthesis given only a single input RGB image
    or 2D label mask, as well as 3D interpolation of appearance and semantics.},
    YEAR = {2020},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2003.12673v2},
    FILE = {2003.12673v2.pdf}
}

@article{saito2020pifuhd,
    AUTHOR = {Shunsuke Saito and Tomas Simon and Jason Saragih and Hanbyul Joo},
    TITLE = {PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution
    3D Human Digitization},
    EPRINT = {2004.00452v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Recent advances in image-based 3D human shape estimation have been driven by
    the significant improvement in representation power afforded by deep neural
    networks. Although current approaches have demonstrated the potential in real
    world settings, they still fail to produce reconstructions with the level of
    detail often present in the input images. We argue that this limitation stems
    primarily form two conflicting requirements; accurate predictions require large
    context, but precise predictions require high resolution. Due to memory
    limitations in current hardware, previous approaches tend to take low
    resolution images as input to cover large spatial context, and produce less
    precise (or low resolution) 3D estimates as a result. We address this
    limitation by formulating a multi-level architecture that is end-to-end
    trainable. A coarse level observes the whole image at lower resolution and
    focuses on holistic reasoning. This provides context to an fine level which
    estimates highly detailed geometry by observing higher-resolution images. We
    demonstrate that our approach significantly outperforms existing
    state-of-the-art techniques on single image human shape reconstruction by fully
    leveraging 1k-resolution input images.},
    YEAR = {2020},
    MONTH = {Apr},
    NOTE = {The IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), 2020},
    URL = {http://arxiv.org/abs/2004.00452v1},
    FILE = {2004.00452v1.pdf}
}

@article{sitzmann2020siren,
    AUTHOR = {Vincent Sitzmann and Julien N. P. Martel and Alexander W. Bergman and David B. Lindell and Gordon Wetzstein},
    TITLE = {Implicit Neural Representations with Periodic Activation Functions},
    EPRINT = {2006.09661v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Implicitly defined, continuous, differentiable signal representations
    parameterized by neural networks have emerged as a powerful paradigm, offering
    many possible benefits over conventional representations. However, current
    network architectures for such implicit neural representations are incapable of
    modeling signals with fine detail, and fail to represent a signal's spatial and
    temporal derivatives, despite the fact that these are essential to many
    physical signals defined implicitly as the solution to partial differential
    equations. We propose to leverage periodic activation functions for implicit
    neural representations and demonstrate that these networks, dubbed sinusoidal
    representation networks or Sirens, are ideally suited for representing complex
    natural signals and their derivatives. We analyze Siren activation statistics
    to propose a principled initialization scheme and demonstrate the
    representation of images, wavefields, video, sound, and their derivatives.
    Further, we show how Sirens can be leveraged to solve challenging boundary
    value problems, such as particular Eikonal equations (yielding signed distance
    functions), the Poisson equation, and the Helmholtz and wave equations. Lastly,
    we combine Sirens with hypernetworks to learn priors over the space of Siren
    functions.},
    YEAR = {2020},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2006.09661v1},
    FILE = {2006.09661v1.pdf}
}

@article{sitzmann2020metasdf,
    AUTHOR = {Vincent Sitzmann and Eric R. Chan and Richard Tucker and Noah Snavely and Gordon Wetzstein},
    TITLE = {MetaSDF: Meta-learning Signed Distance Functions},
    EPRINT = {2006.09662v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Neural implicit shape representations are an emerging paradigm that offers
    many potential benefits over conventional discrete representations, including
    memory efficiency at a high spatial resolution. Generalizing across shapes with
    such neural implicit representations amounts to learning priors over the
    respective function space and enables geometry reconstruction from partial or
    noisy observations. Existing generalization methods rely on conditioning a
    neural network on a low-dimensional latent code that is either regressed by an
    encoder or jointly optimized in the auto-decoder framework. Here, we formalize
    learning of a shape space as a meta-learning problem and leverage
    gradient-based meta-learning algorithms to solve this task. We demonstrate that
    this approach performs on par with auto-decoder based approaches while being an
    order of magnitude faster at test-time inference. We further demonstrate that
    the proposed gradient-based method outperforms encoder-decoder based methods
    that leverage pooling-based set encoders.},
    YEAR = {2020},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2006.09662v1},
    FILE = {2006.09662v1.pdf}
}

@article{tancik2020ffn,
    AUTHOR = {Matthew Tancik and Pratul P. Srinivasan and Ben Mildenhall and Sara Fridovich-Keil and Nithin Raghavan and Utkarsh Singhal and Ravi Ramamoorthi and Jonathan T. Barron and Ren Ng},
    TITLE = {Fourier Features Let Networks Learn High Frequency Functions in Low
    Dimensional Domains},
    EPRINT = {2006.10739v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We show that passing input points through a simple Fourier feature mapping
    enables a multilayer perceptron (MLP) to learn high-frequency functions in
    low-dimensional problem domains. These results shed light on recent advances in
    computer vision and graphics that achieve state-of-the-art results by using
    MLPs to represent complex 3D objects and scenes. Using tools from the neural
    tangent kernel (NTK) literature, we show that a standard MLP fails to learn
    high frequencies both in theory and in practice. To overcome this spectral
    bias, we use a Fourier feature mapping to transform the effective NTK into a
    stationary kernel with a tunable bandwidth. We suggest an approach for
    selecting problem-specific Fourier features that greatly improves the
    performance of MLPs for low-dimensional regression tasks relevant to the
    computer vision and graphics communities.},
    YEAR = {2020},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2006.10739v1},
    FILE = {2006.10739v1.pdf}
}

@article{bi2020deep,
    AUTHOR = {Sai Bi and Zexiang Xu and Kalyan Sunkavalli and Milos Hasan and Yannick Hold-Geoffroy and David Kriegman and Ravi Ramamoorthi},
    TITLE = {Deep Reflectance Volumes: Relightable Reconstructions from Multi-View
    Photometric Images},
    EPRINT = {2007.09892v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present a deep learning approach to reconstruct scene appearance from
    unstructured images captured under collocated point lighting. At the heart of
    Deep Reflectance Volumes is a novel volumetric scene representation consisting
    of opacity, surface normal and reflectance voxel grids. We present a novel
    physically-based differentiable volume ray marching framework to render these
    scene volumes under arbitrary viewpoint and lighting. This allows us to
    optimize the scene volumes to minimize the error between their rendered images
    and the captured images. Our method is able to reconstruct real scenes with
    challenging non-Lambertian reflectance and complex geometry with occlusions and
    shadowing. Moreover, it accurately generalizes to novel viewpoints and
    lighting, including non-collocated lighting, rendering photorealistic images
    that are significantly better than state-of-the-art mesh-based methods. We also
    show that our learned reflectance volumes are editable, allowing for modifying
    the materials of the captured scenes.},
    YEAR = {2020},
    MONTH = {Jul},
    URL = {http://arxiv.org/abs/2007.09892v1},
    FILE = {2007.09892v1.pdf}
}

@article{williams2020neural splines,
    AUTHOR = {Francis Williams and Matthew Trager and Joan Bruna and Denis Zorin},
    TITLE = {Neural Splines: Fitting 3D Surfaces with Infinitely-Wide Neural Networks},
    EPRINT = {2006.13782v3},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present Neural Splines, a technique for 3D surface reconstruction that is
    based on random feature kernels arising from infinitely-wide shallow ReLU
    networks. Our method achieves state-of-the-art results, outperforming recent
    neural network-based techniques and widely used Poisson Surface Reconstruction
    (which, as we demonstrate, can also be viewed as a type of kernel method).
    Because our approach is based on a simple kernel formulation, it is easy to
    analyze and can be accelerated by general techniques designed for kernel-based
    learning. We provide explicit analytical expressions for our kernel and argue
    that our formulation can be seen as a generalization of cubic spline
    interpolation to higher dimensions. In particular, the RKHS norm associated
    with Neural Splines biases toward smooth interpolants.},
    YEAR = {2020},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2006.13782v3},
    FILE = {2006.13782v3.pdf}
}

@article{schwarz2020graf,
    AUTHOR = {Katja Schwarz and Yiyi Liao and Michael Niemeyer and Andreas Geiger},
    TITLE = {GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis},
    EPRINT = {2007.02442v4},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {While 2D generative adversarial networks have enabled high-resolution image
    synthesis, they largely lack an understanding of the 3D world and the image
    formation process. Thus, they do not provide precise control over camera
    viewpoint or object pose. To address this problem, several recent approaches
    leverage intermediate voxel-based representations in combination with
    differentiable rendering. However, existing methods either produce low image
    resolution or fall short in disentangling camera and scene properties, e.g.,
    the object identity may vary with the viewpoint. In this paper, we propose a
    generative model for radiance fields which have recently proven successful for
    novel view synthesis of a single scene. In contrast to voxel-based
    representations, radiance fields are not confined to a coarse discretization of
    the 3D space, yet allow for disentangling camera and scene properties while
    degrading gracefully in the presence of reconstruction ambiguity. By
    introducing a multi-scale patch-based discriminator, we demonstrate synthesis
    of high-resolution images while training our model from unposed 2D images
    alone. We systematically analyze our approach on several challenging synthetic
    and real-world datasets. Our experiments reveal that radiance fields are a
    powerful representation for generative image synthesis, leading to 3D
    consistent models that render with high fidelity.},
    YEAR = {2020},
    MONTH = {Jul},
    NOTE = {Advances in Neural Information Processing Systems, NeurIPS 2020},
    URL = {http://arxiv.org/abs/2007.02442v4},
    FILE = {2007.02442v4.pdf}
}

@article{liu2020nsvf,
    AUTHOR = {Lingjie Liu and Jiatao Gu and Kyaw Zaw Lin and Tat-Seng Chua and Christian Theobalt},
    TITLE = {Neural Sparse Voxel Fields},
    EPRINT = {2007.11571v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Photo-realistic free-viewpoint rendering of real-world scenes using classical
    computer graphics techniques is challenging, because it requires the difficult
    step of capturing detailed appearance and geometry models. Recent studies have
    demonstrated promising results by learning scene representations that
    implicitly encode both geometry and appearance without 3D supervision. However,
    existing approaches in practice often show blurry renderings caused by the
    limited network capacity or the difficulty in finding accurate intersections of
    camera rays with the scene geometry. Synthesizing high-resolution imagery from
    these representations often requires time-consuming optical ray marching. In
    this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene
    representation for fast and high-quality free-viewpoint rendering. NSVF defines
    a set of voxel-bounded implicit fields organized in a sparse voxel octree to
    model local properties in each cell. We progressively learn the underlying
    voxel structures with a differentiable ray-marching operation from only a set
    of posed RGB images. With the sparse voxel octree structure, rendering novel
    views can be accelerated by skipping the voxels containing no relevant scene
    content. Our method is typically over 10 times faster than the state-of-the-art
    (namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving
    higher quality results. Furthermore, by utilizing an explicit sparse voxel
    representation, our method can easily be applied to scene editing and scene
    composition. We also demonstrate several challenging tasks, including
    multi-scene learning, free-viewpoint rendering of a moving human, and
    large-scale scene rendering. Code and data are available at our website:
    https://github.com/facebookresearch/NSVF.},
    YEAR = {2020},
    MONTH = {Jul},
    URL = {http://arxiv.org/abs/2007.11571v2},
    FILE = {2007.11571v2.pdf}
}

@article{hani2020corn,
    AUTHOR = {Nicolai Hani and Selim Engin and Jun-Jee Chao and Volkan Isler},
    TITLE = {Continuous Object Representation Networks: Novel View Synthesis without
    Target View Supervision},
    EPRINT = {2007.15627v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Novel View Synthesis (NVS) is concerned with synthesizing views under camera
    viewpoint transformations from one or multiple input images. NVS requires
    explicit reasoning about 3D object structure and unseen parts of the scene to
    synthesize convincing results. As a result, current approaches typically rely
    on supervised training with either ground truth 3D models or multiple target
    images. We propose Continuous Object Representation Networks (CORN), a
    conditional architecture that encodes an input image's geometry and appearance
    that map to a 3D consistent scene representation. We can train CORN with only
    two source images per object by combining our model with a neural renderer. A
    key feature of CORN is that it requires no ground truth 3D models or target
    view supervision. Regardless, CORN performs well on challenging tasks such as
    novel view synthesis and single-view 3D reconstruction and achieves performance
    comparable to state-of-the-art approaches that use direct supervision. For
    up-to-date information, data, and code, please see our project page:
    https://nicolaihaeni.github.io/corn/.},
    YEAR = {2020},
    MONTH = {Jul},
    URL = {http://arxiv.org/abs/2007.15627v2},
    FILE = {2007.15627v2.pdf}
}

@article{martin-brualla2020nerfw,
    AUTHOR = {Ricardo Martin-Brualla and Noha Radwan and Mehdi S. M. Sajjadi and Jonathan T. Barron and Alexey Dosovitskiy and Daniel Duckworth},
    TITLE = {NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo
    Collections},
    EPRINT = {2008.02268v3},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present a learning-based method for synthesizing novel views of complex
    scenes using only unstructured collections of in-the-wild photographs. We build
    on Neural Radiance Fields (NeRF), which uses the weights of a multilayer
    perceptron to model the density and color of a scene as a function of 3D
    coordinates. While NeRF works well on images of static subjects captured under
    controlled settings, it is incapable of modeling many ubiquitous, real-world
    phenomena in uncontrolled images, such as variable illumination or transient
    occluders. We introduce a series of extensions to NeRF to address these issues,
    thereby enabling accurate reconstructions from unstructured image collections
    taken from the internet. We apply our system, dubbed NeRF-W, to internet photo
    collections of famous landmarks, and demonstrate temporally consistent novel
    view renderings that are significantly closer to photorealism than the prior
    state of the art.},
    YEAR = {2020},
    MONTH = {Aug},
    URL = {http://arxiv.org/abs/2008.02268v3},
    FILE = {2008.02268v3.pdf}
}

@article{bi2020neural,
    AUTHOR = {Sai Bi and Zexiang Xu and Pratul Srinivasan and Ben Mildenhall and Kalyan Sunkavalli and Milos Hasan and Yannick Hold-Geoffroy and David Kriegman and Ravi Ramamoorthi},
    TITLE = {Neural Reflectance Fields for Appearance Acquisition},
    EPRINT = {2008.03824v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present Neural Reflectance Fields, a novel deep scene representation that
    encodes volume density, normal and reflectance properties at any 3D point in a
    scene using a fully-connected neural network. We combine this representation
    with a physically-based differentiable ray marching framework that can render
    images from a neural reflectance field under any viewpoint and light. We
    demonstrate that neural reflectance fields can be estimated from images
    captured with a simple collocated camera-light setup, and accurately model the
    appearance of real-world scenes with complex geometry and reflectance. Once
    estimated, they can be used to render photo-realistic images under novel
    viewpoint and (non-collocated) lighting conditions and accurately reproduce
    challenging effects like specularities, shadows and occlusions. This allows us
    to perform high-quality view synthesis and relighting that is significantly
    better than previous methods. We also demonstrate that we can compose the
    estimated neural reflectance field of a real scene with traditional scene
    models and render them using standard Monte Carlo rendering engines. Our work
    thus enables a complete pipeline from high-quality and practical appearance
    acquisition to 3D scene composition and rendering.},
    YEAR = {2020},
    MONTH = {Aug},
    URL = {http://arxiv.org/abs/2008.03824v2},
    FILE = {2008.03824v2.pdf}
}

@article{davies2020on,
    AUTHOR = {Thomas Davies and Derek Nowrouzezahrai and Alec Jacobson},
    TITLE = {On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes},
    EPRINT = {2009.09808v3},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.GR},
    ABSTRACT = {A neural implicit outputs a number indicating whether the given query point
    in space is inside, outside, or on a surface. Many prior works have focused on
    _latent-encoded_ neural implicits, where a latent vector encoding of a specific
    shape is also fed as input. While affording latent-space interpolation, this
    comes at the cost of reconstruction accuracy for any _single_ shape. Training a
    specific network for each 3D shape, a _weight-encoded_ neural implicit may
    forgo the latent vector and focus reconstruction accuracy on the details of a
    single shape. While previously considered as an intermediary representation for
    3D scanning tasks or as a toy-problem leading up to latent-encoding tasks,
    weight-encoded neural implicits have not yet been taken seriously as a 3D shape
    representation. In this paper, we establish that weight-encoded neural
    implicits meet the criteria of a first-class 3D shape representation. We
    introduce a suite of technical contributions to improve reconstruction
    accuracy, convergence, and robustness when learning the signed distance field
    induced by a polygonal mesh -- the _de facto_ standard representation. Viewed
    as a lossy compression, our conversion outperforms standard techniques from
    geometry processing. Compared to previous latent- and weight-encoded neural
    implicits we demonstrate superior robustness, scalability, and performance.},
    YEAR = {2020},
    MONTH = {Sep},
    URL = {http://arxiv.org/abs/2009.09808v3},
    FILE = {2009.09808v3.pdf}
}

@article{chibane2020implicit,
    AUTHOR = {Julian Chibane and Gerard Pons-Moll},
    TITLE = {Implicit Feature Networks for Texture Completion from Partial 3D Data},
    EPRINT = {2009.09458v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Prior work to infer 3D texture use either texture atlases, which require
    uv-mappings and hence have discontinuities, or colored voxels, which are memory
    inefficient and limited in resolution. Recent work, predicts RGB color at every
    XYZ coordinate forming a texture field, but focus on completing texture given a
    single 2D image. Instead, we focus on 3D texture and geometry completion from
    partial and incomplete 3D scans. IF-Nets have recently achieved
    state-of-the-art results on 3D geometry completion using a multi-scale deep
    feature encoding, but the outputs lack texture. In this work, we generalize
    IF-Nets to texture completion from partial textured scans of humans and
    arbitrary objects. Our key insight is that 3D texture completion benefits from
    incorporating local and global deep features extracted from both the 3D partial
    texture and completed geometry. Specifically, given the partial 3D texture and
    the 3D geometry completed with IF-Nets, our model successfully in-paints the
    missing texture parts in consistence with the completed geometry. Our model won
    the SHARP ECCV'20 challenge, achieving highest performance on all challenges.},
    YEAR = {2020},
    MONTH = {Sep},
    NOTE = {SHARP Workshop, European Conference on Computer Vision (ECCV),
    2020},
    URL = {http://arxiv.org/abs/2009.09458v1},
    FILE = {2009.09458v1.pdf}
}

@article{trevithick2020grf,
    AUTHOR = {Alex Trevithick and Bo Yang},
    TITLE = {GRF: Learning a General Radiance Field for 3D Representation and
    Rendering},
    EPRINT = {2010.04595v3},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present a simple yet powerful neural network that implicitly represents
    and renders 3D objects and scenes only from 2D observations. The network models
    3D geometries as a general radiance field, which takes a set of 2D images with
    camera poses and intrinsics as input, constructs an internal representation for
    each point of the 3D space, and then renders the corresponding appearance and
    geometry of that point viewed from an arbitrary position. The key to our
    approach is to learn local features for each pixel in 2D images and to then
    project these features to 3D points, thus yielding general and rich point
    representations. We additionally integrate an attention mechanism to aggregate
    pixel features from multiple 2D views, such that visual occlusions are
    implicitly taken into account. Extensive experiments demonstrate that our
    method can generate high-quality and realistic novel views for novel objects,
    unseen categories and challenging real-world scenes.},
    YEAR = {2020},
    MONTH = {Oct},
    URL = {http://arxiv.org/abs/2010.04595v3},
    FILE = {2010.04595v3.pdf}
}

@article{zhang2020nerf++,
    AUTHOR = {Kai Zhang and Gernot Riegler and Noah Snavely and Vladlen Koltun},
    TITLE = {NeRF++: Analyzing and Improving Neural Radiance Fields},
    EPRINT = {2010.07492v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a
    variety of capture settings, including 360 capture of bounded scenes and
    forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer
    perceptrons (MLPs) representing view-invariant opacity and view-dependent color
    volumes to a set of training images, and samples novel views based on volume
    rendering techniques. In this technical report, we first remark on radiance
    fields and their potential ambiguities, namely the shape-radiance ambiguity,
    and analyze NeRF's success in avoiding such ambiguities. Second, we address a
    parametrization issue involved in applying NeRF to 360 captures of objects
    within large-scale, unbounded 3D scenes. Our method improves view synthesis
    fidelity in this challenging scenario. Code is available at
    https://github.com/Kai-46/nerfplusplus.},
    YEAR = {2020},
    MONTH = {Oct},
    URL = {http://arxiv.org/abs/2010.07492v2},
    FILE = {2010.07492v2.pdf}
}

@article{lin2020sdfsrn,
    AUTHOR = {Chen-Hsuan Lin and Chaoyang Wang and Simon Lucey},
    TITLE = {SDF-SRN: Learning Signed Distance 3D Object Reconstruction from Static
    Images},
    EPRINT = {2010.10505v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Dense 3D object reconstruction from a single image has recently witnessed
    remarkable advances, but supervising neural networks with ground-truth 3D
    shapes is impractical due to the laborious process of creating paired
    image-shape datasets. Recent efforts have turned to learning 3D reconstruction
    without 3D supervision from RGB images with annotated 2D silhouettes,
    dramatically reducing the cost and effort of annotation. These techniques,
    however, remain impractical as they still require multi-view annotations of the
    same object instance during training. As a result, most experimental efforts to
    date have been limited to synthetic datasets. In this paper, we address this
    issue and propose SDF-SRN, an approach that requires only a single view of
    objects at training time, offering greater utility for real-world scenarios.
    SDF-SRN learns implicit 3D shape representations to handle arbitrary shape
    topologies that may exist in the datasets. To this end, we derive a novel
    differentiable rendering formulation for learning signed distance functions
    (SDF) from 2D silhouettes. Our method outperforms the state of the art under
    challenging single-view supervision settings on both synthetic and real-world
    datasets.},
    YEAR = {2020},
    MONTH = {Oct},
    URL = {http://arxiv.org/abs/2010.10505v1},
    FILE = {2010.10505v1.pdf}
}

@article{chibane2020ndf,
    AUTHOR = {Julian Chibane and Aymen Mir and Gerard Pons-Moll},
    TITLE = {Neural Unsigned Distance Fields for Implicit Function Learning},
    EPRINT = {2010.13938v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {In this work we target a learnable output representation that allows
    continuous, high resolution outputs of arbitrary shape. Recent works represent
    3D surfaces implicitly with a Neural Network, thereby breaking previous
    barriers in resolution, and ability to represent diverse topologies. However,
    neural implicit representations are limited to closed surfaces, which divide
    the space into inside and outside. Many real world objects such as walls of a
    scene scanned by a sensor, clothing, or a car with inner structures are not
    closed. This constitutes a significant barrier, in terms of data pre-processing
    (objects need to be artificially closed creating artifacts), and the ability to
    output open surfaces. In this work, we propose Neural Distance Fields (NDF), a
    neural network based model which predicts the unsigned distance field for
    arbitrary 3D shapes given sparse point clouds. NDF represent surfaces at high
    resolutions as prior implicit models, but do not require closed surface data,
    and significantly broaden the class of representable shapes in the output. NDF
    allow to extract the surface as very dense point clouds and as meshes. We also
    show that NDF allow for surface normal calculation and can be rendered using a
    slight modification of sphere tracing. We find NDF can be used for multi-target
    regression (multiple outputs for one input) with techniques that have been
    exclusively used for rendering in graphics. Experiments on ShapeNet show that
    NDF, while simple, is the state-of-the art, and allows to reconstruct shapes
    with inner structures, such as the chairs inside a bus. Notably, we show that
    NDF are not restricted to 3D shapes, and can approximate more general open
    surfaces such as curves, manifolds, and functions. Code is available for
    research at https://virtualhumans.mpi-inf.mpg.de/ndf/.},
    YEAR = {2020},
    MONTH = {Oct},
    NOTE = {Neural Information Processing Systems (NeurIPS) 2020},
    URL = {http://arxiv.org/abs/2010.13938v1},
    FILE = {2010.13938v1.pdf}
}

@article{ost2020neural,
    AUTHOR = {Julian Ost and Fahim Mannan and Nils Thuerey and Julian Knodt and Felix Heide},
    TITLE = {Neural Scene Graphs for Dynamic Scenes},
    EPRINT = {2011.10379v3},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Recent implicit neural rendering methods have demonstrated that it is
    possible to learn accurate view synthesis for complex scenes by predicting
    their volumetric density and color supervised solely by a set of RGB images.
    However, existing methods are restricted to learning efficient representations
    of static scenes that encode all scene objects into a single neural network,
    and lack the ability to represent dynamic scenes and decompositions into
    individual scene objects. In this work, we present the first neural rendering
    method that decomposes dynamic scenes into scene graphs. We propose a learned
    scene graph representation, which encodes object transformation and radiance,
    to efficiently render novel arrangements and views of the scene. To this end,
    we learn implicitly encoded scenes, combined with a jointly learned latent
    representation to describe objects with a single implicit function. We assess
    the proposed method on synthetic and real automotive data, validating that our
    approach learns dynamic scenes -- only by observing a video of this scene --
    and allows for rendering novel photo-realistic views of novel scene
    compositions with unseen sets of objects at unseen poses.},
    YEAR = {2020},
    MONTH = {Nov},
    URL = {http://arxiv.org/abs/2011.10379v3},
    FILE = {2011.10379v3.pdf}
}

@article{niemeyer2020giraffe,
    AUTHOR = {Michael Niemeyer and Andreas Geiger},
    TITLE = {GIRAFFE: Representing Scenes as Compositional Generative Neural Feature
    Fields},
    EPRINT = {2011.12100v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Deep generative models allow for photorealistic image synthesis at high
    resolutions. But for many applications, this is not enough: content creation
    also needs to be controllable. While several recent works investigate how to
    disentangle underlying factors of variation in the data, most of them operate
    in 2D and hence ignore that our world is three-dimensional. Further, only few
    works consider the compositional nature of scenes. Our key hypothesis is that
    incorporating a compositional 3D scene representation into the generative model
    leads to more controllable image synthesis. Representing scenes as
    compositional generative neural feature fields allows us to disentangle one or
    multiple objects from the background as well as individual objects' shapes and
    appearances while learning from unstructured and unposed image collections
    without any additional supervision. Combining this scene representation with a
    neural rendering pipeline yields a fast and realistic image synthesis model. As
    evidenced by our experiments, our model is able to disentangle individual
    objects and allows for translating and rotating them in the scene as well as
    changing the camera pose.},
    YEAR = {2020},
    MONTH = {Nov},
    URL = {http://arxiv.org/abs/2011.12100v2},
    FILE = {2011.12100v2.pdf}
}

@article{park2020dnerf, nerfies,
    AUTHOR = {Keunhong Park and Utkarsh Sinha and Jonathan T. Barron and Sofien Bouaziz and Dan B Goldman and Steven M. Seitz and Ricardo Martin-Brualla},
    TITLE = {Nerfies: Deformable Neural Radiance Fields},
    EPRINT = {2011.12948v4},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present the first method capable of photorealistically reconstructing
    deformable scenes using photos/videos captured casually from mobile phones. Our
    approach augments neural radiance fields (NeRF) by optimizing an additional
    continuous volumetric deformation field that warps each observed point into a
    canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone
    to local minima, and propose a coarse-to-fine optimization method for
    coordinate-based models that allows for more robust optimization. By adapting
    principles from geometry processing and physical simulation to NeRF-like
    models, we propose an elastic regularization of the deformation field that
    further improves robustness. We show that our method can turn casually captured
    selfie photos/videos into deformable NeRF models that allow for photorealistic
    renderings of the subject from arbitrary viewpoints, which we dub "nerfies." We
    evaluate our method by collecting time-synchronized data using a rig with two
    mobile phones, yielding train/validation images of the same pose at different
    viewpoints. We show that our method faithfully reconstructs non-rigidly
    deforming scenes and reproduces unseen views with high fidelity.},
    YEAR = {2020},
    MONTH = {Nov},
    URL = {http://arxiv.org/abs/2011.12948v4},
    FILE = {2011.12948v4.pdf}
}

@article{rebain2020derf,
    AUTHOR = {Daniel Rebain and Wei Jiang and Soroosh Yazdani and Ke Li and Kwang Moo Yi and Andrea Tagliasacchi},
    TITLE = {DeRF: Decomposed Radiance Fields},
    EPRINT = {2011.12490v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {With the advent of Neural Radiance Fields (NeRF), neural networks can now
    render novel views of a 3D scene with quality that fools the human eye. Yet,
    generating these images is very computationally intensive, limiting their
    applicability in practical scenarios. In this paper, we propose a technique
    based on spatial decomposition capable of mitigating this issue. Our key
    observation is that there are diminishing returns in employing larger (deeper
    and/or wider) networks. Hence, we propose to spatially decompose a scene and
    dedicate smaller networks for each decomposed part. When working together,
    these networks can render the whole scene. This allows us near-constant
    inference time regardless of the number of decomposed parts. Moreover, we show
    that a Voronoi spatial decomposition is preferable for this purpose, as it is
    provably compatible with the Painter's Algorithm for efficient and GPU-friendly
    rendering. Our experiments show that for real-world scenes, our method provides
    up to 3x more efficient inference than NeRF (with the same rendering quality),
    or an improvement of up to 1.0~dB in PSNR (for the same inference cost).},
    YEAR = {2020},
    MONTH = {Nov},
    URL = {http://arxiv.org/abs/2011.12490v1},
    FILE = {2011.12490v1.pdf}
}

@article{xian2020spacetime,
    AUTHOR = {Wenqi Xian and Jia-Bin Huang and Johannes Kopf and Changil Kim},
    TITLE = {Space-time Neural Irradiance Fields for Free-Viewpoint Video},
    EPRINT = {2011.12950v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present a method that learns a spatiotemporal neural irradiance field for
    dynamic scenes from a single video. Our learned representation enables
    free-viewpoint rendering of the input video. Our method builds upon recent
    advances in implicit representations. Learning a spatiotemporal irradiance
    field from a single video poses significant challenges because the video
    contains only one observation of the scene at any point in time. The 3D
    geometry of a scene can be legitimately represented in numerous ways since
    varying geometry (motion) can be explained with varying appearance and vice
    versa. We address this ambiguity by constraining the time-varying geometry of
    our dynamic scene representation using the scene depth estimated from video
    depth estimation methods, aggregating contents from individual frames into a
    single global representation. We provide an extensive quantitative evaluation
    and demonstrate compelling free-viewpoint rendering results.},
    YEAR = {2020},
    MONTH = {Nov},
    URL = {http://arxiv.org/abs/2011.12950v2},
    FILE = {2011.12950v2.pdf}
}

@article{li2020nsff,
    AUTHOR = {Zhengqi Li and Simon Niklaus and Noah Snavely and Oliver Wang},
    TITLE = {Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes},
    EPRINT = {2011.13084v3},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present a method to perform novel view and time synthesis of dynamic
    scenes, requiring only a monocular video with known camera poses as input. To
    do this, we introduce Neural Scene Flow Fields, a new representation that
    models the dynamic scene as a time-variant continuous function of appearance,
    geometry, and 3D scene motion. Our representation is optimized through a neural
    network to fit the observed input views. We show that our representation can be
    used for complex dynamic scenes, including thin structures, view-dependent
    effects, and natural degrees of motion. We conduct a number of experiments that
    demonstrate our approach significantly outperforms recent monocular view
    synthesis methods, and show qualitative results of space-time view synthesis on
    a variety of real-world videos.},
    YEAR = {2020},
    MONTH = {Nov},
    URL = {http://arxiv.org/abs/2011.13084v3},
    FILE = {2011.13084v3.pdf}
}

@article{pumarola2020dnerf,
    AUTHOR = {Albert Pumarola and Enric Corona and Gerard Pons-Moll and Francesc Moreno-Noguer},
    TITLE = {D-NeRF: Neural Radiance Fields for Dynamic Scenes},
    EPRINT = {2011.13961v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Neural rendering techniques combining machine learning with geometric
    reasoning have arisen as one of the most promising approaches for synthesizing
    novel views of a scene from a sparse set of images. Among these, stands out the
    Neural radiance fields (NeRF), which trains a deep network to map 5D input
    coordinates (representing spatial location and viewing direction) into a volume
    density and view-dependent emitted radiance. However, despite achieving an
    unprecedented level of photorealism on the generated images, NeRF is only
    applicable to static scenes, where the same spatial location can be queried
    from different images. In this paper we introduce D-NeRF, a method that extends
    neural radiance fields to a dynamic domain, allowing to reconstruct and render
    novel images of objects under rigid and non-rigid motions from a \emph{single}
    camera moving around the scene. For this purpose we consider time as an
    additional input to the system, and split the learning process in two main
    stages: one that encodes the scene into a canonical space and another that maps
    this canonical representation into the deformed scene at a particular time.
    Both mappings are simultaneously learned using fully-connected networks. Once
    the networks are trained, D-NeRF can render novel images, controlling both the
    camera view and the time variable, and thus, the object movement. We
    demonstrate the effectiveness of our approach on scenes with objects under
    rigid, articulated and non-rigid motions. Code, model weights and the dynamic
    scenes dataset will be released.},
    YEAR = {2020},
    MONTH = {Nov},
    URL = {http://arxiv.org/abs/2011.13961v1},
    FILE = {2011.13961v1.pdf}
}

@article{yenamandra2020i3dmm,
    AUTHOR = {Tarun Yenamandra and Ayush Tewari and Florian Bernard and Hans-Peter Seidel and Mohamed Elgharib and Daniel Cremers and Christian Theobalt},
    TITLE = {i3DMM: Deep Implicit 3D Morphable Model of Human Heads},
    EPRINT = {2011.14143v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present the first deep implicit 3D morphable model (i3DMM) of full heads.
    Unlike earlier morphable face models it not only captures identity-specific
    geometry, texture, and expressions of the frontal face, but also models the
    entire head, including hair. We collect a new dataset consisting of 64 people
    with different expressions and hairstyles to train i3DMM. Our approach has the
    following favorable properties: (i) It is the first full head morphable model
    that includes hair. (ii) In contrast to mesh-based models it can be trained on
    merely rigidly aligned scans, without requiring difficult non-rigid
    registration. (iii) We design a novel architecture to decouple the shape model
    into an implicit reference shape and a deformation of this reference shape.
    With that, dense correspondences between shapes can be learned implicitly. (iv)
    This architecture allows us to semantically disentangle the geometry and color
    components, as color is learned in the reference space. Geometry is further
    disentangled as identity, expressions, and hairstyle, while color is
    disentangled as identity and hairstyle components. We show the merits of i3DMM
    using ablation studies, comparisons to state-of-the-art models, and
    applications such as semantic head editing and texture transfer. We will make
    our model publicly available.},
    YEAR = {2020},
    MONTH = {Nov},
    URL = {http://arxiv.org/abs/2011.14143v1},
    FILE = {2011.14143v1.pdf}
}

@article{bozic2020neural deformation graphs,
    AUTHOR = {Aljaz Bozic and Pablo Palafox and Michael Zollhofer and Justus Thies and Angela Dai and Matthias Niessner},
    TITLE = {Neural Deformation Graphs for Globally-consistent Non-rigid
    Reconstruction},
    EPRINT = {2012.01451v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We introduce Neural Deformation Graphs for globally-consistent deformation
    tracking and 3D reconstruction of non-rigid objects. Specifically, we
    implicitly model a deformation graph via a deep neural network. This neural
    deformation graph does not rely on any object-specific structure and, thus, can
    be applied to general non-rigid deformation tracking. Our method globally
    optimizes this neural graph on a given sequence of depth camera observations of
    a non-rigidly moving object. Based on explicit viewpoint consistency as well as
    inter-frame graph and surface consistency constraints, the underlying network
    is trained in a self-supervised fashion. We additionally optimize for the
    geometry of the object with an implicit deformable multi-MLP shape
    representation. Our approach does not assume sequential input data, thus
    enabling robust tracking of fast motions or even temporally disconnected
    recordings. Our experiments demonstrate that our Neural Deformation Graphs
    outperform state-of-the-art non-rigid reconstruction approaches both
    qualitatively and quantitatively, with 64% improved reconstruction and 62%
    improved deformation tracking performance.},
    YEAR = {2020},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2012.01451v1},
    FILE = {2012.01451v1.pdf}
}

@article{yu2020pixelnerf,
    AUTHOR = {Alex Yu and Vickie Ye and Matthew Tancik and Angjoo Kanazawa},
    TITLE = {pixelNeRF: Neural Radiance Fields from One or Few Images},
    EPRINT = {2012.02190v3},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We propose pixelNeRF, a learning framework that predicts a continuous neural
    scene representation conditioned on one or few input images. The existing
    approach for constructing neural radiance fields involves optimizing the
    representation to every scene independently, requiring many calibrated views
    and significant compute time. We take a step towards resolving these
    shortcomings by introducing an architecture that conditions a NeRF on image
    inputs in a fully convolutional manner. This allows the network to be trained
    across multiple scenes to learn a scene prior, enabling it to perform novel
    view synthesis in a feed-forward manner from a sparse set of views (as few as
    one). Leveraging the volume rendering approach of NeRF, our model can be
    trained directly from images with no explicit 3D supervision. We conduct
    extensive experiments on ShapeNet benchmarks for single image novel view
    synthesis tasks with held-out objects as well as entire unseen categories. We
    further demonstrate the flexibility of pixelNeRF by demonstrating it on
    multi-object ShapeNet scenes and real scenes from the DTU dataset. In all
    cases, pixelNeRF outperforms current state-of-the-art baselines for novel view
    synthesis and single image 3D reconstruction. For the video and code, please
    visit the project website: https://alexyu.net/pixelnerf},
    YEAR = {2020},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2012.02190v3},
    FILE = {2012.02190v3.pdf}
}

@article{tancik2020learned,
    AUTHOR = {Matthew Tancik and Ben Mildenhall and Terrance Wang and Divi Schmidt and Pratul P. Srinivasan and Jonathan T. Barron and Ren Ng},
    TITLE = {Learned Initializations for Optimizing Coordinate-Based Neural
    Representations},
    EPRINT = {2012.02189v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Coordinate-based neural representations have shown significant promise as an
    alternative to discrete, array-based representations for complex low
    dimensional signals. However, optimizing a coordinate-based network from
    randomly initialized weights for each new signal is inefficient. We propose
    applying standard meta-learning algorithms to learn the initial weight
    parameters for these fully-connected networks based on the underlying class of
    signals being represented (e.g., images of faces or 3D models of chairs).
    Despite requiring only a minor change in implementation, using these learned
    initial weights enables faster convergence during optimization and can serve as
    a strong prior over the signal class being modeled, resulting in better
    generalization when only partial observations of a given signal are available.
    We explore these benefits across a variety of tasks, including representing 2D
    images, reconstructing CT scans, and recovering 3D shapes and scenes from 2D
    image observations.},
    YEAR = {2020},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2012.02189v2},
    FILE = {2012.02189v2.pdf}
}

@article{lindell2020autoint,
    AUTHOR = {David B. Lindell and Julien N. P. Martel and Gordon Wetzstein},
    TITLE = {AutoInt: Automatic Integration for Fast Neural Volume Rendering},
    EPRINT = {2012.01714v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Numerical integration is a foundational technique in scientific computing and
    is at the core of many computer vision applications. Among these applications,
    neural volume rendering has recently been proposed as a new paradigm for view
    synthesis, achieving photorealistic image quality. However, a fundamental
    obstacle to making these methods practical is the extreme computational and
    memory requirements caused by the required volume integrations along the
    rendered rays during training and inference. Millions of rays, each requiring
    hundreds of forward passes through a neural network are needed to approximate
    those integrations with Monte Carlo sampling. Here, we propose automatic
    integration, a new framework for learning efficient, closed-form solutions to
    integrals using coordinate-based neural networks. For training, we instantiate
    the computational graph corresponding to the derivative of the network. The
    graph is fitted to the signal to integrate. After optimization, we reassemble
    the graph to obtain a network that represents the antiderivative. By the
    fundamental theorem of calculus, this enables the calculation of any definite
    integral in two evaluations of the network. Applying this approach to neural
    rendering, we improve a tradeoff between rendering speed and image quality:
    improving render times by greater than 10 times with a tradeoff of slightly
    reduced image quality.},
    YEAR = {2020},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2012.01714v2},
    FILE = {2012.01714v2.pdf}
}

@article{gafni2020nerface,
    AUTHOR = {Guy Gafni and Justus Thies and Michael Zollhofer and Matthias Niessner},
    TITLE = {Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar
    Reconstruction},
    EPRINT = {2012.03065v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present dynamic neural radiance fields for modeling the appearance and
    dynamics of a human face. Digitally modeling and reconstructing a talking human
    is a key building-block for a variety of applications. Especially, for
    telepresence applications in AR or VR, a faithful reproduction of the
    appearance including novel viewpoints or head-poses is required. In contrast to
    state-of-the-art approaches that model the geometry and material properties
    explicitly, or are purely image-based, we introduce an implicit representation
    of the head based on scene representation networks. To handle the dynamics of
    the face, we combine our scene representation network with a low-dimensional
    morphable model which provides explicit control over pose and expressions. We
    use volumetric rendering to generate images from this hybrid representation and
    demonstrate that such a dynamic neural scene representation can be learned from
    monocular input data only, without the need of a specialized capture setup. In
    our experiments, we show that this learned volumetric representation allows for
    photo-realistic image generation that surpasses the quality of state-of-the-art
    video-based reenactment methods.},
    YEAR = {2020},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2012.03065v1},
    FILE = {2012.03065v1.pdf}
}

@article{srinivasan2020nerv,
    AUTHOR = {Pratul P. Srinivasan and Boyang Deng and Xiuming Zhang and Matthew Tancik and Ben Mildenhall and Jonathan T. Barron},
    TITLE = {NeRV: Neural Reflectance and Visibility Fields for Relighting and View
    Synthesis},
    EPRINT = {2012.03927v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present a method that takes as input a set of images of a scene
    illuminated by unconstrained known lighting, and produces as output a 3D
    representation that can be rendered from novel viewpoints under arbitrary
    lighting conditions. Our method represents the scene as a continuous volumetric
    function parameterized as MLPs whose inputs are a 3D location and whose outputs
    are the following scene properties at that input location: volume density,
    surface normal, material parameters, distance to the first surface intersection
    in any direction, and visibility of the external environment in any direction.
    Together, these allow us to render novel views of the object under arbitrary
    lighting, including indirect illumination effects. The predicted visibility and
    surface intersection fields are critical to our model's ability to simulate
    direct and indirect illumination during training, because the brute-force
    techniques used by prior work are intractable for lighting conditions outside
    of controlled setups with a single light. Our method outperforms alternative
    approaches for recovering relightable 3D scene representations, and performs
    well in complex lighting settings that have posed a significant challenge to
    prior work.},
    YEAR = {2020},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2012.03927v1},
    FILE = {2012.03927v1.pdf}
}

@article{boss2020nerd,
    AUTHOR = {Mark Boss and Raphael Braun and Varun Jampani and Jonathan T. Barron and Ce Liu and Hendrik P. A. Lensch},
    TITLE = {NeRD: Neural Reflectance Decomposition from Image Collections},
    EPRINT = {2012.03918v3},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Decomposing a scene into its shape, reflectance, and illumination is a
    challenging but essential problem in computer vision and graphics. This problem
    is inherently more challenging when the illumination is not a single light
    source under laboratory conditions but is instead an unconstrained
    environmental illumination. Though recent work has shown that implicit
    representations can be used to model the radiance field of an object, these
    techniques only enable view synthesis and not relighting. Additionally,
    evaluating these radiance fields is resource and time-intensive. By decomposing
    a scene into explicit representations, any rendering framework can be leveraged
    to generate novel views under any illumination in real-time. NeRD is a method
    that achieves this decomposition by introducing physically-based rendering to
    neural radiance fields. Even challenging non-Lambertian reflectances, complex
    geometry, and unknown illumination can be decomposed into high-quality models.
    The datasets and code is available on the project page:
    https://markboss.me/publication/2021-nerd/},
    YEAR = {2020},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2012.03918v3},
    FILE = {2012.03918v3.pdf}
}

@article{yen-chen2020inerf,
    AUTHOR = {Lin Yen-Chen and Pete Florence and Jonathan T. Barron and Alberto Rodriguez and Phillip Isola and Tsung-Yi Lin},
    TITLE = {INeRF: Inverting Neural Radiance Fields for Pose Estimation},
    EPRINT = {2012.05877v3},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present iNeRF, a framework that performs mesh-free pose estimation by
    "inverting" a Neural RadianceField (NeRF). NeRFs have been shown to be
    remarkably effective for the task of view synthesis - synthesizing
    photorealistic novel views of real-world scenes or objects. In this work, we
    investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free,
    RGB-only 6DoF pose estimation - given an image, find the translation and
    rotation of a camera relative to a 3D object or scene. Our method assumes that
    no object mesh models are available during either training or test time.
    Starting from an initial pose estimate, we use gradient descent to minimize the
    residual between pixels rendered from a NeRF and pixels in an observed image.
    In our experiments, we first study 1) how to sample rays during pose refinement
    for iNeRF to collect informative gradients and 2) how different batch sizes of
    rays affect iNeRF on a synthetic dataset. We then show that for complex
    real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating
    the camera poses of novel images and using these images as additional training
    data for NeRF. Finally, we show iNeRF can perform category-level object pose
    estimation, including object instances not seen during training, with RGB
    images by inverting a NeRF model inferred from a single view.},
    YEAR = {2020},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2012.05877v3},
    FILE = {2012.05877v3.pdf}
}

@article{gao2020portraitnerf,
    AUTHOR = {Chen Gao and Yichang Shih and Wei-Sheng Lai and Chia-Kai Liang and Jia-Bin Huang},
    TITLE = {Portrait Neural Radiance Fields from a Single Image},
    EPRINT = {2012.05903v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present a method for estimating Neural Radiance Fields (NeRF) from a
    single headshot portrait. While NeRF has demonstrated high-quality view
    synthesis, it requires multiple images of static scenes and thus impractical
    for casual captures and moving subjects. In this work, we propose to pretrain
    the weights of a multilayer perceptron (MLP), which implicitly models the
    volumetric density and colors, with a meta-learning framework using a light
    stage portrait dataset. To improve the generalization to unseen faces, we train
    the MLP in the canonical coordinate space approximated by 3D face morphable
    models. We quantitatively evaluate the method using controlled captures and
    demonstrate the generalization to real portrait images, showing favorable
    results against state-of-the-arts.},
    YEAR = {2020},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2012.05903v2},
    FILE = {2012.05903v2.pdf}
}

@article{yifan2020isopoints,
    AUTHOR = {Wang Yifan and Shihao Wu and Cengiz Oztireli and Olga Sorkine-Hornung},
    TITLE = {Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid
    Representations},
    EPRINT = {2012.06434v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Neural implicit functions have emerged as a powerful representation for
    surfaces in 3D. Such a function can encode a high quality surface with
    intricate details into the parameters of a deep neural network. However,
    optimizing for the parameters for accurate and robust reconstructions remains a
    challenge, especially when the input data is noisy or incomplete. In this work,
    we develop a hybrid neural surface representation that allows us to impose
    geometry-aware sampling and regularization, which significantly improves the
    fidelity of reconstructions. We propose to use \emph{iso-points} as an explicit
    representation for a neural implicit function. These points are computed and
    updated on-the-fly during training to capture important geometric features and
    impose geometric constraints on the optimization. We demonstrate that our
    method can be adopted to improve state-of-the-art techniques for reconstructing
    neural implicit surfaces from multi-view images or point clouds. Quantitative
    and qualitative evaluations show that, compared with existing sampling and
    optimization methods, our approach allows faster convergence, better
    generalization, and accurate recovery of details and topology.},
    YEAR = {2020},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2012.06434v2},
    FILE = {2012.06434v2.pdf}
}

@article{yang2020deep,
    AUTHOR = {Mingyue Yang and Yuxin Wen and Weikai Chen and Yongwei Chen and Kui Jia},
    TITLE = {Deep Optimized Priors for 3D Shape Modeling and Reconstruction},
    EPRINT = {2012.07241v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Many learning-based approaches have difficulty scaling to unseen data, as the
    generality of its learned prior is limited to the scale and variations of the
    training samples. This holds particularly true with 3D learning tasks, given
    the sparsity of 3D datasets available. We introduce a new learning framework
    for 3D modeling and reconstruction that greatly improves the generalization
    ability of a deep generator. Our approach strives to connect the good ends of
    both learning-based and optimization-based methods. In particular, unlike the
    common practice that fixes the pre-trained priors at test time, we propose to
    further optimize the learned prior and latent code according to the input
    physical measurements after the training. We show that the proposed strategy
    effectively breaks the barriers constrained by the pre-trained priors and could
    lead to high-quality adaptation to unseen data. We realize our framework using
    the implicit surface representation and validate the efficacy of our approach
    in a variety of challenging tasks that take highly sparse or collapsed
    observations as input. Experimental results show that our approach compares
    favorably with the state-of-the-art methods in terms of both generality and
    accuracy.},
    YEAR = {2020},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2012.07241v1},
    FILE = {2012.07241v1.pdf}
}

@article{guo2020osfs,
    AUTHOR = {Michelle Guo and Alireza Fathi and Jiajun Wu and Thomas Funkhouser},
    TITLE = {Object-Centric Neural Scene Rendering},
    EPRINT = {2012.08503v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present a method for composing photorealistic scenes from captured images
    of objects. Our work builds upon neural radiance fields (NeRFs), which
    implicitly model the volumetric density and directionally-emitted radiance of a
    scene. While NeRFs synthesize realistic pictures, they only model static scenes
    and are closely tied to specific imaging conditions. This property makes NeRFs
    hard to generalize to new scenarios, including new lighting or new arrangements
    of objects. Instead of learning a scene radiance field as a NeRF does, we
    propose to learn object-centric neural scattering functions (OSFs), a
    representation that models per-object light transport implicitly using a
    lighting- and view-dependent neural network. This enables rendering scenes even
    when objects or lights move, without retraining. Combined with a volumetric
    path tracing procedure, our framework is capable of rendering both intra- and
    inter-object light transport effects including occlusions, specularities,
    shadows, and indirect illumination. We evaluate our approach on scene
    composition and show that it generalizes to novel illumination conditions,
    producing photorealistic, physically accurate renderings of multi-object
    scenes.},
    YEAR = {2020},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2012.08503v1},
    FILE = {2012.08503v1.pdf}
}

@article{du2020nerflow,
    AUTHOR = {Yilun Du and Yinan Zhang and Hong-Xing Yu and Joshua B. Tenenbaum and Jiajun Wu},
    TITLE = {Neural Radiance Flow for 4D View Synthesis and Video Processing},
    EPRINT = {2012.09790v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present a method, Neural Radiance Flow (NeRFlow),to learn a 4D
    spatial-temporal representation of a dynamic scene from a set of RGB images.
    Key to our approach is the use of a neural implicit representation that learns
    to capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing
    consistency across different modalities, our representation enables multi-view
    rendering in diverse dynamic scenes, including water pouring, robotic
    interaction, and real images, outperforming state-of-the-art methods for
    spatial-temporal view synthesis. Our approach works even when inputs images are
    captured with only one camera. We further demonstrate that the learned
    representation can serve as an implicit scene prior, enabling video processing
    tasks such as image super-resolution and de-noising without any additional
    supervision.},
    YEAR = {2020},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2012.09790v1},
    FILE = {2012.09790v1.pdf}
}

@article{wang2020learning,
    AUTHOR = {Ziyan Wang and Timur Bagautdinov and Stephen Lombardi and Tomas Simon and Jason Saragih and Jessica Hodgins and Michael Zollhofer},
    TITLE = {Learning Compositional Radiance Fields of Dynamic Human Heads},
    EPRINT = {2012.09955v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Photorealistic rendering of dynamic humans is an important ability for
    telepresence systems, virtual shopping, synthetic data generation, and more.
    Recently, neural rendering methods, which combine techniques from computer
    graphics and machine learning, have created high-fidelity models of humans and
    objects. Some of these methods do not produce results with high-enough fidelity
    for driveable human models (Neural Volumes) whereas others have extremely long
    rendering times (NeRF). We propose a novel compositional 3D representation that
    combines the best of previous methods to produce both higher-resolution and
    faster results. Our representation bridges the gap between discrete and
    continuous volumetric representations by combining a coarse 3D-structure-aware
    grid of animation codes with a continuous learned scene function that maps
    every position and its corresponding local animation code to its view-dependent
    emitted radiance and local volume density. Differentiable volume rendering is
    employed to compute photo-realistic novel views of the human head and upper
    body as well as to train our novel representation end-to-end using only 2D
    supervision. In addition, we show that the learned dynamic radiance field can
    be used to synthesize novel unseen expressions based on a global animation
    code. Our approach achieves state-of-the-art results for synthesizing novel
    views of dynamic human heads and the upper body.},
    YEAR = {2020},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2012.09955v1},
    FILE = {2012.09955v1.pdf}
}

@article{tretschk2020nrnerf,
    AUTHOR = {Edgar Tretschk and Ayush Tewari and Vladislav Golyanik and Michael Zollhofer and Christoph Lassner and Christian Theobalt},
    TITLE = {Non-Rigid Neural Radiance Fields: Reconstruction and Novel View
    Synthesis of a Dynamic Scene From Monocular Video},
    EPRINT = {2012.12247v4},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and
    novel view synthesis approach for general non-rigid dynamic scenes. Our
    approach takes RGB images of a dynamic scene as input (e.g., from a monocular
    video recording), and creates a high-quality space-time geometry and appearance
    representation. We show that a single handheld consumer-grade camera is
    sufficient to synthesize sophisticated renderings of a dynamic scene from novel
    virtual camera views, e.g. a `bullet-time' video effect. NR-NeRF disentangles
    the dynamic scene into a canonical volume and its deformation. Scene
    deformation is implemented as ray bending, where straight rays are deformed
    non-rigidly. We also propose a novel rigidity network to better constrain rigid
    regions of the scene, leading to more stable results. The ray bending and
    rigidity network are trained without explicit supervision. Our formulation
    enables dense correspondence estimation across views and time, and compelling
    video editing applications such as motion exaggeration. Our code will be open
    sourced.},
    YEAR = {2020},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2012.12247v4},
    FILE = {2012.12247v4.pdf}
}

@article{yuan2020star,
    AUTHOR = {Wentao Yuan and Zhaoyang Lv and Tanner Schmidt and Steven Lovegrove},
    TITLE = {STaR: Self-supervised Tracking and Reconstruction of Rigid Objects in
    Motion with Neural Rendering},
    EPRINT = {2101.01602v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present STaR, a novel method that performs Self-supervised Tracking and
    Reconstruction of dynamic scenes with rigid motion from multi-view RGB videos
    without any manual annotation. Recent work has shown that neural networks are
    surprisingly effective at the task of compressing many views of a scene into a
    learned function which maps from a viewing ray to an observed radiance value
    via volume rendering. Unfortunately, these methods lose all their predictive
    power once any object in the scene has moved. In this work, we explicitly model
    rigid motion of objects in the context of neural representations of radiance
    fields. We show that without any additional human specified supervision, we can
    reconstruct a dynamic scene with a single rigid object in motion by
    simultaneously decomposing it into its two constituent parts and encoding each
    with its own neural representation. We achieve this by jointly optimizing the
    parameters of two neural radiance fields and a set of rigid poses which align
    the two fields at each frame. On both synthetic and real world datasets, we
    demonstrate that our method can render photorealistic novel views, where
    novelty is measured on both spatial and temporal axes. Our factored
    representation furthermore enables animation of unseen object motion.},
    YEAR = {2020},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2101.01602v1},
    FILE = {2101.01602v1.pdf}
}

@article{peng2020neural body,
    AUTHOR = {Sida Peng and Yuanqing Zhang and Yinghao Xu and Qianqian Wang and Qing Shuai and Hujun Bao and Xiaowei Zhou},
    TITLE = {Neural Body: Implicit Neural Representations with Structured Latent
    Codes for Novel View Synthesis of Dynamic Humans},
    EPRINT = {2012.15838v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {This paper addresses the challenge of novel view synthesis for a human
    performer from a very sparse set of camera views. Some recent works have shown
    that learning implicit neural representations of 3D scenes achieves remarkable
    view synthesis quality given dense input views. However, the representation
    learning will be ill-posed if the views are highly sparse. To solve this
    ill-posed problem, our key idea is to integrate observations over video frames.
    To this end, we propose Neural Body, a new human body representation which
    assumes that the learned neural representations at different frames share the
    same set of latent codes anchored to a deformable mesh, so that the
    observations across frames can be naturally integrated. The deformable mesh
    also provides geometric guidance for the network to learn 3D representations
    more efficiently. To evaluate our approach, we create a multi-view dataset
    named ZJU-MoCap that captures performers with complex motions. Experiments on
    ZJU-MoCap show that our approach outperforms prior works by a large margin in
    terms of novel view synthesis quality. We also demonstrate the capability of
    our approach to reconstruct a moving person from a monocular video on the
    People-Snapshot dataset. The code and dataset are available at
    https://zju3dv.github.io/neuralbody/.},
    YEAR = {2020},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2012.15838v2},
    FILE = {2012.15838v2.pdf}
}

@article{shen2021nonlineofsight,
    AUTHOR = {Siyuan Shen and Zi Wang and Ping Liu and Zhengqing Pan and Ruiqian Li and Tian Gao and Shiying Li and Jingyi Yu},
    TITLE = {Non-line-of-Sight Imaging via Neural Transient Fields},
    EPRINT = {2101.00373v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {eess.IV},
    ABSTRACT = {We present a neural modeling framework for Non-Line-of-Sight (NLOS) imaging.
    Previous solutions have sought to explicitly recover the 3D geometry (e.g., as
    point clouds) or voxel density (e.g., within a pre-defined volume) of the
    hidden scene. In contrast, inspired by the recent Neural Radiance Field (NeRF)
    approach, we use a multi-layer perceptron (MLP) to represent the neural
    transient field or NeTF. However, NeTF measures the transient over spherical
    wavefronts rather than the radiance along lines. We therefore formulate a
    spherical volume NeTF reconstruction pipeline, applicable to both confocal and
    non-confocal setups. Compared with NeRF, NeTF samples a much sparser set of
    viewpoints (scanning spots) and the sampling is highly uneven. We thus
    introduce a Monte Carlo technique to improve the robustness in the
    reconstruction. Comprehensive experiments on synthetic and real datasets
    demonstrate NeTF provides higher quality reconstruction and preserves fine
    details largely missing in the state-of-the-art.},
    YEAR = {2021},
    MONTH = {Jan},
    URL = {http://arxiv.org/abs/2101.00373v2},
    FILE = {2101.00373v2.pdf}
}

@article{raj2021pva,
    AUTHOR = {Amit Raj and Michael Zollhoefer and Tomas Simon and Jason Saragih and Shunsuke Saito and James Hays and Stephen Lombardi},
    TITLE = {PVA: Pixel-aligned Volumetric Avatars},
    EPRINT = {2101.02697v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Acquisition and rendering of photo-realistic human heads is a highly
    challenging research problem of particular importance for virtual telepresence.
    Currently, the highest quality is achieved by volumetric approaches trained in
    a person specific manner on multi-view data. These models better represent fine
    structure, such as hair, compared to simpler mesh-based models. Volumetric
    models typically employ a global code to represent facial expressions, such
    that they can be driven by a small set of animation parameters. While such
    architectures achieve impressive rendering quality, they can not easily be
    extended to the multi-identity setting. In this paper, we devise a novel
    approach for predicting volumetric avatars of the human head given just a small
    number of inputs. We enable generalization across identities by a novel
    parameterization that combines neural radiance fields with local, pixel-aligned
    features extracted directly from the inputs, thus sidestepping the need for
    very deep or complex networks. Our approach is trained in an end-to-end manner
    solely based on a photometric re-rendering loss without requiring explicit 3D
    supervision.We demonstrate that our approach outperforms the existing state of
    the art in terms of quality and is able to generate faithful facial expressions
    in a multi-identity setting.},
    YEAR = {2021},
    MONTH = {Jan},
    URL = {http://arxiv.org/abs/2101.02697v1},
    FILE = {2101.02697v1.pdf}
}

@article{takikawa2021neural,
    AUTHOR = {Towaki Takikawa and Joey Litalien and Kangxue Yin and Karsten Kreis and Charles Loop and Derek Nowrouzezahrai and Alec Jacobson and Morgan McGuire and Sanja Fidler},
    TITLE = {Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D
    Shapes},
    EPRINT = {2101.10994v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Neural signed distance functions (SDFs) are emerging as an effective
    representation for 3D shapes. State-of-the-art methods typically encode the SDF
    with a large, fixed-size neural network to approximate complex shapes with
    implicit surfaces. Rendering with these large networks is, however,
    computationally expensive since it requires many forward passes through the
    network for every pixel, making these representations impractical for real-time
    graphics. We introduce an efficient neural representation that, for the first
    time, enables real-time rendering of high-fidelity neural SDFs, while achieving
    state-of-the-art geometry reconstruction quality. We represent implicit
    surfaces using an octree-based feature volume which adaptively fits shapes with
    multiple discrete levels of detail (LODs), and enables continuous LOD with SDF
    interpolation. We further develop an efficient algorithm to directly render our
    novel neural SDF representation in real-time by querying only the necessary
    LODs with sparse octree traversal. We show that our representation is 2-3
    orders of magnitude more efficient in terms of rendering speed compared to
    previous works. Furthermore, it produces state-of-the-art reconstruction
    quality for complex shapes under both 3D geometric and 2D image-space metrics.},
    YEAR = {2021},
    MONTH = {Jan},
    URL = {http://arxiv.org/abs/2101.10994v1},
    FILE = {2101.10994v1.pdf}
}

@article{costain2021towards,
    AUTHOR = {Theo W. Costain and Victor Adrian Prisacariu},
    TITLE = {Towards Generalising Neural Implicit Representations},
    EPRINT = {2101.12690v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Neural implicit representations have shown substantial improvements in
    efficiently storing 3D data, when compared to conventional formats. However,
    the focus of existing work has mainly been on storage and subsequent
    reconstruction. In this work, we show that training neural representations for
    reconstruction tasks alongside conventional tasks can produce more general
    encodings that admit equal quality reconstructions to single task training,
    whilst improving results on conventional tasks when compared to single task
    encodings. We reformulate the semantic segmentation task, creating a more
    representative task for implicit representation contexts, and through
    multi-task experiments on reconstruction, classification, and segmentation,
    show our approach learns feature rich encodings that admit equal performance
    for each task.},
    YEAR = {2021},
    MONTH = {Jan},
    URL = {http://arxiv.org/abs/2101.12690v2},
    FILE = {2101.12690v2.pdf}
}

@article{sun2021coil,
    AUTHOR = {Yu Sun and Jiaming Liu and Mingyang Xie and Brendt Wohlberg and Ulugbek S. Kamilov},
    TITLE = {CoIL: Coordinate-based Internal Learning for Imaging Inverse Problems},
    EPRINT = {2102.05181v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {eess.IV},
    ABSTRACT = {We propose Coordinate-based Internal Learning (CoIL) as a new deep-learning
    (DL) methodology for the continuous representation of measurements. Unlike
    traditional DL methods that learn a mapping from the measurements to the
    desired image, CoIL trains a multilayer perceptron (MLP) to encode the complete
    measurement field by mapping the coordinates of the measurements to their
    responses. CoIL is a self-supervised method that requires no training examples
    besides the measurements of the test object itself. Once the MLP is trained,
    CoIL generates new measurements that can be used within a majority of image
    reconstruction methods. We validate CoIL on sparse-view computed tomography
    using several widely-used reconstruction methods, including purely model-based
    methods and those based on DL. Our results demonstrate the ability of CoIL to
    consistently improve the performance of all the considered methods by providing
    high-fidelity measurement fields.},
    YEAR = {2021},
    MONTH = {Feb},
    URL = {http://arxiv.org/abs/2102.05181v1},
    FILE = {2102.05181v1.pdf}
}

@article{su2021anerf,
    AUTHOR = {Shih-Yang Su and Frank Yu and Michael Zollhoefer and Helge Rhodin},
    TITLE = {A-NeRF: Surface-free Human 3D Pose Refinement via Neural Rendering},
    EPRINT = {2102.06199v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {While deep learning has reshaped the classical motion capture pipeline,
    generative, analysis-by-synthesis elements are still in use to recover fine
    details if a high-quality 3D model of the user is available. Unfortunately,
    obtaining such a model for every user a priori is challenging, time-consuming,
    and limits the application scenarios. We propose a novel test-time optimization
    approach for monocular motion capture that learns a volumetric body model of
    the user in a self-supervised manner. To this end, our approach combines the
    advantages of neural radiance fields with an articulated skeleton
    representation. Our proposed skeleton embedding serves as a common reference
    that links constraints across time, thereby reducing the number of required
    camera views from traditionally dozens of calibrated cameras, down to a single
    uncalibrated one. As a starting point, we employ the output of an off-the-shelf
    model that predicts the 3D skeleton pose. The volumetric body shape and
    appearance is then learned from scratch, while jointly refining the initial
    pose estimate. Our approach is self-supervised and does not require any
    additional ground truth labels for appearance, pose, or 3D shape. We
    demonstrate that our novel combination of a discriminative pose estimation
    technique with surface-free analysis-by-synthesis outperforms purely
    discriminative monocular pose estimation approaches and generalizes well to
    multiple views.},
    YEAR = {2021},
    MONTH = {Feb},
    URL = {http://arxiv.org/abs/2102.06199v1},
    FILE = {2102.06199v1.pdf}
}

@article{wang2021nerf--,
    AUTHOR = {Zirui Wang and Shangzhe Wu and Weidi Xie and Min Chen and Victor Adrian Prisacariu},
    TITLE = {NeRF--: Neural Radiance Fields Without Known Camera Parameters},
    EPRINT = {2102.07064v3},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {This paper tackles the problem of novel view synthesis (NVS) from 2D images
    without known camera poses and intrinsics. Among various NVS techniques, Neural
    Radiance Field (NeRF) has recently gained popularity due to its remarkable
    synthesis quality. Existing NeRF-based approaches assume that the camera
    parameters associated with each input image are either directly accessible at
    training, or can be accurately estimated with conventional techniques based on
    correspondences, such as Structure-from-Motion. In this work, we propose an
    end-to-end framework, termed NeRF--, for training NeRF models given only RGB
    images, without pre-computed camera parameters. Specifically, we show that the
    camera parameters, including both intrinsics and extrinsics, can be
    automatically discovered via joint optimisation during the training of the NeRF
    model. On the standard LLFF benchmark, our model achieves comparable novel view
    synthesis results compared to the baseline trained with COLMAP pre-computed
    camera parameters. We also conduct extensive analyses to understand the model
    behaviour under different camera trajectories, and show that in scenarios where
    COLMAP fails, our model still produces robust results.},
    YEAR = {2021},
    MONTH = {Feb},
    URL = {http://arxiv.org/abs/2102.07064v3},
    FILE = {2102.07064v3.pdf}
}

@article{rematas2021sharf,
    AUTHOR = {Konstantinos Rematas and Ricardo Martin-Brualla and Vittorio Ferrari},
    TITLE = {ShaRF: Shape-conditioned Radiance Fields from a Single View},
    EPRINT = {2102.08860v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present a method for estimating neural scenes representations of objects
    given only a single image. The core of our method is the estimation of a
    geometric scaffold for the object and its use as a guide for the reconstruction
    of the underlying radiance field. Our formulation is based on a generative
    process that first maps a latent code to a voxelized shape, and then renders it
    to an image, with the object appearance being controlled by a second latent
    code. During inference, we optimize both the latent codes and the networks to
    fit a test image of a new object. The explicit disentanglement of shape and
    appearance allows our model to be fine-tuned given a single image. We can then
    render new views in a geometrically consistent manner and they represent
    faithfully the input object. Additionally, our method is able to generalize to
    images outside of the training domain (more realistic renderings and even real
    photographs). Finally, the inferred geometric scaffold is itself an accurate
    estimate of the object's 3D shape. We demonstrate in several experiments the
    effectiveness of our approach in both synthetic and real images.},
    YEAR = {2021},
    MONTH = {Feb},
    URL = {http://arxiv.org/abs/2102.08860v2},
    FILE = {2102.08860v2.pdf}
}

@article{zehnder2021ntopo,
    AUTHOR = {Jonas Zehnder and Yue Li and Stelian Coros and Bernhard Thomaszewski},
    TITLE = {NTopo: Mesh-free Topology Optimization using Implicit Neural
    Representations},
    EPRINT = {2102.10782v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.LG},
    ABSTRACT = {Recent advances in implicit neural representations show great promise when it
    comes to generating numerical solutions to partial differential equations
    (PDEs). Compared to conventional alternatives, such representations employ
    parameterized neural networks to define, in a mesh-free manner, signals that
    are highly-detailed, continuous, and fully differentiable. Most prior works aim
    to exploit these benefits in order to solve PDE-governed forward problems, or
    associated inverse problems that are defined by a small number of parameters.
    In this work, we present a novel machine learning approach to tackle topology
    optimization (TO) problems. Topology optimization refers to an important class
    of inverse problems that typically feature very high-dimensional parameter
    spaces and objective landscapes which are highly non-linear. To effectively
    leverage neural representations in the context of TO problems, we use
    multilayer perceptrons (MLPs) to parameterize both density and displacement
    fields. Using sensitivity analysis with a moving mean squared error, we show
    that our formulation can be used to efficiently minimize traditional structural
    compliance objectives. As we show through our experiments, a major benefit of
    our approach is that it enables self-supervised learning of continuous solution
    spaces to topology optimization problems.},
    YEAR = {2021},
    MONTH = {Feb},
    URL = {http://arxiv.org/abs/2102.10782v1},
    FILE = {2102.10782v1.pdf}
}

@article{wang2021ibrnet,
    AUTHOR = {Qianqian Wang and Zhicheng Wang and Kyle Genova and Pratul Srinivasan and Howard Zhou and Jonathan T. Barron and Ricardo Martin-Brualla and Noah Snavely and Thomas Funkhouser},
    TITLE = {IBRNet: Learning Multi-View Image-Based Rendering},
    EPRINT = {2102.13090v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present a method that synthesizes novel views of complex scenes by
    interpolating a sparse set of nearby views. The core of our method is a network
    architecture that includes a multilayer perceptron and a ray transformer that
    estimates radiance and volume density at continuous 5D locations (3D spatial
    locations and 2D viewing directions), drawing appearance information on the fly
    from multiple source views. By drawing on source views at render time, our
    method hearkens back to classic work on image-based rendering (IBR), and allows
    us to render high-resolution imagery. Unlike neural scene representation work
    that optimizes per-scene functions for rendering, we learn a generic view
    interpolation function that generalizes to novel scenes. We render images using
    classic volume rendering, which is fully differentiable and allows us to train
    using only multi-view posed images as supervision. Experiments show that our
    method outperforms recent novel view synthesis methods that also seek to
    generalize to novel scenes. Further, if fine-tuned on each scene, our method is
    competitive with state-of-the-art single-scene neural rendering methods.
    Project page: https://ibrnet.github.io/},
    YEAR = {2021},
    MONTH = {Feb},
    URL = {http://arxiv.org/abs/2102.13090v2},
    FILE = {2102.13090v2.pdf}
}

@article{xiang2021neutex,
    AUTHOR = {Fanbo Xiang and Zexiang Xu and Milos Hasan and Yannick Hold-Geoffroy and Kalyan Sunkavalli and Hao Su},
    TITLE = {NeuTex: Neural Texture Mapping for Volumetric Neural Rendering},
    EPRINT = {2103.00762v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Recent work has demonstrated that volumetric scene representations combined
    with differentiable volume rendering can enable photo-realistic rendering for
    challenging scenes that mesh reconstruction fails on. However, these methods
    entangle geometry and appearance in a "black-box" volume that cannot be edited.
    Instead, we present an approach that explicitly disentangles
    geometry--represented as a continuous 3D volume--from appearance--represented
    as a continuous 2D texture map. We achieve this by introducing a 3D-to-2D
    texture mapping (or surface parameterization) network into volumetric
    representations. We constrain this texture mapping network using an additional
    2D-to-3D inverse mapping network and a novel cycle consistency loss to make 3D
    surface points map to 2D texture points that map back to the original 3D
    points. We demonstrate that this representation can be reconstructed using only
    multi-view image supervision and generates high-quality rendering results. More
    importantly, by separating geometry and texture, we allow users to edit
    appearance by simply editing 2D texture maps.},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.00762v1},
    FILE = {2103.00762v1.pdf}
}

@article{lombardi2021mvp,
    AUTHOR = {Stephen Lombardi and Tomas Simon and Gabriel Schwartz and Michael Zollhoefer and Yaser Sheikh and Jason Saragih},
    TITLE = {Mixture of Volumetric Primitives for Efficient Neural Rendering},
    EPRINT = {2103.01954v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.GR},
    ABSTRACT = {Real-time rendering and animation of humans is a core function in games,
    movies, and telepresence applications. Existing methods have a number of
    drawbacks we aim to address with our work. Triangle meshes have difficulty
    modeling thin structures like hair, volumetric representations like Neural
    Volumes are too low-resolution given a reasonable memory budget, and
    high-resolution implicit representations like Neural Radiance Fields are too
    slow for use in real-time applications. We present Mixture of Volumetric
    Primitives (MVP), a representation for rendering dynamic 3D content that
    combines the completeness of volumetric representations with the efficiency of
    primitive-based rendering, e.g., point-based or mesh-based methods. Our
    approach achieves this by leveraging spatially shared computation with a
    deconvolutional architecture and by minimizing computation in empty regions of
    space with volumetric primitives that can move to cover only occupied regions.
    Our parameterization supports the integration of correspondence and tracking
    constraints, while being robust to areas where classical tracking fails, such
    as around thin or translucent structures and areas with large topological
    variability. MVP is a hybrid that generalizes both volumetric and
    primitive-based representations. Through a series of extensive experiments we
    demonstrate that it inherits the strengths of each, while avoiding many of
    their limitations. We also compare our approach to several state-of-the-art
    methods and demonstrate that MVP produces superior results in terms of quality
    and runtime performance.},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.01954v2},
    FILE = {2103.01954v2.pdf}
}

@article{dupont2021coin,
    AUTHOR = {Emilien Dupont and Adam Golinski and Milad Alizadeh and Yee Whye Teh and Arnaud Doucet},
    TITLE = {COIN: COmpression with Implicit Neural representations},
    EPRINT = {2103.03123v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {eess.IV},
    ABSTRACT = {We propose a new simple approach for image compression: instead of storing
    the RGB values for each pixel of an image, we store the weights of a neural
    network overfitted to the image. Specifically, to encode an image, we fit it
    with an MLP which maps pixel locations to RGB values. We then quantize and
    store the weights of this MLP as a code for the image. To decode the image, we
    simply evaluate the MLP at every pixel location. We found that this simple
    approach outperforms JPEG at low bit-rates, even without entropy coding or
    learning a distribution over weights. While our framework is not yet
    competitive with state of the art compression methods, we show that it has
    various attractive properties which could make it a viable alternative to other
    neural data compression approaches.},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.03123v2},
    FILE = {2103.03123v2.pdf}
}

@article{li2021dynerf,
    AUTHOR = {Tianye Li and Mira Slavcheva and Michael Zollhoefer and Simon Green and Christoph Lassner and Changil Kim and Tanner Schmidt and Steven Lovegrove and Michael Goesele and Zhaoyang Lv},
    TITLE = {Neural 3D Video Synthesis},
    EPRINT = {2103.02597v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We propose a novel approach for 3D video synthesis that is able to represent
    multi-view video recordings of a dynamic real-world scene in a compact, yet
    expressive representation that enables high-quality view synthesis and motion
    interpolation. Our approach takes the high quality and compactness of static
    neural radiance fields in a new direction: to a model-free, dynamic setting. At
    the core of our approach is a novel time-conditioned neural radiance fields
    that represents scene dynamics using a set of compact latent codes. To exploit
    the fact that changes between adjacent frames of a video are typically small
    and locally consistent, we propose two novel strategies for efficient training
    of our neural network: 1) An efficient hierarchical training scheme, and 2) an
    importance sampling strategy that selects the next rays for training based on
    the temporal variation of the input videos. In combination, these two
    strategies significantly boost the training speed, lead to fast convergence of
    the training process, and enable high quality results. Our learned
    representation is highly compact and able to represent a 10 second 30 FPS
    multi-view video recording by 18 cameras with a model size of just 28MB. We
    demonstrate that our method can render high-fidelity wide-angle novel views at
    over 1K resolution, even for highly complex and dynamic scenes. We perform an
    extensive qualitative and quantitative evaluation that shows that our approach
    outperforms the current state of the art. We include additional video and
    information at: https://neural-3d-video.github.io/},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.02597v1},
    FILE = {2103.02597v1.pdf}
}

@article{neff2021donerf,
    AUTHOR = {Thomas Neff and Pascal Stadlbauer and Mathias Parger and Andreas Kurz and Joerg H. Mueller and Chakravarty R. Alla Chaitanya and Anton Kaplanyan and Markus Steinberger},
    TITLE = {DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields
    using Depth Oracle Networks},
    EPRINT = {2103.03231v4},
    DOI = {10.1111/cgf.14340},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {The recent research explosion around implicit neural representations, such as
    NeRF, shows that there is immense potential for implicitly storing high-quality
    scene and lighting information in compact neural networks. However, one major
    limitation preventing the use of NeRF in real-time rendering applications is
    the prohibitive computational cost of excessive network evaluations along each
    view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural
    representations closer to practical rendering of synthetic content in real-time
    applications, such as games and virtual reality. We show that the number of
    samples required for each view ray can be significantly reduced when samples
    are placed around surfaces in the scene without compromising image quality. To
    this end, we propose a depth oracle network that predicts ray sample locations
    for each view ray with a single network evaluation. We show that using a
    classification network around logarithmically discretized and spherically
    warped depth values is essential to encode surface locations rather than
    directly estimating depth. The combination of these techniques leads to DONeRF,
    our compact dual network design with a depth oracle network as its first step
    and a locally sampled shading network for ray accumulation. With DONeRF, we
    reduce the inference costs by up to 48x compared to NeRF when conditioning on
    available ground truth depth information. Compared to concurrent acceleration
    methods for raymarching-based neural representations, DONeRF does not require
    additional memory for explicit caching or acceleration structures, and can
    render interactively (20 frames per second) on a single GPU.},
    YEAR = {2021},
    MONTH = {Mar},
    NOTE = {Computer Graphics Forum Volume 40, Issue 4, 2021},
    URL = {http://arxiv.org/abs/2103.03231v4},
    FILE = {2103.03231v4.pdf}
}

@article{wizadwongsa2021nex,
    AUTHOR = {Suttisak Wizadwongsa and Pakkapon Phongthawee and Jiraphon Yenphraphai and Supasorn Suwajanakorn},
    TITLE = {NeX: Real-time View Synthesis with Neural Basis Expansion},
    EPRINT = {2103.05606v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present NeX, a new approach to novel view synthesis based on enhancements
    of multiplane image (MPI) that can reproduce next-level view-dependent effects
    -- in real time. Unlike traditional MPI that uses a set of simple RGB$\alpha$
    planes, our technique models view-dependent effects by instead parameterizing
    each pixel as a linear combination of basis functions learned from a neural
    network. Moreover, we propose a hybrid implicit-explicit modeling strategy that
    improves upon fine detail and produces state-of-the-art results. Our method is
    evaluated on benchmark forward-facing datasets as well as our newly-introduced
    dataset designed to test the limit of view-dependent modeling with
    significantly more challenging effects such as rainbow reflections on a CD. Our
    method achieves the best overall scores across all major metrics on these
    datasets with more than 1000$\times$ faster rendering time than the state of
    the art. For real-time demos, visit https://nex-mpi.github.io/},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.05606v2},
    FILE = {2103.05606v2.pdf}
}

@article{guo2021adnerf,
    AUTHOR = {Yudong Guo and Keyu Chen and Sen Liang and Yong-Jin Liu and Hujun Bao and Juyong Zhang},
    TITLE = {AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis},
    EPRINT = {2103.11078v3},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Generating high-fidelity talking head video by fitting with the input audio
    sequence is a challenging problem that receives considerable attentions
    recently. In this paper, we address this problem with the aid of neural scene
    representation networks. Our method is completely different from existing
    methods that rely on intermediate representations like 2D landmarks or 3D face
    models to bridge the gap between audio input and video output. Specifically,
    the feature of input audio signal is directly fed into a conditional implicit
    function to generate a dynamic neural radiance field, from which a
    high-fidelity talking-head video corresponding to the audio signal is
    synthesized using volume rendering. Another advantage of our framework is that
    not only the head (with hair) region is synthesized as previous methods did,
    but also the upper body is generated via two individual neural radiance fields.
    Experimental results demonstrate that our novel framework can (1) produce
    high-fidelity and natural results, and (2) support free adjustment of audio
    signals, viewing directions, and background images. Code is available at
    https://github.com/YudongGuo/AD-NeRF.},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.11078v3},
    FILE = {2103.11078v3.pdf}
}

@article{kellnhofer2021nlr,
    AUTHOR = {Petr Kellnhofer and Lars Jebe and Andrew Jones and Ryan Spicer and Kari Pulli and Gordon Wetzstein},
    TITLE = {Neural Lumigraph Rendering},
    EPRINT = {2103.11571v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Novel view synthesis is a challenging and ill-posed inverse rendering
    problem. Neural rendering techniques have recently achieved photorealistic
    image quality for this task. State-of-the-art (SOTA) neural volume rendering
    approaches, however, are slow to train and require minutes of inference (i.e.,
    rendering) time for high image resolutions. We adopt high-capacity neural scene
    representations with periodic activations for jointly optimizing an implicit
    surface and a radiance field of a scene supervised exclusively with posed 2D
    images. Our neural rendering pipeline accelerates SOTA neural volume rendering
    by about two orders of magnitude and our implicit surface representation is
    unique in allowing us to export a mesh with view-dependent texture information.
    Thus, like other implicit surface representations, ours is compatible with
    traditional graphics pipelines, enabling real-time rendering rates, while
    achieving unprecedented image quality compared to other surface methods. We
    assess the quality of our approach using existing datasets as well as
    high-quality 3D face data captured with a custom multi-camera rig.},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.11571v1},
    FILE = {2103.11571v1.pdf}
}

@article{sucar2021imap,
    AUTHOR = {Edgar Sucar and Shikun Liu and Joseph Ortiz and Andrew J. Davison},
    TITLE = {iMAP: Implicit Mapping and Positioning in Real-Time},
    EPRINT = {2103.12352v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We show for the first time that a multilayer perceptron (MLP) can serve as
    the only scene representation in a real-time SLAM system for a handheld RGB-D
    camera. Our network is trained in live operation without prior data, building a
    dense, scene-specific implicit 3D model of occupancy and colour which is also
    immediately used for tracking.
    Achieving real-time SLAM via continual training of a neural network against a
    live image stream requires significant innovation. Our iMAP algorithm uses a
    keyframe structure and multi-processing computation flow, with dynamic
    information-guided pixel sampling for speed, with tracking at 10 Hz and global
    map updating at 2 Hz. The advantages of an implicit MLP over standard dense
    SLAM techniques include efficient geometry representation with automatic detail
    control and smooth, plausible filling-in of unobserved regions such as the back
    surfaces of objects.},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.12352v1},
    FILE = {2103.12352v1.pdf}
}

@article{barron2021mipnerf,
    AUTHOR = {Jonathan T. Barron and Ben Mildenhall and Matthew Tancik and Peter Hedman and Ricardo Martin-Brualla and Pratul P. Srinivasan},
    TITLE = {Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance
    Fields},
    EPRINT = {2103.13415v3},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {The rendering procedure used by neural radiance fields (NeRF) samples a scene
    with a single ray per pixel and may therefore produce renderings that are
    excessively blurred or aliased when training or testing images observe scene
    content at different resolutions. The straightforward solution of supersampling
    by rendering with multiple rays per pixel is impractical for NeRF, because
    rendering each ray requires querying a multilayer perceptron hundreds of times.
    Our solution, which we call "mip-NeRF" (a la "mipmap"), extends NeRF to
    represent the scene at a continuously-valued scale. By efficiently rendering
    anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable
    aliasing artifacts and significantly improves NeRF's ability to represent fine
    details, while also being 7% faster than NeRF and half the size. Compared to
    NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with
    NeRF and by 60% on a challenging multiscale variant of that dataset that we
    present. Mip-NeRF is also able to match the accuracy of a brute-force
    supersampled NeRF on our multiscale dataset while being 22x faster.},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.13415v3},
    FILE = {2103.13415v3.pdf}
}

@article{yu2021nerfsh, plenoctrees,
    AUTHOR = {Alex Yu and Ruilong Li and Matthew Tancik and Hao Li and Ren Ng and Angjoo Kanazawa},
    TITLE = {PlenOctrees for Real-time Rendering of Neural Radiance Fields},
    EPRINT = {2103.14024v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We introduce a method to render Neural Radiance Fields (NeRFs) in real time
    using PlenOctrees, an octree-based 3D representation which supports
    view-dependent effects. Our method can render 800x800 images at more than 150
    FPS, which is over 3000 times faster than conventional NeRFs. We do so without
    sacrificing quality while preserving the ability of NeRFs to perform
    free-viewpoint rendering of scenes with arbitrary geometry and view-dependent
    effects. Real-time performance is achieved by pre-tabulating the NeRF into a
    PlenOctree. In order to preserve view-dependent effects such as specularities,
    we factorize the appearance via closed-form spherical basis functions.
    Specifically, we show that it is possible to train NeRFs to predict a spherical
    harmonic representation of radiance, removing the viewing direction as an input
    to the neural network. Furthermore, we show that PlenOctrees can be directly
    optimized to further minimize the reconstruction loss, which leads to equal or
    better quality compared to competing methods. Moreover, this octree
    optimization step can be used to reduce the training time, as we no longer need
    to wait for the NeRF training to converge fully. Our real-time neural rendering
    approach may potentially enable new applications such as 6-DOF industrial and
    product visualizations, as well as next generation AR/VR systems. PlenOctrees
    are amenable to in-browser rendering as well; please visit the project page for
    the interactive online demo, as well as video and code:
    https://alexyu.net/plenoctrees},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.14024v2},
    FILE = {2103.14024v2.pdf}
}

@article{reiser2021kilonerf,
    AUTHOR = {Christian Reiser and Songyou Peng and Yiyi Liao and Andreas Geiger},
    TITLE = {KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs},
    EPRINT = {2103.13744v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {NeRF synthesizes novel views of a scene with unprecedented quality by fitting
    a neural radiance field to RGB images. However, NeRF requires querying a deep
    Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering
    times, even on modern GPUs. In this paper, we demonstrate that real-time
    rendering is possible by utilizing thousands of tiny MLPs instead of one single
    large MLP. In our setting, each individual MLP only needs to represent parts of
    the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining
    this divide-and-conquer strategy with further optimizations, rendering is
    accelerated by three orders of magnitude compared to the original NeRF model
    without incurring high storage costs. Further, using teacher-student
    distillation for training, we show that this speed-up can be achieved without
    sacrificing visual quality.},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.13744v2},
    FILE = {2103.13744v2.pdf}
}

@article{hedman2021snerg,
    AUTHOR = {Peter Hedman and Pratul P. Srinivasan and Ben Mildenhall and Jonathan T. Barron and Paul Debevec},
    TITLE = {Baking Neural Radiance Fields for Real-Time View Synthesis},
    EPRINT = {2103.14645v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Neural volumetric representations such as Neural Radiance Fields (NeRF) have
    emerged as a compelling technique for learning to represent 3D scenes from
    images with the goal of rendering photorealistic images of the scene from
    unobserved viewpoints. However, NeRF's computational requirements are
    prohibitive for real-time applications: rendering views from a trained NeRF
    requires querying a multilayer perceptron (MLP) hundreds of times per ray. We
    present a method to train a NeRF, then precompute and store (i.e. "bake") it as
    a novel representation called a Sparse Neural Radiance Grid (SNeRG) that
    enables real-time rendering on commodity hardware. To achieve this, we
    introduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid
    representation with learned feature vectors. The resulting scene representation
    retains NeRF's ability to render fine geometric details and view-dependent
    appearance, is compact (averaging less than 90 MB per scene), and can be
    rendered in real-time (higher than 30 frames per second on a laptop GPU).
    Actual screen captures are shown in our video.},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.14645v1},
    FILE = {2103.14645v1.pdf}
}

@article{li2021nemi,
    AUTHOR = {Jiaxin Li and Zijian Feng and Qi She and Henghui Ding and Changhu Wang and Gim Hee Lee},
    TITLE = {MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis},
    EPRINT = {2103.14910v3},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {In this paper, we propose MINE to perform novel view synthesis and depth
    estimation via dense 3D reconstruction from a single image. Our approach is a
    continuous depth generalization of the Multiplane Images (MPI) by introducing
    the NEural radiance fields (NeRF). Given a single image as input, MINE predicts
    a 4-channel image (RGB and volume density) at arbitrary depth values to jointly
    reconstruct the camera frustum and fill in occluded contents. The reconstructed
    and inpainted frustum can then be easily rendered into novel RGB or depth views
    using differentiable rendering. Extensive experiments on RealEstate10K, KITTI
    and Flowers Light Fields show that our MINE outperforms state-of-the-art by a
    large margin in novel view synthesis. We also achieve competitive results in
    depth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our
    source code is available at https://github.com/vincentfung13/MINE},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.14910v3},
    FILE = {2103.14910v3.pdf}
}

@article{zhi2021semanticnerf,
    AUTHOR = {Shuaifeng Zhi and Tristan Laidlow and Stefan Leutenegger and Andrew J. Davison},
    TITLE = {In-Place Scene Labelling and Understanding with Implicit Scene
    Representation},
    EPRINT = {2103.15875v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Semantic labelling is highly correlated with geometry and radiance
    reconstruction, as scene entities with similar shape and appearance are more
    likely to come from similar classes. Recent implicit neural reconstruction
    techniques are appealing as they do not require prior training data, but the
    same fully self-supervised approach is not possible for semantics because
    labels are human-defined properties.
    We extend neural radiance fields (NeRF) to jointly encode semantics with
    appearance and geometry, so that complete and accurate 2D semantic labels can
    be achieved using a small amount of in-place annotations specific to the scene.
    The intrinsic multi-view consistency and smoothness of NeRF benefit semantics
    by enabling sparse labels to efficiently propagate. We show the benefit of this
    approach when labels are either sparse or very noisy in room-scale scenes. We
    demonstrate its advantageous properties in various interesting applications
    such as an efficient scene labelling tool, novel semantic view synthesis, label
    denoising, super-resolution, label interpolation and multi-view semantic label
    fusion in visual semantic mapping systems.},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.15875v2},
    FILE = {2103.15875v2.pdf}
}

@article{chen2021nvsnerf,
    AUTHOR = {Anpei Chen and Zexiang Xu and Fuqiang Zhao and Xiaoshuai Zhang and Fanbo Xiang and Jingyi Yu and Hao Su},
    TITLE = {MVSNeRF: Fast Generalizable Radiance Field Reconstruction from
    Multi-View Stereo},
    EPRINT = {2103.15595v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present MVSNeRF, a novel neural rendering approach that can efficiently
    reconstruct neural radiance fields for view synthesis. Unlike prior works on
    neural radiance fields that consider per-scene optimization on densely captured
    images, we propose a generic deep neural network that can reconstruct radiance
    fields from only three nearby input views via fast network inference. Our
    approach leverages plane-swept cost volumes (widely used in multi-view stereo)
    for geometry-aware scene reasoning, and combines this with physically based
    volume rendering for neural radiance field reconstruction. We train our network
    on real objects in the DTU dataset, and test it on three different datasets to
    evaluate its effectiveness and generalizability. Our approach can generalize
    across scenes (even indoor scenes, completely different from our training
    scenes of objects) and generate realistic view synthesis results using only
    three input images, significantly outperforming concurrent works on
    generalizable radiance field reconstruction. Moreover, if dense images are
    captured, our estimated radiance field representation can be easily fine-tuned;
    this leads to fast per-scene reconstruction with higher rendering quality and
    substantially less optimization time than NeRF.},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.15595v2},
    FILE = {2103.15595v2.pdf}
}

@article{meng2021gnerf,
    AUTHOR = {Quan Meng and Anpei Chen and Haimin Luo and Minye Wu and Hao Su and Lan Xu and Xuming He and Jingyi Yu},
    TITLE = {GNeRF: GAN-based Neural Radiance Field without Posed Camera},
    EPRINT = {2103.15606v3},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We introduce GNeRF, a framework to marry Generative Adversarial Networks
    (GAN) with Neural Radiance Field (NeRF) reconstruction for the complex
    scenarios with unknown and even randomly initialized camera poses. Recent
    NeRF-based advances have gained popularity for remarkable realistic novel view
    synthesis. However, most of them heavily rely on accurate camera poses
    estimation, while few recent methods can only optimize the unknown camera poses
    in roughly forward-facing scenes with relatively short camera trajectories and
    require rough camera poses initialization. Differently, our GNeRF only utilizes
    randomly initialized poses for complex outside-in scenarios. We propose a novel
    two-phases end-to-end framework. The first phase takes the use of GANs into the
    new realm for optimizing coarse camera poses and radiance fields jointly, while
    the second phase refines them with additional photometric loss. We overcome
    local minima using a hybrid and iterative optimization scheme. Extensive
    experiments on a variety of synthetic and natural scenes demonstrate the
    effectiveness of GNeRF. More impressively, our approach outperforms the
    baselines favorably in those scenes with repeated patterns or even low textures
    that are regarded as extremely challenging before.},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.15606v3},
    FILE = {2103.15606v3.pdf}
}

@article{henzler2021unsupervised,
    AUTHOR = {Philipp Henzler and Jeremy Reizenstein and Patrick Labatut and Roman Shapovalov and Tobias Ritschel and Andrea Vedaldi and David Novotny},
    TITLE = {Unsupervised Learning of 3D Object Categories from Videos in the Wild},
    EPRINT = {2103.16552v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Our goal is to learn a deep network that, given a small number of images of
    an object of a given category, reconstructs it in 3D. While several recent
    works have obtained analogous results using synthetic data or assuming the
    availability of 2D primitives such as keypoints, we are interested in working
    with challenging real data and with no manual annotations. We thus focus on
    learning a model from multiple views of a large collection of object instances.
    We contribute with a new large dataset of object centric videos suitable for
    training and benchmarking this class of models. We show that existing
    techniques leveraging meshes, voxels, or implicit surfaces, which work well for
    reconstructing isolated objects, fail on this challenging data. Finally, we
    propose a new neural network design, called warp-conditioned ray embedding
    (WCR), which significantly improves reconstruction while obtaining a detailed
    implicit representation of the object surface and texture, also compensating
    for the noise in the initial SfM reconstruction that bootstrapped the learning
    process. Our evaluation demonstrates performance improvements over several deep
    monocular reconstruction baselines on existing benchmarks and on our novel
    dataset.},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.16552v1},
    FILE = {2103.16552v1.pdf}
}

@article{deng2021foveated,
    AUTHOR = {Nianchen Deng and Zhenyi He and Jiannan Ye and Praneeth Chakravarthula and Xubo Yang and Qi Sun},
    TITLE = {Foveated Neural Radiance Fields for Real-Time and Egocentric Virtual
    Reality},
    EPRINT = {2103.16365v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.GR},
    ABSTRACT = {Traditional high-quality 3D graphics requires large volumes of fine-detailed
    scene data for rendering. This demand compromises computational efficiency and
    local storage resources. Specifically, it becomes more concerning for future
    wearable and portable virtual and augmented reality (VR/AR) displays. Recent
    approaches to combat this problem include remote rendering/streaming and neural
    representations of 3D assets. These approaches have redefined the traditional
    local storage-rendering pipeline by distributed computing or compression of
    large data. However, these methods typically suffer from high latency or low
    quality for practical visualization of large immersive virtual scenes, notably
    with extra high resolution and refresh rate requirements for VR applications
    such as gaming and design.
    Tailored for the future portable, low-storage, and energy-efficient VR
    platforms, we present the first gaze-contingent 3D neural representation and
    view synthesis method. We incorporate the human psychophysics of visual- and
    stereo-acuity into an egocentric neural representation of 3D scenery.
    Furthermore, we jointly optimize the latency/performance and visual quality,
    while mutually bridging human perception and neural scene synthesis, to achieve
    perceptually high-quality immersive interaction. Both objective analysis and
    subjective study demonstrate the effectiveness of our approach in significantly
    reducing local storage volume and synthesis latency (up to 99% reduction in
    both data size and computational time), while simultaneously presenting
    high-fidelity rendering, with perceptual quality identical to that of fully
    locally stored and rendered high-quality imagery.},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.16365v1},
    FILE = {2103.16365v1.pdf}
}

@article{niemeyer2021campari,
    AUTHOR = {Michael Niemeyer and Andreas Geiger},
    TITLE = {CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields},
    EPRINT = {2103.17269v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Tremendous progress in deep generative models has led to photorealistic image
    synthesis. While achieving compelling results, most approaches operate in the
    two-dimensional image domain, ignoring the three-dimensional nature of our
    world. Several recent works therefore propose generative models which are
    3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably to
    the image plane. This leads to impressive 3D consistency, but incorporating
    such a bias comes at a price: the camera needs to be modeled as well. Current
    approaches assume fixed intrinsics and a predefined prior over camera pose
    ranges. As a result, parameter tuning is typically required for real-world
    data, and results degrade if the data distribution is not matched. Our key
    hypothesis is that learning a camera generator jointly with the image generator
    leads to a more principled approach to 3D-aware image synthesis. Further, we
    propose to decompose the scene into a background and foreground model, leading
    to more efficient and disentangled scene representations. While training from
    raw, unposed image collections, we learn a 3D- and camera-aware generative
    model which faithfully recovers not only the image but also the camera data
    distribution. At test time, our model generates images with explicit control
    over the camera as well as the shape and appearance of the scene.},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.17269v1},
    FILE = {2103.17269v1.pdf}
}

@article{palafox2021npms,
    AUTHOR = {Pablo Palafox and Aljaz Bozic and Justus Thies and Matthias Niessner and Angela Dai},
    TITLE = {NPMs: Neural Parametric Models for 3D Deformable Shapes},
    EPRINT = {2104.00702v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Parametric 3D models have enabled a wide variety of tasks in computer
    graphics and vision, such as modeling human bodies, faces, and hands. However,
    the construction of these parametric models is often tedious, as it requires
    heavy manual tweaking, and they struggle to represent additional complexity and
    details such as wrinkles or clothing. To this end, we propose Neural Parametric
    Models (NPMs), a novel, learned alternative to traditional, parametric 3D
    models, which does not require hand-crafted, object-specific constraints. In
    particular, we learn to disentangle 4D dynamics into latent-space
    representations of shape and pose, leveraging the flexibility of recent
    developments in learned implicit functions. Crucially, once learned, our neural
    parametric models of shape and pose enable optimization over the learned spaces
    to fit to new observations, similar to the fitting of a traditional parametric
    model, e.g., SMPL. This enables NPMs to achieve a significantly more accurate
    and detailed representation of observed deformable sequences. We show that NPMs
    improve notably over both parametric and non-parametric state of the art in
    reconstruction and tracking of monocular depth sequences of clothed humans and
    hands. Latent-space interpolation as well as shape/pose transfer experiments
    further demonstrate the usefulness of NPMs. Code is publicly available at
    https://pablopalafox.github.io/npms.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.00702v2},
    FILE = {2104.00702v2.pdf}
}

@article{zhu2021rgbd,
    AUTHOR = {Luyang Zhu and Arsalan Mousavian and Yu Xiang and Hammad Mazhar and Jozef van Eenbergen and Shoubhik Debnath and Dieter Fox},
    TITLE = {RGB-D Local Implicit Function for Depth Completion of Transparent
    Objects},
    EPRINT = {2104.00622v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Majority of the perception methods in robotics require depth information
    provided by RGB-D cameras. However, standard 3D sensors fail to capture depth
    of transparent objects due to refraction and absorption of light. In this
    paper, we introduce a new approach for depth completion of transparent objects
    from a single RGB-D image. Key to our approach is a local implicit neural
    representation built on ray-voxel pairs that allows our method to generalize to
    unseen objects and achieve fast inference speed. Based on this representation,
    we present a novel framework that can complete missing depth given noisy RGB-D
    input. We further improve the depth estimation iteratively using a
    self-correcting refinement model. To train the whole pipeline, we build a large
    scale synthetic dataset with transparent objects. Experiments demonstrate that
    our method performs significantly better than the current state-of-the-art
    methods on both synthetic and real world data. In addition, our approach
    improves the inference speed by a factor of 20 compared to the previous best
    method, ClearGrasp. Code and dataset will be released at
    https://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.00622v1},
    FILE = {2104.00622v1.pdf}
}

@article{zhang2021physg,
    AUTHOR = {Kai Zhang and Fujun Luan and Qianqian Wang and Kavita Bala and Noah Snavely},
    TITLE = {PhySG: Inverse Rendering with Spherical Gaussians for Physics-based
    Material Editing and Relighting},
    EPRINT = {2104.00674v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present PhySG, an end-to-end inverse rendering pipeline that includes a
    fully differentiable renderer and can reconstruct geometry, materials, and
    illumination from scratch from a set of RGB input images. Our framework
    represents specular BRDFs and environmental illumination using mixtures of
    spherical Gaussians, and represents geometry as a signed distance function
    parameterized as a Multi-Layer Perceptron. The use of spherical Gaussians
    allows us to efficiently solve for approximate light transport, and our method
    works on scenes with challenging non-Lambertian reflectance captured under
    natural, static illumination. We demonstrate, with both synthetic and real
    data, that our reconstructions not only enable rendering of novel viewpoints,
    but also physics-based appearance editing of materials and illumination.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.00674v1},
    FILE = {2104.00674v1.pdf}
}

@article{kosiorek2021nerfvae,
    AUTHOR = {Adam R. Kosiorek and Heiko Strathmann and Daniel Zoran and Pol Moreno and Rosalia Schneider and Sona Mokra and Danilo J. Rezende},
    TITLE = {NeRF-VAE: A Geometry Aware 3D Scene Generative Model},
    EPRINT = {2104.00587v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {stat.ML},
    ABSTRACT = {We propose NeRF-VAE, a 3D scene generative model that incorporates geometric
    structure via NeRF and differentiable volume rendering. In contrast to NeRF,
    our model takes into account shared structure across scenes, and is able to
    infer the structure of a novel scene -- without the need to re-train -- using
    amortized inference. NeRF-VAE's explicit 3D rendering process further contrasts
    previous generative models with convolution-based rendering which lacks
    geometric structure. Our model is a VAE that learns a distribution over
    radiance fields by conditioning them on a latent scene representation. We show
    that, once trained, NeRF-VAE is able to infer and render
    geometrically-consistent scenes from previously unseen 3D environments using
    very few input images. We further demonstrate that NeRF-VAE generalizes well to
    out-of-distribution cameras, while convolutional models do not. Finally, we
    introduce and study an attention-based conditioning mechanism of NeRF-VAE's
    decoder, which improves model performance.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.00587v1},
    FILE = {2104.00587v1.pdf}
}

@article{devries2021unconstrained,
    AUTHOR = {Terrance DeVries and Miguel Angel Bautista and Nitish Srivastava and Graham W. Taylor and Joshua M. Susskind},
    TITLE = {Unconstrained Scene Generation with Locally Conditioned Radiance Fields},
    EPRINT = {2104.00670v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We tackle the challenge of learning a distribution over complex, realistic,
    indoor scenes. In this paper, we introduce Generative Scene Networks (GSN),
    which learns to decompose scenes into a collection of many local radiance
    fields that can be rendered from a free moving camera. Our model can be used as
    a prior to generate new scenes, or to complete a scene given only sparse 2D
    observations. Recent work has shown that generative models of radiance fields
    can capture properties such as multi-view consistency and view-dependent
    lighting. However, these models are specialized for constrained viewing of
    single objects, such as cars or faces. Due to the size and complexity of
    realistic indoor environments, existing models lack the representational
    capacity to adequately capture them. Our decomposition scheme scales to larger
    and more complex scenes while preserving details and diversity, and the learned
    prior enables high-quality rendering from viewpoints that are significantly
    different from observed viewpoints. When compared to existing models, GSN
    produces quantitatively higher-quality scene renderings across several
    different scene datasets.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.00670v1},
    FILE = {2104.00670v1.pdf}
}

@article{jain2021dietnerf,
    AUTHOR = {Ajay Jain and Matthew Tancik and Pieter Abbeel},
    TITLE = {Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis},
    EPRINT = {2104.00677v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present DietNeRF, a 3D neural scene representation estimated from a few
    images. Neural Radiance Fields (NeRF) learn a continuous volumetric
    representation of a scene through multi-view consistency, and can be rendered
    from novel viewpoints by ray casting. While NeRF has an impressive ability to
    reconstruct geometry and fine details given many images, up to 100 for
    challenging 360{\deg} scenes, it often finds a degenerate solution to its image
    reconstruction objective when only a few input views are available. To improve
    few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic
    consistency loss that encourages realistic renderings at novel poses. DietNeRF
    is trained on individual scenes to (1) correctly render given input views from
    the same pose, and (2) match high-level semantic attributes across different,
    random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary
    poses. We extract these semantics using a pre-trained visual encoder such as
    CLIP, a Vision Transformer trained on hundreds of millions of diverse
    single-view, 2D photographs mined from the web with natural language
    supervision. In experiments, DietNeRF improves the perceptual quality of
    few-shot view synthesis when learned from scratch, can render novel views with
    as few as one observed image when pre-trained on a multi-view dataset, and
    produces plausible completions of completely unobserved regions.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.00677v1},
    FILE = {2104.00677v1.pdf}
}

@article{Fu_2021,
    DOI = {10.1088/1742-6596/1880/1/012034},
    URL = {https://doi.org/10.1088/1742-6596/1880/1/012034},
    YEAR = {apr},
    PUBLISHER = {{IOP} Publishing},
    VOLUME = {1880},
    NUMBER = {1},
    PAGES = {012034},
    AUTHOR = {Bofeng Fu and Zheng Wang},
    TITLE = {Multi-scene Representation Learning with Neural Radiance Fields},
    JOURNAL = {Journal of Physics: Conference Series},
    ABSTRACT = {Getting representations of multiple objects or scenes is a raising research topic in Machine Learning (ML) community. Here, we propose a multi-scene representation model that can learn the representation of complex scenes and reconstruct them in high resolution given novel viewing directions. Our method represents a single scene with fully-connected layers. Each set of fully-connected layers are controlled by hyper-networks for multiple scenes modeling. For each scene, we take 3D coordinates (x, y, z) and 2D view-point orientations (, ) as inputs. A set of fully-connected layers output volume density and RGB values at given 3D spatial positions. Then, we render the output volume density and RGB values along the camera rays into images using volume density rendering techniques. During training process, we optimize a continuous volume scene function with a small amount of input viewing directions. By designing versatile embedding module and multi-scene representation networks, our model can render photographic images with novel viewing directions for different complex scenes. Experiment results demonstrate the neural rendering and multi-scene representation abilities of our model. Several thorough experiments show that our method outperforms previous model on both reconstruction precision and scenes generation ability from novel viewing directions.}
}

@article{stelzner2021obsurf,
    AUTHOR = {Karl Stelzner and Kristian Kersting and Adam R. Kosiorek},
    TITLE = {Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation},
    EPRINT = {2104.01148v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present ObSuRF, a method which turns a single image of a scene into a 3D
    model represented as a set of Neural Radiance Fields (NeRFs), with each NeRF
    corresponding to a different object. A single forward pass of an encoder
    network outputs a set of latent vectors describing the objects in the scene.
    These vectors are used independently to condition a NeRF decoder, defining the
    geometry and appearance of each object. We make learning more computationally
    efficient by deriving a novel loss, which allows training NeRFs on RGB-D inputs
    without explicit ray marching. After confirming that the model performs equal
    or better than state of the art on three 2D image segmentation benchmarks, we
    apply it to two multi-object 3D datasets: A multiview version of CLEVR, and a
    novel dataset in which scenes are populated by ShapeNet models. We find that
    after training ObSuRF on RGB-D views of training scenes, it is capable of not
    only recovering the 3D geometry of a scene depicted in a single input image,
    but also to segment it into objects, despite receiving no supervision in that
    regard.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.01148v1},
    FILE = {2104.01148v1.pdf}
}

@article{jiang2021giga,
    AUTHOR = {Zhenyu Jiang and Yifeng Zhu and Maxwell Svetlik and Kuan Fang and Yuke Zhu},
    TITLE = {Synergies Between Affordance and Geometry: 6-DoF Grasp Detection via
    Implicit Representations},
    EPRINT = {2104.01542v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.RO},
    ABSTRACT = {Grasp detection in clutter requires the robot to reason about the 3D scene
    from incomplete and noisy perception. In this work, we draw insight that 3D
    reconstruction and grasp learning are two intimately connected tasks, both of
    which require a fine-grained understanding of local geometry details. We thus
    propose to utilize the synergies between grasp affordance and 3D reconstruction
    through multi-task learning of a shared representation. Our model takes
    advantage of deep implicit functions, a continuous and memory-efficient
    representation, to enable differentiable training of both tasks. We train the
    model on self-supervised grasp trials data in simulation. Evaluation is
    conducted on a clutter removal task, where the robot clears cluttered objects
    by grasping them one at a time. The experimental results in simulation and on
    the real robot have demonstrated that the use of implicit neural
    representations and joint learning of grasp affordance and 3D reconstruction
    have led to state-of-the-art grasping results. Our method outperforms baselines
    by over 10% in terms of grasp success rate. Additional results and videos can
    be found at https://sites.google.com/view/rpl-giga2021},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.01542v2},
    FILE = {2104.01542v2.pdf}
}

@article{chan2020pigan,
    AUTHOR = {Eric R. Chan and Marco Monteiro and Petr Kellnhofer and Jiajun Wu and Gordon Wetzstein},
    TITLE = {pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware
    Image Synthesis},
    EPRINT = {2012.00926v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We have witnessed rapid progress on 3D-aware image synthesis, leveraging
    recent advances in generative visual models and neural rendering. Existing
    approaches however fall short in two ways: first, they may lack an underlying
    3D representation or rely on view-inconsistent rendering, hence synthesizing
    images that are not multi-view consistent; second, they often depend upon
    representation network architectures that are not expressive enough, and their
    results thus lack in image quality. We propose a novel generative model, named
    Periodic Implicit Generative Adversarial Networks ($\pi$-GAN or pi-GAN), for
    high-quality 3D-aware image synthesis. $\pi$-GAN leverages neural
    representations with periodic activation functions and volumetric rendering to
    represent scenes as view-consistent 3D representations with fine detail. The
    proposed approach obtains state-of-the-art results for 3D-aware image synthesis
    with multiple real and synthetic datasets.},
    YEAR = {2020},
    MONTH = {Dec},
    URL = {http://arxiv.org/abs/2012.00926v2},
    FILE = {2012.00926v2.pdf}
}

@article{luo2021convolutional,
    AUTHOR = {Haimin Luo and Anpei Chen and Qixuan Zhang and Bai Pang and Minye Wu and Lan Xu and Jingyi Yu},
    TITLE = {Convolutional Neural Opacity Radiance Fields},
    EPRINT = {2104.01772v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Photo-realistic modeling and rendering of fuzzy objects with complex opacity
    are critical for numerous immersive VR/AR applications, but it suffers from
    strong view-dependent brightness, color. In this paper, we propose a novel
    scheme to generate opacity radiance fields with a convolutional neural renderer
    for fuzzy objects, which is the first to combine both explicit opacity
    supervision and convolutional mechanism into the neural radiance field
    framework so as to enable high-quality appearance and global consistent alpha
    mattes generation in arbitrary novel views. More specifically, we propose an
    efficient sampling strategy along with both the camera rays and image plane,
    which enables efficient radiance field sampling and learning in a patch-wise
    manner, as well as a novel volumetric feature integration scheme that generates
    per-patch hybrid feature embeddings to reconstruct the view-consistent
    fine-detailed appearance and opacity output. We further adopt a patch-wise
    adversarial training scheme to preserve both high-frequency appearance and
    opacity details in a self-supervised framework. We also introduce an effective
    multi-view image capture system to capture high-quality color and alpha maps
    for challenging fuzzy objects. Extensive experiments on existing and our new
    challenging fuzzy object dataset demonstrate that our method achieves
    photo-realistic, globally consistent, and fined detailed appearance and opacity
    free-viewpoint rendering for various fuzzy objects.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.01772v1},
    FILE = {2104.01772v1.pdf}
}

@article{wang2021mirrornerf,
    AUTHOR = {Ziyu Wang and Liao Wang and Fuqiang Zhao and Minye Wu and Lan Xu and Jingyi Yu},
    TITLE = {MirrorNeRF: One-shot Neural Portrait Radiance Field from Multi-mirror
    Catadioptric Imaging},
    EPRINT = {2104.02607v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Photo-realistic neural reconstruction and rendering of the human portrait are
    critical for numerous VR/AR applications. Still, existing solutions inherently
    rely on multi-view capture settings, and the one-shot solution to get rid of
    the tedious multi-view synchronization and calibration remains extremely
    challenging. In this paper, we propose MirrorNeRF - a one-shot neural portrait
    free-viewpoint rendering approach using a catadioptric imaging system with
    multiple sphere mirrors and a single high-resolution digital camera, which is
    the first to combine neural radiance field with catadioptric imaging so as to
    enable one-shot photo-realistic human portrait reconstruction and rendering, in
    a low-cost and casual capture setting. More specifically, we propose a
    light-weight catadioptric system design with a sphere mirror array to enable
    diverse ray sampling in the continuous 3D space as well as an effective online
    calibration for the camera and the mirror array. Our catadioptric imaging
    system can be easily deployed with a low budget and the casual capture ability
    for convenient daily usages. We introduce a novel neural warping radiance field
    representation to learn a continuous displacement field that implicitly
    compensates for the misalignment due to our flexible system setting. We further
    propose a density regularization scheme to leverage the inherent geometry
    information from the catadioptric data in a self-supervision manner, which not
    only improves the training efficiency but also provides more effective density
    supervision for higher rendering quality. Extensive experiments demonstrate the
    effectiveness and robustness of our scheme to achieve one-shot photo-realistic
    and high-quality appearance free-viewpoint rendering for human portrait scenes.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.02607v2},
    FILE = {2104.02607v2.pdf}
}

@article{noguchi2021narf,
    AUTHOR = {Atsuhiro Noguchi and Xiao Sun and Stephen Lin and Tatsuya Harada},
    TITLE = {Neural Articulated Radiance Field},
    EPRINT = {2104.03110v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present Neural Articulated Radiance Field (NARF), a novel deformable 3D
    representation for articulated objects learned from images. While recent
    advances in 3D implicit representation have made it possible to learn models of
    complex objects, learning pose-controllable representations of articulated
    objects remains a challenge, as current methods require 3D shape supervision
    and are unable to render appearance. In formulating an implicit representation
    of 3D articulated objects, our method considers only the rigid transformation
    of the most relevant object part in solving for the radiance field at each 3D
    location. In this way, the proposed method represents pose-dependent changes
    without significantly increasing the computational complexity. NARF is fully
    differentiable and can be trained from images with pose annotations. Moreover,
    through the use of an autoencoder, it can learn appearance variations over
    multiple instances of an object class. Experiments show that the proposed
    method is efficient and can generalize well to novel poses. The code is
    available for research purposes at https://github.com/nogu-atsu/NARF},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.03110v2},
    FILE = {2104.03110v2.pdf}
}

@article{saito2021scanimate,
    AUTHOR = {Shunsuke Saito and Jinlong Yang and Qianli Ma and Michael J. Black},
    TITLE = {SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks},
    EPRINT = {2104.03313v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present SCANimate, an end-to-end trainable framework that takes raw 3D
    scans of a clothed human and turns them into an animatable avatar. These
    avatars are driven by pose parameters and have realistic clothing that moves
    and deforms naturally. SCANimate does not rely on a customized mesh template or
    surface mesh registration. We observe that fitting a parametric 3D body model,
    like SMPL, to a clothed human scan is tractable while surface registration of
    the body topology to the scan is often not, because clothing can deviate
    significantly from the body shape. We also observe that articulated
    transformations are invertible, resulting in geometric cycle consistency in the
    posed and unposed shapes. These observations lead us to a weakly supervised
    learning method that aligns scans into a canonical pose by disentangling
    articulated deformations without template-based surface registration.
    Furthermore, to complete missing regions in the aligned scans while modeling
    pose-dependent deformations, we introduce a locally pose-aware implicit
    function that learns to complete and model geometry with learned pose
    correctives. In contrast to commonly used global pose embeddings, our local
    pose conditioning significantly reduces long-range spurious correlations and
    improves generalization to unseen poses, especially when training data is
    limited. Our method can be applied to pose-aware appearance modeling to
    generate a fully textured avatar. We demonstrate our approach on various
    clothing types with different amounts of training data, outperforming existing
    solutions and other variants in terms of fidelity and generality in every
    setting. The code is available at https://scanimate.is.tue.mpg.de.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.03313v2},
    FILE = {2104.03313v2.pdf}
}

@article{chen2021directposenet,
    AUTHOR = {Shuai Chen and Zirui Wang and Victor Prisacariu},
    TITLE = {Direct-PoseNet: Absolute Pose Regression with Photometric Consistency},
    EPRINT = {2104.04073v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present a relocalization pipeline, which combines an absolute pose
    regression (APR) network with a novel view synthesis based direct matching
    module, offering superior accuracy while maintaining low inference time. Our
    contribution is twofold: i) we design a direct matching module that supplies a
    photometric supervision signal to refine the pose regression network via
    differentiable rendering; ii) we modify the rotation representation from the
    classical quaternion to SO(3) in pose regression, removing the need for
    balancing rotation and translation loss terms. As a result, our network
    Direct-PoseNet achieves state-of-the-art performance among all other
    single-image APR methods on the 7-Scenes benchmark and the LLFF dataset.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.04073v1},
    FILE = {2104.04073v1.pdf}
}

@article{chen2021snarf,
    AUTHOR = {Xu Chen and Yufeng Zheng and Michael J. Black and Otmar Hilliges and Andreas Geiger},
    TITLE = {SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural
    Implicit Shapes},
    EPRINT = {2104.03953v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Neural implicit surface representations have emerged as a promising paradigm
    to capture 3D shapes in a continuous and resolution-independent manner.
    However, adapting them to articulated shapes is non-trivial. Existing
    approaches learn a backward warp field that maps deformed to canonical points.
    However, this is problematic since the backward warp field is pose dependent
    and thus requires large amounts of data to learn. To address this, we introduce
    SNARF, which combines the advantages of linear blend skinning (LBS) for
    polygonal meshes with those of neural implicit surfaces by learning a forward
    deformation field without direct supervision. This deformation field is defined
    in canonical, pose-independent space, allowing for generalization to unseen
    poses. Learning the deformation field from posed meshes alone is challenging
    since the correspondences of deformed points are defined implicitly and may not
    be unique under changes of topology. We propose a forward skinning model that
    finds all canonical correspondences of any deformed point using iterative root
    finding. We derive analytical gradients via implicit differentiation, enabling
    end-to-end training from 3D meshes with bone transformations. Compared to
    state-of-the-art neural implicit representations, our approach generalizes
    better to unseen poses while preserving accuracy. We demonstrate our method in
    challenging scenarios on (clothed) 3D humans in diverse and unseen poses.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.03953v1},
    FILE = {2104.03953v1.pdf}
}

@article{mehta2021modulated,
    AUTHOR = {Ishit Mehta and Michael Gharbi and Connelly Barnes and Eli Shechtman and Ravi Ramamoorthi and Manmohan Chandraker},
    TITLE = {Modulated Periodic Activations for Generalizable Local Functional
    Representations},
    EPRINT = {2104.03960v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Multi-Layer Perceptrons (MLPs) make powerful functional representations for
    sampling and reconstruction problems involving low-dimensional signals like
    images,shapes and light fields. Recent works have significantly improved their
    ability to represent high-frequency content by using periodic activations or
    positional encodings. This often came at the expense of generalization: modern
    methods are typically optimized for a single signal. We present a new
    representation that generalizes to multiple instances and achieves
    state-of-the-art fidelity. We use a dual-MLP architecture to encode the
    signals. A synthesis network creates a functional mapping from a
    low-dimensional input (e.g. pixel-position) to the output domain (e.g. RGB
    color). A modulation network maps a latent code corresponding to the target
    signal to parameters that modulate the periodic activations of the synthesis
    network. We also propose a local-functional representation which enables
    generalization. The signal's domain is partitioned into a regular grid,with
    each tile represented by a latent code. At test time, the signal is encoded
    with high-fidelity by inferring (or directly optimizing) the latent code-book.
    Our approach produces generalizable functional representations of images,
    videos and shapes, and achieves higher reconstruction quality than prior works
    that are optimized for a single signal.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.03960v1},
    FILE = {2104.03960v1.pdf}
}

@article{azinovic2021neural,
    AUTHOR = {Dejan Azinovic and Ricardo Martin-Brualla and Dan B Goldman and Matthias Niessner and Justus Thies},
    TITLE = {Neural RGB-D Surface Reconstruction},
    EPRINT = {2104.04532v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {In this work, we explore how to leverage the success of implicit novel view
    synthesis methods for surface reconstruction. Methods which learn a neural
    radiance field have shown amazing image synthesis results, but the underlying
    geometry representation is only a coarse approximation of the real geometry. We
    demonstrate how depth measurements can be incorporated into the radiance field
    formulation to produce more detailed and complete reconstruction results than
    using methods based on either color or depth data alone. In contrast to a
    density field as the underlying geometry representation, we propose to learn a
    deep neural network which stores a truncated signed distance field. Using this
    representation, we show that one can still leverage differentiable volume
    rendering to estimate color values of the observed images during training to
    compute a reconstruction loss. This is beneficial for learning the signed
    distance field in regions with missing depth measurements. Furthermore, we
    correct misalignment errors of the camera, improving the overall reconstruction
    quality. In several experiments, we showcase our method and compare to existing
    works on classical RGB-D fusion and learned representations.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.04532v1},
    FILE = {2104.04532v1.pdf}
}

@article{lu2021compressive,
    AUTHOR = {Yuzhe Lu and Kairong Jiang and Joshua A. Levine and Matthew Berger},
    TITLE = {Compressive Neural Representations of Volumetric Scalar Fields},
    EPRINT = {2104.04523v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.LG},
    ABSTRACT = {We present an approach for compressing volumetric scalar fields using
    implicit neural representations. Our approach represents a scalar field as a
    learned function, wherein a neural network maps a point in the domain to an
    output scalar value. By setting the number of weights of the neural network to
    be smaller than the input size, we achieve compressed representations of scalar
    fields, thus framing compression as a type of function approximation. Combined
    with carefully quantizing network weights, we show that this approach yields
    highly compact representations that outperform state-of-the-art volume
    compression approaches. The conceptual simplicity of our approach enables a
    number of benefits, such as support for time-varying scalar fields, optimizing
    to preserve spatial gradients, and random-access field evaluation. We study the
    impact of network design choices on compression performance, highlighting how
    simple network architectures are effective for a broad range of volumes.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.04523v1},
    FILE = {2104.04523v1.pdf}
}

@article{lin2021barf,
    AUTHOR = {Chen-Hsuan Lin and Wei-Chiu Ma and Antonio Torralba and Simon Lucey},
    TITLE = {BARF: Bundle-Adjusting Neural Radiance Fields},
    EPRINT = {2104.06405v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Neural Radiance Fields (NeRF) have recently gained a surge of interest within
    the computer vision community for its power to synthesize photorealistic novel
    views of real-world scenes. One limitation of NeRF, however, is its requirement
    of accurate camera poses to learn the scene representations. In this paper, we
    propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from
    imperfect (or even unknown) camera poses -- the joint problem of learning
    neural 3D representations and registering camera frames. We establish a
    theoretical connection to classical image alignment and show that
    coarse-to-fine registration is also applicable to NeRF. Furthermore, we show
    that na\"ively applying positional encoding in NeRF has a negative impact on
    registration with a synthesis-based objective. Experiments on synthetic and
    real-world data show that BARF can effectively optimize the neural scene
    representations and resolve large camera pose misalignment at the same time.
    This enables view synthesis and localization of video sequences from unknown
    camera poses, opening up new avenues for visual localization systems (e.g.
    SLAM) and potential applications for dense 3D mapping and reconstruction.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.06405v2},
    FILE = {2104.06405v2.pdf}
}

@article{chibane2021srf,
    AUTHOR = {Julian Chibane and Aayush Bansal and Verica Lazova and Gerard Pons-Moll},
    TITLE = {Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views
    of Novel Scenes},
    EPRINT = {2104.06935v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Recent neural view synthesis methods have achieved impressive quality and
    realism, surpassing classical pipelines which rely on multi-view
    reconstruction. State-of-the-Art methods, such as NeRF, are designed to learn a
    single scene with a neural network and require dense multi-view inputs. Testing
    on a new scene requires re-training from scratch, which takes 2-3 days. In this
    work, we introduce Stereo Radiance Fields (SRF), a neural view synthesis
    approach that is trained end-to-end, generalizes to new scenes, and requires
    only sparse views at test time. The core idea is a neural architecture inspired
    by classical multi-view stereo methods, which estimates surface points by
    finding similar image regions in stereo images. In SRF, we predict color and
    density for each 3D point given an encoding of its stereo correspondence in the
    input images. The encoding is implicitly learned by an ensemble of pair-wise
    similarities -- emulating classical stereo. Experiments show that SRF learns
    structure instead of overfitting on a scene. We train on multiple scenes of the
    DTU dataset and generalize to new ones without re-training, requiring only 10
    sparse and spread-out views as input. We show that 10-15 minutes of fine-tuning
    further improve the results, achieving significantly sharper, more detailed
    results than scene-specific models. The code, model, and videos are available
    at https://virtualhumans.mpi-inf.mpg.de/srf/.},
    YEAR = {2021},
    MONTH = {Apr},
    NOTE = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
    2021},
    URL = {http://arxiv.org/abs/2104.06935v1},
    FILE = {2104.06935v1.pdf}
}

@article{hao2021gancraft,
    AUTHOR = {Zekun Hao and Arun Mallya and Serge Belongie and Ming-Yu Liu},
    TITLE = {GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds},
    EPRINT = {2104.07659v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present GANcraft, an unsupervised neural rendering framework for
    generating photorealistic images of large 3D block worlds such as those created
    in Minecraft. Our method takes a semantic block world as input, where each
    block is assigned a semantic label such as dirt, grass, or water. We represent
    the world as a continuous volumetric function and train our model to render
    view-consistent photorealistic images for a user-controlled camera. In the
    absence of paired ground truth real images for the block world, we devise a
    training technique based on pseudo-ground truth and adversarial training. This
    stands in contrast to prior work on neural rendering for view synthesis, which
    requires ground truth images to estimate scene geometry and view-dependent
    appearance. In addition to camera trajectory, GANcraft allows user control over
    both scene semantics and output style. Experimental results with comparison to
    strong baselines show the effectiveness of GANcraft on this novel task of
    photorealistic 3D block world synthesis. The project website is available at
    https://nvlabs.github.io/GANcraft/ .},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.07659v1},
    FILE = {2104.07659v1.pdf}
}

@article{mu2021asdf,
    AUTHOR = {Jiteng Mu and Weichao Qiu and Adam Kortylewski and Alan Yuille and Nuno Vasconcelos and Xiaolong Wang},
    TITLE = {A-SDF: Learning Disentangled Signed Distance Functions for Articulated
    Shape Representation},
    EPRINT = {2104.07645v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Recent work has made significant progress on using implicit functions, as a
    continuous representation for 3D rigid object shape reconstruction. However,
    much less effort has been devoted to modeling general articulated objects.
    Compared to rigid objects, articulated objects have higher degrees of freedom,
    which makes it hard to generalize to unseen shapes. To deal with the large
    shape variance, we introduce Articulated Signed Distance Functions (A-SDF) to
    represent articulated shapes with a disentangled latent space, where we have
    separate codes for encoding shape and articulation. We assume no prior
    knowledge on part geometry, articulation status, joint type, joint axis, and
    joint location. With this disentangled continuous representation, we
    demonstrate that we can control the articulation input and animate unseen
    instances with unseen joint angles. Furthermore, we propose a Test-Time
    Adaptation inference algorithm to adjust our model during inference. We
    demonstrate our model generalize well to out-of-distribution and unseen data,
    e.g., partial point clouds and real-world depth images.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.07645v1},
    FILE = {2104.07645v1.pdf}
}

@article{garbin2021fastnerf,
    AUTHOR = {Stephan J. Garbin and Marek Kowalski and Matthew Johnson and Jamie Shotton and Julien Valentin},
    TITLE = {FastNeRF: High-Fidelity Neural Rendering at 200FPS},
    EPRINT = {2103.10380v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Recent work on Neural Radiance Fields (NeRF) showed how neural networks can
    be used to encode complex 3D environments that can be rendered
    photorealistically from novel viewpoints. Rendering these images is very
    computationally demanding and recent improvements are still a long way from
    enabling interactive rates, even on high-end hardware. Motivated by scenarios
    on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based
    system capable of rendering high fidelity photorealistic images at 200Hz on a
    high-end consumer GPU. The core of our method is a graphics-inspired
    factorization that allows for (i) compactly caching a deep radiance map at each
    position in space, (ii) efficiently querying that map using ray directions to
    estimate the pixel values in the rendered image. Extensive experiments show
    that the proposed method is 3000 times faster than the original NeRF algorithm
    and at least an order of magnitude faster than existing work on accelerating
    NeRF, while maintaining visual quality and extensibility.},
    YEAR = {2021},
    MONTH = {Mar},
    URL = {http://arxiv.org/abs/2103.10380v2},
    FILE = {2103.10380v2.pdf}
}

@article{xie2021fignerf,
    AUTHOR = {Christopher Xie and Keunhong Park and Ricardo Martin-Brualla and Matthew Brown},
    TITLE = {FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category
    Modelling},
    EPRINT = {2104.08418v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We investigate the use of Neural Radiance Fields (NeRF) to learn high quality
    3D object category models from collections of input images. In contrast to
    previous work, we are able to do this whilst simultaneously separating
    foreground objects from their varying backgrounds. We achieve this via a
    2-component NeRF model, FiG-NeRF, that prefers explanation of the scene as a
    geometrically constant background and a deformable foreground that represents
    the object category. We show that this method can learn accurate 3D object
    category models using only photometric supervision and casually captured images
    of the objects. Additionally, our 2-part decomposition allows the model to
    perform accurate and crisp amodal segmentation. We quantitatively evaluate our
    method with view synthesis and image fidelity metrics, using synthetic,
    lab-captured, and in-the-wild data. Our results demonstrate convincing 3D
    object category modelling that exceed the performance of existing methods.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.08418v1},
    FILE = {2104.08418v1.pdf}
}

@article{hertz2021sape,
    AUTHOR = {Amir Hertz and Or Perel and Raja Giryes and Olga Sorkine-Hornung and Daniel Cohen-Or},
    TITLE = {SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization},
    EPRINT = {2104.09125v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.LG},
    ABSTRACT = {Multilayer-perceptrons (MLP) are known to struggle with learning functions of
    high-frequencies, and in particular cases with wide frequency bands. We present
    a spatially adaptive progressive encoding (SAPE) scheme for input signals of
    MLP networks, which enables them to better fit a wide range of frequencies
    without sacrificing training stability or requiring any domain specific
    preprocessing. SAPE gradually unmasks signal components with increasing
    frequencies as a function of time and space. The progressive exposure of
    frequencies is monitored by a feedback loop throughout the neural optimization
    process, allowing changes to propagate at different rates among local spatial
    portions of the signal space. We demonstrate the advantage of SAPE on a variety
    of domains and applications, including regression of low dimensional signals
    and images, representation learning of occupancy networks, and a geometric task
    of mesh transfer between 3D shapes.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.09125v2},
    FILE = {2104.09125v2.pdf}
}

@article{derksen2021snerf,
    AUTHOR = {Dawa Derksen and Dario Izzo},
    TITLE = {Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry},
    EPRINT = {2104.09877v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present a new generic method for shadow-aware multi-view satellite
    photogrammetry of Earth Observation scenes. Our proposed method, the Shadow
    Neural Radiance Field (S-NeRF) follows recent advances in implicit volumetric
    representation learning. For each scene, we train S-NeRF using very high
    spatial resolution optical images taken from known viewing angles. The learning
    requires no labels or shape priors: it is self-supervised by an image
    reconstruction loss. To accommodate for changing light source conditions both
    from a directional light source (the Sun) and a diffuse light source (the sky),
    we extend the NeRF approach in two ways. First, direct illumination from the
    Sun is modeled via a local light source visibility field. Second, indirect
    illumination from a diffuse light source is learned as a non-local color field
    as a function of the position of the Sun. Quantitatively, the combination of
    these factors reduces the altitude and color errors in shaded areas, compared
    to NeRF. The S-NeRF methodology not only performs novel view synthesis and full
    3D shape estimation, it also enables shadow detection, albedo synthesis, and
    transient object filtering, without any explicit shape supervision.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.09877v1},
    FILE = {2104.09877v1.pdf}
}

@article{oechsle2021unisurf,
    AUTHOR = {Michael Oechsle and Songyou Peng and Andreas Geiger},
    TITLE = {UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for
    Multi-View Reconstruction},
    EPRINT = {2104.10078v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Neural implicit 3D representations have emerged as a powerful paradigm for
    reconstructing surfaces from multi-view images and synthesizing novel views.
    Unfortunately, existing methods such as DVR or IDR require accurate per-pixel
    object masks as supervision. At the same time, neural radiance fields have
    revolutionized novel view synthesis. However, NeRF's estimated volume density
    does not admit accurate surface reconstruction. Our key insight is that
    implicit surface models and radiance fields can be formulated in a unified way,
    enabling both surface and volume rendering using the same model. This unified
    perspective enables novel, more efficient sampling procedures and the ability
    to reconstruct accurate surfaces without input masks. We compare our method on
    the DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments
    demonstrate that we outperform NeRF in terms of reconstruction quality while
    performing on par with IDR without requiring masks.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.10078v1},
    FILE = {2104.10078v1.pdf}
}

@article{reed2021inr,
    AUTHOR = {Albert W. Reed and Hyojin Kim and Rushil Anirudh and K. Aditya Mohan and Kyle Champley and Jingu Kang and Suren Jayasuriya},
    TITLE = {Dynamic CT Reconstruction from Limited Views with Implicit Neural
    Representations and Parametric Motion Fields},
    EPRINT = {2104.11745v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {eess.IV},
    ABSTRACT = {Reconstructing dynamic, time-varying scenes with computed tomography (4D-CT)
    is a challenging and ill-posed problem common to industrial and medical
    settings. Existing 4D-CT reconstructions are designed for sparse sampling
    schemes that require fast CT scanners to capture multiple, rapid revolutions
    around the scene in order to generate high quality results. However, if the
    scene is moving too fast, then the sampling occurs along a limited view and is
    difficult to reconstruct due to spatiotemporal ambiguities. In this work, we
    design a reconstruction pipeline using implicit neural representations coupled
    with a novel parametric motion field warping to perform limited view 4D-CT
    reconstruction of rapidly deforming scenes. Importantly, we utilize a
    differentiable analysis-by-synthesis approach to compare with captured x-ray
    sinogram data in a self-supervised fashion. Thus, our resulting optimization
    method requires no training data to reconstruct the scene. We demonstrate that
    our proposed system robustly reconstructs scenes containing deformable and
    periodic motion and validate against state-of-the-art baselines. Further, we
    demonstrate an ability to reconstruct continuous spatiotemporal representations
    of our scenes and upsample them to arbitrary volumes and frame rates
    post-optimization. This research opens a new avenue for implicit neural
    representations in computed tomography reconstruction in general.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.11745v1},
    FILE = {2104.11745v1.pdf}
}

@article{knodt2021neural raytracing,
    AUTHOR = {Julian Knodt and Seung-Hwan Baek and Felix Heide},
    TITLE = {Neural Ray-Tracing: Learning Surfaces and Reflectance for Relighting and
    View Synthesis},
    EPRINT = {2104.13562v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Recent neural rendering methods have demonstrated accurate view interpolation
    by predicting volumetric density and color with a neural network. Although such
    volumetric representations can be supervised on static and dynamic scenes,
    existing methods implicitly bake the complete scene light transport into a
    single neural network for a given scene, including surface modeling,
    bidirectional scattering distribution functions, and indirect lighting effects.
    In contrast to traditional rendering pipelines, this prohibits changing surface
    reflectance, illumination, or composing other objects in the scene.
    In this work, we explicitly model the light transport between scene surfaces
    and we rely on traditional integration schemes and the rendering equation to
    reconstruct a scene. The proposed method allows BSDF recovery with unknown
    light conditions and classic light transports such as pathtracing. By learning
    decomposed transport with surface representations established in conventional
    rendering methods, the method naturally facilitates editing shape, reflectance,
    lighting and scene composition. The method outperforms NeRV for relighting
    under known lighting conditions, and produces realistic reconstructions for
    relit and edited scenes. We validate the proposed approach for scene editing,
    relighting and reflectance estimation learned from synthetic and captured views
    on a subset of NeRV's datasets.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.13562v1},
    FILE = {2104.13562v1.pdf}
}

@article{zhang2021stnerf,
    AUTHOR = {Jiakai Zhang and Xinhang Liu and Xinyi Ye and Fuqiang Zhao and Yanshun Zhang and Minye Wu and Yingliang Zhang and Lan Xu and Jingyi Yu},
    TITLE = {Editable Free-viewpoint Video Using a Layered Neural Representation},
    EPRINT = {2104.14786v1},
    DOI = {10.1145/3450626.3459756},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Generating free-viewpoint videos is critical for immersive VR/AR experience
    but recent neural advances still lack the editing ability to manipulate the
    visual perception for large dynamic scenes. To fill this gap, in this paper we
    propose the first approach for editable photo-realistic free-viewpoint video
    generation for large-scale dynamic scenes using only sparse 16 cameras. The
    core of our approach is a new layered neural representation, where each dynamic
    entity including the environment itself is formulated into a space-time
    coherent neural layered radiance representation called ST-NeRF. Such layered
    representation supports fully perception and realistic manipulation of the
    dynamic scene whilst still supporting a free viewing experience in a wide
    range. In our ST-NeRF, the dynamic entity/layer is represented as continuous
    functions, which achieves the disentanglement of location, deformation as well
    as the appearance of the dynamic entity in a continuous and self-supervised
    manner. We propose a scene parsing 4D label map tracking to disentangle the
    spatial information explicitly, and a continuous deform module to disentangle
    the temporal motion implicitly. An object-aware volume rendering scheme is
    further introduced for the re-assembling of all the neural layers. We adopt a
    novel layered loss and motion-aware ray sampling strategy to enable efficient
    training for a large dynamic scene with multiple performers, Our framework
    further enables a variety of editing functions, i.e., manipulating the scale
    and location, duplicating or retiming individual neural layers to create
    numerous visual effects while preserving high realism. Extensive experiments
    demonstrate the effectiveness of our approach to achieve high-quality,
    photo-realistic, and editable free-viewpoint video generation for dynamic
    scenes.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.14786v1},
    FILE = {2104.14786v1.pdf}
}

@article{bird2021cnerf,
    AUTHOR = {Thomas Bird and Johannes Balle and Saurabh Singh and Philip A. Chou},
    TITLE = {3D Scene Compression through Entropy Penalized Neural Representation
    Functions},
    EPRINT = {2104.12456v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Some forms of novel visual media enable the viewer to explore a 3D scene from
    arbitrary viewpoints, by interpolating between a discrete set of original
    views. Compared to 2D imagery, these types of applications require much larger
    amounts of storage space, which we seek to reduce. Existing approaches for
    compressing 3D scenes are based on a separation of compression and rendering:
    each of the original views is compressed using traditional 2D image formats;
    the receiver decompresses the views and then performs the rendering. We unify
    these steps by directly compressing an implicit representation of the scene, a
    function that maps spatial coordinates to a radiance vector field, which can
    then be queried to render arbitrary viewpoints. The function is implemented as
    a neural network and jointly trained for reconstruction as well as
    compressibility, in an end-to-end manner, with the use of an entropy penalty on
    the parameters. Our method significantly outperforms a state-of-the-art
    conventional approach for scene compression, achieving simultaneously higher
    quality reconstructions and lower bitrates. Furthermore, we show that the
    performance at lower bitrates can be improved by jointly representing multiple
    scenes using a soft form of parameter sharing.},
    YEAR = {2021},
    MONTH = {Apr},
    URL = {http://arxiv.org/abs/2104.12456v1},
    FILE = {2104.12456v1.pdf}
}

@article{peng2021animatable,
    AUTHOR = {Sida Peng and Junting Dong and Qianqian Wang and Shangzhan Zhang and Qing Shuai and Hujun Bao and Xiaowei Zhou},
    TITLE = {Animatable Neural Radiance Fields for Human Body Modeling},
    EPRINT = {2105.02872v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {This paper addresses the challenge of reconstructing an animatable human
    model from a multi-view video. Some recent works have proposed to decompose a
    dynamic scene into a canonical neural radiance field and a set of deformation
    fields that map observation-space points to the canonical space, thereby
    enabling them to learn the dynamic scene from images. However, they represent
    the deformation field as translational vector field or SE(3) field, which makes
    the optimization highly under-constrained. Moreover, these representations
    cannot be explicitly controlled by input motions. Instead, we introduce neural
    blend weight fields to produce the deformation fields. Based on the
    skeleton-driven deformation, blend weight fields are used with 3D human
    skeletons to generate observation-to-canonical and canonical-to-observation
    correspondences. Since 3D human skeletons are more observable, they can
    regularize the learning of deformation fields. Moreover, the learned blend
    weight fields can be combined with input skeletal motions to generate new
    deformation fields to animate the human model. Experiments show that our
    approach significantly outperforms recent human synthesis methods. The code
    will be available at https://zju3dv.github.io/animatable_nerf/.},
    YEAR = {2021},
    MONTH = {May},
    URL = {http://arxiv.org/abs/2105.02872v1},
    FILE = {2105.02872v1.pdf}
}

@article{martel2021acorn,
    AUTHOR = {Julien N. P. Martel and David B. Lindell and Connor Z. Lin and Eric R. Chan and Marco Monteiro and Gordon Wetzstein},
    TITLE = {ACORN: Adaptive Coordinate Networks for Neural Scene Representation},
    EPRINT = {2105.02788v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Neural representations have emerged as a new paradigm for applications in
    rendering, imaging, geometric modeling, and simulation. Compared to traditional
    representations such as meshes, point clouds, or volumes they can be flexibly
    incorporated into differentiable learning-based pipelines. While recent
    improvements to neural representations now make it possible to represent
    signals with fine details at moderate resolutions (e.g., for images and 3D
    shapes), adequately representing large-scale or complex scenes has proven a
    challenge. Current neural representations fail to accurately represent images
    at resolutions greater than a megapixel or 3D scenes with more than a few
    hundred thousand polygons. Here, we introduce a new hybrid implicit-explicit
    network architecture and training strategy that adaptively allocates resources
    during training and inference based on the local complexity of a signal of
    interest. Our approach uses a multiscale block-coordinate decomposition,
    similar to a quadtree or octree, that is optimized during training. The network
    architecture operates in two stages: using the bulk of the network parameters,
    a coordinate encoder generates a feature grid in a single forward pass. Then,
    hundreds or thousands of samples within each block can be efficiently evaluated
    using a lightweight feature decoder. With this hybrid implicit-explicit network
    architecture, we demonstrate the first experiments that fit gigapixel images to
    nearly 40 dB peak signal-to-noise ratio. Notably this represents an increase in
    scale of over 1000x compared to the resolution of previously demonstrated
    image-fitting experiments. Moreover, our approach is able to represent 3D
    shapes significantly faster and better than previous techniques; it reduces
    training times from days to hours or minutes and memory requirements by over an
    order of magnitude.},
    YEAR = {2021},
    MONTH = {May},
    URL = {http://arxiv.org/abs/2105.02788v1},
    FILE = {2105.02788v1.pdf}
}

@article{isik2021neural,
    AUTHOR = {Berivan Isik},
    TITLE = {Neural 3D Scene Compression via Model Compression},
    EPRINT = {2105.03120v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Rendering 3D scenes requires access to arbitrary viewpoints from the scene.
    Storage of such a 3D scene can be done in two ways; (1) storing 2D images taken
    from the 3D scene that can reconstruct the scene back through interpolations,
    or (2) storing a representation of the 3D scene itself that already encodes
    views from all directions. So far, traditional 3D compression methods have
    focused on the first type of storage and compressed the original 2D images with
    image compression techniques. With this approach, the user first decodes the
    stored 2D images and then renders the 3D scene. However, this separated
    procedure is inefficient since a large amount of 2D images have to be stored.
    In this work, we take a different approach and compress a functional
    representation of 3D scenes. In particular, we introduce a method to compress
    3D scenes by compressing the neural networks that represent the scenes as
    neural radiance fields. Our method provides more efficient storage of 3D scenes
    since it does not store 2D images -- which are redundant when we render the
    scene from the neural functional representation.},
    YEAR = {2021},
    MONTH = {May},
    URL = {http://arxiv.org/abs/2105.03120v1},
    FILE = {2105.03120v1.pdf}
}

@article{mergy2021visionbased,
    AUTHOR = {Anne Mergy and Gurvan Lecuyer and Dawa Derksen and Dario Izzo},
    TITLE = {Vision-based Neural Scene Representations for Spacecraft},
    EPRINT = {2105.06405v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {In advanced mission concepts with high levels of autonomy, spacecraft need to
    internally model the pose and shape of nearby orbiting objects. Recent works in
    neural scene representations show promising results for inferring generic
    three-dimensional scenes from optical images. Neural Radiance Fields (NeRF)
    have shown success in rendering highly specular surfaces using a large number
    of images and their pose. More recently, Generative Radiance Fields (GRAF)
    achieved full volumetric reconstruction of a scene from unposed images only,
    thanks to the use of an adversarial framework to train a NeRF. In this paper,
    we compare and evaluate the potential of NeRF and GRAF to render novel views
    and extract the 3D shape of two different spacecraft, the Soil Moisture and
    Ocean Salinity satellite of ESA's Living Planet Programme and a generic cube
    sat. Considering the best performances of both models, we observe that NeRF has
    the ability to render more accurate images regarding the material specularity
    of the spacecraft and its pose. For its part, GRAF generates precise novel
    views with accurate details even when parts of the satellites are shadowed
    while having the significant advantage of not needing any information about the
    relative pose.},
    YEAR = {2021},
    MONTH = {May},
    URL = {http://arxiv.org/abs/2105.06405v1},
    FILE = {2105.06405v1.pdf}
}

@article{chen2021nefnet,
    AUTHOR = {Jintai Chen and Xiangshang Zheng and Hongyun Yu and Danny Z. Chen and Jian Wu},
    TITLE = {Electrocardio Panorama: Synthesizing New ECG Views with Self-supervision},
    EPRINT = {2105.06293v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {eess.SP},
    ABSTRACT = {Multi-lead electrocardiogram (ECG) provides clinical information of
    heartbeats from several fixed viewpoints determined by the lead positioning.
    However, it is often not satisfactory to visualize ECG signals in these fixed
    and limited views, as some clinically useful information is represented only
    from a few specific ECG viewpoints. For the first time, we propose a new
    concept, Electrocardio Panorama, which allows visualizing ECG signals from any
    queried viewpoints. To build Electrocardio Panorama, we assume that an
    underlying electrocardio field exists, representing locations, magnitudes, and
    directions of ECG signals. We present a Neural electrocardio field Network
    (Nef-Net), which first predicts the electrocardio field representation by using
    a sparse set of one or few input ECG views and then synthesizes Electrocardio
    Panorama based on the predicted representations. Specially, to better
    disentangle electrocardio field information from viewpoint biases, a new
    Angular Encoding is proposed to process viewpoint angles. Also, we propose a
    self-supervised learning approach called Standin Learning, which helps model
    the electrocardio field without direct supervision. Further, with very few
    modifications, Nef-Net can also synthesize ECG signals from scratch.
    Experiments verify that our Nef-Net performs well on Electrocardio Panorama
    synthesis, and outperforms the previous work on the auxiliary tasks (ECG view
    transformation and ECG synthesis from scratch). The codes and the division
    labels of cardiac cycles and ECG deflections on Tianchi ECG and PTB datasets
    are available at https://github.com/WhatAShot/Electrocardio-Panorama.},
    YEAR = {2021},
    MONTH = {May},
    NOTE = {the 30th International Joint Conference on Artificial Intelligence
    (2021)},
    URL = {http://arxiv.org/abs/2105.06293v1},
    FILE = {2105.06293v1.pdf}
}

@article{wang2021dctnerf,
    AUTHOR = {Chaoyang Wang and Ben Eckart and Simon Lucey and Orazio Gallo},
    TITLE = {Neural Trajectory Fields for Dynamic Novel View Synthesis},
    EPRINT = {2105.05994v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Recent approaches to render photorealistic views from a limited set of
    photographs have pushed the boundaries of our interactions with pictures of
    static scenes. The ability to recreate moments, that is, time-varying
    sequences, is perhaps an even more interesting scenario, but it remains largely
    unsolved. We introduce DCT-NeRF, a coordinatebased neural representation for
    dynamic scenes. DCTNeRF learns smooth and stable trajectories over the input
    sequence for each point in space. This allows us to enforce consistency between
    any two frames in the sequence, which results in high quality reconstruction,
    particularly in dynamic regions.},
    YEAR = {2021},
    MONTH = {May},
    URL = {http://arxiv.org/abs/2105.05994v1},
    FILE = {2105.05994v1.pdf}
}

@article{liu2021editing,
    AUTHOR = {Steven Liu and Xiuming Zhang and Zhoutong Zhang and Richard Zhang and Jun-Yan Zhu and Bryan Russell},
    TITLE = {Editing Conditional Radiance Fields},
    EPRINT = {2105.06466v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {A neural radiance field (NeRF) is a scene model supporting high-quality view
    synthesis, optimized per scene. In this paper, we explore enabling user editing
    of a category-level NeRF - also known as a conditional radiance field - trained
    on a shape category. Specifically, we introduce a method for propagating coarse
    2D user scribbles to the 3D space, to modify the color or shape of a local
    region. First, we propose a conditional radiance field that incorporates new
    modular network components, including a shape branch that is shared across
    object instances. Observing multiple instances of the same category, our model
    learns underlying part semantics without any supervision, thereby allowing the
    propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair
    seat). Next, we propose a hybrid network update strategy that targets specific
    network components, which balances efficiency and accuracy. During user
    interaction, we formulate an optimization problem that both satisfies the
    user's constraints and preserves the original object structure. We demonstrate
    our approach on various editing tasks over three shape datasets and show that
    it outperforms prior neural editing approaches. Finally, we edit the appearance
    and shape of a real photograph and show that the edit propagates to
    extrapolated novel views.},
    YEAR = {2021},
    MONTH = {May},
    URL = {http://arxiv.org/abs/2105.06466v2},
    FILE = {2105.06466v2.pdf}
}

@article{gao2021dynamic,
    AUTHOR = {Chen Gao and Ayush Saraf and Johannes Kopf and Jia-Bin Huang},
    TITLE = {Dynamic View Synthesis from Dynamic Monocular Video},
    EPRINT = {2105.06468v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present an algorithm for generating novel views at arbitrary viewpoints
    and any input time step given a monocular video of a dynamic scene. Our work
    builds upon recent advances in neural implicit representation and uses
    continuous and differentiable functions for modeling the time-varying structure
    and the appearance of the scene. We jointly train a time-invariant static NeRF
    and a time-varying dynamic NeRF, and learn how to blend the results in an
    unsupervised manner. However, learning this implicit function from a single
    video is highly ill-posed (with infinitely many solutions that match the input
    video). To resolve the ambiguity, we introduce regularization losses to
    encourage a more physically plausible solution. We show extensive quantitative
    and qualitative results of dynamic view synthesis from casually captured
    videos.},
    YEAR = {2021},
    MONTH = {May},
    URL = {http://arxiv.org/abs/2105.06468v1},
    FILE = {2105.06468v1.pdf}
}

@article{liu2021neulf,
    AUTHOR = {Celong Liu and Zhong Li and Junsong Yuan and Yi Xu},
    TITLE = {NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field},
    EPRINT = {2105.07112v4},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {In this paper, we present an efficient and robust deep learning solution for
    novel view synthesis of complex scenes. In our approach, a 3D scene is
    represented as a light field, i.e., a set of rays, each of which has a
    corresponding color when reaching the image plane. For efficient novel view
    rendering, we adopt a 4D parameterization of the light field, where each ray is
    characterized by a 4D parameter. We then formulate the light field as a 4D
    function that maps 4D coordinates to corresponding color values. We train a
    deep fully connected network to optimize this implicit function and memorize
    the 3D scene. Then, the scene-specific model is used to synthesize novel views.
    Different from previous light field approaches which require dense view
    sampling to reliably render novel views, our method can render novel views by
    sampling rays and querying the color for each ray from the network directly,
    thus enabling high-quality light field rendering with a sparser set of training
    images. Our method achieves state-of-the-art novel view synthesis results while
    maintaining an interactive frame rate.},
    YEAR = {2021},
    MONTH = {May},
    URL = {http://arxiv.org/abs/2105.07112v4},
    FILE = {2105.07112v4.pdf}
}

@article{yang2021recursivenerf,
    AUTHOR = {Guo-Wei Yang and Wen-Yang Zhou and Hao-Yang Peng and Dun Liang and Tai-Jiang Mu and Shi-Min Hu},
    TITLE = {Recursive-NeRF: An Efficient and Dynamically Growing NeRF},
    EPRINT = {2105.09103v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {View synthesis methods using implicit continuous shape representations
    learned from a set of images, such as the Neural Radiance Field (NeRF) method,
    have gained increasing attention due to their high quality imagery and
    scalability to high resolution. However, the heavy computation required by its
    volumetric approach prevents NeRF from being useful in practice; minutes are
    taken to render a single image of a few megapixels. Now, an image of a scene
    can be rendered in a level-of-detail manner, so we posit that a complicated
    region of the scene should be represented by a large neural network while a
    small neural network is capable of encoding a simple region, enabling a balance
    between efficiency and quality. Recursive-NeRF is our embodiment of this idea,
    providing an efficient and adaptive rendering and training approach for NeRF.
    The core of Recursive-NeRF learns uncertainties for query coordinates,
    representing the quality of the predicted color and volumetric intensity at
    each level. Only query coordinates with high uncertainties are forwarded to the
    next level to a bigger neural network with a more powerful representational
    capability. The final rendered image is a composition of results from neural
    networks of all levels. Our evaluation on three public datasets shows that
    Recursive-NeRF is more efficient than NeRF while providing state-of-the-art
    quality. The code will be available at https://github.com/Gword/Recursive-NeRF.},
    YEAR = {2021},
    MONTH = {May},
    URL = {http://arxiv.org/abs/2105.09103v1},
    FILE = {2105.09103v1.pdf}
}

@article{hadadan2021neural,
    AUTHOR = {Saeed Hadadan and Shuhong Chen and Matthias Zwicker},
    TITLE = {Neural Radiosity},
    EPRINT = {2105.12319v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.GR},
    ABSTRACT = {We introduce Neural Radiosity, an algorithm to solve the rendering equation
    by minimizing the norm of its residual similar as in traditional radiosity
    techniques. Traditional basis functions used in radiosity techniques, such as
    piecewise polynomials or meshless basis functions are typically limited to
    representing isotropic scattering from diffuse surfaces. Instead, we propose to
    leverage neural networks to represent the full four-dimensional radiance
    distribution, directly optimizing network parameters to minimize the norm of
    the residual. Our approach decouples solving the rendering equation from
    rendering (perspective) images similar as in traditional radiosity techniques,
    and allows us to efficiently synthesize arbitrary views of a scene. In
    addition, we propose a network architecture using geometric learnable features
    that improves convergence of our solver compared to previous techniques. Our
    approach leads to an algorithm that is simple to implement, and we demonstrate
    its effectiveness on a variety of scenes with non-diffuse surfaces.},
    YEAR = {2021},
    MONTH = {May},
    URL = {http://arxiv.org/abs/2105.12319v1},
    FILE = {2105.12319v1.pdf}
}

@article{chiang2021stylizing,
    AUTHOR = {Pei-Ze Chiang and Meng-Shiun Tsai and Hung-Yu Tseng and Wei-sheng Lai and Wei-Chen Chiu},
    TITLE = {Stylizing 3D Scene via Implicit Representation and HyperNetwork},
    EPRINT = {2105.13016v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {In this work, we aim to address the 3D scene stylization problem - generating
    stylized images of the scene at arbitrary novel view angles. A straightforward
    solution is to combine existing novel view synthesis and image/video style
    transfer approaches, which often leads to blurry results or inconsistent
    appearance. Inspired by the high quality results of the neural radiance fields
    (NeRF) method, we propose a joint framework to directly render novel views with
    the desired style. Our framework consists of two components: an implicit
    representation of the 3D scene with the neural radiance field model, and a
    hypernetwork to transfer the style information into the scene representation.
    In particular, our implicit representation model disentangles the scene into
    the geometry and appearance branches, and the hypernetwork learns to predict
    the parameters of the appearance branch from the reference style image. To
    alleviate the training difficulties and memory burden, we propose a two-stage
    training procedure and a patch sub-sampling approach to optimize the style and
    content losses with the neural radiance field model. After optimization, our
    model is able to render consistent novel views at arbitrary view angles with
    arbitrary style. Both quantitative evaluation and human subject study have
    demonstrated that the proposed method generates faithful stylization results
    with consistent appearance across different views.},
    YEAR = {2021},
    MONTH = {May},
    URL = {http://arxiv.org/abs/2105.13016v2},
    FILE = {2105.13016v2.pdf}
}

@article{izzo2021geodesynets,
    AUTHOR = {Dario Izzo and Pablo Gomez},
    TITLE = {Geodesy of irregular small bodies via neural density fields: geodesyNets},
    EPRINT = {2105.13031v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {astro-ph.EP},
    ABSTRACT = {We present a novel approach based on artificial neural networks, so-called
    geodesyNets, and present compelling evidence of their ability to serve as
    accurate geodetic models of highly irregular bodies using minimal prior
    information on the body. The approach does not rely on the body shape
    information but, if available, can harness it. GeodesyNets learn a
    three-dimensional, differentiable, function representing the body density,
    which we call neural density field. The body shape, as well as other geodetic
    properties, can easily be recovered. We investigate six different shapes
    including the bodies 101955 Bennu, 67P Churyumov-Gerasimenko, 433 Eros and
    25143 Itokawa for which shape models developed during close proximity surveys
    are available. Both heterogeneous and homogeneous mass distributions are
    considered. The gravitational acceleration computed from the trained
    geodesyNets models, as well as the inferred body shape, show great accuracy in
    all cases with a relative error on the predicted acceleration smaller than 1\%
    even close to the asteroid surface. When the body shape information is
    available, geodesyNets can seamlessly exploit it and be trained to represent a
    high-fidelity neural density field able to give insights into the internal
    structure of the body. This work introduces a new unexplored approach to
    geodesy, adding a powerful tool to consolidated ones based on spherical
    harmonics, mascon models and polyhedral gravity.},
    YEAR = {2021},
    MONTH = {May},
    URL = {http://arxiv.org/abs/2105.13031v1},
    FILE = {2105.13031v1.pdf}
}

@article{zhang2021nerfactor,
    AUTHOR = {Xiuming Zhang and Pratul P. Srinivasan and Boyang Deng and Paul Debevec and William T. Freeman and Jonathan T. Barron},
    TITLE = {NeRFactor: Neural Factorization of Shape and Reflectance Under an
    Unknown Illumination},
    EPRINT = {2106.01970v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We address the problem of recovering the shape and spatially-varying
    reflectance of an object from posed multi-view images of the object illuminated
    by one unknown lighting condition. This enables the rendering of novel views of
    the object under arbitrary environment lighting and editing of the object's
    material properties. The key to our approach, which we call Neural Radiance
    Factorization (NeRFactor), is to distill the volumetric geometry of a Neural
    Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object
    into a surface representation and then jointly refine the geometry while
    solving for the spatially-varying reflectance and the environment lighting.
    Specifically, NeRFactor recovers 3D neural fields of surface normals, light
    visibility, albedo, and Bidirectional Reflectance Distribution Functions
    (BRDFs) without any supervision, using only a re-rendering loss, simple
    smoothness priors, and a data-driven BRDF prior learned from real-world BRDF
    measurements. By explicitly modeling light visibility, NeRFactor is able to
    separate shadows from albedo and synthesize realistic soft or hard shadows
    under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D
    models for free-viewpoint relighting in this challenging and underconstrained
    capture setup for both synthetic and real scenes. Qualitative and quantitative
    experiments show that NeRFactor outperforms classic and deep learning-based
    state of the art across various tasks. Our code and data are available at
    people.csail.mit.edu/xiuming/projects/nerfactor/.},
    YEAR = {2021},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2106.01970v1},
    FILE = {2106.01970v1.pdf}
}

@article{wang2021spline,
    AUTHOR = {Peng-Shuai Wang and Yang Liu and Yu-Qi Yang and Xin Tong},
    TITLE = {Spline Positional Encoding for Learning 3D Implicit Signed Distance
    Fields},
    EPRINT = {2106.01553v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Multilayer perceptrons (MLPs) have been successfully used to represent 3D
    shapes implicitly and compactly, by mapping 3D coordinates to the corresponding
    signed distance values or occupancy values. In this paper, we propose a novel
    positional encoding scheme, called Spline Positional Encoding, to map the input
    coordinates to a high dimensional space before passing them to MLPs, for
    helping to recover 3D signed distance fields with fine-scale geometric details
    from unorganized 3D point clouds. We verified the superiority of our approach
    over other positional encoding schemes on tasks of 3D shape reconstruction from
    input point clouds and shape space learning. The efficacy of our approach
    extended to image reconstruction is also demonstrated and evaluated.},
    YEAR = {2021},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2106.01553v1},
    FILE = {2106.01553v1.pdf}
}

@article{liu2021na,
    AUTHOR = {Lingjie Liu and Marc Habermann and Viktor Rudnev and Kripasindhu Sarkar and Jiatao Gu and Christian Theobalt},
    TITLE = {Neural Actor: Neural Free-view Synthesis of Human Actors with Pose
    Control},
    EPRINT = {2106.02019v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We propose Neural Actor (NA), a new method for high-quality synthesis of
    humans from arbitrary viewpoints and under arbitrary controllable poses. Our
    method is built upon recent neural scene representation and rendering works
    which learn representations of geometry and appearance from only 2D images.
    While existing works demonstrated compelling rendering of static scenes and
    playback of dynamic scenes, photo-realistic reconstruction and rendering of
    humans with neural implicit methods, in particular under user-controlled novel
    poses, is still difficult. To address this problem, we utilize a coarse body
    model as the proxy to unwarp the surrounding 3D space into a canonical pose. A
    neural radiance field learns pose-dependent geometric deformations and pose-
    and view-dependent appearance effects in the canonical space from multi-view
    video input. To synthesize novel views of high fidelity dynamic geometry and
    appearance, we leverage 2D texture maps defined on the body model as latent
    variables for predicting residual deformations and the dynamic appearance.
    Experiments demonstrate that our method achieves better quality than the
    state-of-the-arts on playback as well as novel pose synthesis, and can even
    generalize well to new poses that starkly differ from the training poses.
    Furthermore, our method also supports body shape control of the synthesized
    results.},
    YEAR = {2021},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2106.02019v1},
    FILE = {2106.02019v1.pdf}
}

@article{sitzmann2021lfns,
    AUTHOR = {Vincent Sitzmann and Semon Rezchikov and William T. Freeman and Joshua B. Tenenbaum and Fredo Durand},
    TITLE = {Light Field Networks: Neural Scene Representations with
    Single-Evaluation Rendering},
    EPRINT = {2106.02634v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Inferring representations of 3D scenes from 2D observations is a fundamental
    problem of computer graphics, computer vision, and artificial intelligence.
    Emerging 3D-structured neural scene representations are a promising approach to
    3D scene understanding. In this work, we propose a novel neural scene
    representation, Light Field Networks or LFNs, which represent both geometry and
    appearance of the underlying 3D scene in a 360-degree, four-dimensional light
    field parameterized via a neural implicit representation. Rendering a ray from
    an LFN requires only a *single* network evaluation, as opposed to hundreds of
    evaluations per ray for ray-marching or volumetric based renderers in
    3D-structured neural scene representations. In the setting of simple scenes, we
    leverage meta-learning to learn a prior over LFNs that enables multi-view
    consistent light field reconstruction from as little as a single image
    observation. This results in dramatic reductions in time and memory complexity,
    and enables real-time rendering. The cost of storing a 360-degree light field
    via an LFN is two orders of magnitude lower than conventional methods such as
    the Lumigraph. Utilizing the analytical differentiability of neural implicit
    representations and a novel parameterization of light space, we further
    demonstrate the extraction of sparse depth maps from LFNs.},
    YEAR = {2021},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2106.02634v1},
    FILE = {2106.02634v1.pdf}
}

@article{rebain2021dmf,
    AUTHOR = {Daniel Rebain and Ke Li and Vincent Sitzmann and Soroosh Yazdani and Kwang Moo Yi and Andrea Tagliasacchi},
    TITLE = {Deep Medial Fields},
    EPRINT = {2106.03804v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.GR},
    ABSTRACT = {Implicit representations of geometry, such as occupancy fields or signed
    distance fields (SDF), have recently re-gained popularity in encoding 3D solid
    shape in a functional form. In this work, we introduce medial fields: a field
    function derived from the medial axis transform (MAT) that makes available
    information about the underlying 3D geometry that is immediately useful for a
    number of downstream tasks. In particular, the medial field encodes the local
    thickness of a 3D shape, and enables O(1) projection of a query point onto the
    medial axis. To construct the medial field we require nothing but the SDF of
    the shape itself, thus allowing its straightforward incorporation in any
    application that relies on signed distance fields. Working in unison with the
    O(1) surface projection supported by the SDF, the medial field opens the door
    for an entirely new set of efficient, shape-aware operations on implicit
    representations. We present three such applications, including a modification
    to sphere tracing that renders implicit representations with better convergence
    properties, a fast construction method for memory-efficient rigid-body
    collision proxies, and an efficient approximation of ambient occlusion that
    remains stable with respect to viewpoint variations.},
    YEAR = {2021},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2106.03804v1},
    FILE = {2106.03804v1.pdf}
}

@article{chen2021mocoflow,
    AUTHOR = {Xuelin Chen and Weiyu Li and Daniel Cohen-Or and Niloy J. Mitra and Baoquan Chen},
    TITLE = {MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in Stationary
    Monocular Cameras},
    EPRINT = {2106.04477v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Synthesizing novel views of dynamic humans from stationary monocular cameras
    is a popular scenario. This is particularly attractive as it does not require
    static scenes, controlled environments, or specialized hardware. In contrast to
    techniques that exploit multi-view observations to constrain the modeling,
    given a single fixed viewpoint only, the problem of modeling the dynamic scene
    is significantly more under-constrained and ill-posed. In this paper, we
    introduce Neural Motion Consensus Flow (MoCo-Flow), a representation that
    models the dynamic scene using a 4D continuous time-variant function. The
    proposed representation is learned by an optimization which models a dynamic
    scene that minimizes the error of rendering all observation images. At the
    heart of our work lies a novel optimization formulation, which is constrained
    by a motion consensus regularization on the motion flow. We extensively
    evaluate MoCo-Flow on several datasets that contain human motions of varying
    complexity, and compare, both qualitatively and quantitatively, to several
    baseline methods and variants of our methods. Pretrained model, code, and data
    will be released for research purposes upon paper acceptance.},
    YEAR = {2021},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2106.04477v1},
    FILE = {2106.04477v1.pdf}
}

@article{shao2021doublefield,
    AUTHOR = {Ruizhi Shao and Hongwen Zhang and He Zhang and Yanpei Cao and Tao Yu and Yebin Liu},
    TITLE = {DoubleField: Bridging the Neural Surface and Radiance Fields for
    High-fidelity Human Rendering},
    EPRINT = {2106.03798v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We introduce DoubleField, a novel representation combining the merits of both
    surface field and radiance field for high-fidelity human rendering. Within
    DoubleField, the surface field and radiance field are associated together by a
    shared feature embedding and a surface-guided sampling strategy. In this way,
    DoubleField has a continuous but disentangled learning space for geometry and
    appearance modeling, which supports fast training, inference, and finetuning.
    To achieve high-fidelity free-viewpoint rendering, DoubleField is further
    augmented to leverage ultra-high-resolution inputs, where a view-to-view
    transformer and a transfer learning scheme are introduced for more efficient
    learning and finetuning from sparse-view inputs at original resolutions. The
    efficacy of DoubleField is validated by the quantitative evaluations on several
    datasets and the qualitative results in a real-world sparse multi-view system,
    showing its superior capability for photo-realistic free-viewpoint human
    rendering. For code and demo video, please refer to our project page:
    http://www.liuyebin.com/dbfield/dbfield.html.},
    YEAR = {2021},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2106.03798v2},
    FILE = {2106.03798v2.pdf}
}

@article{yifan2021idf,
    AUTHOR = {Wang Yifan and Lukas Rahmann and Olga Sorkine-Hornung},
    TITLE = {Geometry-Consistent Neural Shape Representation with Implicit
    Displacement Fields},
    EPRINT = {2106.05187v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present implicit displacement fields, a novel representation for detailed
    3D geometry. Inspired by a classic surface deformation technique, displacement
    mapping, our method represents a complex surface as a smooth base surface plus
    a displacement along the base's normal directions, resulting in a
    frequency-based shape decomposition, where the high frequency signal is
    constrained geometrically by the low frequency signal. Importantly, this
    disentanglement is unsupervised thanks to a tailored architectural design that
    has an innate frequency hierarchy by construction. We explore implicit
    displacement field surface reconstruction and detail transfer and demonstrate
    superior representational power, training stability and generalizability.},
    YEAR = {2021},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2106.05187v2},
    FILE = {2106.05187v2.pdf}
}

@article{henderson2021unsupervised,
    AUTHOR = {Paul Henderson and Christoph H. Lampert and Bernd Bickel},
    TITLE = {Unsupervised Video Prediction from a Single Frame by Estimating 3D
    Dynamic Scene Structure},
    EPRINT = {2106.09051v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Our goal in this work is to generate realistic videos given just one initial
    frame as input. Existing unsupervised approaches to this task do not consider
    the fact that a video typically shows a 3D environment, and that this should
    remain coherent from frame to frame even as the camera and objects move. We
    address this by developing a model that first estimates the latent 3D structure
    of the scene, including the segmentation of any moving objects. It then
    predicts future frames by simulating the object and camera dynamics, and
    rendering the resulting views. Importantly, it is trained end-to-end using only
    the unsupervised objective of predicting future frames, without any 3D
    information nor segmentation annotations. Experiments on two challenging
    datasets of natural videos show that our model can estimate 3D structure and
    motion segmentation from a single frame, and hence generate plausible and
    varied predictions.},
    YEAR = {2021},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2106.09051v1},
    FILE = {2106.09051v1.pdf}
}

@article{wang2021neus,
    AUTHOR = {Peng Wang and Lingjie Liu and Yuan Liu and Christian Theobalt and Taku Komura and Wenping Wang},
    TITLE = {NeuS: Learning Neural Implicit Surfaces by Volume Rendering for
    Multi-view Reconstruction},
    EPRINT = {2106.10689v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present a novel neural surface reconstruction method, called NeuS, for
    reconstructing objects and scenes with high fidelity from 2D image inputs.
    Existing neural surface reconstruction approaches, such as DVR and IDR, require
    foreground mask as supervision, easily get trapped in local minima, and
    therefore struggle with the reconstruction of objects with severe
    self-occlusion or thin structures. Meanwhile, recent neural methods for novel
    view synthesis, such as NeRF and its variants, use volume rendering to produce
    a neural scene representation with robustness of optimization, even for highly
    complex objects. However, extracting high-quality surfaces from this learned
    implicit representation is difficult because there are not sufficient surface
    constraints in the representation. In NeuS, we propose to represent a surface
    as the zero-level set of a signed distance function (SDF) and develop a new
    volume rendering method to train a neural SDF representation. We observe that
    the conventional volume rendering method causes inherent geometric errors (i.e.
    bias) for surface reconstruction, and therefore propose a new formulation that
    is free of bias in the first order of approximation, thus leading to more
    accurate surface reconstruction even without the mask supervision. Experiments
    on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the
    state-of-the-arts in high-quality surface reconstruction, especially for
    objects and scenes with complex structures and self-occlusion.},
    YEAR = {2021},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2106.10689v1},
    FILE = {2106.10689v1.pdf}
}

@article{hsu2021omninerf,
    AUTHOR = {Ching-Yu Hsu and Cheng Sun and Hwann-Tzong Chen},
    TITLE = {Moving in a 360 World: Synthesizing Panoramic Parallaxes from a Single
    Panorama},
    EPRINT = {2106.10859v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present Omnidirectional Neural Radiance Fields (OmniNeRF), the first
    method to the application of parallax-enabled novel panoramic view synthesis.
    Recent works for novel view synthesis focus on perspective images with limited
    field-of-view and require sufficient pictures captured in a specific condition.
    Conversely, OmniNeRF can generate panorama images for unknown viewpoints given
    a single equirectangular image as training data. To this end, we propose to
    augment the single RGB-D panorama by projecting back and forth between a 3D
    world and different 2D panoramic coordinates at different virtual camera
    positions. By doing so, we are able to optimize an Omnidirectional Neural
    Radiance Field with visible pixels collecting from omnidirectional viewing
    angles at a fixed center for the estimation of new viewing angles from varying
    camera positions. As a result, the proposed OmniNeRF achieves convincing
    renderings of novel panoramic views that exhibit the parallax effect. We
    showcase the effectiveness of each of our proposals on both synthetic and
    real-world datasets.},
    YEAR = {2021},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2106.10859v1},
    FILE = {2106.10859v1.pdf}
}

@article{yariv2021volsdf,
    AUTHOR = {Lior Yariv and Jiatao Gu and Yoni Kasten and Yaron Lipman},
    TITLE = {Volume Rendering of Neural Implicit Surfaces},
    EPRINT = {2106.12052v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Neural volume rendering became increasingly popular recently due to its
    success in synthesizing novel views of a scene from a sparse set of input
    images. So far, the geometry learned by neural volume rendering techniques was
    modeled using a generic density function. Furthermore, the geometry itself was
    extracted using an arbitrary level set of the density function leading to a
    noisy, often low fidelity reconstruction. The goal of this paper is to improve
    geometry representation and reconstruction in neural volume rendering. We
    achieve that by modeling the volume density as a function of the geometry. This
    is in contrast to previous work modeling the geometry as a function of the
    volume density. In more detail, we define the volume density function as
    Laplace's cumulative distribution function (CDF) applied to a signed distance
    function (SDF) representation. This simple density representation has three
    benefits: (i) it provides a useful inductive bias to the geometry learned in
    the neural volume rendering process; (ii) it facilitates a bound on the opacity
    approximation error, leading to an accurate sampling of the viewing ray.
    Accurate sampling is important to provide a precise coupling of geometry and
    radiance; and (iii) it allows efficient unsupervised disentanglement of shape
    and appearance in volume rendering. Applying this new density representation to
    challenging scene multiview datasets produced high quality geometry
    reconstructions, outperforming relevant baselines. Furthermore, switching shape
    and appearance between scenes is possible due to the disentanglement of the
    two.},
    YEAR = {2021},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2106.12052v1},
    FILE = {2106.12052v1.pdf}
}

@article{wang2021metaavatar,
    AUTHOR = {Shaofei Wang and Marko Mihajlovic and Qianli Ma and Andreas Geiger and Siyu Tang},
    TITLE = {MetaAvatar: Learning Animatable Clothed Human Models from Few Depth
    Images},
    EPRINT = {2106.11944v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {In this paper, we aim to create generalizable and controllable neural signed
    distance fields (SDFs) that represent clothed humans from monocular depth
    observations. Recent advances in deep learning, especially neural implicit
    representations, have enabled human shape reconstruction and controllable
    avatar generation from different sensor inputs. However, to generate realistic
    cloth deformations from novel input poses, watertight meshes or dense full-body
    scans are usually needed as inputs. Furthermore, due to the difficulty of
    effectively modeling pose-dependent cloth deformations for diverse body shapes
    and cloth types, existing approaches resort to per-subject/cloth-type
    optimization from scratch, which is computationally expensive. In contrast, we
    propose an approach that can quickly generate realistic clothed human avatars,
    represented as controllable neural SDFs, given only monocular depth images. We
    achieve this by using meta-learning to learn an initialization of a
    hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is
    conditioned on human poses and represents a clothed neural avatar that deforms
    non-rigidly according to the input poses. Meanwhile, it is meta-learned to
    effectively incorporate priors of diverse body shapes and cloth types and thus
    can be much faster to fine-tune, compared to models trained from scratch. We
    qualitatively and quantitatively show that our approach outperforms
    state-of-the-art approaches that require complete meshes as inputs while our
    approach requires only depth frames as inputs and runs orders of magnitudes
    faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very
    robust, being the first to generate avatars with realistic dynamic cloth
    deformations given as few as 8 monocular depth frames.},
    YEAR = {2021},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2106.11944v1},
    FILE = {2106.11944v1.pdf}
}

@article{muller2021realtime,
    AUTHOR = {Thomas Muller and Fabrice Rousselle and Jan Novak and Alexander Keller},
    TITLE = {Real-time Neural Radiance Caching for Path Tracing},
    EPRINT = {2106.12372v2},
    DOI = {10.1145/3450626.3459812},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.GR},
    ABSTRACT = {We present a real-time neural radiance caching method for path-traced global
    illumination. Our system is designed to handle fully dynamic scenes, and makes
    no assumptions about the lighting, geometry, and materials. The data-driven
    nature of our approach sidesteps many difficulties of caching algorithms, such
    as locating, interpolating, and updating cache points. Since pretraining neural
    networks to handle novel, dynamic scenes is a formidable generalization
    challenge, we do away with pretraining and instead achieve generalization via
    adaptation, i.e. we opt for training the radiance cache while rendering. We
    employ self-training to provide low-noise training targets and simulate
    infinite-bounce transport by merely iterating few-bounce training updates. The
    updates and cache queries incur a mild overhead -- about 2.6ms on full HD
    resolution -- thanks to a streaming implementation of the neural network that
    fully exploits modern hardware. We demonstrate significant noise reduction at
    the cost of little induced bias, and report state-of-the-art, real-time
    performance on a number of challenging scenarios.},
    YEAR = {2021},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2106.12372v2},
    FILE = {2106.12372v2.pdf}
}

@article{park2021hypernerf,
    AUTHOR = {Keunhong Park and Utkarsh Sinha and Peter Hedman and Jonathan T. Barron and Sofien Bouaziz and Dan B Goldman and Ricardo Martin-Brualla and Steven M. Seitz},
    TITLE = {HyperNeRF: A Higher-Dimensional Representation for Topologically Varying
    Neural Radiance Fields},
    EPRINT = {2106.13228v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Neural Radiance Fields (NeRF) are able to reconstruct scenes with
    unprecedented fidelity, and various recent works have extended NeRF to handle
    dynamic scenes. A common approach to reconstruct such non-rigid scenes is
    through the use of a learned deformation field mapping from coordinates in each
    input image into a canonical template coordinate space. However, these
    deformation-based approaches struggle to model changes in topology, as
    topological changes require a discontinuity in the deformation field, but these
    deformation fields are necessarily continuous. We address this limitation by
    lifting NeRFs into a higher dimensional space, and by representing the 5D
    radiance field corresponding to each individual input image as a slice through
    this "hyper-space". Our method is inspired by level set methods, which model
    the evolution of surfaces as slices through a higher dimensional surface. We
    evaluate our method on two tasks: (i) interpolating smoothly between "moments",
    i.e., configurations of the scene, seen in the input images while maintaining
    visual plausibility, and (ii) novel-view synthesis at fixed moments. We show
    that our method, which we dub HyperNeRF, outperforms existing methods on both
    tasks by significant margins. Compared to Nerfies, HyperNeRF reduces average
    error rates by 8.6% for interpolation and 8.8% for novel-view synthesis, as
    measured by LPIPS.},
    YEAR = {2021},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2106.13228v1},
    FILE = {2106.13228v1.pdf}
}

@article{chen2021animatable,
    AUTHOR = {Jianchuan Chen and Ying Zhang and Di Kang and Xuefei Zhe and Linchao Bao and Huchuan Lu},
    TITLE = {Animatable Neural Radiance Fields from Monocular RGB Video},
    EPRINT = {2106.13629v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {We present animatable neural radiance fields for detailed human avatar
    creation from monocular videos. Our approach extends neural radiance fields
    (NeRF) to the dynamic scenes with human movements via introducing explicit
    pose-guided deformation while learning the scene representation network. In
    particular, we estimate the human pose for each frame and learn a constant
    canonical space for the detailed human template, which enables natural shape
    deformation from the observation space to the canonical space under the
    explicit control of the pose parameters. To compensate for inaccurate pose
    estimation, we introduce the pose refinement strategy that updates the initial
    pose during the learning process, which not only helps to learn more accurate
    human reconstruction but also accelerates the convergence. In experiments we
    show that the proposed approach achieves 1) implicit human geometry and
    appearance reconstruction with high-quality details, 2) photo-realistic
    rendering of the human from arbitrary views, and 3) animation of the human with
    arbitrary poses.},
    YEAR = {2021},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2106.13629v1},
    FILE = {2106.13629v1.pdf}
}

@article{bergman2021metanlr++,
    AUTHOR = {Alexander W. Bergman and Petr Kellnhofer and Gordon Wetzstein},
    TITLE = {Fast Training of Neural Lumigraph Representations using Meta Learning},
    EPRINT = {2106.14942v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {Novel view synthesis is a long-standing problem in machine learning and
    computer vision. Significant progress has recently been made in developing
    neural scene representations and rendering techniques that synthesize
    photorealistic images from arbitrary views. These representations, however, are
    extremely slow to train and often also slow to render. Inspired by neural
    variants of image-based rendering, we develop a new neural rendering approach
    with the goal of quickly learning a high-quality representation which can also
    be rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a
    unique combination of a neural shape representation and 2D CNN-based image
    feature extraction, aggregation, and re-projection. To push representation
    convergence times down to minutes, we leverage meta learning to learn neural
    shape and image feature priors which accelerate training. The optimized shape
    and image features can then be extracted using traditional graphics techniques
    and rendered in real time. We show that MetaNLR++ achieves similar or better
    novel view synthesis results in a fraction of the time that competing methods
    require.},
    YEAR = {2021},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2106.14942v1},
    FILE = {2106.14942v1.pdf}
}

@article{wu2021irem,
    AUTHOR = {Qing Wu and Yuwei Li and Lan Xu and Ruiming Feng and Hongjiang Wei and Qing Yang and Boliang Yu and Xiaozhao Liu and Jingyi Yu and Yuyao Zhang},
    TITLE = {IREM: High-Resolution Magnetic Resonance (MR) Image Reconstruction via
    Implicit Neural Representation},
    EPRINT = {2106.15097v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {eess.IV},
    ABSTRACT = {For collecting high-quality high-resolution (HR) MR image, we propose a novel
    image reconstruction network named IREM, which is trained on multiple
    low-resolution (LR) MR images and achieve an arbitrary up-sampling rate for HR
    image reconstruction. In this work, we suppose the desired HR image as an
    implicit continuous function of the 3D image spatial coordinate and the
    thick-slice LR images as several sparse discrete samplings of this function.
    Then the super-resolution (SR) task is to learn the continuous volumetric
    function from a limited observations using an fully-connected neural network
    combined with Fourier feature positional encoding. By simply minimizing the
    error between the network prediction and the acquired LR image intensity across
    each imaging plane, IREM is trained to represent a continuous model of the
    observed tissue anatomy. Experimental results indicate that IREM succeeds in
    representing high frequency image feature, and in real scene data collection,
    IREM reduces scan time and achieves high-quality high-resolution MR imaging in
    terms of SNR and local image detail.},
    YEAR = {2021},
    MONTH = {Jun},
    URL = {http://arxiv.org/abs/2106.15097v1},
    FILE = {2106.15097v1.pdf}
}

@article{zheng2021rethinking,
    AUTHOR = {Jianqiao Zheng and Sameera Ramasinghe and Simon Lucey},
    TITLE = {Rethinking Positional Encoding},
    EPRINT = {2107.02561v2},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.LG},
    ABSTRACT = {It is well noted that coordinate based MLPs benefit greatly -- in terms of
    preserving high-frequency information -- through the encoding of coordinate
    positions as an array of Fourier features. Hitherto, the rationale for the
    effectiveness of these positional encodings has been solely studied through a
    Fourier lens. In this paper, we strive to broaden this understanding by showing
    that alternative non-Fourier embedding functions can indeed be used for
    positional encoding. Moreover, we show that their performance is entirely
    determined by a trade-off between the stable rank of the embedded matrix and
    the distance preservation between embedded coordinates. We further establish
    that the now ubiquitous Fourier feature mapping of position is a special case
    that fulfills these conditions. Consequently, we present a more general theory
    to analyze positional encoding in terms of shifted basis functions. To this
    end, we develop the necessary theoretical formulae and empirically verify that
    our theoretical claims hold in practice. Codes available at
    https://github.com/osiriszjq/Rethinking-positional-encoding.},
    YEAR = {2021},
    MONTH = {Jul},
    URL = {http://arxiv.org/abs/2107.02561v2},
    FILE = {2107.02561v2.pdf}
}

@article{deng2021dsnerf,
    AUTHOR = {Kangle Deng and Andrew Liu and Jun-Yan Zhu and Deva Ramanan},
    TITLE = {Depth-supervised NeRF: Fewer Views and Faster Training for Free},
    EPRINT = {2107.02791v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.CV},
    ABSTRACT = {One common failure mode of Neural Radiance Field (NeRF) models is fitting
    incorrect geometries when given an insufficient number of input views. We
    propose DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning
    neural radiance fields that takes advantage of readily-available depth
    supervision. Our key insight is that sparse depth supervision can be used to
    regularize the learned geometry, a crucial component for effectively rendering
    novel views using NeRF. We exploit the fact that current NeRF pipelines require
    images with known camera poses that are typically estimated by running
    structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that
    can be used as ``free" depth supervision during training: we simply add a loss
    to ensure that depth rendered along rays that intersect these 3D points is
    close to the observed depth. We find that DS-NeRF can render more accurate
    images given fewer training views while training 2-6x faster. With only two
    training views on real-world images, DS-NeRF significantly outperforms NeRF as
    well as other sparse-view variants. We show that our loss is compatible with
    these NeRF models, demonstrating that depth is a cheap and easily digestible
    supervisory signal. Finally, we show that DS-NeRF supports other types of depth
    supervision such as scanned depth sensors and RGBD reconstruction outputs.},
    YEAR = {2021},
    MONTH = {Jul},
    URL = {http://arxiv.org/abs/2107.02791v1},
    FILE = {2107.02791v1.pdf}
}

@article{li20213d,
    AUTHOR = {Yunzhu Li and Shuang Li and Vincent Sitzmann and Pulkit Agrawal and Antonio Torralba},
    TITLE = {3D Neural Scene Representations for Visuomotor Control},
    EPRINT = {2107.04004v1},
    ARCHIVEPREFIX = {arXiv},
    PRIMARYCLASS = {cs.RO},
    ABSTRACT = {Humans have a strong intuitive understanding of the 3D environment around us.
    The mental model of the physics in our brain applies to objects of different
    materials and enables us to perform a wide range of manipulation tasks that are
    far beyond the reach of current robots. In this work, we desire to learn models
    for dynamic 3D scenes purely from 2D visual observations. Our model combines
    Neural Radiance Fields (NeRF) and time contrastive learning with an
    autoencoding framework, which learns viewpoint-invariant 3D-aware scene
    representations. We show that a dynamics model, constructed over the learned
    representation space, enables visuomotor control for challenging manipulation
    tasks involving both rigid bodies and fluids, where the target is specified in
    a viewpoint different from what the robot operates on. When coupled with an
    auto-decoding framework, it can even support goal specification from camera
    viewpoints that are outside the training distribution. We further demonstrate
    the richness of the learned 3D dynamics model by performing future prediction
    and novel view synthesis. Finally, we provide detailed ablation studies
    regarding different system designs and qualitative analysis of the learned
    representations.},
    YEAR = {2021},
    MONTH = {Jul},
    URL = {http://arxiv.org/abs/2107.04004v1},
    FILE = {2107.04004v1.pdf}
}

@article{Pan:21,
    AUTHOR = {Hujie Pan and Di Xiao and Fuhao Zhang and Xuesong Li and Min Xu},
    JOURNAL = {Opt. Express},
    KEYWORDS = {Computational imaging; Computed tomography; Light fields; Light propagation; Neural networks; Propagation methods},
    NUMBER = {15},
    PAGES = {23682--23700},
    PUBLISHER = {OSA},
    TITLE = {Adaptive weight matrix and phantom intensity learning for computed tomography of chemiluminescence},
    VOLUME = {29},
    MONTH = {Jul},
    YEAR = {2021},
    URL = {http://www.opticsexpress.org/abstract.cfm?URI=oe-29-15-23682},
    DOI = {10.1364/OE.427459},
    ABSTRACT = {Classic algebraic reconstruction technique (ART) for computed tomography requires pre-determined weights of the voxels for the projected pixel values to build the equations. However, such weights cannot be accurately obtained in the application of chemiluminescence measurements due to the high physical complexity and computation resources required. Moreover, streaks arise in the results from ART method especially with imperfect projections. In this study, we propose a semi-case-wise learning-based method named Weight Encode Reconstruction Network (WERNet) to co-learn the target phantom intensities and the adaptive weight matrix of the case without labeling the target voxel set and thus offers a more applicable solution for computed tomography problems. Both numerical and experimental validations were conducted to evaluate the algorithm. In the numerical test, with the help of gradient normalization, the WERNet reconstructed voxel set with a high accuracy and showed a higher capability of denoising compared to the classic ART methods. In the experimental test, WERNet produces comparable results to the ART method while having a better performance in avoiding the streaks. Furthermore, with the adaptive weight matrix, WERNet is not sensitive to the ensemble intensity of the projection which shows much better robustness than ART method.},
}

