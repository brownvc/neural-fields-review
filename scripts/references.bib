@inproceedings{gargan1998approximating,
  TITLE = {Approximating reflectance functions using neural networks},
  AUTHOR = {Gargan, David and Neelamkavil, Francis},
  BOOKTITLE = {Eurographics Workshop on Rendering Techniques},
  PAGES = {23--34},
  YEAR = {1998},
  ORGANIZATION = {Springer}
 }

@book{lim20043d,
  TITLE = {3D Object Reconstruction and Representation Using Neural Networks},
  AUTHOR = {Lim, Wen Peng and Shamsuddin, Siti Mariyam},
  YEAR = {2004},
  PUBLISHER = {Universiti Teknologi Malaysia}
 }

@article{yang2017foldingnet,
  AUTHOR = {Yaoqing Yang and Chen Feng and Yiru Shen and Dong Tian},
  TITLE = {FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation},
  EPRINT = {1712.07262v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent deep networks that directly handle points in a point set, e.g.,PointNet, have been state-of-the-art for supervised learning tasks on pointclouds such as classification and segmentation. In this work, a novelend-to-end deep auto-encoder is proposed to address unsupervised learningchallenges on point clouds. On the encoder side, a graph-based enhancement isenforced to promote local structures on top of PointNet. Then, a novelfolding-based decoder deforms a canonical 2D grid onto the underlying 3D objectsurface of a point cloud, achieving low reconstruction errors even for objectswith delicate structures. The proposed decoder only uses about 7% parameters ofa decoder with fully-connected neural networks, yet leads to a morediscriminative representation that achieves higher linear SVM classificationaccuracy than the benchmark. In addition, the proposed decoder structure isshown, in theory, to be a generic architecture that is able to reconstruct anarbitrary point cloud from a 2D grid. Our code is available athttp://www.merl.com/research/license#FoldingNet},
  YEAR = {2017},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/1712.07262v2},
  FILE = {1712.07262v2.pdf}
 }

@article{groueix2018atlasnet,
  AUTHOR = {Thibault Groueix and Matthew Fisher and Vladimir G. Kim and Bryan C. Russell and Mathieu Aubry},
  TITLE = {AtlasNet: A Papier-Mache Approach to Learning 3D Surface Generation},
  EPRINT = {1802.05384v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce a method for learning to generate the surface of 3D shapes. Ourapproach represents a 3D shape as a collection of parametric surface elementsand, in contrast to methods generating voxel grids or point clouds, naturallyinfers a surface representation of the shape. Beyond its novelty, our new shapegeneration framework, AtlasNet, comes with significant advantages, such asimproved precision and generalization capabilities, and the possibility togenerate a shape of arbitrary resolution without memory issues. We demonstratethese benefits and compare to strong baselines on the ShapeNet benchmark fortwo applications: (i) auto-encoding shapes, and (ii) single-view reconstructionfrom a still image. We also provide results showing its potential for otherapplications, such as morphing, parametrization, super-resolution, matching,and co-segmentation.},
  YEAR = {2018},
  MONTH = {Feb},
  URL = {http://arxiv.org/abs/1802.05384v3},
  FILE = {1802.05384v3.pdf}
 }

@article{williams2018deep,
  AUTHOR = {Francis Williams and Teseo Schneider and Claudio Silva and Denis Zorin and Joan Bruna and Daniele Panozzo},
  TITLE = {Deep Geometric Prior for Surface Reconstruction},
  EPRINT = {1811.10943v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {The reconstruction of a discrete surface from a point cloud is a fundamentalgeometry processing problem that has been studied for decades, with manymethods developed. We propose the use of a deep neural network as a geometricprior for surface reconstruction. Specifically, we overfit a neural networkrepresenting a local chart parameterization to part of an input point cloudusing the Wasserstein distance as a measure of approximation. By jointlyfitting many such networks to overlapping parts of the point cloud, whileenforcing a consistency condition, we compute a manifold atlas. By samplingthis atlas, we can produce a dense reconstruction of the surface approximatingthe input cloud. The entire procedure does not require any training data orexplicit regularization, yet, we show that it is able to perform remarkablywell: not introducing typical overfitting artifacts, and approximating sharpfeatures closely at the same time. We experimentally show that this geometricprior produces good results for both man-made objects containing sharp featuresand smoother organic objects, as well as noisy inputs. We compare our methodwith a number of well-known reconstruction methods on a standard surfacereconstruction benchmark.},
  YEAR = {2018},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/1811.10943v2},
  FILE = {1811.10943v2.pdf}
 }

@article{chen2018imnet,
  AUTHOR = {Zhiqin Chen and Hao Zhang},
  TITLE = {Learning Implicit Fields for Generative Shape Modeling},
  EPRINT = {1812.02822v5},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {We advocate the use of implicit fields for learning generative models ofshapes and introduce an implicit field decoder, called IM-NET, for shapegeneration, aimed at improving the visual quality of the generated shapes. Animplicit field assigns a value to each point in 3D space, so that a shape canbe extracted as an iso-surface. IM-NET is trained to perform this assignment bymeans of a binary classifier. Specifically, it takes a point coordinate, alongwith a feature vector encoding a shape, and outputs a value which indicateswhether the point is outside the shape or not. By replacing conventionaldecoders by our implicit decoder for representation learning (via IM-AE) andshape generation (via IM-GAN), we demonstrate superior results for tasks suchas generative shape modeling, interpolation, and single-view 3D reconstruction,particularly in terms of visual quality. Code and supplementary material areavailable at https://github.com/czq142857/implicit-decoder.},
  YEAR = {2018},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/1812.02822v5},
  FILE = {1812.02822v5.pdf}
 }

@article{mescheder2018occupancy networks,
  AUTHOR = {Lars Mescheder and Michael Oechsle and Michael Niemeyer and Sebastian Nowozin and Andreas Geiger},
  TITLE = {Occupancy Networks: Learning 3D Reconstruction in Function Space},
  EPRINT = {1812.03828v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {With the advent of deep neural networks, learning-based approaches for 3Dreconstruction have gained popularity. However, unlike for images, in 3D thereis no canonical representation which is both computationally and memoryefficient yet allows for representing high-resolution geometry of arbitrarytopology. Many of the state-of-the-art learning-based 3D reconstructionapproaches can hence only represent very coarse 3D geometry or are limited to arestricted domain. In this paper, we propose Occupancy Networks, a newrepresentation for learning-based 3D reconstruction methods. Occupancy networksimplicitly represent the 3D surface as the continuous decision boundary of adeep neural network classifier. In contrast to existing approaches, ourrepresentation encodes a description of the 3D output at infinite resolutionwithout excessive memory footprint. We validate that our representation canefficiently encode 3D structure and can be inferred from various kinds ofinput. Our experiments demonstrate competitive results, both qualitatively andquantitatively, for the challenging tasks of 3D reconstruction from singleimages, noisy point clouds and coarse discrete voxel grids. We believe thatoccupancy networks will become a useful tool in a wide variety oflearning-based 3D tasks.},
  YEAR = {2018},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/1812.03828v2},
  FILE = {1812.03828v2.pdf}
 }

@article{park2019deepsdf,
  AUTHOR = {Jeong Joon Park and Peter Florence and Julian Straub and Richard Newcombe and Steven Lovegrove},
  TITLE = {DeepSDF: Learning Continuous Signed Distance Functions for ShapeRepresentation},
  EPRINT = {1901.05103v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Computer graphics, 3D computer vision and robotics communities have producedmultiple approaches to representing 3D geometry for rendering andreconstruction. These provide trade-offs across fidelity, efficiency andcompression capabilities. In this work, we introduce DeepSDF, a learnedcontinuous Signed Distance Function (SDF) representation of a class of shapesthat enables high quality shape representation, interpolation and completionfrom partial and noisy 3D input data. DeepSDF, like its classical counterpart,represents a shape's surface by a continuous volumetric field: the magnitude ofa point in the field represents the distance to the surface boundary and thesign indicates whether the region is inside (-) or outside (+) of the shape,hence our representation implicitly encodes a shape's boundary as thezero-level-set of the learned function while explicitly representing theclassification of space as being part of the shapes interior or not. Whileclassical SDF's both in analytical or discretized voxel form typicallyrepresent the surface of a single shape, DeepSDF can represent an entire classof shapes. Furthermore, we show state-of-the-art performance for learned 3Dshape representation and completion while reducing the model size by an orderof magnitude compared with previous work.},
  YEAR = {2019},
  MONTH = {Jan},
  URL = {http://arxiv.org/abs/1901.05103v1},
  FILE = {1901.05103v1.pdf}
 }

@article{saito2019pifu,
  AUTHOR = {Shunsuke Saito and Zeng Huang and Ryota Natsume and Shigeo Morishima and Angjoo Kanazawa and Hao Li},
  TITLE = {PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed HumanDigitization},
  EPRINT = {1905.05172v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce Pixel-aligned Implicit Function (PIFu), a highly effectiveimplicit representation that locally aligns pixels of 2D images with the globalcontext of their corresponding 3D object. Using PIFu, we propose an end-to-enddeep learning method for digitizing highly detailed clothed humans that caninfer both 3D surface and texture from a single image, and optionally, multipleinput images. Highly intricate shapes, such as hairstyles, clothing, as well astheir variations and deformations can be digitized in a unified way. Comparedto existing representations used for 3D deep learning, PIFu can producehigh-resolution surfaces including largely unseen regions such as the back of aperson. In particular, it is memory efficient unlike the voxel representation,can handle arbitrary topology, and the resulting surface is spatially alignedwith the input image. Furthermore, while previous techniques are designed toprocess either a single image or multiple views, PIFu extends naturally toarbitrary number of views. We demonstrate high-resolution and robustreconstructions on real world images from the DeepFashion dataset, whichcontains a variety of challenging clothing types. Our method achievesstate-of-the-art performance on a public benchmark and outperforms the priorwork for clothed human digitization from a single image.},
  YEAR = {2019},
  MONTH = {May},
  NOTE = {The IEEE International Conference on Computer Vision (ICCV), 2019,
  PP.2304-2314},
  URL = {http://arxiv.org/abs/1905.05172v3},
  FILE = {1905.05172v3.pdf}
 }

@article{oechsle2019texturefields,
  AUTHOR = {Michael Oechsle and Lars Mescheder and Michael Niemeyer and Thilo Strauss and Andreas Geiger},
  TITLE = {Texture Fields: Learning Texture Representations in Function Space},
  EPRINT = {1905.07259v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In recent years, substantial progress has been achieved in learning-basedreconstruction of 3D objects. At the same time, generative models were proposedthat can generate highly realistic images. However, despite this success inthese closely related tasks, texture reconstruction of 3D objects has receivedlittle attention from the research community and state-of-the-art methods areeither limited to comparably low resolution or constrained experimental setups.A major reason for these limitations is that common representations of textureare inefficient or hard to interface for modern deep learning techniques. Inthis paper, we propose Texture Fields, a novel texture representation which isbased on regressing a continuous 3D function parameterized with a neuralnetwork. Our approach circumvents limiting factors like shape discretizationand parameterization, as the proposed texture representation is independent ofthe shape representation of the 3D object. We show that Texture Fields are ableto represent high frequency texture and naturally blend with modern deeplearning techniques. Experimentally, we find that Texture Fields comparefavorably to state-of-the-art methods for conditional texture reconstruction of3D objects and enable learning of probabilistic generative models for texturingunseen 3D models. We believe that Texture Fields will become an importantbuilding block for the next generation of generative 3D models.},
  YEAR = {2019},
  MONTH = {May},
  URL = {http://arxiv.org/abs/1905.07259v1},
  FILE = {1905.07259v1.pdf}
 }

@article{atzmon2019controlling,
  AUTHOR = {Matan Atzmon and Niv Haim and Lior Yariv and Ofer Israelov and Haggai Maron and Yaron Lipman},
  TITLE = {Controlling Neural Level Sets},
  EPRINT = {1905.11911v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {The level sets of neural networks represent fundamental properties such asdecision boundaries of classifiers and are used to model non-linear manifolddata such as curves and surfaces. Thus, methods for controlling the neurallevel sets could find many applications in machine learning.In this paper we present a simple and scalable approach to directly controllevel sets of a deep neural network. Our method consists of two parts: (i)sampling of the neural level sets, and (ii) relating the samples' positions tothe network parameters. The latter is achieved by a sample network that isconstructed by adding a single fixed linear layer to the original network. Inturn, the sample network can be used to incorporate the level set samples intoa loss function of interest.We have tested our method on three different learning tasks: improvinggeneralization to unseen data, training networks robust to adversarial attacks,and curve and surface reconstruction from point clouds. For surfacereconstruction, we produce high fidelity surfaces directly from raw 3D pointclouds. When training small to medium networks to be robust to adversarialattacks we obtain robust accuracy comparable to state-of-the-art methods.},
  YEAR = {2019},
  MONTH = {May},
  URL = {http://arxiv.org/abs/1905.11911v2},
  FILE = {1905.11911v2.pdf}
 }

@article{sitzmann2019srn,
  AUTHOR = {Vincent Sitzmann and Michael Zollhofer and Gordon Wetzstein},
  TITLE = {Scene Representation Networks: Continuous 3D-Structure-Aware NeuralScene Representations},
  EPRINT = {1906.01618v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Unsupervised learning with generative models has the potential of discoveringrich representations of 3D scenes. While geometric deep learning has explored3D-structure-aware representations of scene geometry, these models typicallyrequire explicit 3D supervision. Emerging neural scene representations can betrained only with posed 2D images, but existing methods ignore thethree-dimensional structure of scenes. We propose Scene Representation Networks(SRNs), a continuous, 3D-structure-aware scene representation that encodes bothgeometry and appearance. SRNs represent scenes as continuous functions that mapworld coordinates to a feature representation of local scene properties. Byformulating the image formation as a differentiable ray-marching algorithm,SRNs can be trained end-to-end from only 2D images and their camera poses,without access to depth or shape. This formulation naturally generalizes acrossscenes, learning powerful geometry and appearance priors in the process. Wedemonstrate the potential of SRNs by evaluating them for novel view synthesis,few-shot reconstruction, joint shape and appearance interpolation, andunsupervised discovery of a non-rigid face model.},
  YEAR = {2019},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/1906.01618v2},
  FILE = {1906.01618v2.pdf}
 }

@article{lombardi2019nv,
  AUTHOR = {Stephen Lombardi and Tomas Simon and Jason Saragih and Gabriel Schwartz and Andreas Lehrmann and Yaser Sheikh},
  TITLE = {Neural Volumes: Learning Dynamic Renderable Volumes from Images},
  EPRINT = {1906.07751v1},
  DOI = {10.1145/3306346.3323020},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {Modeling and rendering of dynamic scenes is challenging, as natural scenesoften contain complex phenomena such as thin structures, evolving topology,translucency, scattering, occlusion, and biological motion. Mesh-basedreconstruction and tracking often fail in these cases, and other approaches(e.g., light field video) typically rely on constrained viewing conditions,which limit interactivity. We circumvent these difficulties by presenting alearning-based approach to representing dynamic objects inspired by theintegral projection model used in tomographic imaging. The approach issupervised directly from 2D images in a multi-view capture setting and does notrequire explicit reconstruction or tracking of the object. Our method has twoprimary components: an encoder-decoder network that transforms input imagesinto a 3D volume representation, and a differentiable ray-marching operationthat enables end-to-end training. By virtue of its 3D representation, ourconstruction extrapolates better to novel viewpoints compared to screen-spacerendering techniques. The encoder-decoder architecture learns a latentrepresentation of a dynamic scene that enables us to produce novel contentsequences not seen during training. To overcome memory limitations ofvoxel-based representations, we learn a dynamic irregular grid structureimplemented with a warp field during ray-marching. This structure greatlyimproves the apparent resolution and reduces grid-like artifacts and jaggedmotion. Finally, we demonstrate how to incorporate surface-basedrepresentations into our volumetric-learning framework for applications wherethe highest resolution is required, using facial performance capture as a casein point.},
  YEAR = {2019},
  MONTH = {Jun},
  NOTE = {ACM Transactions on Graphics (SIGGRAPH 2019) 38, 4, Article 65},
  URL = {http://arxiv.org/abs/1906.07751v1},
  FILE = {1906.07751v1.pdf}
 }

@article{deprelle2019learning,
  AUTHOR = {Theo Deprelle and Thibault Groueix and Matthew Fisher and Vladimir G. Kim and Bryan C. Russell and Mathieu Aubry},
  TITLE = {Learning elementary structures for 3D shape generation and matching},
  EPRINT = {1908.04725v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose to represent shapes as the deformation and combination oflearnable elementary 3D structures, which are primitives resulting fromtraining over a collection of shape. We demonstrate that the learned elementary3D structures lead to clear improvements in 3D shape generation and matching.More precisely, we present two complementary approaches for learning elementarystructures: (i) patch deformation learning and (ii) point translation learning.Both approaches can be extended to abstract structures of higher dimensions forimproved results. We evaluate our method on two tasks: reconstructing ShapeNetobjects and estimating dense correspondences between human scans (FAUST interchallenge). We show 16% improvement over surface deformation approaches forshape reconstruction and outperform FAUST inter challenge state of the art by6%.},
  YEAR = {2019},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/1908.04725v2},
  FILE = {1908.04725v2.pdf}
 }

@inproceedings{niemeyer2019occupancy,
  TITLE = {Occupancy flow: 4d reconstruction by learning particle dynamics},
  AUTHOR = {Niemeyer, Michael and Mescheder, Lars and Oechsle, Michael and Geiger, Andreas},
  BOOKTITLE = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  PAGES = {5379--5389},
  YEAR = {2019}
 }

@article{liu2019learning,
  AUTHOR = {Shichen Liu and Shunsuke Saito and Weikai Chen and Hao Li},
  TITLE = {Learning to Infer Implicit Surfaces without 3D Supervision},
  EPRINT = {1911.00767v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent advances in 3D deep learning have shown that it is possible to trainhighly effective deep models for 3D shape generation, directly from 2D images.This is particularly interesting since the availability of 3D models is stilllimited compared to the massive amount of accessible 2D images, which isinvaluable for training. The representation of 3D surfaces itself is a keyfactor for the quality and resolution of the 3D output. While explicitrepresentations, such as point clouds and voxels, can span a wide range ofshape variations, their resolutions are often limited. Mesh-basedrepresentations are more efficient but are limited by their ability to handlevarying topologies. Implicit surfaces, however, can robustly handle complexshapes, topologies, and also provide flexible resolution control. We addressthe fundamental problem of learning implicit surfaces for shape inferencewithout the need of 3D supervision. Despite their advantages, it remainsnontrivial to (1) formulate a differentiable connection between implicitsurfaces and their 2D renderings, which is needed for image-based supervision;and (2) ensure precise geometric properties and control, such as localsmoothness. In particular, sampling implicit surfaces densely is also known tobe a computationally demanding and very slow operation. To this end, we proposea novel ray-based field probing technique for efficient image-to-fieldsupervision, as well as a general geometric regularizer for implicit surfaces,which provides natural shape priors in unconstrained regions. We demonstratethe effectiveness of our framework on the task of single-view image-based 3Dshape digitization and show how we outperform state-of-the-art techniques bothquantitatively and qualitatively.},
  YEAR = {2019},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/1911.00767v1},
  FILE = {1911.00767v1.pdf}
 }

@article{atzmon2019sal,
  AUTHOR = {Matan Atzmon and Yaron Lipman},
  TITLE = {SAL: Sign Agnostic Learning of Shapes from Raw Data},
  EPRINT = {1911.10414v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recently, neural networks have been used as implicit representations forsurface reconstruction, modelling, learning, and generation. So far, trainingneural networks to be implicit representations of surfaces required trainingdata sampled from a ground-truth signed implicit functions such as signeddistance or occupancy functions, which are notoriously hard to compute.In this paper we introduce Sign Agnostic Learning (SAL), a deep learningapproach for learning implicit shape representations directly from raw,unsigned geometric data, such as point clouds and triangle soups.We have tested SAL on the challenging problem of surface reconstruction froman un-oriented point cloud, as well as end-to-end human shape space learningdirectly from raw scans dataset, and achieved state of the art reconstructionscompared to current approaches. We believe SAL opens the door to many geometricdeep learning applications with real-world data, alleviating the usualpainstaking, often manual pre-process.},
  YEAR = {2019},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/1911.10414v2},
  FILE = {1911.10414v2.pdf}
 }

@article{liu2019dist,
  AUTHOR = {Shaohui Liu and Yinda Zhang and Songyou Peng and Boxin Shi and Marc Pollefeys and Zhaopeng Cui},
  TITLE = {DIST: Rendering Deep Implicit Signed Distance Function withDifferentiable Sphere Tracing},
  EPRINT = {1911.13225v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose a differentiable sphere tracing algorithm to bridge the gapbetween inverse graphics methods and the recently proposed deep learning basedimplicit signed distance function. Due to the nature of the implicit function,the rendering process requires tremendous function queries, which isparticularly problematic when the function is represented as a neural network.We optimize both the forward and backward passes of our rendering layer to makeit run efficiently with affordable memory consumption on a commodity graphicscard. Our rendering method is fully differentiable such that losses can bedirectly computed on the rendered 2D observations, and the gradients can bepropagated backwards to optimize the 3D geometry. We show that our renderingmethod can effectively reconstruct accurate 3D shapes from various inputs, suchas sparse depth and multi-view images, through inverse optimization. With thegeometry based reasoning, our 3D shape prediction methods show excellentgeneralization capability and robustness against various noises.},
  YEAR = {2019},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/1911.13225v2},
  FILE = {1911.13225v2.pdf}
 }

@article{genova2019ldif,
  AUTHOR = {Kyle Genova and Forrester Cole and Avneesh Sud and Aaron Sarna and Thomas Funkhouser},
  TITLE = {Local Deep Implicit Functions for 3D Shape},
  EPRINT = {1912.06126v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {The goal of this project is to learn a 3D shape representation that enablesaccurate surface reconstruction, compact storage, efficient computation,consistency for similar shapes, generalization across diverse shape categories,and inference from depth camera observations. Towards this end, we introduceLocal Deep Implicit Functions (LDIF), a 3D shape representation that decomposesspace into a structured set of learned implicit functions. We provide networksthat infer the space decomposition and local deep implicit functions from a 3Dmesh or posed depth image. During experiments, we find that it provides 10.3points higher surface reconstruction accuracy (F-Score) than thestate-of-the-art (OccNet), while requiring fewer than 1 percent of the networkparameters. Experiments on posed depth image completion and generalization tounseen classes show 15.8 and 17.8 point improvements over the state-of-the-art,while producing a structured 3D representation for each input with consistencyacross diverse shape collections.},
  YEAR = {2019},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/1912.06126v2},
  FILE = {1912.06126v2.pdf}
 }

@article{niemeyer2019dvr,
  AUTHOR = {Michael Niemeyer and Lars Mescheder and Michael Oechsle and Andreas Geiger},
  TITLE = {Differentiable Volumetric Rendering: Learning Implicit 3DRepresentations without 3D Supervision},
  EPRINT = {1912.07372v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Learning-based 3D reconstruction methods have shown impressive results.However, most methods require 3D supervision which is often hard to obtain forreal-world datasets. Recently, several works have proposed differentiablerendering techniques to train reconstruction models from RGB images.Unfortunately, these approaches are currently restricted to voxel- andmesh-based representations, suffering from discretization or low resolution. Inthis work, we propose a differentiable rendering formulation for implicit shapeand texture representations. Implicit representations have recently gainedpopularity as they represent shape and texture continuously. Our key insight isthat depth gradients can be derived analytically using the concept of implicitdifferentiation. This allows us to learn implicit shape and texturerepresentations directly from RGB images. We experimentally show that oursingle-view reconstructions rival those learned with full 3D supervision.Moreover, we find that our method can be used for multi-view 3D reconstruction,directly resulting in watertight meshes.},
  YEAR = {2019},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/1912.07372v2},
  FILE = {1912.07372v2.pdf}
 }

@article{gropp2020igr,
  AUTHOR = {Amos Gropp and Lior Yariv and Niv Haim and Matan Atzmon and Yaron Lipman},
  TITLE = {Implicit Geometric Regularization for Learning Shapes},
  EPRINT = {2002.10099v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {Representing shapes as level sets of neural networks has been recently provedto be useful for different shape analysis and reconstruction tasks. So far,such representations were computed using either: (i) pre-computed implicitshape representations; or (ii) loss functions explicitly defined over theneural level sets. In this paper we offer a new paradigm for computing highfidelity implicit neural representations directly from raw data (i.e., pointclouds, with or without normal information). We observe that a rather simpleloss function, encouraging the neural network to vanish on the input pointcloud and to have a unit norm gradient, possesses an implicit geometricregularization property that favors smooth and natural zero level set surfaces,avoiding bad zero-loss solutions. We provide a theoretical analysis of thisproperty for the linear case, and show that, in practice, our method leads tostate of the art implicit neural representations with higher level-of-detailsand fidelity compared to previous methods.},
  YEAR = {2020},
  MONTH = {Feb},
  URL = {http://arxiv.org/abs/2002.10099v2},
  FILE = {2002.10099v2.pdf}
 }

@article{chibane2020ifnets,
  AUTHOR = {Julian Chibane and Thiemo Alldieck and Gerard Pons-Moll},
  TITLE = {Implicit Functions in Feature Space for 3D Shape Reconstruction andCompletion},
  EPRINT = {2003.01456v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {While many works focus on 3D reconstruction from images, in this paper, wefocus on 3D shape reconstruction and completion from a variety of 3D inputs,which are deficient in some respect: low and high resolution voxels, sparse anddense point clouds, complete or incomplete. Processing of such 3D inputs is anincreasingly important problem as they are the output of 3D scanners, which arebecoming more accessible, and are the intermediate output of 3D computer visionalgorithms. Recently, learned implicit functions have shown great promise asthey produce continuous reconstructions. However, we identified two limitationsin reconstruction from 3D inputs: 1) details present in the input data are notretained, and 2) poor reconstruction of articulated humans. To solve this, wepropose Implicit Feature Networks (IF-Nets), which deliver continuous outputs,can handle multiple topologies, and complete shapes for missing or sparse inputdata retaining the nice properties of recent learned implicit functions, butcritically they can also retain detail when it is present in the input data,and can reconstruct articulated humans. Our work differs from prior work in twocrucial aspects. First, instead of using a single vector to encode a 3D shape,we extract a learnable 3-dimensional multi-scale tensor of deep features, whichis aligned with the original Euclidean space embedding the shape. Second,instead of classifying x-y-z point coordinates directly, we classify deepfeatures extracted from the tensor at a continuous query point. We show thatthis forces our model to make decisions based on global and local shapestructure, as opposed to point coordinates, which are arbitrary under Euclideantransformations. Experiments demonstrate that IF-Nets clearly outperform priorwork in 3D object reconstruction in ShapeNet, and obtain significantly moreaccurate 3D human reconstructions.},
  YEAR = {2020},
  MONTH = {Mar},
  NOTE = {{IEEE} Conference on Computer Vision and Pattern Recognition
  (CVPR)2020},
  URL = {http://arxiv.org/abs/2003.01456v2},
  FILE = {2003.01456v2.pdf}
 }

@article{peng2020convolutional,
  AUTHOR = {Songyou Peng and Michael Niemeyer and Lars Mescheder and Marc Pollefeys and Andreas Geiger},
  TITLE = {Convolutional Occupancy Networks},
  EPRINT = {2003.04618v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recently, implicit neural representations have gained popularity forlearning-based 3D reconstruction. While demonstrating promising results, mostimplicit approaches are limited to comparably simple geometry of single objectsand do not scale to more complicated or large-scale scenes. The key limitingfactor of implicit methods is their simple fully-connected network architecturewhich does not allow for integrating local information in the observations orincorporating inductive biases such as translational equivariance. In thispaper, we propose Convolutional Occupancy Networks, a more flexible implicitrepresentation for detailed reconstruction of objects and 3D scenes. Bycombining convolutional encoders with implicit occupancy decoders, our modelincorporates inductive biases, enabling structured reasoning in 3D space. Weinvestigate the effectiveness of the proposed representation by reconstructingcomplex geometry from noisy point clouds and low-resolution voxelrepresentations. We empirically find that our method enables the fine-grainedimplicit 3D reconstruction of single objects, scales to large indoor scenes,and generalizes well from synthetic to real data.},
  YEAR = {2020},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2003.04618v2},
  FILE = {2003.04618v2.pdf}
 }

@article{mildenhall2020nerf,
  AUTHOR = {Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
  TITLE = {NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
  EPRINT = {2003.08934v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method that achieves state-of-the-art results for synthesizingnovel views of complex scenes by optimizing an underlying continuous volumetricscene function using a sparse set of input views. Our algorithm represents ascene using a fully-connected (non-convolutional) deep network, whose input isa single continuous 5D coordinate (spatial location $(x,y,z)$ and viewingdirection $(\theta, \phi)$) and whose output is the volume density andview-dependent emitted radiance at that spatial location. We synthesize viewsby querying 5D coordinates along camera rays and use classic volume renderingtechniques to project the output colors and densities into an image. Becausevolume rendering is naturally differentiable, the only input required tooptimize our representation is a set of images with known camera poses. Wedescribe how to effectively optimize neural radiance fields to renderphotorealistic novel views of scenes with complicated geometry and appearance,and demonstrate results that outperform prior work on neural rendering and viewsynthesis. View synthesis results are best viewed as videos, so we urge readersto view our supplementary video for convincing comparisons.},
  YEAR = {2020},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2003.08934v2},
  FILE = {2003.08934v2.pdf}
 }

@article{yariv2020idr,
  AUTHOR = {Lior Yariv and Yoni Kasten and Dror Moran and Meirav Galun and Matan Atzmon and Ronen Basri and Yaron Lipman},
  TITLE = {Multiview Neural Surface Reconstruction by Disentangling Geometry andAppearance},
  EPRINT = {2003.09852v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this work we address the challenging problem of multiview 3D surfacereconstruction. We introduce a neural network architecture that simultaneouslylearns the unknown geometry, camera parameters, and a neural renderer thatapproximates the light reflected from the surface towards the camera. Thegeometry is represented as a zero level-set of a neural network, while theneural renderer, derived from the rendering equation, is capable of(implicitly) modeling a wide set of lighting conditions and materials. Wetrained our network on real world 2D images of objects with different materialproperties, lighting conditions, and noisy camera initializations from the DTUMVS dataset. We found our model to produce state of the art 3D surfacereconstructions with high fidelity, resolution and detail.},
  YEAR = {2020},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2003.09852v3},
  FILE = {2003.09852v3.pdf}
 }

@article{chabra2020deepls,
  AUTHOR = {Rohan Chabra and Jan Eric Lenssen and Eddy Ilg and Tanner Schmidt and Julian Straub and Steven Lovegrove and Richard Newcombe},
  TITLE = {Deep Local Shapes: Learning Local SDF Priors for Detailed 3DReconstruction},
  EPRINT = {2003.10983v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Efficiently reconstructing complex and intricate surfaces at scale is along-standing goal in machine perception. To address this problem we introduceDeep Local Shapes (DeepLS), a deep shape representation that enables encodingand reconstruction of high-quality 3D shapes without prohibitive memoryrequirements. DeepLS replaces the dense volumetric signed distance function(SDF) representation used in traditional surface reconstruction systems with aset of locally learned continuous SDFs defined by a neural network, inspired byrecent work such as DeepSDF. Unlike DeepSDF, which represents an object-levelSDF with a neural network and a single latent code, we store a grid ofindependent latent codes, each responsible for storing information aboutsurfaces in a small local neighborhood. This decomposition of scenes into localshapes simplifies the prior distribution that the network must learn, and alsoenables efficient inference. We demonstrate the effectiveness andgeneralization power of DeepLS by showing object shape encoding andreconstructions of full scenes, where DeepLS delivers high compression,accuracy, and local shape completion.},
  YEAR = {2020},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2003.10983v3},
  FILE = {2003.10983v3.pdf}
 }

@article{kohli2020semantic,
  AUTHOR = {Amit Kohli and Vincent Sitzmann and Gordon Wetzstein},
  TITLE = {Semantic Implicit Neural Scene Representations With Semi-SupervisedTraining},
  EPRINT = {2003.12673v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {The recent success of implicit neural scene representations has presented aviable new method for how we capture and store 3D scenes. Unlike conventional3D representations, such as point clouds, which explicitly store sceneproperties in discrete, localized units, these implicit representations encodea scene in the weights of a neural network which can be queried at anycoordinate to produce these same scene properties. Thus far, implicitrepresentations have primarily been optimized to estimate only the appearanceand/or 3D geometry information in a scene. We take the next step anddemonstrate that an existing implicit representation (SRNs) is actuallymulti-modal; it can be further leveraged to perform per-point semanticsegmentation while retaining its ability to represent appearance and geometry.To achieve this multi-modal behavior, we utilize a semi-supervised learningstrategy atop the existing pre-trained scene representation. Our method issimple, general, and only requires a few tens of labeled 2D segmentation masksin order to achieve dense 3D semantic segmentation. We explore two novelapplications for this semantically aware implicit neural scene representation:3D novel view and semantic label synthesis given only a single input RGB imageor 2D label mask, as well as 3D interpolation of appearance and semantics.},
  YEAR = {2020},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2003.12673v2},
  FILE = {2003.12673v2.pdf}
 }

@article{saito2020pifuhd,
  AUTHOR = {Shunsuke Saito and Tomas Simon and Jason Saragih and Hanbyul Joo},
  TITLE = {PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution3D Human Digitization},
  EPRINT = {2004.00452v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent advances in image-based 3D human shape estimation have been driven bythe significant improvement in representation power afforded by deep neuralnetworks. Although current approaches have demonstrated the potential in realworld settings, they still fail to produce reconstructions with the level ofdetail often present in the input images. We argue that this limitation stemsprimarily form two conflicting requirements; accurate predictions require largecontext, but precise predictions require high resolution. Due to memorylimitations in current hardware, previous approaches tend to take lowresolution images as input to cover large spatial context, and produce lessprecise (or low resolution) 3D estimates as a result. We address thislimitation by formulating a multi-level architecture that is end-to-endtrainable. A coarse level observes the whole image at lower resolution andfocuses on holistic reasoning. This provides context to an fine level whichestimates highly detailed geometry by observing higher-resolution images. Wedemonstrate that our approach significantly outperforms existingstate-of-the-art techniques on single image human shape reconstruction by fullyleveraging 1k-resolution input images.},
  YEAR = {2020},
  MONTH = {Apr},
  NOTE = {The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR),2020},
  URL = {http://arxiv.org/abs/2004.00452v1},
  FILE = {2004.00452v1.pdf}
 }

@article{hao2020dualsdf,
  AUTHOR = {Zekun Hao and Hadar Averbuch-Elor and Noah Snavely and Serge Belongie},
  TITLE = {DualSDF: Semantic Shape Manipulation using a Two-Level Representation},
  EPRINT = {2004.02869v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We are seeing a Cambrian explosion of 3D shape representations for use inmachine learning. Some representations seek high expressive power in capturinghigh-resolution detail. Other approaches seek to represent shapes ascompositions of simple parts, which are intuitive for people to understand andeasy to edit and manipulate. However, it is difficult to achieve both fidelityand interpretability in the same representation. We propose DualSDF, arepresentation expressing shapes at two levels of granularity, one capturingfine details and the other representing an abstracted proxy shape using simpleand semantically consistent shape primitives. To achieve a tight couplingbetween the two representations, we use a variational objective over a sharedlatent space. Our two-level model gives rise to a new shape manipulationtechnique in which a user can interactively manipulate the coarse proxy shapeand see the changes instantly mirrored in the high-resolution shape. Moreover,our model actively augments and guides the manipulation towards producingsemantically meaningful shapes, making complex manipulations possible withminimal user input.},
  YEAR = {2020},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2004.02869v1},
  FILE = {2004.02869v1.pdf}
 }

@article{jiang2020meshfreeflownet,
  AUTHOR = {Chiyu Max Jiang and Soheil Esmaeilzadeh and Kamyar Azizzadenesheli and Karthik Kashinath and Mustafa Mustafa and Hamdi A. Tchelepi and Philip Marcus and Prabhat and Anima Anandkumar},
  TITLE = {MeshfreeFlowNet: A Physics-Constrained Deep Continuous Space-TimeSuper-Resolution Framework},
  EPRINT = {2005.01463v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {We propose MeshfreeFlowNet, a novel deep learning-based super-resolutionframework to generate continuous (grid-free) spatio-temporal solutions from thelow-resolution inputs. While being computationally efficient, MeshfreeFlowNetaccurately recovers the fine-scale quantities of interest. MeshfreeFlowNetallows for: (i) the output to be sampled at all spatio-temporal resolutions,(ii) a set of Partial Differential Equation (PDE) constraints to be imposed,and (iii) training on fixed-size inputs on arbitrarily sized spatio-temporaldomains owing to its fully convolutional encoder. We empirically study theperformance of MeshfreeFlowNet on the task of super-resolution of turbulentflows in the Rayleigh-Benard convection problem. Across a diverse set ofevaluation metrics, we show that MeshfreeFlowNet significantly outperformsexisting baselines. Furthermore, we provide a large scale implementation ofMeshfreeFlowNet and show that it efficiently scales across large clusters,achieving 96.80% scaling efficiency on up to 128 GPUs and a training time ofless than 4 minutes.},
  YEAR = {2020},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2005.01463v2},
  FILE = {2005.01463v2.pdf}
 }

@article{sitzmann2020siren,
  AUTHOR = {Vincent Sitzmann and Julien N. P. Martel and Alexander W. Bergman and David B. Lindell and Gordon Wetzstein},
  TITLE = {Implicit Neural Representations with Periodic Activation Functions},
  EPRINT = {2006.09661v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicitly defined, continuous, differentiable signal representationsparameterized by neural networks have emerged as a powerful paradigm, offeringmany possible benefits over conventional representations. However, currentnetwork architectures for such implicit neural representations are incapable ofmodeling signals with fine detail, and fail to represent a signal's spatial andtemporal derivatives, despite the fact that these are essential to manyphysical signals defined implicitly as the solution to partial differentialequations. We propose to leverage periodic activation functions for implicitneural representations and demonstrate that these networks, dubbed sinusoidalrepresentation networks or Sirens, are ideally suited for representing complexnatural signals and their derivatives. We analyze Siren activation statisticsto propose a principled initialization scheme and demonstrate therepresentation of images, wavefields, video, sound, and their derivatives.Further, we show how Sirens can be leveraged to solve challenging boundaryvalue problems, such as particular Eikonal equations (yielding signed distancefunctions), the Poisson equation, and the Helmholtz and wave equations. Lastly,we combine Sirens with hypernetworks to learn priors over the space of Sirenfunctions.},
  YEAR = {2020},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2006.09661v1},
  FILE = {2006.09661v1.pdf}
 }

@article{sitzmann2020metasdf,
  AUTHOR = {Vincent Sitzmann and Eric R. Chan and Richard Tucker and Noah Snavely and Gordon Wetzstein},
  TITLE = {MetaSDF: Meta-learning Signed Distance Functions},
  EPRINT = {2006.09662v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural implicit shape representations are an emerging paradigm that offersmany potential benefits over conventional discrete representations, includingmemory efficiency at a high spatial resolution. Generalizing across shapes withsuch neural implicit representations amounts to learning priors over therespective function space and enables geometry reconstruction from partial ornoisy observations. Existing generalization methods rely on conditioning aneural network on a low-dimensional latent code that is either regressed by anencoder or jointly optimized in the auto-decoder framework. Here, we formalizelearning of a shape space as a meta-learning problem and leveragegradient-based meta-learning algorithms to solve this task. We demonstrate thatthis approach performs on par with auto-decoder based approaches while being anorder of magnitude faster at test-time inference. We further demonstrate thatthe proposed gradient-based method outperforms encoder-decoder based methodsthat leverage pooling-based set encoders.},
  YEAR = {2020},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2006.09662v1},
  FILE = {2006.09662v1.pdf}
 }

@article{tancik2020ffn,
  AUTHOR = {Matthew Tancik and Pratul P. Srinivasan and Ben Mildenhall and Sara Fridovich-Keil and Nithin Raghavan and Utkarsh Singhal and Ravi Ramamoorthi and Jonathan T. Barron and Ren Ng},
  TITLE = {Fourier Features Let Networks Learn High Frequency Functions in LowDimensional Domains},
  EPRINT = {2006.10739v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We show that passing input points through a simple Fourier feature mappingenables a multilayer perceptron (MLP) to learn high-frequency functions inlow-dimensional problem domains. These results shed light on recent advances incomputer vision and graphics that achieve state-of-the-art results by usingMLPs to represent complex 3D objects and scenes. Using tools from the neuraltangent kernel (NTK) literature, we show that a standard MLP fails to learnhigh frequencies both in theory and in practice. To overcome this spectralbias, we use a Fourier feature mapping to transform the effective NTK into astationary kernel with a tunable bandwidth. We suggest an approach forselecting problem-specific Fourier features that greatly improves theperformance of MLPs for low-dimensional regression tasks relevant to thecomputer vision and graphics communities.},
  YEAR = {2020},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2006.10739v1},
  FILE = {2006.10739v1.pdf}
 }

@article{bi2020deep,
  AUTHOR = {Sai Bi and Zexiang Xu and Kalyan Sunkavalli and Milos Hasan and Yannick Hold-Geoffroy and David Kriegman and Ravi Ramamoorthi},
  TITLE = {Deep Reflectance Volumes: Relightable Reconstructions from Multi-ViewPhotometric Images},
  EPRINT = {2007.09892v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a deep learning approach to reconstruct scene appearance fromunstructured images captured under collocated point lighting. At the heart ofDeep Reflectance Volumes is a novel volumetric scene representation consistingof opacity, surface normal and reflectance voxel grids. We present a novelphysically-based differentiable volume ray marching framework to render thesescene volumes under arbitrary viewpoint and lighting. This allows us tooptimize the scene volumes to minimize the error between their rendered imagesand the captured images. Our method is able to reconstruct real scenes withchallenging non-Lambertian reflectance and complex geometry with occlusions andshadowing. Moreover, it accurately generalizes to novel viewpoints andlighting, including non-collocated lighting, rendering photorealistic imagesthat are significantly better than state-of-the-art mesh-based methods. We alsoshow that our learned reflectance volumes are editable, allowing for modifyingthe materials of the captured scenes.},
  YEAR = {2020},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2007.09892v1},
  FILE = {2007.09892v1.pdf}
 }

@article{williams2020neural splines,
  AUTHOR = {Francis Williams and Matthew Trager and Joan Bruna and Denis Zorin},
  TITLE = {Neural Splines: Fitting 3D Surfaces with Infinitely-Wide Neural Networks},
  EPRINT = {2006.13782v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Neural Splines, a technique for 3D surface reconstruction that isbased on random feature kernels arising from infinitely-wide shallow ReLUnetworks. Our method achieves state-of-the-art results, outperforming recentneural network-based techniques and widely used Poisson Surface Reconstruction(which, as we demonstrate, can also be viewed as a type of kernel method).Because our approach is based on a simple kernel formulation, it is easy toanalyze and can be accelerated by general techniques designed for kernel-basedlearning. We provide explicit analytical expressions for our kernel and arguethat our formulation can be seen as a generalization of cubic splineinterpolation to higher dimensions. In particular, the RKHS norm associatedwith Neural Splines biases toward smooth interpolants.},
  YEAR = {2020},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2006.13782v3},
  FILE = {2006.13782v3.pdf}
 }

@article{schwarz2020graf,
  AUTHOR = {Katja Schwarz and Yiyi Liao and Michael Niemeyer and Andreas Geiger},
  TITLE = {GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis},
  EPRINT = {2007.02442v4},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {While 2D generative adversarial networks have enabled high-resolution imagesynthesis, they largely lack an understanding of the 3D world and the imageformation process. Thus, they do not provide precise control over cameraviewpoint or object pose. To address this problem, several recent approachesleverage intermediate voxel-based representations in combination withdifferentiable rendering. However, existing methods either produce low imageresolution or fall short in disentangling camera and scene properties, e.g.,the object identity may vary with the viewpoint. In this paper, we propose agenerative model for radiance fields which have recently proven successful fornovel view synthesis of a single scene. In contrast to voxel-basedrepresentations, radiance fields are not confined to a coarse discretization ofthe 3D space, yet allow for disentangling camera and scene properties whiledegrading gracefully in the presence of reconstruction ambiguity. Byintroducing a multi-scale patch-based discriminator, we demonstrate synthesisof high-resolution images while training our model from unposed 2D imagesalone. We systematically analyze our approach on several challenging syntheticand real-world datasets. Our experiments reveal that radiance fields are apowerful representation for generative image synthesis, leading to 3Dconsistent models that render with high fidelity.},
  YEAR = {2020},
  MONTH = {Jul},
  NOTE = {Advances in Neural Information Processing Systems, NeurIPS 2020},
  URL = {http://arxiv.org/abs/2007.02442v4},
  FILE = {2007.02442v4.pdf}
 }

@article{liu2020nsvf,
  AUTHOR = {Lingjie Liu and Jiatao Gu and Kyaw Zaw Lin and Tat-Seng Chua and Christian Theobalt},
  TITLE = {Neural Sparse Voxel Fields},
  EPRINT = {2007.11571v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Photo-realistic free-viewpoint rendering of real-world scenes using classicalcomputer graphics techniques is challenging, because it requires the difficultstep of capturing detailed appearance and geometry models. Recent studies havedemonstrated promising results by learning scene representations thatimplicitly encode both geometry and appearance without 3D supervision. However,existing approaches in practice often show blurry renderings caused by thelimited network capacity or the difficulty in finding accurate intersections ofcamera rays with the scene geometry. Synthesizing high-resolution imagery fromthese representations often requires time-consuming optical ray marching. Inthis work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scenerepresentation for fast and high-quality free-viewpoint rendering. NSVF definesa set of voxel-bounded implicit fields organized in a sparse voxel octree tomodel local properties in each cell. We progressively learn the underlyingvoxel structures with a differentiable ray-marching operation from only a setof posed RGB images. With the sparse voxel octree structure, rendering novelviews can be accelerated by skipping the voxels containing no relevant scenecontent. Our method is typically over 10 times faster than the state-of-the-art(namely, NeRF(Mildenhall et al., 2020)) at inference time while achievinghigher quality results. Furthermore, by utilizing an explicit sparse voxelrepresentation, our method can easily be applied to scene editing and scenecomposition. We also demonstrate several challenging tasks, includingmulti-scene learning, free-viewpoint rendering of a moving human, andlarge-scale scene rendering. Code and data are available at our website:https://github.com/facebookresearch/NSVF.},
  YEAR = {2020},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2007.11571v2},
  FILE = {2007.11571v2.pdf}
 }

@article{xu2020ladybird,
  AUTHOR = {Yifan Xu and Tianqi Fan and Yi Yuan and Gurprit Singh},
  TITLE = {Ladybird: Quasi-Monte Carlo Sampling for Deep Implicit Field Based 3DReconstruction with Symmetry},
  EPRINT = {2007.13393v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Deep implicit field regression methods are effective for 3D reconstructionfrom single-view images. However, the impact of different sampling patterns onthe reconstruction quality is not well-understood. In this work, we first studythe effect of point set discrepancy on the network training. Based on FarthestPoint Sampling algorithm, we propose a sampling scheme that theoreticallyencourages better generalization performance, and results in fast convergencefor SGD-based optimization algorithms. Secondly, based on the reflectivesymmetry of an object, we propose a feature fusion method that alleviatesissues due to self-occlusions which makes it difficult to utilize local imagefeatures. Our proposed system Ladybird is able to create high quality 3D objectreconstructions from a single input image. We evaluate Ladybird on a largescale 3D dataset (ShapeNet) demonstrating highly competitive results in termsof Chamfer distance, Earth Mover's distance and Intersection Over Union (IoU).},
  YEAR = {2020},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2007.13393v1},
  FILE = {2007.13393v1.pdf}
 }

@article{hani2020corn,
  AUTHOR = {Nicolai Hani and Selim Engin and Jun-Jee Chao and Volkan Isler},
  TITLE = {Continuous Object Representation Networks: Novel View Synthesis withoutTarget View Supervision},
  EPRINT = {2007.15627v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Novel View Synthesis (NVS) is concerned with synthesizing views under cameraviewpoint transformations from one or multiple input images. NVS requiresexplicit reasoning about 3D object structure and unseen parts of the scene tosynthesize convincing results. As a result, current approaches typically relyon supervised training with either ground truth 3D models or multiple targetimages. We propose Continuous Object Representation Networks (CORN), aconditional architecture that encodes an input image's geometry and appearancethat map to a 3D consistent scene representation. We can train CORN with onlytwo source images per object by combining our model with a neural renderer. Akey feature of CORN is that it requires no ground truth 3D models or targetview supervision. Regardless, CORN performs well on challenging tasks such asnovel view synthesis and single-view 3D reconstruction and achieves performancecomparable to state-of-the-art approaches that use direct supervision. Forup-to-date information, data, and code, please see our project page:https://nicolaihaeni.github.io/corn/.},
  YEAR = {2020},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2007.15627v2},
  FILE = {2007.15627v2.pdf}
 }

@article{tretschk2020patchnets,
  AUTHOR = {Edgar Tretschk and Ayush Tewari and Vladislav Golyanik and Michael Zollhofer and Carsten Stoll and Christian Theobalt},
  TITLE = {PatchNets: Patch-Based Generalizable Deep Implicit 3D ShapeRepresentations},
  EPRINT = {2008.01639v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicit surface representations, such as signed-distance functions, combinedwith deep learning have led to impressive models which can represent detailedshapes of objects with arbitrary topology. Since a continuous function islearned, the reconstructions can also be extracted at any arbitrary resolution.However, large datasets such as ShapeNet are required to train such models. Inthis paper, we present a new mid-level patch-based surface representation. Atthe level of patches, objects across different categories share similarities,which leads to more generalizable models. We then introduce a novel method tolearn this patch-based representation in a canonical space, such that it is asobject-agnostic as possible. We show that our representation trained on onecategory of objects from ShapeNet can also well represent detailed shapes fromany other category. In addition, it can be trained using much fewer shapes,compared to existing approaches. We show several applications of our newrepresentation, including shape interpolation and partial point cloudcompletion. Due to explicit control over positions, orientations and scales ofpatches, our representation is also more controllable compared to object-levelrepresentations, which enables us to deform encoded shapes non-rigidly.},
  YEAR = {2020},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2008.01639v2},
  FILE = {2008.01639v2.pdf}
 }

@article{martin-brualla2020nerfw,
  AUTHOR = {Ricardo Martin-Brualla and Noha Radwan and Mehdi S. M. Sajjadi and Jonathan T. Barron and Alexey Dosovitskiy and Daniel Duckworth},
  TITLE = {NeRF in the Wild: Neural Radiance Fields for Unconstrained PhotoCollections},
  EPRINT = {2008.02268v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a learning-based method for synthesizing novel views of complexscenes using only unstructured collections of in-the-wild photographs. We buildon Neural Radiance Fields (NeRF), which uses the weights of a multilayerperceptron to model the density and color of a scene as a function of 3Dcoordinates. While NeRF works well on images of static subjects captured undercontrolled settings, it is incapable of modeling many ubiquitous, real-worldphenomena in uncontrolled images, such as variable illumination or transientoccluders. We introduce a series of extensions to NeRF to address these issues,thereby enabling accurate reconstructions from unstructured image collectionstaken from the internet. We apply our system, dubbed NeRF-W, to internet photocollections of famous landmarks, and demonstrate temporally consistent novelview renderings that are significantly closer to photorealism than the priorstate of the art.},
  YEAR = {2020},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2008.02268v3},
  FILE = {2008.02268v3.pdf}
 }

@article{bi2020neural,
  AUTHOR = {Sai Bi and Zexiang Xu and Pratul Srinivasan and Ben Mildenhall and Kalyan Sunkavalli and Milos Hasan and Yannick Hold-Geoffroy and David Kriegman and Ravi Ramamoorthi},
  TITLE = {Neural Reflectance Fields for Appearance Acquisition},
  EPRINT = {2008.03824v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Neural Reflectance Fields, a novel deep scene representation thatencodes volume density, normal and reflectance properties at any 3D point in ascene using a fully-connected neural network. We combine this representationwith a physically-based differentiable ray marching framework that can renderimages from a neural reflectance field under any viewpoint and light. Wedemonstrate that neural reflectance fields can be estimated from imagescaptured with a simple collocated camera-light setup, and accurately model theappearance of real-world scenes with complex geometry and reflectance. Onceestimated, they can be used to render photo-realistic images under novelviewpoint and (non-collocated) lighting conditions and accurately reproducechallenging effects like specularities, shadows and occlusions. This allows usto perform high-quality view synthesis and relighting that is significantlybetter than previous methods. We also demonstrate that we can compose theestimated neural reflectance field of a real scene with traditional scenemodels and render them using standard Monte Carlo rendering engines. Our workthus enables a complete pipeline from high-quality and practical appearanceacquisition to 3D scene composition and rendering.},
  YEAR = {2020},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2008.03824v2},
  FILE = {2008.03824v2.pdf}
 }

@article{davies2020on,
  AUTHOR = {Thomas Davies and Derek Nowrouzezahrai and Alec Jacobson},
  TITLE = {On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes},
  EPRINT = {2009.09808v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {A neural implicit outputs a number indicating whether the given query pointin space is inside, outside, or on a surface. Many prior works have focused on_latent-encoded_ neural implicits, where a latent vector encoding of a specificshape is also fed as input. While affording latent-space interpolation, thiscomes at the cost of reconstruction accuracy for any _single_ shape. Training aspecific network for each 3D shape, a _weight-encoded_ neural implicit mayforgo the latent vector and focus reconstruction accuracy on the details of asingle shape. While previously considered as an intermediary representation for3D scanning tasks or as a toy-problem leading up to latent-encoding tasks,weight-encoded neural implicits have not yet been taken seriously as a 3D shaperepresentation. In this paper, we establish that weight-encoded neuralimplicits meet the criteria of a first-class 3D shape representation. Weintroduce a suite of technical contributions to improve reconstructionaccuracy, convergence, and robustness when learning the signed distance fieldinduced by a polygonal mesh -- the _de facto_ standard representation. Viewedas a lossy compression, our conversion outperforms standard techniques fromgeometry processing. Compared to previous latent- and weight-encoded neuralimplicits we demonstrate superior robustness, scalability, and performance.},
  YEAR = {2020},
  MONTH = {Sep},
  URL = {http://arxiv.org/abs/2009.09808v3},
  FILE = {2009.09808v3.pdf}
 }

@article{chibane2020implicit,
  AUTHOR = {Julian Chibane and Gerard Pons-Moll},
  TITLE = {Implicit Feature Networks for Texture Completion from Partial 3D Data},
  EPRINT = {2009.09458v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Prior work to infer 3D texture use either texture atlases, which requireuv-mappings and hence have discontinuities, or colored voxels, which are memoryinefficient and limited in resolution. Recent work, predicts RGB color at everyXYZ coordinate forming a texture field, but focus on completing texture given asingle 2D image. Instead, we focus on 3D texture and geometry completion frompartial and incomplete 3D scans. IF-Nets have recently achievedstate-of-the-art results on 3D geometry completion using a multi-scale deepfeature encoding, but the outputs lack texture. In this work, we generalizeIF-Nets to texture completion from partial textured scans of humans andarbitrary objects. Our key insight is that 3D texture completion benefits fromincorporating local and global deep features extracted from both the 3D partialtexture and completed geometry. Specifically, given the partial 3D texture andthe 3D geometry completed with IF-Nets, our model successfully in-paints themissing texture parts in consistence with the completed geometry. Our model wonthe SHARP ECCV'20 challenge, achieving highest performance on all challenges.},
  YEAR = {2020},
  MONTH = {Sep},
  NOTE = {SHARP Workshop, European Conference on Computer Vision (ECCV),
  2020},
  URL = {http://arxiv.org/abs/2009.09458v1},
  FILE = {2009.09458v1.pdf}
 }

@article{li2021nemi,
  AUTHOR = {Jiaxin Li and Zijian Feng and Qi She and Henghui Ding and Changhu Wang and Gim Hee Lee},
  TITLE = {MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis},
  EPRINT = {2103.14910v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper, we propose MINE to perform novel view synthesis and depthestimation via dense 3D reconstruction from a single image. Our approach is acontinuous depth generalization of the Multiplane Images (MPI) by introducingthe NEural radiance fields (NeRF). Given a single image as input, MINE predictsa 4-channel image (RGB and volume density) at arbitrary depth values to jointlyreconstruct the camera frustum and fill in occluded contents. The reconstructedand inpainted frustum can then be easily rendered into novel RGB or depth viewsusing differentiable rendering. Extensive experiments on RealEstate10K, KITTIand Flowers Light Fields show that our MINE outperforms state-of-the-art by alarge margin in novel view synthesis. We also achieve competitive results indepth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Oursource code is available at https://github.com/vincentfung13/MINE},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.14910v3},
  FILE = {2103.14910v3.pdf}
 }

@article{bemana2020xfields,
  AUTHOR = {Mojtaba Bemana and Karol Myszkowski and Hans-Peter Seidel and Tobias Ritschel},
  TITLE = {X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation},
  EPRINT = {2010.00450v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We suggest to represent an X-Field -a set of 2D images taken across differentview, time or illumination conditions, i.e., video, light field, reflectancefields or combinations thereof-by learning a neural network (NN) to map theirview, time or light coordinates to 2D images. Executing this NN at newcoordinates results in joint view, time or light interpolation. The key idea tomake this workable is a NN that already knows the "basic tricks" of graphics(lighting, 3D projection, occlusion) in a hard-coded and differentiable form.The NN represents the input to that rendering as an implicit map, that for anyview, time, or light coordinate and for any pixel can quantify how it will moveif view, time or light coordinates change (Jacobian of pixel position withrespect to view, time, illumination, etc.). Our X-Field representation istrained for one scene within minutes, leading to a compact set of trainableparameters and hence real-time navigation in view, time and illumination.},
  YEAR = {2020},
  MONTH = {Oct},
  URL = {http://arxiv.org/abs/2010.00450v1},
  FILE = {2010.00450v1.pdf}
 }

@article{trevithick2020grf,
  AUTHOR = {Alex Trevithick and Bo Yang},
  TITLE = {GRF: Learning a General Radiance Field for 3D Representation andRendering},
  EPRINT = {2010.04595v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a simple yet powerful neural network that implicitly representsand renders 3D objects and scenes only from 2D observations. The network models3D geometries as a general radiance field, which takes a set of 2D images withcamera poses and intrinsics as input, constructs an internal representation foreach point of the 3D space, and then renders the corresponding appearance andgeometry of that point viewed from an arbitrary position. The key to ourapproach is to learn local features for each pixel in 2D images and to thenproject these features to 3D points, thus yielding general and rich pointrepresentations. We additionally integrate an attention mechanism to aggregatepixel features from multiple 2D views, such that visual occlusions areimplicitly taken into account. Extensive experiments demonstrate that ourmethod can generate high-quality and realistic novel views for novel objects,unseen categories and challenging real-world scenes.},
  YEAR = {2020},
  MONTH = {Oct},
  URL = {http://arxiv.org/abs/2010.04595v3},
  FILE = {2010.04595v3.pdf}
 }

@article{zhang2020nerf++,
  AUTHOR = {Kai Zhang and Gernot Riegler and Noah Snavely and Vladlen Koltun},
  TITLE = {NeRF++: Analyzing and Improving Neural Radiance Fields},
  EPRINT = {2010.07492v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural Radiance Fields (NeRF) achieve impressive view synthesis results for avariety of capture settings, including 360 capture of bounded scenes andforward-facing capture of bounded and unbounded scenes. NeRF fits multi-layerperceptrons (MLPs) representing view-invariant opacity and view-dependent colorvolumes to a set of training images, and samples novel views based on volumerendering techniques. In this technical report, we first remark on radiancefields and their potential ambiguities, namely the shape-radiance ambiguity,and analyze NeRF's success in avoiding such ambiguities. Second, we address aparametrization issue involved in applying NeRF to 360 captures of objectswithin large-scale, unbounded 3D scenes. Our method improves view synthesisfidelity in this challenging scenario. Code is available athttps://github.com/Kai-46/nerfplusplus.},
  YEAR = {2020},
  MONTH = {Oct},
  URL = {http://arxiv.org/abs/2010.07492v2},
  FILE = {2010.07492v2.pdf}
 }

@article{lin2020sdfsrn,
  AUTHOR = {Chen-Hsuan Lin and Chaoyang Wang and Simon Lucey},
  TITLE = {SDF-SRN: Learning Signed Distance 3D Object Reconstruction from StaticImages},
  EPRINT = {2010.10505v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Dense 3D object reconstruction from a single image has recently witnessedremarkable advances, but supervising neural networks with ground-truth 3Dshapes is impractical due to the laborious process of creating pairedimage-shape datasets. Recent efforts have turned to learning 3D reconstructionwithout 3D supervision from RGB images with annotated 2D silhouettes,dramatically reducing the cost and effort of annotation. These techniques,however, remain impractical as they still require multi-view annotations of thesame object instance during training. As a result, most experimental efforts todate have been limited to synthetic datasets. In this paper, we address thisissue and propose SDF-SRN, an approach that requires only a single view ofobjects at training time, offering greater utility for real-world scenarios.SDF-SRN learns implicit 3D shape representations to handle arbitrary shapetopologies that may exist in the datasets. To this end, we derive a noveldifferentiable rendering formulation for learning signed distance functions(SDF) from 2D silhouettes. Our method outperforms the state of the art underchallenging single-view supervision settings on both synthetic and real-worlddatasets.},
  YEAR = {2020},
  MONTH = {Oct},
  URL = {http://arxiv.org/abs/2010.10505v1},
  FILE = {2010.10505v1.pdf}
 }

@article{chibane2020ndf,
  AUTHOR = {Julian Chibane and Aymen Mir and Gerard Pons-Moll},
  TITLE = {Neural Unsigned Distance Fields for Implicit Function Learning},
  EPRINT = {2010.13938v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this work we target a learnable output representation that allowscontinuous, high resolution outputs of arbitrary shape. Recent works represent3D surfaces implicitly with a Neural Network, thereby breaking previousbarriers in resolution, and ability to represent diverse topologies. However,neural implicit representations are limited to closed surfaces, which dividethe space into inside and outside. Many real world objects such as walls of ascene scanned by a sensor, clothing, or a car with inner structures are notclosed. This constitutes a significant barrier, in terms of data pre-processing(objects need to be artificially closed creating artifacts), and the ability tooutput open surfaces. In this work, we propose Neural Distance Fields (NDF), aneural network based model which predicts the unsigned distance field forarbitrary 3D shapes given sparse point clouds. NDF represent surfaces at highresolutions as prior implicit models, but do not require closed surface data,and significantly broaden the class of representable shapes in the output. NDFallow to extract the surface as very dense point clouds and as meshes. We alsoshow that NDF allow for surface normal calculation and can be rendered using aslight modification of sphere tracing. We find NDF can be used for multi-targetregression (multiple outputs for one input) with techniques that have beenexclusively used for rendering in graphics. Experiments on ShapeNet show thatNDF, while simple, is the state-of-the art, and allows to reconstruct shapeswith inner structures, such as the chairs inside a bus. Notably, we show thatNDF are not restricted to 3D shapes, and can approximate more general opensurfaces such as curves, manifolds, and functions. Code is available forresearch at https://virtualhumans.mpi-inf.mpg.de/ndf/.},
  YEAR = {2020},
  MONTH = {Oct},
  NOTE = {Neural Information Processing Systems (NeurIPS) 2020},
  URL = {http://arxiv.org/abs/2010.13938v1},
  FILE = {2010.13938v1.pdf}
 }

@article{ost2020neural,
  AUTHOR = {Julian Ost and Fahim Mannan and Nils Thuerey and Julian Knodt and Felix Heide},
  TITLE = {Neural Scene Graphs for Dynamic Scenes},
  EPRINT = {2011.10379v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent implicit neural rendering methods have demonstrated that it ispossible to learn accurate view synthesis for complex scenes by predictingtheir volumetric density and color supervised solely by a set of RGB images.However, existing methods are restricted to learning efficient representationsof static scenes that encode all scene objects into a single neural network,and lack the ability to represent dynamic scenes and decompositions intoindividual scene objects. In this work, we present the first neural renderingmethod that decomposes dynamic scenes into scene graphs. We propose a learnedscene graph representation, which encodes object transformation and radiance,to efficiently render novel arrangements and views of the scene. To this end,we learn implicitly encoded scenes, combined with a jointly learned latentrepresentation to describe objects with a single implicit function. We assessthe proposed method on synthetic and real automotive data, validating that ourapproach learns dynamic scenes -- only by observing a video of this scene --and allows for rendering novel photo-realistic views of novel scenecompositions with unseen sets of objects at unseen poses.},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.10379v3},
  FILE = {2011.10379v3.pdf}
 }

@article{niemeyer2020giraffe,
  AUTHOR = {Michael Niemeyer and Andreas Geiger},
  TITLE = {GIRAFFE: Representing Scenes as Compositional Generative Neural FeatureFields},
  EPRINT = {2011.12100v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Deep generative models allow for photorealistic image synthesis at highresolutions. But for many applications, this is not enough: content creationalso needs to be controllable. While several recent works investigate how todisentangle underlying factors of variation in the data, most of them operatein 2D and hence ignore that our world is three-dimensional. Further, only fewworks consider the compositional nature of scenes. Our key hypothesis is thatincorporating a compositional 3D scene representation into the generative modelleads to more controllable image synthesis. Representing scenes ascompositional generative neural feature fields allows us to disentangle one ormultiple objects from the background as well as individual objects' shapes andappearances while learning from unstructured and unposed image collectionswithout any additional supervision. Combining this scene representation with aneural rendering pipeline yields a fast and realistic image synthesis model. Asevidenced by our experiments, our model is able to disentangle individualobjects and allows for translating and rotating them in the scene as well aschanging the camera pose.},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.12100v2},
  FILE = {2011.12100v2.pdf}
 }

@article{skorokhodov2020inrgan,
  AUTHOR = {Ivan Skorokhodov and Savva Ignatyev and Mohamed Elhoseiny},
  TITLE = {Adversarial Generation of Continuous Images},
  EPRINT = {2011.12026v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In most existing learning systems, images are typically viewed as 2D pixelarrays. However, in another paradigm gaining popularity, a 2D image isrepresented as an implicit neural representation (INR) - an MLP that predictsan RGB pixel value given its (x,y) coordinate. In this paper, we propose twonovel architectural techniques for building INR-based image decoders:factorized multiplicative modulation and multi-scale INRs, and use them tobuild a state-of-the-art continuous image GAN. Previous attempts to adapt INRsfor image generation were limited to MNIST-like datasets and do not scale tocomplex real-world data. Our proposed INR-GAN architecture improves theperformance of continuous image generators by several times, greatly reducingthe gap between continuous image GANs and pixel-based ones. Apart from that, weexplore several exciting properties of the INR-based decoders, likeout-of-the-box superresolution, meaningful image-space interpolation,accelerated inference of low-resolution images, an ability to extrapolateoutside of image boundaries, and strong geometric prior. The project page islocated at https://universome.github.io/inr-gan.},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.12026v2},
  FILE = {2011.12026v2.pdf}
 }

@article{park2020dnerf,
  AUTHOR = {Keunhong Park and Utkarsh Sinha and Jonathan T. Barron and Sofien Bouaziz and Dan B Goldman and Steven M. Seitz and Ricardo Martin-Brualla},
  TITLE = {Nerfies: Deformable Neural Radiance Fields},
  EPRINT = {2011.12948v4},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present the first method capable of photorealistically reconstructingdeformable scenes using photos/videos captured casually from mobile phones. Ourapproach augments neural radiance fields (NeRF) by optimizing an additionalcontinuous volumetric deformation field that warps each observed point into acanonical 5D NeRF. We observe that these NeRF-like deformation fields are proneto local minima, and propose a coarse-to-fine optimization method forcoordinate-based models that allows for more robust optimization. By adaptingprinciples from geometry processing and physical simulation to NeRF-likemodels, we propose an elastic regularization of the deformation field thatfurther improves robustness. We show that our method can turn casually capturedselfie photos/videos into deformable NeRF models that allow for photorealisticrenderings of the subject from arbitrary viewpoints, which we dub "nerfies." Weevaluate our method by collecting time-synchronized data using a rig with twomobile phones, yielding train/validation images of the same pose at differentviewpoints. We show that our method faithfully reconstructs non-rigidlydeforming scenes and reproduces unseen views with high fidelity.},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.12948v4},
  FILE = {2011.12948v4.pdf}
 }

@article{rebain2020derf,
  AUTHOR = {Daniel Rebain and Wei Jiang and Soroosh Yazdani and Ke Li and Kwang Moo Yi and Andrea Tagliasacchi},
  TITLE = {DeRF: Decomposed Radiance Fields},
  EPRINT = {2011.12490v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {With the advent of Neural Radiance Fields (NeRF), neural networks can nowrender novel views of a 3D scene with quality that fools the human eye. Yet,generating these images is very computationally intensive, limiting theirapplicability in practical scenarios. In this paper, we propose a techniquebased on spatial decomposition capable of mitigating this issue. Our keyobservation is that there are diminishing returns in employing larger (deeperand/or wider) networks. Hence, we propose to spatially decompose a scene anddedicate smaller networks for each decomposed part. When working together,these networks can render the whole scene. This allows us near-constantinference time regardless of the number of decomposed parts. Moreover, we showthat a Voronoi spatial decomposition is preferable for this purpose, as it isprovably compatible with the Painter's Algorithm for efficient and GPU-friendlyrendering. Our experiments show that for real-world scenes, our method providesup to 3x more efficient inference than NeRF (with the same rendering quality),or an improvement of up to 1.0~dB in PSNR (for the same inference cost).},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.12490v1},
  FILE = {2011.12490v1.pdf}
 }

@article{xian2020spacetime,
  AUTHOR = {Wenqi Xian and Jia-Bin Huang and Johannes Kopf and Changil Kim},
  TITLE = {Space-time Neural Irradiance Fields for Free-Viewpoint Video},
  EPRINT = {2011.12950v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method that learns a spatiotemporal neural irradiance field fordynamic scenes from a single video. Our learned representation enablesfree-viewpoint rendering of the input video. Our method builds upon recentadvances in implicit representations. Learning a spatiotemporal irradiancefield from a single video poses significant challenges because the videocontains only one observation of the scene at any point in time. The 3Dgeometry of a scene can be legitimately represented in numerous ways sincevarying geometry (motion) can be explained with varying appearance and viceversa. We address this ambiguity by constraining the time-varying geometry ofour dynamic scene representation using the scene depth estimated from videodepth estimation methods, aggregating contents from individual frames into asingle global representation. We provide an extensive quantitative evaluationand demonstrate compelling free-viewpoint rendering results.},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.12950v2},
  FILE = {2011.12950v2.pdf}
 }

@article{li2020nsff,
  AUTHOR = {Zhengqi Li and Simon Niklaus and Noah Snavely and Oliver Wang},
  TITLE = {Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes},
  EPRINT = {2011.13084v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method to perform novel view and time synthesis of dynamicscenes, requiring only a monocular video with known camera poses as input. Todo this, we introduce Neural Scene Flow Fields, a new representation thatmodels the dynamic scene as a time-variant continuous function of appearance,geometry, and 3D scene motion. Our representation is optimized through a neuralnetwork to fit the observed input views. We show that our representation can beused for complex dynamic scenes, including thin structures, view-dependenteffects, and natural degrees of motion. We conduct a number of experiments thatdemonstrate our approach significantly outperforms recent monocular viewsynthesis methods, and show qualitative results of space-time view synthesis ona variety of real-world videos.},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.13084v3},
  FILE = {2011.13084v3.pdf}
 }

@article{pumarola2020dnerf,
  AUTHOR = {Albert Pumarola and Enric Corona and Gerard Pons-Moll and Francesc Moreno-Noguer},
  TITLE = {D-NeRF: Neural Radiance Fields for Dynamic Scenes},
  EPRINT = {2011.13961v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural rendering techniques combining machine learning with geometricreasoning have arisen as one of the most promising approaches for synthesizingnovel views of a scene from a sparse set of images. Among these, stands out theNeural radiance fields (NeRF), which trains a deep network to map 5D inputcoordinates (representing spatial location and viewing direction) into a volumedensity and view-dependent emitted radiance. However, despite achieving anunprecedented level of photorealism on the generated images, NeRF is onlyapplicable to static scenes, where the same spatial location can be queriedfrom different images. In this paper we introduce D-NeRF, a method that extendsneural radiance fields to a dynamic domain, allowing to reconstruct and rendernovel images of objects under rigid and non-rigid motions from a \emph{single}camera moving around the scene. For this purpose we consider time as anadditional input to the system, and split the learning process in two mainstages: one that encodes the scene into a canonical space and another that mapsthis canonical representation into the deformed scene at a particular time.Both mappings are simultaneously learned using fully-connected networks. Oncethe networks are trained, D-NeRF can render novel images, controlling both thecamera view and the time variable, and thus, the object movement. Wedemonstrate the effectiveness of our approach on scenes with objects underrigid, articulated and non-rigid motions. Code, model weights and the dynamicscenes dataset will be released.},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.13961v1},
  FILE = {2011.13961v1.pdf}
 }

@article{anokhin2020cips,
  AUTHOR = {Ivan Anokhin and Kirill Demochkin and Taras Khakhulin and Gleb Sterkin and Victor Lempitsky and Denis Korzhenkov},
  TITLE = {Image Generators with Conditionally-Independent Pixel Synthesis},
  EPRINT = {2011.13775v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Existing image generator networks rely heavily on spatial convolutions and,optionally, self-attention blocks in order to gradually synthesize images in acoarse-to-fine manner. Here, we present a new architecture for imagegenerators, where the color value at each pixel is computed independently giventhe value of a random latent vector and the coordinate of that pixel. Nospatial convolutions or similar operations that propagate information acrosspixels are involved during the synthesis. We analyze the modeling capabilitiesof such generators when trained in an adversarial fashion, and observe the newgenerators to achieve similar generation quality to state-of-the-artconvolutional generators. We also investigate several interesting propertiesunique to the new architecture.},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.13775v1},
  FILE = {2011.13775v1.pdf}
 }

@article{yenamandra2020i3dmm,
  AUTHOR = {Tarun Yenamandra and Ayush Tewari and Florian Bernard and Hans-Peter Seidel and Mohamed Elgharib and Daniel Cremers and Christian Theobalt},
  TITLE = {i3DMM: Deep Implicit 3D Morphable Model of Human Heads},
  EPRINT = {2011.14143v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present the first deep implicit 3D morphable model (i3DMM) of full heads.Unlike earlier morphable face models it not only captures identity-specificgeometry, texture, and expressions of the frontal face, but also models theentire head, including hair. We collect a new dataset consisting of 64 peoplewith different expressions and hairstyles to train i3DMM. Our approach has thefollowing favorable properties: (i) It is the first full head morphable modelthat includes hair. (ii) In contrast to mesh-based models it can be trained onmerely rigidly aligned scans, without requiring difficult non-rigidregistration. (iii) We design a novel architecture to decouple the shape modelinto an implicit reference shape and a deformation of this reference shape.With that, dense correspondences between shapes can be learned implicitly. (iv)This architecture allows us to semantically disentangle the geometry and colorcomponents, as color is learned in the reference space. Geometry is furtherdisentangled as identity, expressions, and hairstyle, while color isdisentangled as identity and hairstyle components. We show the merits of i3DMMusing ablation studies, comparisons to state-of-the-art models, andapplications such as semantic head editing and texture transfer. We will makeour model publicly available.},
  YEAR = {2020},
  MONTH = {Nov},
  URL = {http://arxiv.org/abs/2011.14143v1},
  FILE = {2011.14143v1.pdf}
 }

@article{bozic2020neural,
  AUTHOR = {Aljaz Bozic and Pablo Palafox and Michael Zollhofer and Justus Thies and Angela Dai and Matthias Niessner},
  TITLE = {Neural Deformation Graphs for Globally-consistent Non-rigidReconstruction},
  EPRINT = {2012.01451v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce Neural Deformation Graphs for globally-consistent deformationtracking and 3D reconstruction of non-rigid objects. Specifically, weimplicitly model a deformation graph via a deep neural network. This neuraldeformation graph does not rely on any object-specific structure and, thus, canbe applied to general non-rigid deformation tracking. Our method globallyoptimizes this neural graph on a given sequence of depth camera observations ofa non-rigidly moving object. Based on explicit viewpoint consistency as well asinter-frame graph and surface consistency constraints, the underlying networkis trained in a self-supervised fashion. We additionally optimize for thegeometry of the object with an implicit deformable multi-MLP shaperepresentation. Our approach does not assume sequential input data, thusenabling robust tracking of fast motions or even temporally disconnectedrecordings. Our experiments demonstrate that our Neural Deformation Graphsoutperform state-of-the-art non-rigid reconstruction approaches bothqualitatively and quantitatively, with 64% improved reconstruction and 62%improved deformation tracking performance.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.01451v1},
  FILE = {2012.01451v1.pdf}
 }

@article{yu2020pixelnerf,
  AUTHOR = {Alex Yu and Vickie Ye and Matthew Tancik and Angjoo Kanazawa},
  TITLE = {pixelNeRF: Neural Radiance Fields from One or Few Images},
  EPRINT = {2012.02190v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose pixelNeRF, a learning framework that predicts a continuous neuralscene representation conditioned on one or few input images. The existingapproach for constructing neural radiance fields involves optimizing therepresentation to every scene independently, requiring many calibrated viewsand significant compute time. We take a step towards resolving theseshortcomings by introducing an architecture that conditions a NeRF on imageinputs in a fully convolutional manner. This allows the network to be trainedacross multiple scenes to learn a scene prior, enabling it to perform novelview synthesis in a feed-forward manner from a sparse set of views (as few asone). Leveraging the volume rendering approach of NeRF, our model can betrained directly from images with no explicit 3D supervision. We conductextensive experiments on ShapeNet benchmarks for single image novel viewsynthesis tasks with held-out objects as well as entire unseen categories. Wefurther demonstrate the flexibility of pixelNeRF by demonstrating it onmulti-object ShapeNet scenes and real scenes from the DTU dataset. In allcases, pixelNeRF outperforms current state-of-the-art baselines for novel viewsynthesis and single image 3D reconstruction. For the video and code, pleasevisit the project website: https://alexyu.net/pixelnerf},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.02190v3},
  FILE = {2012.02190v3.pdf}
 }

@article{tancik2020learned,
  AUTHOR = {Matthew Tancik and Ben Mildenhall and Terrance Wang and Divi Schmidt and Pratul P. Srinivasan and Jonathan T. Barron and Ren Ng},
  TITLE = {Learned Initializations for Optimizing Coordinate-Based NeuralRepresentations},
  EPRINT = {2012.02189v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Coordinate-based neural representations have shown significant promise as analternative to discrete, array-based representations for complex lowdimensional signals. However, optimizing a coordinate-based network fromrandomly initialized weights for each new signal is inefficient. We proposeapplying standard meta-learning algorithms to learn the initial weightparameters for these fully-connected networks based on the underlying class ofsignals being represented (e.g., images of faces or 3D models of chairs).Despite requiring only a minor change in implementation, using these learnedinitial weights enables faster convergence during optimization and can serve asa strong prior over the signal class being modeled, resulting in bettergeneralization when only partial observations of a given signal are available.We explore these benefits across a variety of tasks, including representing 2Dimages, reconstructing CT scans, and recovering 3D shapes and scenes from 2Dimage observations.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.02189v2},
  FILE = {2012.02189v2.pdf}
 }

@article{lindell2020autoint,
  AUTHOR = {David B. Lindell and Julien N. P. Martel and Gordon Wetzstein},
  TITLE = {AutoInt: Automatic Integration for Fast Neural Volume Rendering},
  EPRINT = {2012.01714v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Numerical integration is a foundational technique in scientific computing andis at the core of many computer vision applications. Among these applications,neural volume rendering has recently been proposed as a new paradigm for viewsynthesis, achieving photorealistic image quality. However, a fundamentalobstacle to making these methods practical is the extreme computational andmemory requirements caused by the required volume integrations along therendered rays during training and inference. Millions of rays, each requiringhundreds of forward passes through a neural network are needed to approximatethose integrations with Monte Carlo sampling. Here, we propose automaticintegration, a new framework for learning efficient, closed-form solutions tointegrals using coordinate-based neural networks. For training, we instantiatethe computational graph corresponding to the derivative of the network. Thegraph is fitted to the signal to integrate. After optimization, we reassemblethe graph to obtain a network that represents the antiderivative. By thefundamental theorem of calculus, this enables the calculation of any definiteintegral in two evaluations of the network. Applying this approach to neuralrendering, we improve a tradeoff between rendering speed and image quality:improving render times by greater than 10 times with a tradeoff of slightlyreduced image quality.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.01714v2},
  FILE = {2012.01714v2.pdf}
 }

@article{gafni2020nerface,
  AUTHOR = {Guy Gafni and Justus Thies and Michael Zollhofer and Matthias Niessner},
  TITLE = {Dynamic Neural Radiance Fields for Monocular 4D Facial AvatarReconstruction},
  EPRINT = {2012.03065v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present dynamic neural radiance fields for modeling the appearance anddynamics of a human face. Digitally modeling and reconstructing a talking humanis a key building-block for a variety of applications. Especially, fortelepresence applications in AR or VR, a faithful reproduction of theappearance including novel viewpoints or head-poses is required. In contrast tostate-of-the-art approaches that model the geometry and material propertiesexplicitly, or are purely image-based, we introduce an implicit representationof the head based on scene representation networks. To handle the dynamics ofthe face, we combine our scene representation network with a low-dimensionalmorphable model which provides explicit control over pose and expressions. Weuse volumetric rendering to generate images from this hybrid representation anddemonstrate that such a dynamic neural scene representation can be learned frommonocular input data only, without the need of a specialized capture setup. Inour experiments, we show that this learned volumetric representation allows forphoto-realistic image generation that surpasses the quality of state-of-the-artvideo-based reenactment methods.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.03065v1},
  FILE = {2012.03065v1.pdf}
 }

@article{shaham2020asapnet,
  AUTHOR = {Tamar Rott Shaham and Michael Gharbi and Richard Zhang and Eli Shechtman and Tomer Michaeli},
  TITLE = {Spatially-Adaptive Pixelwise Networks for Fast Image Translation},
  EPRINT = {2012.02992v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce a new generator architecture, aimed at fast and efficienthigh-resolution image-to-image translation. We design the generator to be anextremely lightweight function of the full-resolution image. In fact, we usepixel-wise networks; that is, each pixel is processed independently of others,through a composition of simple affine transformations and nonlinearities. Wetake three important steps to equip such a seemingly simple function withadequate expressivity. First, the parameters of the pixel-wise networks arespatially varying so they can represent a broader function class than simple1x1 convolutions. Second, these parameters are predicted by a fastconvolutional network that processes an aggressively low-resolutionrepresentation of the input; Third, we augment the input image with asinusoidal encoding of spatial coordinates, which provides an effectiveinductive bias for generating realistic novel high-frequency image content. Asa result, our model is up to 18x faster than state-of-the-art baselines. Weachieve this speedup while generating comparable visual quality acrossdifferent image resolutions and translation domains.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.02992v1},
  FILE = {2012.02992v1.pdf}
 }

@article{srinivasan2020nerv,
  AUTHOR = {Pratul P. Srinivasan and Boyang Deng and Xiuming Zhang and Matthew Tancik and Ben Mildenhall and Jonathan T. Barron},
  TITLE = {NeRV: Neural Reflectance and Visibility Fields for Relighting and ViewSynthesis},
  EPRINT = {2012.03927v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method that takes as input a set of images of a sceneilluminated by unconstrained known lighting, and produces as output a 3Drepresentation that can be rendered from novel viewpoints under arbitrarylighting conditions. Our method represents the scene as a continuous volumetricfunction parameterized as MLPs whose inputs are a 3D location and whose outputsare the following scene properties at that input location: volume density,surface normal, material parameters, distance to the first surface intersectionin any direction, and visibility of the external environment in any direction.Together, these allow us to render novel views of the object under arbitrarylighting, including indirect illumination effects. The predicted visibility andsurface intersection fields are critical to our model's ability to simulatedirect and indirect illumination during training, because the brute-forcetechniques used by prior work are intractable for lighting conditions outsideof controlled setups with a single light. Our method outperforms alternativeapproaches for recovering relightable 3D scene representations, and performswell in complex lighting settings that have posed a significant challenge toprior work.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.03927v1},
  FILE = {2012.03927v1.pdf}
 }

@article{boss2020nerd,
  AUTHOR = {Mark Boss and Raphael Braun and Varun Jampani and Jonathan T. Barron and Ce Liu and Hendrik P. A. Lensch},
  TITLE = {NeRD: Neural Reflectance Decomposition from Image Collections},
  EPRINT = {2012.03918v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Decomposing a scene into its shape, reflectance, and illumination is achallenging but essential problem in computer vision and graphics. This problemis inherently more challenging when the illumination is not a single lightsource under laboratory conditions but is instead an unconstrainedenvironmental illumination. Though recent work has shown that implicitrepresentations can be used to model the radiance field of an object, thesetechniques only enable view synthesis and not relighting. Additionally,evaluating these radiance fields is resource and time-intensive. By decomposinga scene into explicit representations, any rendering framework can be leveragedto generate novel views under any illumination in real-time. NeRD is a methodthat achieves this decomposition by introducing physically-based rendering toneural radiance fields. Even challenging non-Lambertian reflectances, complexgeometry, and unknown illumination can be decomposed into high-quality models.The datasets and code is available on the project page:https://markboss.me/publication/2021-nerd/},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.03918v3},
  FILE = {2012.03918v3.pdf}
 }

@article{yen-chen2020inerf,
  AUTHOR = {Lin Yen-Chen and Pete Florence and Jonathan T. Barron and Alberto Rodriguez and Phillip Isola and Tsung-Yi Lin},
  TITLE = {INeRF: Inverting Neural Radiance Fields for Pose Estimation},
  EPRINT = {2012.05877v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present iNeRF, a framework that performs mesh-free pose estimation by"inverting" a Neural RadianceField (NeRF). NeRFs have been shown to beremarkably effective for the task of view synthesis - synthesizingphotorealistic novel views of real-world scenes or objects. In this work, weinvestigate whether we can apply analysis-by-synthesis via NeRF for mesh-free,RGB-only 6DoF pose estimation - given an image, find the translation androtation of a camera relative to a 3D object or scene. Our method assumes thatno object mesh models are available during either training or test time.Starting from an initial pose estimate, we use gradient descent to minimize theresidual between pixels rendered from a NeRF and pixels in an observed image.In our experiments, we first study 1) how to sample rays during pose refinementfor iNeRF to collect informative gradients and 2) how different batch sizes ofrays affect iNeRF on a synthetic dataset. We then show that for complexreal-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimatingthe camera poses of novel images and using these images as additional trainingdata for NeRF. Finally, we show iNeRF can perform category-level object poseestimation, including object instances not seen during training, with RGBimages by inverting a NeRF model inferred from a single view.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.05877v3},
  FILE = {2012.05877v3.pdf}
 }

@article{gao2020portraitnerf,
  AUTHOR = {Chen Gao and Yichang Shih and Wei-Sheng Lai and Chia-Kai Liang and Jia-Bin Huang},
  TITLE = {Portrait Neural Radiance Fields from a Single Image},
  EPRINT = {2012.05903v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method for estimating Neural Radiance Fields (NeRF) from asingle headshot portrait. While NeRF has demonstrated high-quality viewsynthesis, it requires multiple images of static scenes and thus impracticalfor casual captures and moving subjects. In this work, we propose to pretrainthe weights of a multilayer perceptron (MLP), which implicitly models thevolumetric density and colors, with a meta-learning framework using a lightstage portrait dataset. To improve the generalization to unseen faces, we trainthe MLP in the canonical coordinate space approximated by 3D face morphablemodels. We quantitatively evaluate the method using controlled captures anddemonstrate the generalization to real portrait images, showing favorableresults against state-of-the-arts.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.05903v2},
  FILE = {2012.05903v2.pdf}
 }

@article{yifan2020isopoints,
  AUTHOR = {Wang Yifan and Shihao Wu and Cengiz Oztireli and Olga Sorkine-Hornung},
  TITLE = {Iso-Points: Optimizing Neural Implicit Surfaces with HybridRepresentations},
  EPRINT = {2012.06434v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural implicit functions have emerged as a powerful representation forsurfaces in 3D. Such a function can encode a high quality surface withintricate details into the parameters of a deep neural network. However,optimizing for the parameters for accurate and robust reconstructions remains achallenge, especially when the input data is noisy or incomplete. In this work,we develop a hybrid neural surface representation that allows us to imposegeometry-aware sampling and regularization, which significantly improves thefidelity of reconstructions. We propose to use \emph{iso-points} as an explicitrepresentation for a neural implicit function. These points are computed andupdated on-the-fly during training to capture important geometric features andimpose geometric constraints on the optimization. We demonstrate that ourmethod can be adopted to improve state-of-the-art techniques for reconstructingneural implicit surfaces from multi-view images or point clouds. Quantitativeand qualitative evaluations show that, compared with existing sampling andoptimization methods, our approach allows faster convergence, bettergeneralization, and accurate recovery of details and topology.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.06434v2},
  FILE = {2012.06434v2.pdf}
 }

@article{yang2020deep,
  AUTHOR = {Mingyue Yang and Yuxin Wen and Weikai Chen and Yongwei Chen and Kui Jia},
  TITLE = {Deep Optimized Priors for 3D Shape Modeling and Reconstruction},
  EPRINT = {2012.07241v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Many learning-based approaches have difficulty scaling to unseen data, as thegenerality of its learned prior is limited to the scale and variations of thetraining samples. This holds particularly true with 3D learning tasks, giventhe sparsity of 3D datasets available. We introduce a new learning frameworkfor 3D modeling and reconstruction that greatly improves the generalizationability of a deep generator. Our approach strives to connect the good ends ofboth learning-based and optimization-based methods. In particular, unlike thecommon practice that fixes the pre-trained priors at test time, we propose tofurther optimize the learned prior and latent code according to the inputphysical measurements after the training. We show that the proposed strategyeffectively breaks the barriers constrained by the pre-trained priors and couldlead to high-quality adaptation to unseen data. We realize our framework usingthe implicit surface representation and validate the efficacy of our approachin a variety of challenging tasks that take highly sparse or collapsedobservations as input. Experimental results show that our approach comparesfavorably with the state-of-the-art methods in terms of both generality andaccuracy.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.07241v1},
  FILE = {2012.07241v1.pdf}
 }

@article{guo2020osfs,
  AUTHOR = {Michelle Guo and Alireza Fathi and Jiajun Wu and Thomas Funkhouser},
  TITLE = {Object-Centric Neural Scene Rendering},
  EPRINT = {2012.08503v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method for composing photorealistic scenes from captured imagesof objects. Our work builds upon neural radiance fields (NeRFs), whichimplicitly model the volumetric density and directionally-emitted radiance of ascene. While NeRFs synthesize realistic pictures, they only model static scenesand are closely tied to specific imaging conditions. This property makes NeRFshard to generalize to new scenarios, including new lighting or new arrangementsof objects. Instead of learning a scene radiance field as a NeRF does, wepropose to learn object-centric neural scattering functions (OSFs), arepresentation that models per-object light transport implicitly using alighting- and view-dependent neural network. This enables rendering scenes evenwhen objects or lights move, without retraining. Combined with a volumetricpath tracing procedure, our framework is capable of rendering both intra- andinter-object light transport effects including occlusions, specularities,shadows, and indirect illumination. We evaluate our approach on scenecomposition and show that it generalizes to novel illumination conditions,producing photorealistic, physically accurate renderings of multi-objectscenes.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.08503v1},
  FILE = {2012.08503v1.pdf}
 }

@article{chen2020liif,
  AUTHOR = {Yinbo Chen and Sifei Liu and Xiaolong Wang},
  TITLE = {Learning Continuous Image Representation with Local Implicit ImageFunction},
  EPRINT = {2012.09161v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {How to represent an image? While the visual world is presented in acontinuous manner, machines store and see the images in a discrete way with 2Darrays of pixels. In this paper, we seek to learn a continuous representationfor images. Inspired by the recent progress in 3D reconstruction with implicitneural representation, we propose Local Implicit Image Function (LIIF), whichtakes an image coordinate and the 2D deep features around the coordinate asinputs, predicts the RGB value at a given coordinate as an output. Since thecoordinates are continuous, LIIF can be presented in arbitrary resolution. Togenerate the continuous representation for images, we train an encoder withLIIF representation via a self-supervised task with super-resolution. Thelearned continuous representation can be presented in arbitrary resolution evenextrapolate to x30 higher resolution, where the training tasks are notprovided. We further show that LIIF representation builds a bridge betweendiscrete and continuous representation in 2D, it naturally supports thelearning tasks with size-varied image ground-truths and significantlyoutperforms the method with resizing the ground-truths.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.09161v2},
  FILE = {2012.09161v2.pdf}
 }

@article{du2020nerflow,
  AUTHOR = {Yilun Du and Yinan Zhang and Hong-Xing Yu and Joshua B. Tenenbaum and Jiajun Wu},
  TITLE = {Neural Radiance Flow for 4D View Synthesis and Video Processing},
  EPRINT = {2012.09790v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method, Neural Radiance Flow (NeRFlow),to learn a 4Dspatial-temporal representation of a dynamic scene from a set of RGB images.Key to our approach is the use of a neural implicit representation that learnsto capture the 3D occupancy, radiance, and dynamics of the scene. By enforcingconsistency across different modalities, our representation enables multi-viewrendering in diverse dynamic scenes, including water pouring, roboticinteraction, and real images, outperforming state-of-the-art methods forspatial-temporal view synthesis. Our approach works even when inputs images arecaptured with only one camera. We further demonstrate that the learnedrepresentation can serve as an implicit scene prior, enabling video processingtasks such as image super-resolution and de-noising without any additionalsupervision.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.09790v1},
  FILE = {2012.09790v1.pdf}
 }

@article{wang2020learning,
  AUTHOR = {Ziyan Wang and Timur Bagautdinov and Stephen Lombardi and Tomas Simon and Jason Saragih and Jessica Hodgins and Michael Zollhofer},
  TITLE = {Learning Compositional Radiance Fields of Dynamic Human Heads},
  EPRINT = {2012.09955v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Photorealistic rendering of dynamic humans is an important ability fortelepresence systems, virtual shopping, synthetic data generation, and more.Recently, neural rendering methods, which combine techniques from computergraphics and machine learning, have created high-fidelity models of humans andobjects. Some of these methods do not produce results with high-enough fidelityfor driveable human models (Neural Volumes) whereas others have extremely longrendering times (NeRF). We propose a novel compositional 3D representation thatcombines the best of previous methods to produce both higher-resolution andfaster results. Our representation bridges the gap between discrete andcontinuous volumetric representations by combining a coarse 3D-structure-awaregrid of animation codes with a continuous learned scene function that mapsevery position and its corresponding local animation code to its view-dependentemitted radiance and local volume density. Differentiable volume rendering isemployed to compute photo-realistic novel views of the human head and upperbody as well as to train our novel representation end-to-end using only 2Dsupervision. In addition, we show that the learned dynamic radiance field canbe used to synthesize novel unseen expressions based on a global animationcode. Our approach achieves state-of-the-art results for synthesizing novelviews of dynamic human heads and the upper body.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.09955v1},
  FILE = {2012.09955v1.pdf}
 }

@article{nirkin2020hyperseg,
  AUTHOR = {Yuval Nirkin and Lior Wolf and Tal Hassner},
  TITLE = {HyperSeg: Patch-wise Hypernetwork for Real-time Semantic Segmentation},
  EPRINT = {2012.11582v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a novel, real-time, semantic segmentation network in which theencoder both encodes and generates the parameters (weights) of the decoder.Furthermore, to allow maximal adaptivity, the weights at each decoder blockvary spatially. For this purpose, we design a new type of hypernetwork,composed of a nested U-Net for drawing higher level context features, amulti-headed weight generating module which generates the weights of each blockin the decoder immediately before they are consumed, for efficient memoryutilization, and a primary network that is composed of novel dynamic patch-wiseconvolutions. Despite the usage of less-conventional blocks, our architectureobtains real-time performance. In terms of the runtime vs. accuracy trade-off,we surpass state of the art (SotA) results on popular semantic segmentationbenchmarks: PASCAL VOC 2012 (val. set) and real-time semantic segmentation onCityscapes, and CamVid. The code is available: https://nirkin.com/hyperseg.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.11582v2},
  FILE = {2012.11582v2.pdf}
 }

@article{tretschk2020nrnerf,
  AUTHOR = {Edgar Tretschk and Ayush Tewari and Vladislav Golyanik and Michael Zollhofer and Christoph Lassner and Christian Theobalt},
  TITLE = {Non-Rigid Neural Radiance Fields: Reconstruction and Novel ViewSynthesis of a Dynamic Scene From Monocular Video},
  EPRINT = {2012.12247v4},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction andnovel view synthesis approach for general non-rigid dynamic scenes. Ourapproach takes RGB images of a dynamic scene as input (e.g., from a monocularvideo recording), and creates a high-quality space-time geometry and appearancerepresentation. We show that a single handheld consumer-grade camera issufficient to synthesize sophisticated renderings of a dynamic scene from novelvirtual camera views, e.g. a `bullet-time' video effect. NR-NeRF disentanglesthe dynamic scene into a canonical volume and its deformation. Scenedeformation is implemented as ray bending, where straight rays are deformednon-rigidly. We also propose a novel rigidity network to better constrain rigidregions of the scene, leading to more stable results. The ray bending andrigidity network are trained without explicit supervision. Our formulationenables dense correspondence estimation across views and time, and compellingvideo editing applications such as motion exaggeration. Our code will be opensourced.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.12247v4},
  FILE = {2012.12247v4.pdf}
 }

@article{yuan2020star,
  AUTHOR = {Wentao Yuan and Zhaoyang Lv and Tanner Schmidt and Steven Lovegrove},
  TITLE = {STaR: Self-supervised Tracking and Reconstruction of Rigid Objects inMotion with Neural Rendering},
  EPRINT = {2101.01602v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present STaR, a novel method that performs Self-supervised Tracking andReconstruction of dynamic scenes with rigid motion from multi-view RGB videoswithout any manual annotation. Recent work has shown that neural networks aresurprisingly effective at the task of compressing many views of a scene into alearned function which maps from a viewing ray to an observed radiance valuevia volume rendering. Unfortunately, these methods lose all their predictivepower once any object in the scene has moved. In this work, we explicitly modelrigid motion of objects in the context of neural representations of radiancefields. We show that without any additional human specified supervision, we canreconstruct a dynamic scene with a single rigid object in motion bysimultaneously decomposing it into its two constituent parts and encoding eachwith its own neural representation. We achieve this by jointly optimizing theparameters of two neural radiance fields and a set of rigid poses which alignthe two fields at each frame. On both synthetic and real world datasets, wedemonstrate that our method can render photorealistic novel views, wherenovelty is measured on both spatial and temporal axes. Our factoredrepresentation furthermore enables animation of unseen object motion.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2101.01602v1},
  FILE = {2101.01602v1.pdf}
 }

@article{peng2020neuralbody,
  AUTHOR = {Sida Peng and Yuanqing Zhang and Yinghao Xu and Qianqian Wang and Qing Shuai and Hujun Bao and Xiaowei Zhou},
  TITLE = {Neural Body: Implicit Neural Representations with Structured LatentCodes for Novel View Synthesis of Dynamic Humans},
  EPRINT = {2012.15838v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {This paper addresses the challenge of novel view synthesis for a humanperformer from a very sparse set of camera views. Some recent works have shownthat learning implicit neural representations of 3D scenes achieves remarkableview synthesis quality given dense input views. However, the representationlearning will be ill-posed if the views are highly sparse. To solve thisill-posed problem, our key idea is to integrate observations over video frames.To this end, we propose Neural Body, a new human body representation whichassumes that the learned neural representations at different frames share thesame set of latent codes anchored to a deformable mesh, so that theobservations across frames can be naturally integrated. The deformable meshalso provides geometric guidance for the network to learn 3D representationsmore efficiently. To evaluate our approach, we create a multi-view datasetnamed ZJU-MoCap that captures performers with complex motions. Experiments onZJU-MoCap show that our approach outperforms prior works by a large margin interms of novel view synthesis quality. We also demonstrate the capability ofour approach to reconstruct a moving person from a monocular video on thePeople-Snapshot dataset. The code and dataset are available athttps://zju3dv.github.io/neuralbody/.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.15838v2},
  FILE = {2012.15838v2.pdf}
 }

@article{shen2021nonlineofsight,
  AUTHOR = {Siyuan Shen and Zi Wang and Ping Liu and Zhengqing Pan and Ruiqian Li and Tian Gao and Shiying Li and Jingyi Yu},
  TITLE = {Non-line-of-Sight Imaging via Neural Transient Fields},
  EPRINT = {2101.00373v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.IV},
  ABSTRACT = {We present a neural modeling framework for Non-Line-of-Sight (NLOS) imaging.Previous solutions have sought to explicitly recover the 3D geometry (e.g., aspoint clouds) or voxel density (e.g., within a pre-defined volume) of thehidden scene. In contrast, inspired by the recent Neural Radiance Field (NeRF)approach, we use a multi-layer perceptron (MLP) to represent the neuraltransient field or NeTF. However, NeTF measures the transient over sphericalwavefronts rather than the radiance along lines. We therefore formulate aspherical volume NeTF reconstruction pipeline, applicable to both confocal andnon-confocal setups. Compared with NeRF, NeTF samples a much sparser set ofviewpoints (scanning spots) and the sampling is highly uneven. We thusintroduce a Monte Carlo technique to improve the robustness in thereconstruction. Comprehensive experiments on synthetic and real datasetsdemonstrate NeTF provides higher quality reconstruction and preserves finedetails largely missing in the state-of-the-art.},
  YEAR = {2021},
  MONTH = {Jan},
  URL = {http://arxiv.org/abs/2101.00373v2},
  FILE = {2101.00373v2.pdf}
 }

@article{raj2021pva,
  AUTHOR = {Amit Raj and Michael Zollhoefer and Tomas Simon and Jason Saragih and Shunsuke Saito and James Hays and Stephen Lombardi},
  TITLE = {PVA: Pixel-aligned Volumetric Avatars},
  EPRINT = {2101.02697v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Acquisition and rendering of photo-realistic human heads is a highlychallenging research problem of particular importance for virtual telepresence.Currently, the highest quality is achieved by volumetric approaches trained ina person specific manner on multi-view data. These models better represent finestructure, such as hair, compared to simpler mesh-based models. Volumetricmodels typically employ a global code to represent facial expressions, suchthat they can be driven by a small set of animation parameters. While sucharchitectures achieve impressive rendering quality, they can not easily beextended to the multi-identity setting. In this paper, we devise a novelapproach for predicting volumetric avatars of the human head given just a smallnumber of inputs. We enable generalization across identities by a novelparameterization that combines neural radiance fields with local, pixel-alignedfeatures extracted directly from the inputs, thus sidestepping the need forvery deep or complex networks. Our approach is trained in an end-to-end mannersolely based on a photometric re-rendering loss without requiring explicit 3Dsupervision.We demonstrate that our approach outperforms the existing state ofthe art in terms of quality and is able to generate faithful facial expressionsin a multi-identity setting.},
  YEAR = {2021},
  MONTH = {Jan},
  URL = {http://arxiv.org/abs/2101.02697v1},
  FILE = {2101.02697v1.pdf}
 }

@article{takikawa2021neural,
  AUTHOR = {Towaki Takikawa and Joey Litalien and Kangxue Yin and Karsten Kreis and Charles Loop and Derek Nowrouzezahrai and Alec Jacobson and Morgan McGuire and Sanja Fidler},
  TITLE = {Neural Geometric Level of Detail: Real-time Rendering with Implicit 3DShapes},
  EPRINT = {2101.10994v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural signed distance functions (SDFs) are emerging as an effectiverepresentation for 3D shapes. State-of-the-art methods typically encode the SDFwith a large, fixed-size neural network to approximate complex shapes withimplicit surfaces. Rendering with these large networks is, however,computationally expensive since it requires many forward passes through thenetwork for every pixel, making these representations impractical for real-timegraphics. We introduce an efficient neural representation that, for the firsttime, enables real-time rendering of high-fidelity neural SDFs, while achievingstate-of-the-art geometry reconstruction quality. We represent implicitsurfaces using an octree-based feature volume which adaptively fits shapes withmultiple discrete levels of detail (LODs), and enables continuous LOD with SDFinterpolation. We further develop an efficient algorithm to directly render ournovel neural SDF representation in real-time by querying only the necessaryLODs with sparse octree traversal. We show that our representation is 2-3orders of magnitude more efficient in terms of rendering speed compared toprevious works. Furthermore, it produces state-of-the-art reconstructionquality for complex shapes under both 3D geometric and 2D image-space metrics.},
  YEAR = {2021},
  MONTH = {Jan},
  URL = {http://arxiv.org/abs/2101.10994v1},
  FILE = {2101.10994v1.pdf}
 }

@article{costain2021towards,
  AUTHOR = {Theo W. Costain and Victor Adrian Prisacariu},
  TITLE = {Towards Generalising Neural Implicit Representations},
  EPRINT = {2101.12690v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural implicit representations have shown substantial improvements inefficiently storing 3D data, when compared to conventional formats. However,the focus of existing work has mainly been on storage and subsequentreconstruction. In this work, we show that training neural representations forreconstruction tasks alongside conventional tasks can produce more generalencodings that admit equal quality reconstructions to single task training,whilst improving results on conventional tasks when compared to single taskencodings. We reformulate the semantic segmentation task, creating a morerepresentative task for implicit representation contexts, and throughmulti-task experiments on reconstruction, classification, and segmentation,show our approach learns feature rich encodings that admit equal performancefor each task.},
  YEAR = {2021},
  MONTH = {Jan},
  URL = {http://arxiv.org/abs/2101.12690v2},
  FILE = {2101.12690v2.pdf}
 }

@article{sun2021coil,
  AUTHOR = {Yu Sun and Jiaming Liu and Mingyang Xie and Brendt Wohlberg and Ulugbek S. Kamilov},
  TITLE = {CoIL: Coordinate-based Internal Learning for Imaging Inverse Problems},
  EPRINT = {2102.05181v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.IV},
  ABSTRACT = {We propose Coordinate-based Internal Learning (CoIL) as a new deep-learning(DL) methodology for the continuous representation of measurements. Unliketraditional DL methods that learn a mapping from the measurements to thedesired image, CoIL trains a multilayer perceptron (MLP) to encode the completemeasurement field by mapping the coordinates of the measurements to theirresponses. CoIL is a self-supervised method that requires no training examplesbesides the measurements of the test object itself. Once the MLP is trained,CoIL generates new measurements that can be used within a majority of imagereconstruction methods. We validate CoIL on sparse-view computed tomographyusing several widely-used reconstruction methods, including purely model-basedmethods and those based on DL. Our results demonstrate the ability of CoIL toconsistently improve the performance of all the considered methods by providinghigh-fidelity measurement fields.},
  YEAR = {2021},
  MONTH = {Feb},
  URL = {http://arxiv.org/abs/2102.05181v1},
  FILE = {2102.05181v1.pdf}
 }

@article{su2021anerf,
  AUTHOR = {Shih-Yang Su and Frank Yu and Michael Zollhoefer and Helge Rhodin},
  TITLE = {A-NeRF: Surface-free Human 3D Pose Refinement via Neural Rendering},
  EPRINT = {2102.06199v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {While deep learning has reshaped the classical motion capture pipeline,generative, analysis-by-synthesis elements are still in use to recover finedetails if a high-quality 3D model of the user is available. Unfortunately,obtaining such a model for every user a priori is challenging, time-consuming,and limits the application scenarios. We propose a novel test-time optimizationapproach for monocular motion capture that learns a volumetric body model ofthe user in a self-supervised manner. To this end, our approach combines theadvantages of neural radiance fields with an articulated skeletonrepresentation. Our proposed skeleton embedding serves as a common referencethat links constraints across time, thereby reducing the number of requiredcamera views from traditionally dozens of calibrated cameras, down to a singleuncalibrated one. As a starting point, we employ the output of an off-the-shelfmodel that predicts the 3D skeleton pose. The volumetric body shape andappearance is then learned from scratch, while jointly refining the initialpose estimate. Our approach is self-supervised and does not require anyadditional ground truth labels for appearance, pose, or 3D shape. Wedemonstrate that our novel combination of a discriminative pose estimationtechnique with surface-free analysis-by-synthesis outperforms purelydiscriminative monocular pose estimation approaches and generalizes well tomultiple views.},
  YEAR = {2021},
  MONTH = {Feb},
  URL = {http://arxiv.org/abs/2102.06199v1},
  FILE = {2102.06199v1.pdf}
 }

@article{wang2021nerf--,
  AUTHOR = {Zirui Wang and Shangzhe Wu and Weidi Xie and Min Chen and Victor Adrian Prisacariu},
  TITLE = {NeRF--: Neural Radiance Fields Without Known Camera Parameters},
  EPRINT = {2102.07064v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {This paper tackles the problem of novel view synthesis (NVS) from 2D imageswithout known camera poses and intrinsics. Among various NVS techniques, NeuralRadiance Field (NeRF) has recently gained popularity due to its remarkablesynthesis quality. Existing NeRF-based approaches assume that the cameraparameters associated with each input image are either directly accessible attraining, or can be accurately estimated with conventional techniques based oncorrespondences, such as Structure-from-Motion. In this work, we propose anend-to-end framework, termed NeRF--, for training NeRF models given only RGBimages, without pre-computed camera parameters. Specifically, we show that thecamera parameters, including both intrinsics and extrinsics, can beautomatically discovered via joint optimisation during the training of the NeRFmodel. On the standard LLFF benchmark, our model achieves comparable novel viewsynthesis results compared to the baseline trained with COLMAP pre-computedcamera parameters. We also conduct extensive analyses to understand the modelbehaviour under different camera trajectories, and show that in scenarios whereCOLMAP fails, our model still produces robust results.},
  YEAR = {2021},
  MONTH = {Feb},
  URL = {http://arxiv.org/abs/2102.07064v3},
  FILE = {2102.07064v3.pdf}
 }

@article{rematas2021sharf,
  AUTHOR = {Konstantinos Rematas and Ricardo Martin-Brualla and Vittorio Ferrari},
  TITLE = {ShaRF: Shape-conditioned Radiance Fields from a Single View},
  EPRINT = {2102.08860v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method for estimating neural scenes representations of objectsgiven only a single image. The core of our method is the estimation of ageometric scaffold for the object and its use as a guide for the reconstructionof the underlying radiance field. Our formulation is based on a generativeprocess that first maps a latent code to a voxelized shape, and then renders itto an image, with the object appearance being controlled by a second latentcode. During inference, we optimize both the latent codes and the networks tofit a test image of a new object. The explicit disentanglement of shape andappearance allows our model to be fine-tuned given a single image. We can thenrender new views in a geometrically consistent manner and they representfaithfully the input object. Additionally, our method is able to generalize toimages outside of the training domain (more realistic renderings and even realphotographs). Finally, the inferred geometric scaffold is itself an accurateestimate of the object's 3D shape. We demonstrate in several experiments theeffectiveness of our approach in both synthetic and real images.},
  YEAR = {2021},
  MONTH = {Feb},
  URL = {http://arxiv.org/abs/2102.08860v2},
  FILE = {2102.08860v2.pdf}
 }

@article{zehnder2021ntopo,
  AUTHOR = {Jonas Zehnder and Yue Li and Stelian Coros and Bernhard Thomaszewski},
  TITLE = {NTopo: Mesh-free Topology Optimization using Implicit NeuralRepresentations},
  EPRINT = {2102.10782v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {Recent advances in implicit neural representations show great promise when itcomes to generating numerical solutions to partial differential equations(PDEs). Compared to conventional alternatives, such representations employparameterized neural networks to define, in a mesh-free manner, signals thatare highly-detailed, continuous, and fully differentiable. Most prior works aimto exploit these benefits in order to solve PDE-governed forward problems, orassociated inverse problems that are defined by a small number of parameters.In this work, we present a novel machine learning approach to tackle topologyoptimization (TO) problems. Topology optimization refers to an important classof inverse problems that typically feature very high-dimensional parameterspaces and objective landscapes which are highly non-linear. To effectivelyleverage neural representations in the context of TO problems, we usemultilayer perceptrons (MLPs) to parameterize both density and displacementfields. Using sensitivity analysis with a moving mean squared error, we showthat our formulation can be used to efficiently minimize traditional structuralcompliance objectives. As we show through our experiments, a major benefit ofour approach is that it enables self-supervised learning of continuous solutionspaces to topology optimization problems.},
  YEAR = {2021},
  MONTH = {Feb},
  URL = {http://arxiv.org/abs/2102.10782v1},
  FILE = {2102.10782v1.pdf}
 }

@article{wang2021ibrnet,
  AUTHOR = {Qianqian Wang and Zhicheng Wang and Kyle Genova and Pratul Srinivasan and Howard Zhou and Jonathan T. Barron and Ricardo Martin-Brualla and Noah Snavely and Thomas Funkhouser},
  TITLE = {IBRNet: Learning Multi-View Image-Based Rendering},
  EPRINT = {2102.13090v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a method that synthesizes novel views of complex scenes byinterpolating a sparse set of nearby views. The core of our method is a networkarchitecture that includes a multilayer perceptron and a ray transformer thatestimates radiance and volume density at continuous 5D locations (3D spatiallocations and 2D viewing directions), drawing appearance information on the flyfrom multiple source views. By drawing on source views at render time, ourmethod hearkens back to classic work on image-based rendering (IBR), and allowsus to render high-resolution imagery. Unlike neural scene representation workthat optimizes per-scene functions for rendering, we learn a generic viewinterpolation function that generalizes to novel scenes. We render images usingclassic volume rendering, which is fully differentiable and allows us to trainusing only multi-view posed images as supervision. Experiments show that ourmethod outperforms recent novel view synthesis methods that also seek togeneralize to novel scenes. Further, if fine-tuned on each scene, our method iscompetitive with state-of-the-art single-scene neural rendering methods.Project page: https://ibrnet.github.io/},
  YEAR = {2021},
  MONTH = {Feb},
  URL = {http://arxiv.org/abs/2102.13090v2},
  FILE = {2102.13090v2.pdf}
 }

@article{xiang2021neutex,
  AUTHOR = {Fanbo Xiang and Zexiang Xu and Milos Hasan and Yannick Hold-Geoffroy and Kalyan Sunkavalli and Hao Su},
  TITLE = {NeuTex: Neural Texture Mapping for Volumetric Neural Rendering},
  EPRINT = {2103.00762v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent work has demonstrated that volumetric scene representations combinedwith differentiable volume rendering can enable photo-realistic rendering forchallenging scenes that mesh reconstruction fails on. However, these methodsentangle geometry and appearance in a "black-box" volume that cannot be edited.Instead, we present an approach that explicitly disentanglesgeometry--represented as a continuous 3D volume--from appearance--representedas a continuous 2D texture map. We achieve this by introducing a 3D-to-2Dtexture mapping (or surface parameterization) network into volumetricrepresentations. We constrain this texture mapping network using an additional2D-to-3D inverse mapping network and a novel cycle consistency loss to make 3Dsurface points map to 2D texture points that map back to the original 3Dpoints. We demonstrate that this representation can be reconstructed using onlymulti-view image supervision and generates high-quality rendering results. Moreimportantly, by separating geometry and texture, we allow users to editappearance by simply editing 2D texture maps.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.00762v1},
  FILE = {2103.00762v1.pdf}
 }

@article{lombardi2021mvp,
  AUTHOR = {Stephen Lombardi and Tomas Simon and Gabriel Schwartz and Michael Zollhoefer and Yaser Sheikh and Jason Saragih},
  TITLE = {Mixture of Volumetric Primitives for Efficient Neural Rendering},
  EPRINT = {2103.01954v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {Real-time rendering and animation of humans is a core function in games,movies, and telepresence applications. Existing methods have a number ofdrawbacks we aim to address with our work. Triangle meshes have difficultymodeling thin structures like hair, volumetric representations like NeuralVolumes are too low-resolution given a reasonable memory budget, andhigh-resolution implicit representations like Neural Radiance Fields are tooslow for use in real-time applications. We present Mixture of VolumetricPrimitives (MVP), a representation for rendering dynamic 3D content thatcombines the completeness of volumetric representations with the efficiency ofprimitive-based rendering, e.g., point-based or mesh-based methods. Ourapproach achieves this by leveraging spatially shared computation with adeconvolutional architecture and by minimizing computation in empty regions ofspace with volumetric primitives that can move to cover only occupied regions.Our parameterization supports the integration of correspondence and trackingconstraints, while being robust to areas where classical tracking fails, suchas around thin or translucent structures and areas with large topologicalvariability. MVP is a hybrid that generalizes both volumetric andprimitive-based representations. Through a series of extensive experiments wedemonstrate that it inherits the strengths of each, while avoiding many oftheir limitations. We also compare our approach to several state-of-the-artmethods and demonstrate that MVP produces superior results in terms of qualityand runtime performance.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.01954v2},
  FILE = {2103.01954v2.pdf}
 }

@article{dupont2021coin,
  AUTHOR = {Emilien Dupont and Adam Golinski and Milad Alizadeh and Yee Whye Teh and Arnaud Doucet},
  TITLE = {COIN: COmpression with Implicit Neural representations},
  EPRINT = {2103.03123v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.IV},
  ABSTRACT = {We propose a new simple approach for image compression: instead of storingthe RGB values for each pixel of an image, we store the weights of a neuralnetwork overfitted to the image. Specifically, to encode an image, we fit itwith an MLP which maps pixel locations to RGB values. We then quantize andstore the weights of this MLP as a code for the image. To decode the image, wesimply evaluate the MLP at every pixel location. We found that this simpleapproach outperforms JPEG at low bit-rates, even without entropy coding orlearning a distribution over weights. While our framework is not yetcompetitive with state of the art compression methods, we show that it hasvarious attractive properties which could make it a viable alternative to otherneural data compression approaches.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.03123v2},
  FILE = {2103.03123v2.pdf}
 }

@article{li2021dynerf,
  AUTHOR = {Tianye Li and Mira Slavcheva and Michael Zollhoefer and Simon Green and Christoph Lassner and Changil Kim and Tanner Schmidt and Steven Lovegrove and Michael Goesele and Zhaoyang Lv},
  TITLE = {Neural 3D Video Synthesis},
  EPRINT = {2103.02597v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose a novel approach for 3D video synthesis that is able to representmulti-view video recordings of a dynamic real-world scene in a compact, yetexpressive representation that enables high-quality view synthesis and motioninterpolation. Our approach takes the high quality and compactness of staticneural radiance fields in a new direction: to a model-free, dynamic setting. Atthe core of our approach is a novel time-conditioned neural radiance fieldsthat represents scene dynamics using a set of compact latent codes. To exploitthe fact that changes between adjacent frames of a video are typically smalland locally consistent, we propose two novel strategies for efficient trainingof our neural network: 1) An efficient hierarchical training scheme, and 2) animportance sampling strategy that selects the next rays for training based onthe temporal variation of the input videos. In combination, these twostrategies significantly boost the training speed, lead to fast convergence ofthe training process, and enable high quality results. Our learnedrepresentation is highly compact and able to represent a 10 second 30 FPSmulti-view video recording by 18 cameras with a model size of just 28MB. Wedemonstrate that our method can render high-fidelity wide-angle novel views atover 1K resolution, even for highly complex and dynamic scenes. We perform anextensive qualitative and quantitative evaluation that shows that our approachoutperforms the current state of the art. We include additional video andinformation at: https://neural-3d-video.github.io/},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.02597v1},
  FILE = {2103.02597v1.pdf}
 }

@article{neff2021donerf,
  AUTHOR = {Thomas Neff and Pascal Stadlbauer and Mathias Parger and Andreas Kurz and Joerg H. Mueller and Chakravarty R. Alla Chaitanya and Anton Kaplanyan and Markus Steinberger},
  TITLE = {DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fieldsusing Depth Oracle Networks},
  EPRINT = {2103.03231v4},
  DOI = {10.1111/cgf.14340},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {The recent research explosion around implicit neural representations, such asNeRF, shows that there is immense potential for implicitly storing high-qualityscene and lighting information in compact neural networks. However, one majorlimitation preventing the use of NeRF in real-time rendering applications isthe prohibitive computational cost of excessive network evaluations along eachview ray, requiring dozens of petaFLOPS. In this work, we bring compact neuralrepresentations closer to practical rendering of synthetic content in real-timeapplications, such as games and virtual reality. We show that the number ofsamples required for each view ray can be significantly reduced when samplesare placed around surfaces in the scene without compromising image quality. Tothis end, we propose a depth oracle network that predicts ray sample locationsfor each view ray with a single network evaluation. We show that using aclassification network around logarithmically discretized and sphericallywarped depth values is essential to encode surface locations rather thandirectly estimating depth. The combination of these techniques leads to DONeRF,our compact dual network design with a depth oracle network as its first stepand a locally sampled shading network for ray accumulation. With DONeRF, wereduce the inference costs by up to 48x compared to NeRF when conditioning onavailable ground truth depth information. Compared to concurrent accelerationmethods for raymarching-based neural representations, DONeRF does not requireadditional memory for explicit caching or acceleration structures, and canrender interactively (20 frames per second) on a single GPU.},
  YEAR = {2021},
  MONTH = {Mar},
  NOTE = {Computer Graphics Forum Volume 40, Issue 4, 2021},
  URL = {http://arxiv.org/abs/2103.03231v4},
  FILE = {2103.03231v4.pdf}
 }

@article{wizadwongsa2021nex,
  AUTHOR = {Suttisak Wizadwongsa and Pakkapon Phongthawee and Jiraphon Yenphraphai and Supasorn Suwajanakorn},
  TITLE = {NeX: Real-time View Synthesis with Neural Basis Expansion},
  EPRINT = {2103.05606v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present NeX, a new approach to novel view synthesis based on enhancementsof multiplane image (MPI) that can reproduce next-level view-dependent effects-- in real time. Unlike traditional MPI that uses a set of simple RGB$\alpha$planes, our technique models view-dependent effects by instead parameterizingeach pixel as a linear combination of basis functions learned from a neuralnetwork. Moreover, we propose a hybrid implicit-explicit modeling strategy thatimproves upon fine detail and produces state-of-the-art results. Our method isevaluated on benchmark forward-facing datasets as well as our newly-introduceddataset designed to test the limit of view-dependent modeling withsignificantly more challenging effects such as rainbow reflections on a CD. Ourmethod achieves the best overall scores across all major metrics on thesedatasets with more than 1000$\times$ faster rendering time than the state ofthe art. For real-time demos, visit https://nex-mpi.github.io/},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.05606v2},
  FILE = {2103.05606v2.pdf}
 }

@article{guo2021adnerf,
  AUTHOR = {Yudong Guo and Keyu Chen and Sen Liang and Yong-Jin Liu and Hujun Bao and Juyong Zhang},
  TITLE = {AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis},
  EPRINT = {2103.11078v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Generating high-fidelity talking head video by fitting with the input audiosequence is a challenging problem that receives considerable attentionsrecently. In this paper, we address this problem with the aid of neural scenerepresentation networks. Our method is completely different from existingmethods that rely on intermediate representations like 2D landmarks or 3D facemodels to bridge the gap between audio input and video output. Specifically,the feature of input audio signal is directly fed into a conditional implicitfunction to generate a dynamic neural radiance field, from which ahigh-fidelity talking-head video corresponding to the audio signal issynthesized using volume rendering. Another advantage of our framework is thatnot only the head (with hair) region is synthesized as previous methods did,but also the upper body is generated via two individual neural radiance fields.Experimental results demonstrate that our novel framework can (1) producehigh-fidelity and natural results, and (2) support free adjustment of audiosignals, viewing directions, and background images. Code is available athttps://github.com/YudongGuo/AD-NeRF.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.11078v3},
  FILE = {2103.11078v3.pdf}
 }

@article{kellnhofer2021nlr,
  AUTHOR = {Petr Kellnhofer and Lars Jebe and Andrew Jones and Ryan Spicer and Kari Pulli and Gordon Wetzstein},
  TITLE = {Neural Lumigraph Rendering},
  EPRINT = {2103.11571v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Novel view synthesis is a challenging and ill-posed inverse renderingproblem. Neural rendering techniques have recently achieved photorealisticimage quality for this task. State-of-the-art (SOTA) neural volume renderingapproaches, however, are slow to train and require minutes of inference (i.e.,rendering) time for high image resolutions. We adopt high-capacity neural scenerepresentations with periodic activations for jointly optimizing an implicitsurface and a radiance field of a scene supervised exclusively with posed 2Dimages. Our neural rendering pipeline accelerates SOTA neural volume renderingby about two orders of magnitude and our implicit surface representation isunique in allowing us to export a mesh with view-dependent texture information.Thus, like other implicit surface representations, ours is compatible withtraditional graphics pipelines, enabling real-time rendering rates, whileachieving unprecedented image quality compared to other surface methods. Weassess the quality of our approach using existing datasets as well ashigh-quality 3D face data captured with a custom multi-camera rig.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.11571v1},
  FILE = {2103.11571v1.pdf}
 }

@article{sucar2021imap,
  AUTHOR = {Edgar Sucar and Shikun Liu and Joseph Ortiz and Andrew J. Davison},
  TITLE = {iMAP: Implicit Mapping and Positioning in Real-Time},
  EPRINT = {2103.12352v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We show for the first time that a multilayer perceptron (MLP) can serve asthe only scene representation in a real-time SLAM system for a handheld RGB-Dcamera. Our network is trained in live operation without prior data, building adense, scene-specific implicit 3D model of occupancy and colour which is alsoimmediately used for tracking.Achieving real-time SLAM via continual training of a neural network against alive image stream requires significant innovation. Our iMAP algorithm uses akeyframe structure and multi-processing computation flow, with dynamicinformation-guided pixel sampling for speed, with tracking at 10 Hz and globalmap updating at 2 Hz. The advantages of an implicit MLP over standard denseSLAM techniques include efficient geometry representation with automatic detailcontrol and smooth, plausible filling-in of unobserved regions such as the backsurfaces of objects.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.12352v1},
  FILE = {2103.12352v1.pdf}
 }

@article{barron2021mipnerf,
  AUTHOR = {Jonathan T. Barron and Ben Mildenhall and Matthew Tancik and Peter Hedman and Ricardo Martin-Brualla and Pratul P. Srinivasan},
  TITLE = {Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural RadianceFields},
  EPRINT = {2103.13415v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {The rendering procedure used by neural radiance fields (NeRF) samples a scenewith a single ray per pixel and may therefore produce renderings that areexcessively blurred or aliased when training or testing images observe scenecontent at different resolutions. The straightforward solution of supersamplingby rendering with multiple rays per pixel is impractical for NeRF, becauserendering each ray requires querying a multilayer perceptron hundreds of times.Our solution, which we call "mip-NeRF" (a la "mipmap"), extends NeRF torepresent the scene at a continuously-valued scale. By efficiently renderinganti-aliased conical frustums instead of rays, mip-NeRF reduces objectionablealiasing artifacts and significantly improves NeRF's ability to represent finedetails, while also being 7% faster than NeRF and half the size. Compared toNeRF, mip-NeRF reduces average error rates by 17% on the dataset presented withNeRF and by 60% on a challenging multiscale variant of that dataset that wepresent. Mip-NeRF is also able to match the accuracy of a brute-forcesupersampled NeRF on our multiscale dataset while being 22x faster.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.13415v3},
  FILE = {2103.13415v3.pdf}
 }

@article{yu2021nerfsh, plenoctrees,
  AUTHOR = {Alex Yu and Ruilong Li and Matthew Tancik and Hao Li and Ren Ng and Angjoo Kanazawa},
  TITLE = {PlenOctrees for Real-time Rendering of Neural Radiance Fields},
  EPRINT = {2103.14024v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce a method to render Neural Radiance Fields (NeRFs) in real timeusing PlenOctrees, an octree-based 3D representation which supportsview-dependent effects. Our method can render 800x800 images at more than 150FPS, which is over 3000 times faster than conventional NeRFs. We do so withoutsacrificing quality while preserving the ability of NeRFs to performfree-viewpoint rendering of scenes with arbitrary geometry and view-dependenteffects. Real-time performance is achieved by pre-tabulating the NeRF into aPlenOctree. In order to preserve view-dependent effects such as specularities,we factorize the appearance via closed-form spherical basis functions.Specifically, we show that it is possible to train NeRFs to predict a sphericalharmonic representation of radiance, removing the viewing direction as an inputto the neural network. Furthermore, we show that PlenOctrees can be directlyoptimized to further minimize the reconstruction loss, which leads to equal orbetter quality compared to competing methods. Moreover, this octreeoptimization step can be used to reduce the training time, as we no longer needto wait for the NeRF training to converge fully. Our real-time neural renderingapproach may potentially enable new applications such as 6-DOF industrial andproduct visualizations, as well as next generation AR/VR systems. PlenOctreesare amenable to in-browser rendering as well; please visit the project page forthe interactive online demo, as well as video and code:https://alexyu.net/plenoctrees},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.14024v2},
  FILE = {2103.14024v2.pdf}
 }

@article{reiser2021kilonerf,
  AUTHOR = {Christian Reiser and Songyou Peng and Yiyi Liao and Andreas Geiger},
  TITLE = {KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs},
  EPRINT = {2103.13744v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {NeRF synthesizes novel views of a scene with unprecedented quality by fittinga neural radiance field to RGB images. However, NeRF requires querying a deepMulti-Layer Perceptron (MLP) millions of times, leading to slow renderingtimes, even on modern GPUs. In this paper, we demonstrate that real-timerendering is possible by utilizing thousands of tiny MLPs instead of one singlelarge MLP. In our setting, each individual MLP only needs to represent parts ofthe scene, thus smaller and faster-to-evaluate MLPs can be used. By combiningthis divide-and-conquer strategy with further optimizations, rendering isaccelerated by three orders of magnitude compared to the original NeRF modelwithout incurring high storage costs. Further, using teacher-studentdistillation for training, we show that this speed-up can be achieved withoutsacrificing visual quality.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.13744v2},
  FILE = {2103.13744v2.pdf}
 }

@article{hedman2021snerg,
  AUTHOR = {Peter Hedman and Pratul P. Srinivasan and Ben Mildenhall and Jonathan T. Barron and Paul Debevec},
  TITLE = {Baking Neural Radiance Fields for Real-Time View Synthesis},
  EPRINT = {2103.14645v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural volumetric representations such as Neural Radiance Fields (NeRF) haveemerged as a compelling technique for learning to represent 3D scenes fromimages with the goal of rendering photorealistic images of the scene fromunobserved viewpoints. However, NeRF's computational requirements areprohibitive for real-time applications: rendering views from a trained NeRFrequires querying a multilayer perceptron (MLP) hundreds of times per ray. Wepresent a method to train a NeRF, then precompute and store (i.e. "bake") it asa novel representation called a Sparse Neural Radiance Grid (SNeRG) thatenables real-time rendering on commodity hardware. To achieve this, weintroduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel gridrepresentation with learned feature vectors. The resulting scene representationretains NeRF's ability to render fine geometric details and view-dependentappearance, is compact (averaging less than 90 MB per scene), and can berendered in real-time (higher than 30 frames per second on a laptop GPU).Actual screen captures are shown in our video.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.14645v1},
  FILE = {2103.14645v1.pdf}
 }

@article{li2021mine,
  AUTHOR = {Jiaxin Li and Zijian Feng and Qi She and Henghui Ding and Changhu Wang and Gim Hee Lee},
  TITLE = {MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis},
  EPRINT = {2103.14910v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper, we propose MINE to perform novel view synthesis and depthestimation via dense 3D reconstruction from a single image. Our approach is acontinuous depth generalization of the Multiplane Images (MPI) by introducingthe NEural radiance fields (NeRF). Given a single image as input, MINE predictsa 4-channel image (RGB and volume density) at arbitrary depth values to jointlyreconstruct the camera frustum and fill in occluded contents. The reconstructedand inpainted frustum can then be easily rendered into novel RGB or depth viewsusing differentiable rendering. Extensive experiments on RealEstate10K, KITTIand Flowers Light Fields show that our MINE outperforms state-of-the-art by alarge margin in novel view synthesis. We also achieve competitive results indepth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Oursource code is available at https://github.com/vincentfung13/MINE},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.14910v3},
  FILE = {2103.14910v3.pdf}
 }

@article{zhi2021semanticnerf,
  AUTHOR = {Shuaifeng Zhi and Tristan Laidlow and Stefan Leutenegger and Andrew J. Davison},
  TITLE = {In-Place Scene Labelling and Understanding with Implicit SceneRepresentation},
  EPRINT = {2103.15875v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Semantic labelling is highly correlated with geometry and radiancereconstruction, as scene entities with similar shape and appearance are morelikely to come from similar classes. Recent implicit neural reconstructiontechniques are appealing as they do not require prior training data, but thesame fully self-supervised approach is not possible for semantics becauselabels are human-defined properties.We extend neural radiance fields (NeRF) to jointly encode semantics withappearance and geometry, so that complete and accurate 2D semantic labels canbe achieved using a small amount of in-place annotations specific to the scene.The intrinsic multi-view consistency and smoothness of NeRF benefit semanticsby enabling sparse labels to efficiently propagate. We show the benefit of thisapproach when labels are either sparse or very noisy in room-scale scenes. Wedemonstrate its advantageous properties in various interesting applicationssuch as an efficient scene labelling tool, novel semantic view synthesis, labeldenoising, super-resolution, label interpolation and multi-view semantic labelfusion in visual semantic mapping systems.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.15875v2},
  FILE = {2103.15875v2.pdf}
 }

@article{chen2021nvsnerf,
  AUTHOR = {Anpei Chen and Zexiang Xu and Fuqiang Zhao and Xiaoshuai Zhang and Fanbo Xiang and Jingyi Yu and Hao Su},
  TITLE = {MVSNeRF: Fast Generalizable Radiance Field Reconstruction fromMulti-View Stereo},
  EPRINT = {2103.15595v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present MVSNeRF, a novel neural rendering approach that can efficientlyreconstruct neural radiance fields for view synthesis. Unlike prior works onneural radiance fields that consider per-scene optimization on densely capturedimages, we propose a generic deep neural network that can reconstruct radiancefields from only three nearby input views via fast network inference. Ourapproach leverages plane-swept cost volumes (widely used in multi-view stereo)for geometry-aware scene reasoning, and combines this with physically basedvolume rendering for neural radiance field reconstruction. We train our networkon real objects in the DTU dataset, and test it on three different datasets toevaluate its effectiveness and generalizability. Our approach can generalizeacross scenes (even indoor scenes, completely different from our trainingscenes of objects) and generate realistic view synthesis results using onlythree input images, significantly outperforming concurrent works ongeneralizable radiance field reconstruction. Moreover, if dense images arecaptured, our estimated radiance field representation can be easily fine-tuned;this leads to fast per-scene reconstruction with higher rendering quality andsubstantially less optimization time than NeRF.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.15595v2},
  FILE = {2103.15595v2.pdf}
 }

@article{meng2021gnerf,
  AUTHOR = {Quan Meng and Anpei Chen and Haimin Luo and Minye Wu and Hao Su and Lan Xu and Xuming He and Jingyi Yu},
  TITLE = {GNeRF: GAN-based Neural Radiance Field without Posed Camera},
  EPRINT = {2103.15606v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce GNeRF, a framework to marry Generative Adversarial Networks(GAN) with Neural Radiance Field (NeRF) reconstruction for the complexscenarios with unknown and even randomly initialized camera poses. RecentNeRF-based advances have gained popularity for remarkable realistic novel viewsynthesis. However, most of them heavily rely on accurate camera posesestimation, while few recent methods can only optimize the unknown camera posesin roughly forward-facing scenes with relatively short camera trajectories andrequire rough camera poses initialization. Differently, our GNeRF only utilizesrandomly initialized poses for complex outside-in scenarios. We propose a noveltwo-phases end-to-end framework. The first phase takes the use of GANs into thenew realm for optimizing coarse camera poses and radiance fields jointly, whilethe second phase refines them with additional photometric loss. We overcomelocal minima using a hybrid and iterative optimization scheme. Extensiveexperiments on a variety of synthetic and natural scenes demonstrate theeffectiveness of GNeRF. More impressively, our approach outperforms thebaselines favorably in those scenes with repeated patterns or even low texturesthat are regarded as extremely challenging before.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.15606v3},
  FILE = {2103.15606v3.pdf}
 }

@article{henzler2021unsupervised,
  AUTHOR = {Philipp Henzler and Jeremy Reizenstein and Patrick Labatut and Roman Shapovalov and Tobias Ritschel and Andrea Vedaldi and David Novotny},
  TITLE = {Unsupervised Learning of 3D Object Categories from Videos in the Wild},
  EPRINT = {2103.16552v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Our goal is to learn a deep network that, given a small number of images ofan object of a given category, reconstructs it in 3D. While several recentworks have obtained analogous results using synthetic data or assuming theavailability of 2D primitives such as keypoints, we are interested in workingwith challenging real data and with no manual annotations. We thus focus onlearning a model from multiple views of a large collection of object instances.We contribute with a new large dataset of object centric videos suitable fortraining and benchmarking this class of models. We show that existingtechniques leveraging meshes, voxels, or implicit surfaces, which work well forreconstructing isolated objects, fail on this challenging data. Finally, wepropose a new neural network design, called warp-conditioned ray embedding(WCR), which significantly improves reconstruction while obtaining a detailedimplicit representation of the object surface and texture, also compensatingfor the noise in the initial SfM reconstruction that bootstrapped the learningprocess. Our evaluation demonstrates performance improvements over several deepmonocular reconstruction baselines on existing benchmarks and on our noveldataset.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.16552v1},
  FILE = {2103.16552v1.pdf}
 }

@article{deng2021foveated,
  AUTHOR = {Nianchen Deng and Zhenyi He and Jiannan Ye and Praneeth Chakravarthula and Xubo Yang and Qi Sun},
  TITLE = {Foveated Neural Radiance Fields for Real-Time and Egocentric VirtualReality},
  EPRINT = {2103.16365v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {Traditional high-quality 3D graphics requires large volumes of fine-detailedscene data for rendering. This demand compromises computational efficiency andlocal storage resources. Specifically, it becomes more concerning for futurewearable and portable virtual and augmented reality (VR/AR) displays. Recentapproaches to combat this problem include remote rendering/streaming and neuralrepresentations of 3D assets. These approaches have redefined the traditionallocal storage-rendering pipeline by distributed computing or compression oflarge data. However, these methods typically suffer from high latency or lowquality for practical visualization of large immersive virtual scenes, notablywith extra high resolution and refresh rate requirements for VR applicationssuch as gaming and design.Tailored for the future portable, low-storage, and energy-efficient VRplatforms, we present the first gaze-contingent 3D neural representation andview synthesis method. We incorporate the human psychophysics of visual- andstereo-acuity into an egocentric neural representation of 3D scenery.Furthermore, we jointly optimize the latency/performance and visual quality,while mutually bridging human perception and neural scene synthesis, to achieveperceptually high-quality immersive interaction. Both objective analysis andsubjective study demonstrate the effectiveness of our approach in significantlyreducing local storage volume and synthesis latency (up to 99% reduction inboth data size and computational time), while simultaneously presentinghigh-fidelity rendering, with perceptual quality identical to that of fullylocally stored and rendered high-quality imagery.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.16365v1},
  FILE = {2103.16365v1.pdf}
 }

@article{niemeyer2021campari,
  AUTHOR = {Michael Niemeyer and Andreas Geiger},
  TITLE = {CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields},
  EPRINT = {2103.17269v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Tremendous progress in deep generative models has led to photorealistic imagesynthesis. While achieving compelling results, most approaches operate in thetwo-dimensional image domain, ignoring the three-dimensional nature of ourworld. Several recent works therefore propose generative models which are3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably tothe image plane. This leads to impressive 3D consistency, but incorporatingsuch a bias comes at a price: the camera needs to be modeled as well. Currentapproaches assume fixed intrinsics and a predefined prior over camera poseranges. As a result, parameter tuning is typically required for real-worlddata, and results degrade if the data distribution is not matched. Our keyhypothesis is that learning a camera generator jointly with the image generatorleads to a more principled approach to 3D-aware image synthesis. Further, wepropose to decompose the scene into a background and foreground model, leadingto more efficient and disentangled scene representations. While training fromraw, unposed image collections, we learn a 3D- and camera-aware generativemodel which faithfully recovers not only the image but also the camera datadistribution. At test time, our model generates images with explicit controlover the camera as well as the shape and appearance of the scene.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.17269v1},
  FILE = {2103.17269v1.pdf}
 }

@article{palafox2021npms,
  AUTHOR = {Pablo Palafox and Aljaz Bozic and Justus Thies and Matthias Niessner and Angela Dai},
  TITLE = {NPMs: Neural Parametric Models for 3D Deformable Shapes},
  EPRINT = {2104.00702v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Parametric 3D models have enabled a wide variety of tasks in computergraphics and vision, such as modeling human bodies, faces, and hands. However,the construction of these parametric models is often tedious, as it requiresheavy manual tweaking, and they struggle to represent additional complexity anddetails such as wrinkles or clothing. To this end, we propose Neural ParametricModels (NPMs), a novel, learned alternative to traditional, parametric 3Dmodels, which does not require hand-crafted, object-specific constraints. Inparticular, we learn to disentangle 4D dynamics into latent-spacerepresentations of shape and pose, leveraging the flexibility of recentdevelopments in learned implicit functions. Crucially, once learned, our neuralparametric models of shape and pose enable optimization over the learned spacesto fit to new observations, similar to the fitting of a traditional parametricmodel, e.g., SMPL. This enables NPMs to achieve a significantly more accurateand detailed representation of observed deformable sequences. We show that NPMsimprove notably over both parametric and non-parametric state of the art inreconstruction and tracking of monocular depth sequences of clothed humans andhands. Latent-space interpolation as well as shape/pose transfer experimentsfurther demonstrate the usefulness of NPMs. Code is publicly available athttps://pablopalafox.github.io/npms.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.00702v2},
  FILE = {2104.00702v2.pdf}
 }

@article{zhu2021rgbd,
  AUTHOR = {Luyang Zhu and Arsalan Mousavian and Yu Xiang and Hammad Mazhar and Jozef van Eenbergen and Shoubhik Debnath and Dieter Fox},
  TITLE = {RGB-D Local Implicit Function for Depth Completion of TransparentObjects},
  EPRINT = {2104.00622v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Majority of the perception methods in robotics require depth informationprovided by RGB-D cameras. However, standard 3D sensors fail to capture depthof transparent objects due to refraction and absorption of light. In thispaper, we introduce a new approach for depth completion of transparent objectsfrom a single RGB-D image. Key to our approach is a local implicit neuralrepresentation built on ray-voxel pairs that allows our method to generalize tounseen objects and achieve fast inference speed. Based on this representation,we present a novel framework that can complete missing depth given noisy RGB-Dinput. We further improve the depth estimation iteratively using aself-correcting refinement model. To train the whole pipeline, we build a largescale synthetic dataset with transparent objects. Experiments demonstrate thatour method performs significantly better than the current state-of-the-artmethods on both synthetic and real world data. In addition, our approachimproves the inference speed by a factor of 20 compared to the previous bestmethod, ClearGrasp. Code and dataset will be released athttps://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.00622v1},
  FILE = {2104.00622v1.pdf}
 }

@article{zhang2021physg,
  AUTHOR = {Kai Zhang and Fujun Luan and Qianqian Wang and Kavita Bala and Noah Snavely},
  TITLE = {PhySG: Inverse Rendering with Spherical Gaussians for Physics-basedMaterial Editing and Relighting},
  EPRINT = {2104.00674v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present PhySG, an end-to-end inverse rendering pipeline that includes afully differentiable renderer and can reconstruct geometry, materials, andillumination from scratch from a set of RGB input images. Our frameworkrepresents specular BRDFs and environmental illumination using mixtures ofspherical Gaussians, and represents geometry as a signed distance functionparameterized as a Multi-Layer Perceptron. The use of spherical Gaussiansallows us to efficiently solve for approximate light transport, and our methodworks on scenes with challenging non-Lambertian reflectance captured undernatural, static illumination. We demonstrate, with both synthetic and realdata, that our reconstructions not only enable rendering of novel viewpoints,but also physics-based appearance editing of materials and illumination.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.00674v1},
  FILE = {2104.00674v1.pdf}
 }

@article{kosiorek2021nerfvae,
  AUTHOR = {Adam R. Kosiorek and Heiko Strathmann and Daniel Zoran and Pol Moreno and Rosalia Schneider and Sona Mokra and Danilo J. Rezende},
  TITLE = {NeRF-VAE: A Geometry Aware 3D Scene Generative Model},
  EPRINT = {2104.00587v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {stat.ML},
  ABSTRACT = {We propose NeRF-VAE, a 3D scene generative model that incorporates geometricstructure via NeRF and differentiable volume rendering. In contrast to NeRF,our model takes into account shared structure across scenes, and is able toinfer the structure of a novel scene -- without the need to re-train -- usingamortized inference. NeRF-VAE's explicit 3D rendering process further contrastsprevious generative models with convolution-based rendering which lacksgeometric structure. Our model is a VAE that learns a distribution overradiance fields by conditioning them on a latent scene representation. We showthat, once trained, NeRF-VAE is able to infer and rendergeometrically-consistent scenes from previously unseen 3D environments usingvery few input images. We further demonstrate that NeRF-VAE generalizes well toout-of-distribution cameras, while convolutional models do not. Finally, weintroduce and study an attention-based conditioning mechanism of NeRF-VAE'sdecoder, which improves model performance.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.00587v1},
  FILE = {2104.00587v1.pdf}
 }

@article{devries2021unconstrained,
  AUTHOR = {Terrance DeVries and Miguel Angel Bautista and Nitish Srivastava and Graham W. Taylor and Joshua M. Susskind},
  TITLE = {Unconstrained Scene Generation with Locally Conditioned Radiance Fields},
  EPRINT = {2104.00670v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We tackle the challenge of learning a distribution over complex, realistic,indoor scenes. In this paper, we introduce Generative Scene Networks (GSN),which learns to decompose scenes into a collection of many local radiancefields that can be rendered from a free moving camera. Our model can be used asa prior to generate new scenes, or to complete a scene given only sparse 2Dobservations. Recent work has shown that generative models of radiance fieldscan capture properties such as multi-view consistency and view-dependentlighting. However, these models are specialized for constrained viewing ofsingle objects, such as cars or faces. Due to the size and complexity ofrealistic indoor environments, existing models lack the representationalcapacity to adequately capture them. Our decomposition scheme scales to largerand more complex scenes while preserving details and diversity, and the learnedprior enables high-quality rendering from viewpoints that are significantlydifferent from observed viewpoints. When compared to existing models, GSNproduces quantitatively higher-quality scene renderings across severaldifferent scene datasets.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.00670v1},
  FILE = {2104.00670v1.pdf}
 }

@article{jain2021dietnerf,
  AUTHOR = {Ajay Jain and Matthew Tancik and Pieter Abbeel},
  TITLE = {Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis},
  EPRINT = {2104.00677v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present DietNeRF, a 3D neural scene representation estimated from a fewimages. Neural Radiance Fields (NeRF) learn a continuous volumetricrepresentation of a scene through multi-view consistency, and can be renderedfrom novel viewpoints by ray casting. While NeRF has an impressive ability toreconstruct geometry and fine details given many images, up to 100 forchallenging 360{\deg} scenes, it often finds a degenerate solution to its imagereconstruction objective when only a few input views are available. To improvefew-shot quality, we propose DietNeRF. We introduce an auxiliary semanticconsistency loss that encourages realistic renderings at novel poses. DietNeRFis trained on individual scenes to (1) correctly render given input views fromthe same pose, and (2) match high-level semantic attributes across different,random poses. Our semantic loss allows us to supervise DietNeRF from arbitraryposes. We extract these semantics using a pre-trained visual encoder such asCLIP, a Vision Transformer trained on hundreds of millions of diversesingle-view, 2D photographs mined from the web with natural languagesupervision. In experiments, DietNeRF improves the perceptual quality offew-shot view synthesis when learned from scratch, can render novel views withas few as one observed image when pre-trained on a multi-view dataset, andproduces plausible completions of completely unobserved regions.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.00677v1},
  FILE = {2104.00677v1.pdf}
 }

@article{Fu_2021,
  DOI = {10.1088/1742-6596/1880/1/012034},
  URL = {https://doi.org/10.1088/1742-6596/1880/1/012034},
  YEAR = {apr},
  PUBLISHER = {{IOP} Publishing},
  VOLUME = {1880},
  NUMBER = {1},
  PAGES = {012034},
  AUTHOR = {Bofeng Fu and Zheng Wang},
  TITLE = {Multi-scene Representation Learning with Neural Radiance Fields},
  JOURNAL = {Journal of Physics: Conference Series},
  ABSTRACT = {Getting representations of multiple objects or scenes is a raising research topic in Machine Learning (ML) community. Here, we propose a multi-scene representation model that can learn the representation of complex scenes and reconstruct them in high resolution given novel viewing directions. Our method represents a single scene with fully-connected layers. Each set of fully-connected layers are controlled by hyper-networks for multiple scenes modeling. For each scene, we take 3D coordinates (x, y, z) and 2D view-point orientations (I,, E,) as inputs. A set of fully-connected layers output volume density and RGB values at given 3D spatial positions. Then, we render the output volume density and RGB values along the camera rays into images using volume density rendering techniques. During training process, we optimize a continuous volume scene function with a small amount of input viewing directions. By designing versatile embedding module and multi-scene representation networks, our model can render photographic images with novel viewing directions for different complex scenes. Experiment results demonstrate the neural rendering and multi-scene representation abilities of our model. Several thorough experiments show that our method outperforms previous model on both reconstruction precision and scenes generation ability from novel viewing directions.}
 }

@article{stelzner2021obsurf,
  AUTHOR = {Karl Stelzner and Kristian Kersting and Adam R. Kosiorek},
  TITLE = {Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation},
  EPRINT = {2104.01148v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present ObSuRF, a method which turns a single image of a scene into a 3Dmodel represented as a set of Neural Radiance Fields (NeRFs), with each NeRFcorresponding to a different object. A single forward pass of an encodernetwork outputs a set of latent vectors describing the objects in the scene.These vectors are used independently to condition a NeRF decoder, defining thegeometry and appearance of each object. We make learning more computationallyefficient by deriving a novel loss, which allows training NeRFs on RGB-D inputswithout explicit ray marching. After confirming that the model performs equalor better than state of the art on three 2D image segmentation benchmarks, weapply it to two multi-object 3D datasets: A multiview version of CLEVR, and anovel dataset in which scenes are populated by ShapeNet models. We find thatafter training ObSuRF on RGB-D views of training scenes, it is capable of notonly recovering the 3D geometry of a scene depicted in a single input image,but also to segment it into objects, despite receiving no supervision in thatregard.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.01148v1},
  FILE = {2104.01148v1.pdf}
 }

@article{jiang2021giga,
  AUTHOR = {Zhenyu Jiang and Yifeng Zhu and Maxwell Svetlik and Kuan Fang and Yuke Zhu},
  TITLE = {Synergies Between Affordance and Geometry: 6-DoF Grasp Detection viaImplicit Representations},
  EPRINT = {2104.01542v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.RO},
  ABSTRACT = {Grasp detection in clutter requires the robot to reason about the 3D scenefrom incomplete and noisy perception. In this work, we draw insight that 3Dreconstruction and grasp learning are two intimately connected tasks, both ofwhich require a fine-grained understanding of local geometry details. We thuspropose to utilize the synergies between grasp affordance and 3D reconstructionthrough multi-task learning of a shared representation. Our model takesadvantage of deep implicit functions, a continuous and memory-efficientrepresentation, to enable differentiable training of both tasks. We train themodel on self-supervised grasp trials data in simulation. Evaluation isconducted on a clutter removal task, where the robot clears cluttered objectsby grasping them one at a time. The experimental results in simulation and onthe real robot have demonstrated that the use of implicit neuralrepresentations and joint learning of grasp affordance and 3D reconstructionhave led to state-of-the-art grasping results. Our method outperforms baselinesby over 10% in terms of grasp success rate. Additional results and videos canbe found at https://sites.google.com/view/rpl-giga2021},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.01542v2},
  FILE = {2104.01542v2.pdf}
 }

@article{chan2020pigan,
  AUTHOR = {Eric R. Chan and Marco Monteiro and Petr Kellnhofer and Jiajun Wu and Gordon Wetzstein},
  TITLE = {pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-AwareImage Synthesis},
  EPRINT = {2012.00926v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We have witnessed rapid progress on 3D-aware image synthesis, leveragingrecent advances in generative visual models and neural rendering. Existingapproaches however fall short in two ways: first, they may lack an underlying3D representation or rely on view-inconsistent rendering, hence synthesizingimages that are not multi-view consistent; second, they often depend uponrepresentation network architectures that are not expressive enough, and theirresults thus lack in image quality. We propose a novel generative model, namedPeriodic Implicit Generative Adversarial Networks ($\pi$-GAN or pi-GAN), forhigh-quality 3D-aware image synthesis. $\pi$-GAN leverages neuralrepresentations with periodic activation functions and volumetric rendering torepresent scenes as view-consistent 3D representations with fine detail. Theproposed approach obtains state-of-the-art results for 3D-aware image synthesiswith multiple real and synthetic datasets.},
  YEAR = {2020},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/2012.00926v2},
  FILE = {2012.00926v2.pdf}
 }

@article{luo2021convolutional,
  AUTHOR = {Haimin Luo and Anpei Chen and Qixuan Zhang and Bai Pang and Minye Wu and Lan Xu and Jingyi Yu},
  TITLE = {Convolutional Neural Opacity Radiance Fields},
  EPRINT = {2104.01772v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Photo-realistic modeling and rendering of fuzzy objects with complex opacityare critical for numerous immersive VR/AR applications, but it suffers fromstrong view-dependent brightness, color. In this paper, we propose a novelscheme to generate opacity radiance fields with a convolutional neural rendererfor fuzzy objects, which is the first to combine both explicit opacitysupervision and convolutional mechanism into the neural radiance fieldframework so as to enable high-quality appearance and global consistent alphamattes generation in arbitrary novel views. More specifically, we propose anefficient sampling strategy along with both the camera rays and image plane,which enables efficient radiance field sampling and learning in a patch-wisemanner, as well as a novel volumetric feature integration scheme that generatesper-patch hybrid feature embeddings to reconstruct the view-consistentfine-detailed appearance and opacity output. We further adopt a patch-wiseadversarial training scheme to preserve both high-frequency appearance andopacity details in a self-supervised framework. We also introduce an effectivemulti-view image capture system to capture high-quality color and alpha mapsfor challenging fuzzy objects. Extensive experiments on existing and our newchallenging fuzzy object dataset demonstrate that our method achievesphoto-realistic, globally consistent, and fined detailed appearance and opacityfree-viewpoint rendering for various fuzzy objects.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.01772v1},
  FILE = {2104.01772v1.pdf}
 }

@article{wang2021mirrornerf,
  AUTHOR = {Ziyu Wang and Liao Wang and Fuqiang Zhao and Minye Wu and Lan Xu and Jingyi Yu},
  TITLE = {MirrorNeRF: One-shot Neural Portrait Radiance Field from Multi-mirrorCatadioptric Imaging},
  EPRINT = {2104.02607v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Photo-realistic neural reconstruction and rendering of the human portrait arecritical for numerous VR/AR applications. Still, existing solutions inherentlyrely on multi-view capture settings, and the one-shot solution to get rid ofthe tedious multi-view synchronization and calibration remains extremelychallenging. In this paper, we propose MirrorNeRF - a one-shot neural portraitfree-viewpoint rendering approach using a catadioptric imaging system withmultiple sphere mirrors and a single high-resolution digital camera, which isthe first to combine neural radiance field with catadioptric imaging so as toenable one-shot photo-realistic human portrait reconstruction and rendering, ina low-cost and casual capture setting. More specifically, we propose alight-weight catadioptric system design with a sphere mirror array to enablediverse ray sampling in the continuous 3D space as well as an effective onlinecalibration for the camera and the mirror array. Our catadioptric imagingsystem can be easily deployed with a low budget and the casual capture abilityfor convenient daily usages. We introduce a novel neural warping radiance fieldrepresentation to learn a continuous displacement field that implicitlycompensates for the misalignment due to our flexible system setting. We furtherpropose a density regularization scheme to leverage the inherent geometryinformation from the catadioptric data in a self-supervision manner, which notonly improves the training efficiency but also provides more effective densitysupervision for higher rendering quality. Extensive experiments demonstrate theeffectiveness and robustness of our scheme to achieve one-shot photo-realisticand high-quality appearance free-viewpoint rendering for human portrait scenes.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.02607v2},
  FILE = {2104.02607v2.pdf}
 }

@article{noguchi2021narf,
  AUTHOR = {Atsuhiro Noguchi and Xiao Sun and Stephen Lin and Tatsuya Harada},
  TITLE = {Neural Articulated Radiance Field},
  EPRINT = {2104.03110v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Neural Articulated Radiance Field (NARF), a novel deformable 3Drepresentation for articulated objects learned from images. While recentadvances in 3D implicit representation have made it possible to learn models ofcomplex objects, learning pose-controllable representations of articulatedobjects remains a challenge, as current methods require 3D shape supervisionand are unable to render appearance. In formulating an implicit representationof 3D articulated objects, our method considers only the rigid transformationof the most relevant object part in solving for the radiance field at each 3Dlocation. In this way, the proposed method represents pose-dependent changeswithout significantly increasing the computational complexity. NARF is fullydifferentiable and can be trained from images with pose annotations. Moreover,through the use of an autoencoder, it can learn appearance variations overmultiple instances of an object class. Experiments show that the proposedmethod is efficient and can generalize well to novel poses. The code isavailable for research purposes at https://github.com/nogu-atsu/NARF},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.03110v2},
  FILE = {2104.03110v2.pdf}
 }

@article{saito2021scanimate,
  AUTHOR = {Shunsuke Saito and Jinlong Yang and Qianli Ma and Michael J. Black},
  TITLE = {SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks},
  EPRINT = {2104.03313v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present SCANimate, an end-to-end trainable framework that takes raw 3Dscans of a clothed human and turns them into an animatable avatar. Theseavatars are driven by pose parameters and have realistic clothing that movesand deforms naturally. SCANimate does not rely on a customized mesh template orsurface mesh registration. We observe that fitting a parametric 3D body model,like SMPL, to a clothed human scan is tractable while surface registration ofthe body topology to the scan is often not, because clothing can deviatesignificantly from the body shape. We also observe that articulatedtransformations are invertible, resulting in geometric cycle consistency in theposed and unposed shapes. These observations lead us to a weakly supervisedlearning method that aligns scans into a canonical pose by disentanglingarticulated deformations without template-based surface registration.Furthermore, to complete missing regions in the aligned scans while modelingpose-dependent deformations, we introduce a locally pose-aware implicitfunction that learns to complete and model geometry with learned posecorrectives. In contrast to commonly used global pose embeddings, our localpose conditioning significantly reduces long-range spurious correlations andimproves generalization to unseen poses, especially when training data islimited. Our method can be applied to pose-aware appearance modeling togenerate a fully textured avatar. We demonstrate our approach on variousclothing types with different amounts of training data, outperforming existingsolutions and other variants in terms of fidelity and generality in everysetting. The code is available at https://scanimate.is.tue.mpg.de.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.03313v2},
  FILE = {2104.03313v2.pdf}
 }

@article{chen2021directposenet,
  AUTHOR = {Shuai Chen and Zirui Wang and Victor Prisacariu},
  TITLE = {Direct-PoseNet: Absolute Pose Regression with Photometric Consistency},
  EPRINT = {2104.04073v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a relocalization pipeline, which combines an absolute poseregression (APR) network with a novel view synthesis based direct matchingmodule, offering superior accuracy while maintaining low inference time. Ourcontribution is twofold: i) we design a direct matching module that supplies aphotometric supervision signal to refine the pose regression network viadifferentiable rendering; ii) we modify the rotation representation from theclassical quaternion to SO(3) in pose regression, removing the need forbalancing rotation and translation loss terms. As a result, our networkDirect-PoseNet achieves state-of-the-art performance among all othersingle-image APR methods on the 7-Scenes benchmark and the LLFF dataset.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.04073v1},
  FILE = {2104.04073v1.pdf}
 }

@article{chen2021snarf,
  AUTHOR = {Xu Chen and Yufeng Zheng and Michael J. Black and Otmar Hilliges and Andreas Geiger},
  TITLE = {SNARF: Differentiable Forward Skinning for Animating Non-Rigid NeuralImplicit Shapes},
  EPRINT = {2104.03953v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural implicit surface representations have emerged as a promising paradigmto capture 3D shapes in a continuous and resolution-independent manner.However, adapting them to articulated shapes is non-trivial. Existingapproaches learn a backward warp field that maps deformed to canonical points.However, this is problematic since the backward warp field is pose dependentand thus requires large amounts of data to learn. To address this, we introduceSNARF, which combines the advantages of linear blend skinning (LBS) forpolygonal meshes with those of neural implicit surfaces by learning a forwarddeformation field without direct supervision. This deformation field is definedin canonical, pose-independent space, allowing for generalization to unseenposes. Learning the deformation field from posed meshes alone is challengingsince the correspondences of deformed points are defined implicitly and may notbe unique under changes of topology. We propose a forward skinning model thatfinds all canonical correspondences of any deformed point using iterative rootfinding. We derive analytical gradients via implicit differentiation, enablingend-to-end training from 3D meshes with bone transformations. Compared tostate-of-the-art neural implicit representations, our approach generalizesbetter to unseen poses while preserving accuracy. We demonstrate our method inchallenging scenarios on (clothed) 3D humans in diverse and unseen poses.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.03953v1},
  FILE = {2104.03953v1.pdf}
 }

@article{mehta2021modulated,
  AUTHOR = {Ishit Mehta and Michael Gharbi and Connelly Barnes and Eli Shechtman and Ravi Ramamoorthi and Manmohan Chandraker},
  TITLE = {Modulated Periodic Activations for Generalizable Local FunctionalRepresentations},
  EPRINT = {2104.03960v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Multi-Layer Perceptrons (MLPs) make powerful functional representations forsampling and reconstruction problems involving low-dimensional signals likeimages,shapes and light fields. Recent works have significantly improved theirability to represent high-frequency content by using periodic activations orpositional encodings. This often came at the expense of generalization: modernmethods are typically optimized for a single signal. We present a newrepresentation that generalizes to multiple instances and achievesstate-of-the-art fidelity. We use a dual-MLP architecture to encode thesignals. A synthesis network creates a functional mapping from alow-dimensional input (e.g. pixel-position) to the output domain (e.g. RGBcolor). A modulation network maps a latent code corresponding to the targetsignal to parameters that modulate the periodic activations of the synthesisnetwork. We also propose a local-functional representation which enablesgeneralization. The signal's domain is partitioned into a regular grid,witheach tile represented by a latent code. At test time, the signal is encodedwith high-fidelity by inferring (or directly optimizing) the latent code-book.Our approach produces generalizable functional representations of images,videos and shapes, and achieves higher reconstruction quality than prior worksthat are optimized for a single signal.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.03960v1},
  FILE = {2104.03960v1.pdf}
 }

@article{azinovic2021neural,
  AUTHOR = {Dejan Azinovic and Ricardo Martin-Brualla and Dan B Goldman and Matthias Niessner and Justus Thies},
  TITLE = {Neural RGB-D Surface Reconstruction},
  EPRINT = {2104.04532v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this work, we explore how to leverage the success of implicit novel viewsynthesis methods for surface reconstruction. Methods which learn a neuralradiance field have shown amazing image synthesis results, but the underlyinggeometry representation is only a coarse approximation of the real geometry. Wedemonstrate how depth measurements can be incorporated into the radiance fieldformulation to produce more detailed and complete reconstruction results thanusing methods based on either color or depth data alone. In contrast to adensity field as the underlying geometry representation, we propose to learn adeep neural network which stores a truncated signed distance field. Using thisrepresentation, we show that one can still leverage differentiable volumerendering to estimate color values of the observed images during training tocompute a reconstruction loss. This is beneficial for learning the signeddistance field in regions with missing depth measurements. Furthermore, wecorrect misalignment errors of the camera, improving the overall reconstructionquality. In several experiments, we showcase our method and compare to existingworks on classical RGB-D fusion and learned representations.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.04532v1},
  FILE = {2104.04532v1.pdf}
 }

@article{lu2021compressive,
  AUTHOR = {Yuzhe Lu and Kairong Jiang and Joshua A. Levine and Matthew Berger},
  TITLE = {Compressive Neural Representations of Volumetric Scalar Fields},
  EPRINT = {2104.04523v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {We present an approach for compressing volumetric scalar fields usingimplicit neural representations. Our approach represents a scalar field as alearned function, wherein a neural network maps a point in the domain to anoutput scalar value. By setting the number of weights of the neural network tobe smaller than the input size, we achieve compressed representations of scalarfields, thus framing compression as a type of function approximation. Combinedwith carefully quantizing network weights, we show that this approach yieldshighly compact representations that outperform state-of-the-art volumecompression approaches. The conceptual simplicity of our approach enables anumber of benefits, such as support for time-varying scalar fields, optimizingto preserve spatial gradients, and random-access field evaluation. We study theimpact of network design choices on compression performance, highlighting howsimple network architectures are effective for a broad range of volumes.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.04523v1},
  FILE = {2104.04523v1.pdf}
 }

@article{lin2021barf,
  AUTHOR = {Chen-Hsuan Lin and Wei-Chiu Ma and Antonio Torralba and Simon Lucey},
  TITLE = {BARF: Bundle-Adjusting Neural Radiance Fields},
  EPRINT = {2104.06405v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural Radiance Fields (NeRF) have recently gained a surge of interest withinthe computer vision community for its power to synthesize photorealistic novelviews of real-world scenes. One limitation of NeRF, however, is its requirementof accurate camera poses to learn the scene representations. In this paper, wepropose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF fromimperfect (or even unknown) camera poses -- the joint problem of learningneural 3D representations and registering camera frames. We establish atheoretical connection to classical image alignment and show thatcoarse-to-fine registration is also applicable to NeRF. Furthermore, we showthat na\"ively applying positional encoding in NeRF has a negative impact onregistration with a synthesis-based objective. Experiments on synthetic andreal-world data show that BARF can effectively optimize the neural scenerepresentations and resolve large camera pose misalignment at the same time.This enables view synthesis and localization of video sequences from unknowncamera poses, opening up new avenues for visual localization systems (e.g.SLAM) and potential applications for dense 3D mapping and reconstruction.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.06405v2},
  FILE = {2104.06405v2.pdf}
 }

@article{chibane2021srf,
  AUTHOR = {Julian Chibane and Aayush Bansal and Verica Lazova and Gerard Pons-Moll},
  TITLE = {Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Viewsof Novel Scenes},
  EPRINT = {2104.06935v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent neural view synthesis methods have achieved impressive quality andrealism, surpassing classical pipelines which rely on multi-viewreconstruction. State-of-the-Art methods, such as NeRF, are designed to learn asingle scene with a neural network and require dense multi-view inputs. Testingon a new scene requires re-training from scratch, which takes 2-3 days. In thiswork, we introduce Stereo Radiance Fields (SRF), a neural view synthesisapproach that is trained end-to-end, generalizes to new scenes, and requiresonly sparse views at test time. The core idea is a neural architecture inspiredby classical multi-view stereo methods, which estimates surface points byfinding similar image regions in stereo images. In SRF, we predict color anddensity for each 3D point given an encoding of its stereo correspondence in theinput images. The encoding is implicitly learned by an ensemble of pair-wisesimilarities -- emulating classical stereo. Experiments show that SRF learnsstructure instead of overfitting on a scene. We train on multiple scenes of theDTU dataset and generalize to new ones without re-training, requiring only 10sparse and spread-out views as input. We show that 10-15 minutes of fine-tuningfurther improve the results, achieving significantly sharper, more detailedresults than scene-specific models. The code, model, and videos are availableat https://virtualhumans.mpi-inf.mpg.de/srf/.},
  YEAR = {2021},
  MONTH = {Apr},
  NOTE = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  2021},
  URL = {http://arxiv.org/abs/2104.06935v1},
  FILE = {2104.06935v1.pdf}
 }

@article{hao2021gancraft,
  AUTHOR = {Zekun Hao and Arun Mallya and Serge Belongie and Ming-Yu Liu},
  TITLE = {GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds},
  EPRINT = {2104.07659v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present GANcraft, an unsupervised neural rendering framework forgenerating photorealistic images of large 3D block worlds such as those createdin Minecraft. Our method takes a semantic block world as input, where eachblock is assigned a semantic label such as dirt, grass, or water. We representthe world as a continuous volumetric function and train our model to renderview-consistent photorealistic images for a user-controlled camera. In theabsence of paired ground truth real images for the block world, we devise atraining technique based on pseudo-ground truth and adversarial training. Thisstands in contrast to prior work on neural rendering for view synthesis, whichrequires ground truth images to estimate scene geometry and view-dependentappearance. In addition to camera trajectory, GANcraft allows user control overboth scene semantics and output style. Experimental results with comparison tostrong baselines show the effectiveness of GANcraft on this novel task ofphotorealistic 3D block world synthesis. The project website is available athttps://nvlabs.github.io/GANcraft/ .},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.07659v1},
  FILE = {2104.07659v1.pdf}
 }

@article{mu2021asdf,
  AUTHOR = {Jiteng Mu and Weichao Qiu and Adam Kortylewski and Alan Yuille and Nuno Vasconcelos and Xiaolong Wang},
  TITLE = {A-SDF: Learning Disentangled Signed Distance Functions for ArticulatedShape Representation},
  EPRINT = {2104.07645v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent work has made significant progress on using implicit functions, as acontinuous representation for 3D rigid object shape reconstruction. However,much less effort has been devoted to modeling general articulated objects.Compared to rigid objects, articulated objects have higher degrees of freedom,which makes it hard to generalize to unseen shapes. To deal with the largeshape variance, we introduce Articulated Signed Distance Functions (A-SDF) torepresent articulated shapes with a disentangled latent space, where we haveseparate codes for encoding shape and articulation. We assume no priorknowledge on part geometry, articulation status, joint type, joint axis, andjoint location. With this disentangled continuous representation, wedemonstrate that we can control the articulation input and animate unseeninstances with unseen joint angles. Furthermore, we propose a Test-TimeAdaptation inference algorithm to adjust our model during inference. Wedemonstrate our model generalize well to out-of-distribution and unseen data,e.g., partial point clouds and real-world depth images.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.07645v1},
  FILE = {2104.07645v1.pdf}
 }

@article{garbin2021fastnerf,
  AUTHOR = {Stephan J. Garbin and Marek Kowalski and Matthew Johnson and Jamie Shotton and Julien Valentin},
  TITLE = {FastNeRF: High-Fidelity Neural Rendering at 200FPS},
  EPRINT = {2103.10380v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent work on Neural Radiance Fields (NeRF) showed how neural networks canbe used to encode complex 3D environments that can be renderedphotorealistically from novel viewpoints. Rendering these images is verycomputationally demanding and recent improvements are still a long way fromenabling interactive rates, even on high-end hardware. Motivated by scenarioson mobile and mixed reality devices, we propose FastNeRF, the first NeRF-basedsystem capable of rendering high fidelity photorealistic images at 200Hz on ahigh-end consumer GPU. The core of our method is a graphics-inspiredfactorization that allows for (i) compactly caching a deep radiance map at eachposition in space, (ii) efficiently querying that map using ray directions toestimate the pixel values in the rendered image. Extensive experiments showthat the proposed method is 3000 times faster than the original NeRF algorithmand at least an order of magnitude faster than existing work on acceleratingNeRF, while maintaining visual quality and extensibility.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.10380v2},
  FILE = {2103.10380v2.pdf}
 }

@article{xie2021fignerf,
  AUTHOR = {Christopher Xie and Keunhong Park and Ricardo Martin-Brualla and Matthew Brown},
  TITLE = {FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object CategoryModelling},
  EPRINT = {2104.08418v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We investigate the use of Neural Radiance Fields (NeRF) to learn high quality3D object category models from collections of input images. In contrast toprevious work, we are able to do this whilst simultaneously separatingforeground objects from their varying backgrounds. We achieve this via a2-component NeRF model, FiG-NeRF, that prefers explanation of the scene as ageometrically constant background and a deformable foreground that representsthe object category. We show that this method can learn accurate 3D objectcategory models using only photometric supervision and casually captured imagesof the objects. Additionally, our 2-part decomposition allows the model toperform accurate and crisp amodal segmentation. We quantitatively evaluate ourmethod with view synthesis and image fidelity metrics, using synthetic,lab-captured, and in-the-wild data. Our results demonstrate convincing 3Dobject category modelling that exceed the performance of existing methods.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.08418v1},
  FILE = {2104.08418v1.pdf}
 }

@article{hertz2021sape,
  AUTHOR = {Amir Hertz and Or Perel and Raja Giryes and Olga Sorkine-Hornung and Daniel Cohen-Or},
  TITLE = {SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization},
  EPRINT = {2104.09125v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {Multilayer-perceptrons (MLP) are known to struggle with learning functions ofhigh-frequencies, and in particular cases with wide frequency bands. We presenta spatially adaptive progressive encoding (SAPE) scheme for input signals ofMLP networks, which enables them to better fit a wide range of frequencieswithout sacrificing training stability or requiring any domain specificpreprocessing. SAPE gradually unmasks signal components with increasingfrequencies as a function of time and space. The progressive exposure offrequencies is monitored by a feedback loop throughout the neural optimizationprocess, allowing changes to propagate at different rates among local spatialportions of the signal space. We demonstrate the advantage of SAPE on a varietyof domains and applications, including regression of low dimensional signalsand images, representation learning of occupancy networks, and a geometric taskof mesh transfer between 3D shapes.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.09125v2},
  FILE = {2104.09125v2.pdf}
 }

@article{derksen2021snerf,
  AUTHOR = {Dawa Derksen and Dario Izzo},
  TITLE = {Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry},
  EPRINT = {2104.09877v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a new generic method for shadow-aware multi-view satellitephotogrammetry of Earth Observation scenes. Our proposed method, the ShadowNeural Radiance Field (S-NeRF) follows recent advances in implicit volumetricrepresentation learning. For each scene, we train S-NeRF using very highspatial resolution optical images taken from known viewing angles. The learningrequires no labels or shape priors: it is self-supervised by an imagereconstruction loss. To accommodate for changing light source conditions bothfrom a directional light source (the Sun) and a diffuse light source (the sky),we extend the NeRF approach in two ways. First, direct illumination from theSun is modeled via a local light source visibility field. Second, indirectillumination from a diffuse light source is learned as a non-local color fieldas a function of the position of the Sun. Quantitatively, the combination ofthese factors reduces the altitude and color errors in shaded areas, comparedto NeRF. The S-NeRF methodology not only performs novel view synthesis and full3D shape estimation, it also enables shadow detection, albedo synthesis, andtransient object filtering, without any explicit shape supervision.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.09877v1},
  FILE = {2104.09877v1.pdf}
 }

@article{oechsle2021unisurf,
  AUTHOR = {Michael Oechsle and Songyou Peng and Andreas Geiger},
  TITLE = {UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields forMulti-View Reconstruction},
  EPRINT = {2104.10078v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural implicit 3D representations have emerged as a powerful paradigm forreconstructing surfaces from multi-view images and synthesizing novel views.Unfortunately, existing methods such as DVR or IDR require accurate per-pixelobject masks as supervision. At the same time, neural radiance fields haverevolutionized novel view synthesis. However, NeRF's estimated volume densitydoes not admit accurate surface reconstruction. Our key insight is thatimplicit surface models and radiance fields can be formulated in a unified way,enabling both surface and volume rendering using the same model. This unifiedperspective enables novel, more efficient sampling procedures and the abilityto reconstruct accurate surfaces without input masks. We compare our method onthe DTU, BlendedMVS, and a synthetic indoor dataset. Our experimentsdemonstrate that we outperform NeRF in terms of reconstruction quality whileperforming on par with IDR without requiring masks.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.10078v1},
  FILE = {2104.10078v1.pdf}
 }

@article{reed2021inr,
  AUTHOR = {Albert W. Reed and Hyojin Kim and Rushil Anirudh and K. Aditya Mohan and Kyle Champley and Jingu Kang and Suren Jayasuriya},
  TITLE = {Dynamic CT Reconstruction from Limited Views with Implicit NeuralRepresentations and Parametric Motion Fields},
  EPRINT = {2104.11745v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.IV},
  ABSTRACT = {Reconstructing dynamic, time-varying scenes with computed tomography (4D-CT)is a challenging and ill-posed problem common to industrial and medicalsettings. Existing 4D-CT reconstructions are designed for sparse samplingschemes that require fast CT scanners to capture multiple, rapid revolutionsaround the scene in order to generate high quality results. However, if thescene is moving too fast, then the sampling occurs along a limited view and isdifficult to reconstruct due to spatiotemporal ambiguities. In this work, wedesign a reconstruction pipeline using implicit neural representations coupledwith a novel parametric motion field warping to perform limited view 4D-CTreconstruction of rapidly deforming scenes. Importantly, we utilize adifferentiable analysis-by-synthesis approach to compare with captured x-raysinogram data in a self-supervised fashion. Thus, our resulting optimizationmethod requires no training data to reconstruct the scene. We demonstrate thatour proposed system robustly reconstructs scenes containing deformable andperiodic motion and validate against state-of-the-art baselines. Further, wedemonstrate an ability to reconstruct continuous spatiotemporal representationsof our scenes and upsample them to arbitrary volumes and frame ratespost-optimization. This research opens a new avenue for implicit neuralrepresentations in computed tomography reconstruction in general.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.11745v1},
  FILE = {2104.11745v1.pdf}
 }

@article{deng2021vector neurons,
  AUTHOR = {Congyue Deng and Or Litany and Yueqi Duan and Adrien Poulenard and Andrea Tagliasacchi and Leonidas Guibas},
  TITLE = {Vector Neurons: A General Framework for SO(3)-Equivariant Networks},
  EPRINT = {2104.12229v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Invariance and equivariance to the rotation group have been widely discussedin the 3D deep learning community for pointclouds. Yet most proposed methodseither use complex mathematical tools that may limit their accessibility, orare tied to specific input data types and network architectures. In this paper,we introduce a general framework built on top of what we call Vector Neuronrepresentations for creating SO(3)-equivariant neural networks for pointcloudprocessing. Extending neurons from 1D scalars to 3D vectors, our vector neuronsenable a simple mapping of SO(3) actions to latent spaces thereby providing aframework for building equivariance in common neural operations -- includinglinear layers, non-linearities, pooling, and normalizations. Due to theirsimplicity, vector neurons are versatile and, as we demonstrate, can beincorporated into diverse network architecture backbones, allowing them toprocess geometry inputs in arbitrary poses. Despite its simplicity, our methodperforms comparably well in accuracy and generalization with other more complexand specialized state-of-the-art methods on classification and segmentationtasks. We also show for the first time a rotation equivariant reconstructionnetwork.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.12229v1},
  FILE = {2104.12229v1.pdf}
 }

@article{knodt2021neural raytracing,
  AUTHOR = {Julian Knodt and Seung-Hwan Baek and Felix Heide},
  TITLE = {Neural Ray-Tracing: Learning Surfaces and Reflectance for Relighting andView Synthesis},
  EPRINT = {2104.13562v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent neural rendering methods have demonstrated accurate view interpolationby predicting volumetric density and color with a neural network. Although suchvolumetric representations can be supervised on static and dynamic scenes,existing methods implicitly bake the complete scene light transport into asingle neural network for a given scene, including surface modeling,bidirectional scattering distribution functions, and indirect lighting effects.In contrast to traditional rendering pipelines, this prohibits changing surfacereflectance, illumination, or composing other objects in the scene.In this work, we explicitly model the light transport between scene surfacesand we rely on traditional integration schemes and the rendering equation toreconstruct a scene. The proposed method allows BSDF recovery with unknownlight conditions and classic light transports such as pathtracing. By learningdecomposed transport with surface representations established in conventionalrendering methods, the method naturally facilitates editing shape, reflectance,lighting and scene composition. The method outperforms NeRV for relightingunder known lighting conditions, and produces realistic reconstructions forrelit and edited scenes. We validate the proposed approach for scene editing,relighting and reflectance estimation learned from synthetic and captured viewson a subset of NeRV's datasets.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.13562v1},
  FILE = {2104.13562v1.pdf}
 }

@article{zhang2021stnerf,
  AUTHOR = {Jiakai Zhang and Xinhang Liu and Xinyi Ye and Fuqiang Zhao and Yanshun Zhang and Minye Wu and Yingliang Zhang and Lan Xu and Jingyi Yu},
  TITLE = {Editable Free-viewpoint Video Using a Layered Neural Representation},
  EPRINT = {2104.14786v1},
  DOI = {10.1145/3450626.3459756},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Generating free-viewpoint videos is critical for immersive VR/AR experiencebut recent neural advances still lack the editing ability to manipulate thevisual perception for large dynamic scenes. To fill this gap, in this paper wepropose the first approach for editable photo-realistic free-viewpoint videogeneration for large-scale dynamic scenes using only sparse 16 cameras. Thecore of our approach is a new layered neural representation, where each dynamicentity including the environment itself is formulated into a space-timecoherent neural layered radiance representation called ST-NeRF. Such layeredrepresentation supports fully perception and realistic manipulation of thedynamic scene whilst still supporting a free viewing experience in a widerange. In our ST-NeRF, the dynamic entity/layer is represented as continuousfunctions, which achieves the disentanglement of location, deformation as wellas the appearance of the dynamic entity in a continuous and self-supervisedmanner. We propose a scene parsing 4D label map tracking to disentangle thespatial information explicitly, and a continuous deform module to disentanglethe temporal motion implicitly. An object-aware volume rendering scheme isfurther introduced for the re-assembling of all the neural layers. We adopt anovel layered loss and motion-aware ray sampling strategy to enable efficienttraining for a large dynamic scene with multiple performers, Our frameworkfurther enables a variety of editing functions, i.e., manipulating the scaleand location, duplicating or retiming individual neural layers to createnumerous visual effects while preserving high realism. Extensive experimentsdemonstrate the effectiveness of our approach to achieve high-quality,photo-realistic, and editable free-viewpoint video generation for dynamicscenes.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.14786v1},
  FILE = {2104.14786v1.pdf}
 }

@article{bird2021cnerf,
  AUTHOR = {Thomas Bird and Johannes Balle and Saurabh Singh and Philip A. Chou},
  TITLE = {3D Scene Compression through Entropy Penalized Neural RepresentationFunctions},
  EPRINT = {2104.12456v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Some forms of novel visual media enable the viewer to explore a 3D scene fromarbitrary viewpoints, by interpolating between a discrete set of originalviews. Compared to 2D imagery, these types of applications require much largeramounts of storage space, which we seek to reduce. Existing approaches forcompressing 3D scenes are based on a separation of compression and rendering:each of the original views is compressed using traditional 2D image formats;the receiver decompresses the views and then performs the rendering. We unifythese steps by directly compressing an implicit representation of the scene, afunction that maps spatial coordinates to a radiance vector field, which canthen be queried to render arbitrary viewpoints. The function is implemented asa neural network and jointly trained for reconstruction as well ascompressibility, in an end-to-end manner, with the use of an entropy penalty onthe parameters. Our method significantly outperforms a state-of-the-artconventional approach for scene compression, achieving simultaneously higherquality reconstructions and lower bitrates. Furthermore, we show that theperformance at lower bitrates can be improved by jointly representing multiplescenes using a soft form of parameter sharing.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.12456v1},
  FILE = {2104.12456v1.pdf}
 }

@article{peng2021animatable,
  AUTHOR = {Sida Peng and Junting Dong and Qianqian Wang and Shangzhan Zhang and Qing Shuai and Hujun Bao and Xiaowei Zhou},
  TITLE = {Animatable Neural Radiance Fields for Human Body Modeling},
  EPRINT = {2105.02872v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {This paper addresses the challenge of reconstructing an animatable humanmodel from a multi-view video. Some recent works have proposed to decompose adynamic scene into a canonical neural radiance field and a set of deformationfields that map observation-space points to the canonical space, therebyenabling them to learn the dynamic scene from images. However, they representthe deformation field as translational vector field or SE(3) field, which makesthe optimization highly under-constrained. Moreover, these representationscannot be explicitly controlled by input motions. Instead, we introduce neuralblend weight fields to produce the deformation fields. Based on theskeleton-driven deformation, blend weight fields are used with 3D humanskeletons to generate observation-to-canonical and canonical-to-observationcorrespondences. Since 3D human skeletons are more observable, they canregularize the learning of deformation fields. Moreover, the learned blendweight fields can be combined with input skeletal motions to generate newdeformation fields to animate the human model. Experiments show that ourapproach significantly outperforms recent human synthesis methods. The codewill be available at https://zju3dv.github.io/animatable_nerf/.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.02872v1},
  FILE = {2105.02872v1.pdf}
 }

@article{martel2021acorn,
  AUTHOR = {Julien N. P. Martel and David B. Lindell and Connor Z. Lin and Eric R. Chan and Marco Monteiro and Gordon Wetzstein},
  TITLE = {ACORN: Adaptive Coordinate Networks for Neural Scene Representation},
  EPRINT = {2105.02788v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural representations have emerged as a new paradigm for applications inrendering, imaging, geometric modeling, and simulation. Compared to traditionalrepresentations such as meshes, point clouds, or volumes they can be flexiblyincorporated into differentiable learning-based pipelines. While recentimprovements to neural representations now make it possible to representsignals with fine details at moderate resolutions (e.g., for images and 3Dshapes), adequately representing large-scale or complex scenes has proven achallenge. Current neural representations fail to accurately represent imagesat resolutions greater than a megapixel or 3D scenes with more than a fewhundred thousand polygons. Here, we introduce a new hybrid implicit-explicitnetwork architecture and training strategy that adaptively allocates resourcesduring training and inference based on the local complexity of a signal ofinterest. Our approach uses a multiscale block-coordinate decomposition,similar to a quadtree or octree, that is optimized during training. The networkarchitecture operates in two stages: using the bulk of the network parameters,a coordinate encoder generates a feature grid in a single forward pass. Then,hundreds or thousands of samples within each block can be efficiently evaluatedusing a lightweight feature decoder. With this hybrid implicit-explicit networkarchitecture, we demonstrate the first experiments that fit gigapixel images tonearly 40 dB peak signal-to-noise ratio. Notably this represents an increase inscale of over 1000x compared to the resolution of previously demonstratedimage-fitting experiments. Moreover, our approach is able to represent 3Dshapes significantly faster and better than previous techniques; it reducestraining times from days to hours or minutes and memory requirements by over anorder of magnitude.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.02788v1},
  FILE = {2105.02788v1.pdf}
 }

@article{isik2021neural,
  AUTHOR = {Berivan Isik},
  TITLE = {Neural 3D Scene Compression via Model Compression},
  EPRINT = {2105.03120v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Rendering 3D scenes requires access to arbitrary viewpoints from the scene.Storage of such a 3D scene can be done in two ways; (1) storing 2D images takenfrom the 3D scene that can reconstruct the scene back through interpolations,or (2) storing a representation of the 3D scene itself that already encodesviews from all directions. So far, traditional 3D compression methods havefocused on the first type of storage and compressed the original 2D images withimage compression techniques. With this approach, the user first decodes thestored 2D images and then renders the 3D scene. However, this separatedprocedure is inefficient since a large amount of 2D images have to be stored.In this work, we take a different approach and compress a functionalrepresentation of 3D scenes. In particular, we introduce a method to compress3D scenes by compressing the neural networks that represent the scenes asneural radiance fields. Our method provides more efficient storage of 3D scenessince it does not store 2D images -- which are redundant when we render thescene from the neural functional representation.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.03120v1},
  FILE = {2105.03120v1.pdf}
 }

@article{mergy2021visionbased,
  AUTHOR = {Anne Mergy and Gurvan Lecuyer and Dawa Derksen and Dario Izzo},
  TITLE = {Vision-based Neural Scene Representations for Spacecraft},
  EPRINT = {2105.06405v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In advanced mission concepts with high levels of autonomy, spacecraft need tointernally model the pose and shape of nearby orbiting objects. Recent works inneural scene representations show promising results for inferring genericthree-dimensional scenes from optical images. Neural Radiance Fields (NeRF)have shown success in rendering highly specular surfaces using a large numberof images and their pose. More recently, Generative Radiance Fields (GRAF)achieved full volumetric reconstruction of a scene from unposed images only,thanks to the use of an adversarial framework to train a NeRF. In this paper,we compare and evaluate the potential of NeRF and GRAF to render novel viewsand extract the 3D shape of two different spacecraft, the Soil Moisture andOcean Salinity satellite of ESA's Living Planet Programme and a generic cubesat. Considering the best performances of both models, we observe that NeRF hasthe ability to render more accurate images regarding the material specularityof the spacecraft and its pose. For its part, GRAF generates precise novelviews with accurate details even when parts of the satellites are shadowedwhile having the significant advantage of not needing any information about therelative pose.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.06405v1},
  FILE = {2105.06405v1.pdf}
 }

@article{chen2021nefnet,
  AUTHOR = {Jintai Chen and Xiangshang Zheng and Hongyun Yu and Danny Z. Chen and Jian Wu},
  TITLE = {Electrocardio Panorama: Synthesizing New ECG Views with Self-supervision},
  EPRINT = {2105.06293v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.SP},
  ABSTRACT = {Multi-lead electrocardiogram (ECG) provides clinical information ofheartbeats from several fixed viewpoints determined by the lead positioning.However, it is often not satisfactory to visualize ECG signals in these fixedand limited views, as some clinically useful information is represented onlyfrom a few specific ECG viewpoints. For the first time, we propose a newconcept, Electrocardio Panorama, which allows visualizing ECG signals from anyqueried viewpoints. To build Electrocardio Panorama, we assume that anunderlying electrocardio field exists, representing locations, magnitudes, anddirections of ECG signals. We present a Neural electrocardio field Network(Nef-Net), which first predicts the electrocardio field representation by usinga sparse set of one or few input ECG views and then synthesizes ElectrocardioPanorama based on the predicted representations. Specially, to betterdisentangle electrocardio field information from viewpoint biases, a newAngular Encoding is proposed to process viewpoint angles. Also, we propose aself-supervised learning approach called Standin Learning, which helps modelthe electrocardio field without direct supervision. Further, with very fewmodifications, Nef-Net can also synthesize ECG signals from scratch.Experiments verify that our Nef-Net performs well on Electrocardio Panoramasynthesis, and outperforms the previous work on the auxiliary tasks (ECG viewtransformation and ECG synthesis from scratch). The codes and the divisionlabels of cardiac cycles and ECG deflections on Tianchi ECG and PTB datasetsare available at https://github.com/WhatAShot/Electrocardio-Panorama.},
  YEAR = {2021},
  MONTH = {May},
  NOTE = {the 30th International Joint Conference on Artificial Intelligence
  (2021)},
  URL = {http://arxiv.org/abs/2105.06293v1},
  FILE = {2105.06293v1.pdf}
 }

@article{wang2021dctnerf,
  AUTHOR = {Chaoyang Wang and Ben Eckart and Simon Lucey and Orazio Gallo},
  TITLE = {Neural Trajectory Fields for Dynamic Novel View Synthesis},
  EPRINT = {2105.05994v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent approaches to render photorealistic views from a limited set ofphotographs have pushed the boundaries of our interactions with pictures ofstatic scenes. The ability to recreate moments, that is, time-varyingsequences, is perhaps an even more interesting scenario, but it remains largelyunsolved. We introduce DCT-NeRF, a coordinatebased neural representation fordynamic scenes. DCTNeRF learns smooth and stable trajectories over the inputsequence for each point in space. This allows us to enforce consistency betweenany two frames in the sequence, which results in high quality reconstruction,particularly in dynamic regions.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.05994v1},
  FILE = {2105.05994v1.pdf}
 }

@article{liu2021editing,
  AUTHOR = {Steven Liu and Xiuming Zhang and Zhoutong Zhang and Richard Zhang and Jun-Yan Zhu and Bryan Russell},
  TITLE = {Editing Conditional Radiance Fields},
  EPRINT = {2105.06466v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {A neural radiance field (NeRF) is a scene model supporting high-quality viewsynthesis, optimized per scene. In this paper, we explore enabling user editingof a category-level NeRF - also known as a conditional radiance field - trainedon a shape category. Specifically, we introduce a method for propagating coarse2D user scribbles to the 3D space, to modify the color or shape of a localregion. First, we propose a conditional radiance field that incorporates newmodular network components, including a shape branch that is shared acrossobject instances. Observing multiple instances of the same category, our modellearns underlying part semantics without any supervision, thereby allowing thepropagation of coarse 2D user scribbles to the entire 3D region (e.g., chairseat). Next, we propose a hybrid network update strategy that targets specificnetwork components, which balances efficiency and accuracy. During userinteraction, we formulate an optimization problem that both satisfies theuser's constraints and preserves the original object structure. We demonstrateour approach on various editing tasks over three shape datasets and show thatit outperforms prior neural editing approaches. Finally, we edit the appearanceand shape of a real photograph and show that the edit propagates toextrapolated novel views.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.06466v2},
  FILE = {2105.06466v2.pdf}
 }

@article{gao2021dynamic,
  AUTHOR = {Chen Gao and Ayush Saraf and Johannes Kopf and Jia-Bin Huang},
  TITLE = {Dynamic View Synthesis from Dynamic Monocular Video},
  EPRINT = {2105.06468v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present an algorithm for generating novel views at arbitrary viewpointsand any input time step given a monocular video of a dynamic scene. Our workbuilds upon recent advances in neural implicit representation and usescontinuous and differentiable functions for modeling the time-varying structureand the appearance of the scene. We jointly train a time-invariant static NeRFand a time-varying dynamic NeRF, and learn how to blend the results in anunsupervised manner. However, learning this implicit function from a singlevideo is highly ill-posed (with infinitely many solutions that match the inputvideo). To resolve the ambiguity, we introduce regularization losses toencourage a more physically plausible solution. We show extensive quantitativeand qualitative results of dynamic view synthesis from casually capturedvideos.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.06468v1},
  FILE = {2105.06468v1.pdf}
 }

@article{liu2021neulf,
  AUTHOR = {Celong Liu and Zhong Li and Junsong Yuan and Yi Xu},
  TITLE = {NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field},
  EPRINT = {2105.07112v4},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper, we present an efficient and robust deep learning solution fornovel view synthesis of complex scenes. In our approach, a 3D scene isrepresented as a light field, i.e., a set of rays, each of which has acorresponding color when reaching the image plane. For efficient novel viewrendering, we adopt a 4D parameterization of the light field, where each ray ischaracterized by a 4D parameter. We then formulate the light field as a 4Dfunction that maps 4D coordinates to corresponding color values. We train adeep fully connected network to optimize this implicit function and memorizethe 3D scene. Then, the scene-specific model is used to synthesize novel views.Different from previous light field approaches which require dense viewsampling to reliably render novel views, our method can render novel views bysampling rays and querying the color for each ray from the network directly,thus enabling high-quality light field rendering with a sparser set of trainingimages. Our method achieves state-of-the-art novel view synthesis results whilemaintaining an interactive frame rate.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.07112v4},
  FILE = {2105.07112v4.pdf}
 }

@article{yang2021recursivenerf,
  AUTHOR = {Guo-Wei Yang and Wen-Yang Zhou and Hao-Yang Peng and Dun Liang and Tai-Jiang Mu and Shi-Min Hu},
  TITLE = {Recursive-NeRF: An Efficient and Dynamically Growing NeRF},
  EPRINT = {2105.09103v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {View synthesis methods using implicit continuous shape representationslearned from a set of images, such as the Neural Radiance Field (NeRF) method,have gained increasing attention due to their high quality imagery andscalability to high resolution. However, the heavy computation required by itsvolumetric approach prevents NeRF from being useful in practice; minutes aretaken to render a single image of a few megapixels. Now, an image of a scenecan be rendered in a level-of-detail manner, so we posit that a complicatedregion of the scene should be represented by a large neural network while asmall neural network is capable of encoding a simple region, enabling a balancebetween efficiency and quality. Recursive-NeRF is our embodiment of this idea,providing an efficient and adaptive rendering and training approach for NeRF.The core of Recursive-NeRF learns uncertainties for query coordinates,representing the quality of the predicted color and volumetric intensity ateach level. Only query coordinates with high uncertainties are forwarded to thenext level to a bigger neural network with a more powerful representationalcapability. The final rendered image is a composition of results from neuralnetworks of all levels. Our evaluation on three public datasets shows thatRecursive-NeRF is more efficient than NeRF while providing state-of-the-artquality. The code will be available at https://github.com/Gword/Recursive-NeRF.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.09103v1},
  FILE = {2105.09103v1.pdf}
 }

@article{hadadan2021neural,
  AUTHOR = {Saeed Hadadan and Shuhong Chen and Matthias Zwicker},
  TITLE = {Neural Radiosity},
  EPRINT = {2105.12319v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {We introduce Neural Radiosity, an algorithm to solve the rendering equationby minimizing the norm of its residual similar as in traditional radiositytechniques. Traditional basis functions used in radiosity techniques, such aspiecewise polynomials or meshless basis functions are typically limited torepresenting isotropic scattering from diffuse surfaces. Instead, we propose toleverage neural networks to represent the full four-dimensional radiancedistribution, directly optimizing network parameters to minimize the norm ofthe residual. Our approach decouples solving the rendering equation fromrendering (perspective) images similar as in traditional radiosity techniques,and allows us to efficiently synthesize arbitrary views of a scene. Inaddition, we propose a network architecture using geometric learnable featuresthat improves convergence of our solver compared to previous techniques. Ourapproach leads to an algorithm that is simple to implement, and we demonstrateits effectiveness on a variety of scenes with non-diffuse surfaces.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.12319v1},
  FILE = {2105.12319v1.pdf}
 }

@article{chiang2021stylizing,
  AUTHOR = {Pei-Ze Chiang and Meng-Shiun Tsai and Hung-Yu Tseng and Wei-sheng Lai and Wei-Chen Chiu},
  TITLE = {Stylizing 3D Scene via Implicit Representation and HyperNetwork},
  EPRINT = {2105.13016v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this work, we aim to address the 3D scene stylization problem - generatingstylized images of the scene at arbitrary novel view angles. A straightforwardsolution is to combine existing novel view synthesis and image/video styletransfer approaches, which often leads to blurry results or inconsistentappearance. Inspired by the high quality results of the neural radiance fields(NeRF) method, we propose a joint framework to directly render novel views withthe desired style. Our framework consists of two components: an implicitrepresentation of the 3D scene with the neural radiance field model, and ahypernetwork to transfer the style information into the scene representation.In particular, our implicit representation model disentangles the scene intothe geometry and appearance branches, and the hypernetwork learns to predictthe parameters of the appearance branch from the reference style image. Toalleviate the training difficulties and memory burden, we propose a two-stagetraining procedure and a patch sub-sampling approach to optimize the style andcontent losses with the neural radiance field model. After optimization, ourmodel is able to render consistent novel views at arbitrary view angles witharbitrary style. Both quantitative evaluation and human subject study havedemonstrated that the proposed method generates faithful stylization resultswith consistent appearance across different views.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.13016v2},
  FILE = {2105.13016v2.pdf}
 }

@article{izzo2021geodesynets,
  AUTHOR = {Dario Izzo and Pablo Gomez},
  TITLE = {Geodesy of irregular small bodies via neural density fields: geodesyNets},
  EPRINT = {2105.13031v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {astro-ph.EP},
  ABSTRACT = {We present a novel approach based on artificial neural networks, so-calledgeodesyNets, and present compelling evidence of their ability to serve asaccurate geodetic models of highly irregular bodies using minimal priorinformation on the body. The approach does not rely on the body shapeinformation but, if available, can harness it. GeodesyNets learn athree-dimensional, differentiable, function representing the body density,which we call neural density field. The body shape, as well as other geodeticproperties, can easily be recovered. We investigate six different shapesincluding the bodies 101955 Bennu, 67P Churyumov-Gerasimenko, 433 Eros and25143 Itokawa for which shape models developed during close proximity surveysare available. Both heterogeneous and homogeneous mass distributions areconsidered. The gravitational acceleration computed from the trainedgeodesyNets models, as well as the inferred body shape, show great accuracy inall cases with a relative error on the predicted acceleration smaller than 1\%even close to the asteroid surface. When the body shape information isavailable, geodesyNets can seamlessly exploit it and be trained to represent ahigh-fidelity neural density field able to give insights into the internalstructure of the body. This work introduces a new unexplored approach togeodesy, adding a powerful tool to consolidated ones based on sphericalharmonics, mascon models and polyhedral gravity.},
  YEAR = {2021},
  MONTH = {May},
  URL = {http://arxiv.org/abs/2105.13031v1},
  FILE = {2105.13031v1.pdf}
 }

@article{zhang2021nerfactor,
  AUTHOR = {Xiuming Zhang and Pratul P. Srinivasan and Boyang Deng and Paul Debevec and William T. Freeman and Jonathan T. Barron},
  TITLE = {NeRFactor: Neural Factorization of Shape and Reflectance Under anUnknown Illumination},
  EPRINT = {2106.01970v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We address the problem of recovering the shape and spatially-varyingreflectance of an object from posed multi-view images of the object illuminatedby one unknown lighting condition. This enables the rendering of novel views ofthe object under arbitrary environment lighting and editing of the object'smaterial properties. The key to our approach, which we call Neural RadianceFactorization (NeRFactor), is to distill the volumetric geometry of a NeuralRadiance Field (NeRF) [Mildenhall et al. 2020] representation of the objectinto a surface representation and then jointly refine the geometry whilesolving for the spatially-varying reflectance and the environment lighting.Specifically, NeRFactor recovers 3D neural fields of surface normals, lightvisibility, albedo, and Bidirectional Reflectance Distribution Functions(BRDFs) without any supervision, using only a re-rendering loss, simplesmoothness priors, and a data-driven BRDF prior learned from real-world BRDFmeasurements. By explicitly modeling light visibility, NeRFactor is able toseparate shadows from albedo and synthesize realistic soft or hard shadowsunder arbitrary lighting conditions. NeRFactor is able to recover convincing 3Dmodels for free-viewpoint relighting in this challenging and underconstrainedcapture setup for both synthetic and real scenes. Qualitative and quantitativeexperiments show that NeRFactor outperforms classic and deep learning-basedstate of the art across various tasks. Our code and data are available atpeople.csail.mit.edu/xiuming/projects/nerfactor/.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.01970v1},
  FILE = {2106.01970v1.pdf}
 }

@article{wang2021spline,
  AUTHOR = {Peng-Shuai Wang and Yang Liu and Yu-Qi Yang and Xin Tong},
  TITLE = {Spline Positional Encoding for Learning 3D Implicit Signed DistanceFields},
  EPRINT = {2106.01553v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Multilayer perceptrons (MLPs) have been successfully used to represent 3Dshapes implicitly and compactly, by mapping 3D coordinates to the correspondingsigned distance values or occupancy values. In this paper, we propose a novelpositional encoding scheme, called Spline Positional Encoding, to map the inputcoordinates to a high dimensional space before passing them to MLPs, forhelping to recover 3D signed distance fields with fine-scale geometric detailsfrom unorganized 3D point clouds. We verified the superiority of our approachover other positional encoding schemes on tasks of 3D shape reconstruction frominput point clouds and shape space learning. The efficacy of our approachextended to image reconstruction is also demonstrated and evaluated.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.01553v1},
  FILE = {2106.01553v1.pdf}
 }

@article{liu2021na,
  AUTHOR = {Lingjie Liu and Marc Habermann and Viktor Rudnev and Kripasindhu Sarkar and Jiatao Gu and Christian Theobalt},
  TITLE = {Neural Actor: Neural Free-view Synthesis of Human Actors with PoseControl},
  EPRINT = {2106.02019v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose Neural Actor (NA), a new method for high-quality synthesis ofhumans from arbitrary viewpoints and under arbitrary controllable poses. Ourmethod is built upon recent neural scene representation and rendering workswhich learn representations of geometry and appearance from only 2D images.While existing works demonstrated compelling rendering of static scenes andplayback of dynamic scenes, photo-realistic reconstruction and rendering ofhumans with neural implicit methods, in particular under user-controlled novelposes, is still difficult. To address this problem, we utilize a coarse bodymodel as the proxy to unwarp the surrounding 3D space into a canonical pose. Aneural radiance field learns pose-dependent geometric deformations and pose-and view-dependent appearance effects in the canonical space from multi-viewvideo input. To synthesize novel views of high fidelity dynamic geometry andappearance, we leverage 2D texture maps defined on the body model as latentvariables for predicting residual deformations and the dynamic appearance.Experiments demonstrate that our method achieves better quality than thestate-of-the-arts on playback as well as novel pose synthesis, and can evengeneralize well to new poses that starkly differ from the training poses.Furthermore, our method also supports body shape control of the synthesizedresults.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.02019v1},
  FILE = {2106.02019v1.pdf}
 }

@article{sitzmann2021lfns,
  AUTHOR = {Vincent Sitzmann and Semon Rezchikov and William T. Freeman and Joshua B. Tenenbaum and Fredo Durand},
  TITLE = {Light Field Networks: Neural Scene Representations withSingle-Evaluation Rendering},
  EPRINT = {2106.02634v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Inferring representations of 3D scenes from 2D observations is a fundamentalproblem of computer graphics, computer vision, and artificial intelligence.Emerging 3D-structured neural scene representations are a promising approach to3D scene understanding. In this work, we propose a novel neural scenerepresentation, Light Field Networks or LFNs, which represent both geometry andappearance of the underlying 3D scene in a 360-degree, four-dimensional lightfield parameterized via a neural implicit representation. Rendering a ray froman LFN requires only a *single* network evaluation, as opposed to hundreds ofevaluations per ray for ray-marching or volumetric based renderers in3D-structured neural scene representations. In the setting of simple scenes, weleverage meta-learning to learn a prior over LFNs that enables multi-viewconsistent light field reconstruction from as little as a single imageobservation. This results in dramatic reductions in time and memory complexity,and enables real-time rendering. The cost of storing a 360-degree light fieldvia an LFN is two orders of magnitude lower than conventional methods such asthe Lumigraph. Utilizing the analytical differentiability of neural implicitrepresentations and a novel parameterization of light space, we furtherdemonstrate the extraction of sparse depth maps from LFNs.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.02634v1},
  FILE = {2106.02634v1.pdf}
 }

@article{rebain2021dmf,
  AUTHOR = {Daniel Rebain and Ke Li and Vincent Sitzmann and Soroosh Yazdani and Kwang Moo Yi and Andrea Tagliasacchi},
  TITLE = {Deep Medial Fields},
  EPRINT = {2106.03804v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {Implicit representations of geometry, such as occupancy fields or signeddistance fields (SDF), have recently re-gained popularity in encoding 3D solidshape in a functional form. In this work, we introduce medial fields: a fieldfunction derived from the medial axis transform (MAT) that makes availableinformation about the underlying 3D geometry that is immediately useful for anumber of downstream tasks. In particular, the medial field encodes the localthickness of a 3D shape, and enables O(1) projection of a query point onto themedial axis. To construct the medial field we require nothing but the SDF ofthe shape itself, thus allowing its straightforward incorporation in anyapplication that relies on signed distance fields. Working in unison with theO(1) surface projection supported by the SDF, the medial field opens the doorfor an entirely new set of efficient, shape-aware operations on implicitrepresentations. We present three such applications, including a modificationto sphere tracing that renders implicit representations with better convergenceproperties, a fast construction method for memory-efficient rigid-bodycollision proxies, and an efficient approximation of ambient occlusion thatremains stable with respect to viewpoint variations.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.03804v1},
  FILE = {2106.03804v1.pdf}
 }

@article{chen2021mocoflow,
  AUTHOR = {Xuelin Chen and Weiyu Li and Daniel Cohen-Or and Niloy J. Mitra and Baoquan Chen},
  TITLE = {MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in StationaryMonocular Cameras},
  EPRINT = {2106.04477v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Synthesizing novel views of dynamic humans from stationary monocular camerasis a popular scenario. This is particularly attractive as it does not requirestatic scenes, controlled environments, or specialized hardware. In contrast totechniques that exploit multi-view observations to constrain the modeling,given a single fixed viewpoint only, the problem of modeling the dynamic sceneis significantly more under-constrained and ill-posed. In this paper, weintroduce Neural Motion Consensus Flow (MoCo-Flow), a representation thatmodels the dynamic scene using a 4D continuous time-variant function. Theproposed representation is learned by an optimization which models a dynamicscene that minimizes the error of rendering all observation images. At theheart of our work lies a novel optimization formulation, which is constrainedby a motion consensus regularization on the motion flow. We extensivelyevaluate MoCo-Flow on several datasets that contain human motions of varyingcomplexity, and compare, both qualitatively and quantitatively, to severalbaseline methods and variants of our methods. Pretrained model, code, and datawill be released for research purposes upon paper acceptance.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.04477v1},
  FILE = {2106.04477v1.pdf}
 }

@article{shao2021doublefield,
  AUTHOR = {Ruizhi Shao and Hongwen Zhang and He Zhang and Yanpei Cao and Tao Yu and Yebin Liu},
  TITLE = {DoubleField: Bridging the Neural Surface and Radiance Fields forHigh-fidelity Human Rendering},
  EPRINT = {2106.03798v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce DoubleField, a novel representation combining the merits of bothsurface field and radiance field for high-fidelity human rendering. WithinDoubleField, the surface field and radiance field are associated together by ashared feature embedding and a surface-guided sampling strategy. In this way,DoubleField has a continuous but disentangled learning space for geometry andappearance modeling, which supports fast training, inference, and finetuning.To achieve high-fidelity free-viewpoint rendering, DoubleField is furtheraugmented to leverage ultra-high-resolution inputs, where a view-to-viewtransformer and a transfer learning scheme are introduced for more efficientlearning and finetuning from sparse-view inputs at original resolutions. Theefficacy of DoubleField is validated by the quantitative evaluations on severaldatasets and the qualitative results in a real-world sparse multi-view system,showing its superior capability for photo-realistic free-viewpoint humanrendering. For code and demo video, please refer to our project page:http://www.liuyebin.com/dbfield/dbfield.html.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.03798v2},
  FILE = {2106.03798v2.pdf}
 }

@article{yifan2021idf,
  AUTHOR = {Wang Yifan and Lukas Rahmann and Olga Sorkine-Hornung},
  TITLE = {Geometry-Consistent Neural Shape Representation with ImplicitDisplacement Fields},
  EPRINT = {2106.05187v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present implicit displacement fields, a novel representation for detailed3D geometry. Inspired by a classic surface deformation technique, displacementmapping, our method represents a complex surface as a smooth base surface plusa displacement along the base's normal directions, resulting in afrequency-based shape decomposition, where the high frequency signal isconstrained geometrically by the low frequency signal. Importantly, thisdisentanglement is unsupervised thanks to a tailored architectural design thathas an innate frequency hierarchy by construction. We explore implicitdisplacement field surface reconstruction and detail transfer and demonstratesuperior representational power, training stability and generalizability.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.05187v2},
  FILE = {2106.05187v2.pdf}
 }

@article{henderson2021unsupervised,
  AUTHOR = {Paul Henderson and Christoph H. Lampert and Bernd Bickel},
  TITLE = {Unsupervised Video Prediction from a Single Frame by Estimating 3DDynamic Scene Structure},
  EPRINT = {2106.09051v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Our goal in this work is to generate realistic videos given just one initialframe as input. Existing unsupervised approaches to this task do not considerthe fact that a video typically shows a 3D environment, and that this shouldremain coherent from frame to frame even as the camera and objects move. Weaddress this by developing a model that first estimates the latent 3D structureof the scene, including the segmentation of any moving objects. It thenpredicts future frames by simulating the object and camera dynamics, andrendering the resulting views. Importantly, it is trained end-to-end using onlythe unsupervised objective of predicting future frames, without any 3Dinformation nor segmentation annotations. Experiments on two challengingdatasets of natural videos show that our model can estimate 3D structure andmotion segmentation from a single frame, and hence generate plausible andvaried predictions.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.09051v1},
  FILE = {2106.09051v1.pdf}
 }

@article{wang2021neus,
  AUTHOR = {Peng Wang and Lingjie Liu and Yuan Liu and Christian Theobalt and Taku Komura and Wenping Wang},
  TITLE = {NeuS: Learning Neural Implicit Surfaces by Volume Rendering forMulti-view Reconstruction},
  EPRINT = {2106.10689v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present a novel neural surface reconstruction method, called NeuS, forreconstructing objects and scenes with high fidelity from 2D image inputs.Existing neural surface reconstruction approaches, such as DVR and IDR, requireforeground mask as supervision, easily get trapped in local minima, andtherefore struggle with the reconstruction of objects with severeself-occlusion or thin structures. Meanwhile, recent neural methods for novelview synthesis, such as NeRF and its variants, use volume rendering to producea neural scene representation with robustness of optimization, even for highlycomplex objects. However, extracting high-quality surfaces from this learnedimplicit representation is difficult because there are not sufficient surfaceconstraints in the representation. In NeuS, we propose to represent a surfaceas the zero-level set of a signed distance function (SDF) and develop a newvolume rendering method to train a neural SDF representation. We observe thatthe conventional volume rendering method causes inherent geometric errors (i.e.bias) for surface reconstruction, and therefore propose a new formulation thatis free of bias in the first order of approximation, thus leading to moreaccurate surface reconstruction even without the mask supervision. Experimentson the DTU dataset and the BlendedMVS dataset show that NeuS outperforms thestate-of-the-arts in high-quality surface reconstruction, especially forobjects and scenes with complex structures and self-occlusion.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.10689v1},
  FILE = {2106.10689v1.pdf}
 }

@article{hsu2021omninerf,
  AUTHOR = {Ching-Yu Hsu and Cheng Sun and Hwann-Tzong Chen},
  TITLE = {Moving in a 360 World: Synthesizing Panoramic Parallaxes from a SinglePanorama},
  EPRINT = {2106.10859v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Omnidirectional Neural Radiance Fields (OmniNeRF), the firstmethod to the application of parallax-enabled novel panoramic view synthesis.Recent works for novel view synthesis focus on perspective images with limitedfield-of-view and require sufficient pictures captured in a specific condition.Conversely, OmniNeRF can generate panorama images for unknown viewpoints givena single equirectangular image as training data. To this end, we propose toaugment the single RGB-D panorama by projecting back and forth between a 3Dworld and different 2D panoramic coordinates at different virtual camerapositions. By doing so, we are able to optimize an Omnidirectional NeuralRadiance Field with visible pixels collecting from omnidirectional viewingangles at a fixed center for the estimation of new viewing angles from varyingcamera positions. As a result, the proposed OmniNeRF achieves convincingrenderings of novel panoramic views that exhibit the parallax effect. Weshowcase the effectiveness of each of our proposals on both synthetic andreal-world datasets.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.10859v1},
  FILE = {2106.10859v1.pdf}
 }

@article{yariv2021volsdf,
  AUTHOR = {Lior Yariv and Jiatao Gu and Yoni Kasten and Yaron Lipman},
  TITLE = {Volume Rendering of Neural Implicit Surfaces},
  EPRINT = {2106.12052v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural volume rendering became increasingly popular recently due to itssuccess in synthesizing novel views of a scene from a sparse set of inputimages. So far, the geometry learned by neural volume rendering techniques wasmodeled using a generic density function. Furthermore, the geometry itself wasextracted using an arbitrary level set of the density function leading to anoisy, often low fidelity reconstruction. The goal of this paper is to improvegeometry representation and reconstruction in neural volume rendering. Weachieve that by modeling the volume density as a function of the geometry. Thisis in contrast to previous work modeling the geometry as a function of thevolume density. In more detail, we define the volume density function asLaplace's cumulative distribution function (CDF) applied to a signed distancefunction (SDF) representation. This simple density representation has threebenefits: (i) it provides a useful inductive bias to the geometry learned inthe neural volume rendering process; (ii) it facilitates a bound on the opacityapproximation error, leading to an accurate sampling of the viewing ray.Accurate sampling is important to provide a precise coupling of geometry andradiance; and (iii) it allows efficient unsupervised disentanglement of shapeand appearance in volume rendering. Applying this new density representation tochallenging scene multiview datasets produced high quality geometryreconstructions, outperforming relevant baselines. Furthermore, switching shapeand appearance between scenes is possible due to the disentanglement of thetwo.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.12052v1},
  FILE = {2106.12052v1.pdf}
 }

@article{wang2021metaavatar,
  AUTHOR = {Shaofei Wang and Marko Mihajlovic and Qianli Ma and Andreas Geiger and Siyu Tang},
  TITLE = {MetaAvatar: Learning Animatable Clothed Human Models from Few DepthImages},
  EPRINT = {2106.11944v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper, we aim to create generalizable and controllable neural signeddistance fields (SDFs) that represent clothed humans from monocular depthobservations. Recent advances in deep learning, especially neural implicitrepresentations, have enabled human shape reconstruction and controllableavatar generation from different sensor inputs. However, to generate realisticcloth deformations from novel input poses, watertight meshes or dense full-bodyscans are usually needed as inputs. Furthermore, due to the difficulty ofeffectively modeling pose-dependent cloth deformations for diverse body shapesand cloth types, existing approaches resort to per-subject/cloth-typeoptimization from scratch, which is computationally expensive. In contrast, wepropose an approach that can quickly generate realistic clothed human avatars,represented as controllable neural SDFs, given only monocular depth images. Weachieve this by using meta-learning to learn an initialization of ahypernetwork that predicts the parameters of neural SDFs. The hypernetwork isconditioned on human poses and represents a clothed neural avatar that deformsnon-rigidly according to the input poses. Meanwhile, it is meta-learned toeffectively incorporate priors of diverse body shapes and cloth types and thuscan be much faster to fine-tune, compared to models trained from scratch. Wequalitatively and quantitatively show that our approach outperformsstate-of-the-art approaches that require complete meshes as inputs while ourapproach requires only depth frames as inputs and runs orders of magnitudesfaster. Furthermore, we demonstrate that our meta-learned hypernetwork is veryrobust, being the first to generate avatars with realistic dynamic clothdeformations given as few as 8 monocular depth frames.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.11944v1},
  FILE = {2106.11944v1.pdf}
 }

@article{muller2021realtime,
  AUTHOR = {Thomas Muller and Fabrice Rousselle and Jan Novak and Alexander Keller},
  TITLE = {Real-time Neural Radiance Caching for Path Tracing},
  EPRINT = {2106.12372v2},
  DOI = {10.1145/3450626.3459812},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {We present a real-time neural radiance caching method for path-traced globalillumination. Our system is designed to handle fully dynamic scenes, and makesno assumptions about the lighting, geometry, and materials. The data-drivennature of our approach sidesteps many difficulties of caching algorithms, suchas locating, interpolating, and updating cache points. Since pretraining neuralnetworks to handle novel, dynamic scenes is a formidable generalizationchallenge, we do away with pretraining and instead achieve generalization viaadaptation, i.e. we opt for training the radiance cache while rendering. Weemploy self-training to provide low-noise training targets and simulateinfinite-bounce transport by merely iterating few-bounce training updates. Theupdates and cache queries incur a mild overhead -- about 2.6ms on full HDresolution -- thanks to a streaming implementation of the neural network thatfully exploits modern hardware. We demonstrate significant noise reduction atthe cost of little induced bias, and report state-of-the-art, real-timeperformance on a number of challenging scenarios.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.12372v2},
  FILE = {2106.12372v2.pdf}
 }

@article{park2021hypernerf,
  AUTHOR = {Keunhong Park and Utkarsh Sinha and Peter Hedman and Jonathan T. Barron and Sofien Bouaziz and Dan B Goldman and Ricardo Martin-Brualla and Steven M. Seitz},
  TITLE = {HyperNeRF: A Higher-Dimensional Representation for Topologically VaryingNeural Radiance Fields},
  EPRINT = {2106.13228v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural Radiance Fields (NeRF) are able to reconstruct scenes withunprecedented fidelity, and various recent works have extended NeRF to handledynamic scenes. A common approach to reconstruct such non-rigid scenes isthrough the use of a learned deformation field mapping from coordinates in eachinput image into a canonical template coordinate space. However, thesedeformation-based approaches struggle to model changes in topology, astopological changes require a discontinuity in the deformation field, but thesedeformation fields are necessarily continuous. We address this limitation bylifting NeRFs into a higher dimensional space, and by representing the 5Dradiance field corresponding to each individual input image as a slice throughthis "hyper-space". Our method is inspired by level set methods, which modelthe evolution of surfaces as slices through a higher dimensional surface. Weevaluate our method on two tasks: (i) interpolating smoothly between "moments",i.e., configurations of the scene, seen in the input images while maintainingvisual plausibility, and (ii) novel-view synthesis at fixed moments. We showthat our method, which we dub HyperNeRF, outperforms existing methods on bothtasks by significant margins. Compared to Nerfies, HyperNeRF reduces averageerror rates by 8.6% for interpolation and 8.8% for novel-view synthesis, asmeasured by LPIPS.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.13228v1},
  FILE = {2106.13228v1.pdf}
 }

@article{chen2021animatable,
  AUTHOR = {Jianchuan Chen and Ying Zhang and Di Kang and Xuefei Zhe and Linchao Bao and Huchuan Lu},
  TITLE = {Animatable Neural Radiance Fields from Monocular RGB Video},
  EPRINT = {2106.13629v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present animatable neural radiance fields for detailed human avatarcreation from monocular videos. Our approach extends neural radiance fields(NeRF) to the dynamic scenes with human movements via introducing explicitpose-guided deformation while learning the scene representation network. Inparticular, we estimate the human pose for each frame and learn a constantcanonical space for the detailed human template, which enables natural shapedeformation from the observation space to the canonical space under theexplicit control of the pose parameters. To compensate for inaccurate poseestimation, we introduce the pose refinement strategy that updates the initialpose during the learning process, which not only helps to learn more accuratehuman reconstruction but also accelerates the convergence. In experiments weshow that the proposed approach achieves 1) implicit human geometry andappearance reconstruction with high-quality details, 2) photo-realisticrendering of the human from arbitrary views, and 3) animation of the human witharbitrary poses.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.13629v1},
  FILE = {2106.13629v1.pdf}
 }

@article{bergman2021metanlr++,
  AUTHOR = {Alexander W. Bergman and Petr Kellnhofer and Gordon Wetzstein},
  TITLE = {Fast Training of Neural Lumigraph Representations using Meta Learning},
  EPRINT = {2106.14942v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Novel view synthesis is a long-standing problem in machine learning andcomputer vision. Significant progress has recently been made in developingneural scene representations and rendering techniques that synthesizephotorealistic images from arbitrary views. These representations, however, areextremely slow to train and often also slow to render. Inspired by neuralvariants of image-based rendering, we develop a new neural rendering approachwith the goal of quickly learning a high-quality representation which can alsobe rendered in real-time. Our approach, MetaNLR++, accomplishes this by using aunique combination of a neural shape representation and 2D CNN-based imagefeature extraction, aggregation, and re-projection. To push representationconvergence times down to minutes, we leverage meta learning to learn neuralshape and image feature priors which accelerate training. The optimized shapeand image features can then be extracted using traditional graphics techniquesand rendered in real time. We show that MetaNLR++ achieves similar or betternovel view synthesis results in a fraction of the time that competing methodsrequire.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.14942v1},
  FILE = {2106.14942v1.pdf}
 }

@article{wu2021irem,
  AUTHOR = {Qing Wu and Yuwei Li and Lan Xu and Ruiming Feng and Hongjiang Wei and Qing Yang and Boliang Yu and Xiaozhao Liu and Jingyi Yu and Yuyao Zhang},
  TITLE = {IREM: High-Resolution Magnetic Resonance (MR) Image Reconstruction viaImplicit Neural Representation},
  EPRINT = {2106.15097v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.IV},
  ABSTRACT = {For collecting high-quality high-resolution (HR) MR image, we propose a novelimage reconstruction network named IREM, which is trained on multiplelow-resolution (LR) MR images and achieve an arbitrary up-sampling rate for HRimage reconstruction. In this work, we suppose the desired HR image as animplicit continuous function of the 3D image spatial coordinate and thethick-slice LR images as several sparse discrete samplings of this function.Then the super-resolution (SR) task is to learn the continuous volumetricfunction from a limited observations using an fully-connected neural networkcombined with Fourier feature positional encoding. By simply minimizing theerror between the network prediction and the acquired LR image intensity acrosseach imaging plane, IREM is trained to represent a continuous model of theobserved tissue anatomy. Experimental results indicate that IREM succeeds inrepresenting high frequency image feature, and in real scene data collection,IREM reduces scan time and achieves high-quality high-resolution MR imaging interms of SNR and local image detail.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.15097v1},
  FILE = {2106.15097v1.pdf}
 }

@article{zheng2021rethinking,
  AUTHOR = {Jianqiao Zheng and Sameera Ramasinghe and Simon Lucey},
  TITLE = {Rethinking Positional Encoding},
  EPRINT = {2107.02561v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.LG},
  ABSTRACT = {It is well noted that coordinate based MLPs benefit greatly -- in terms ofpreserving high-frequency information -- through the encoding of coordinatepositions as an array of Fourier features. Hitherto, the rationale for theeffectiveness of these positional encodings has been solely studied through aFourier lens. In this paper, we strive to broaden this understanding by showingthat alternative non-Fourier embedding functions can indeed be used forpositional encoding. Moreover, we show that their performance is entirelydetermined by a trade-off between the stable rank of the embedded matrix andthe distance preservation between embedded coordinates. We further establishthat the now ubiquitous Fourier feature mapping of position is a special casethat fulfills these conditions. Consequently, we present a more general theoryto analyze positional encoding in terms of shifted basis functions. To thisend, we develop the necessary theoretical formulae and empirically verify thatour theoretical claims hold in practice. Codes available athttps://github.com/osiriszjq/Rethinking-positional-encoding.},
  YEAR = {2021},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2107.02561v2},
  FILE = {2107.02561v2.pdf}
 }

@article{deng2021dsnerf,
  AUTHOR = {Kangle Deng and Andrew Liu and Jun-Yan Zhu and Deva Ramanan},
  TITLE = {Depth-supervised NeRF: Fewer Views and Faster Training for Free},
  EPRINT = {2107.02791v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {One common failure mode of Neural Radiance Field (NeRF) models is fittingincorrect geometries when given an insufficient number of input views. Wepropose DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learningneural radiance fields that takes advantage of readily-available depthsupervision. Our key insight is that sparse depth supervision can be used toregularize the learned geometry, a crucial component for effectively renderingnovel views using NeRF. We exploit the fact that current NeRF pipelines requireimages with known camera poses that are typically estimated by runningstructure-from-motion (SFM). Crucially, SFM also produces sparse 3D points thatcan be used as ``free" depth supervision during training: we simply add a lossto ensure that depth rendered along rays that intersect these 3D points isclose to the observed depth. We find that DS-NeRF can render more accurateimages given fewer training views while training 2-6x faster. With only twotraining views on real-world images, DS-NeRF significantly outperforms NeRF aswell as other sparse-view variants. We show that our loss is compatible withthese NeRF models, demonstrating that depth is a cheap and easily digestiblesupervisory signal. Finally, we show that DS-NeRF supports other types of depthsupervision such as scanned depth sensors and RGBD reconstruction outputs.},
  YEAR = {2021},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2107.02791v1},
  FILE = {2107.02791v1.pdf}
 }

@article{li20213d,
  AUTHOR = {Yunzhu Li and Shuang Li and Vincent Sitzmann and Pulkit Agrawal and Antonio Torralba},
  TITLE = {3D Neural Scene Representations for Visuomotor Control},
  EPRINT = {2107.04004v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.RO},
  ABSTRACT = {Humans have a strong intuitive understanding of the 3D environment around us.The mental model of the physics in our brain applies to objects of differentmaterials and enables us to perform a wide range of manipulation tasks that arefar beyond the reach of current robots. In this work, we desire to learn modelsfor dynamic 3D scenes purely from 2D visual observations. Our model combinesNeural Radiance Fields (NeRF) and time contrastive learning with anautoencoding framework, which learns viewpoint-invariant 3D-aware scenerepresentations. We show that a dynamics model, constructed over the learnedrepresentation space, enables visuomotor control for challenging manipulationtasks involving both rigid bodies and fluids, where the target is specified ina viewpoint different from what the robot operates on. When coupled with anauto-decoding framework, it can even support goal specification from cameraviewpoints that are outside the training distribution. We further demonstratethe richness of the learned 3D dynamics model by performing future predictionand novel view synthesis. Finally, we provide detailed ablation studiesregarding different system designs and qualitative analysis of the learnedrepresentations.},
  YEAR = {2021},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2107.04004v1},
  FILE = {2107.04004v1.pdf}
 }

@article{Pan:21,
  AUTHOR = {Hujie Pan and Di Xiao and Fuhao Zhang and Xuesong Li and Min Xu},
  JOURNAL = {Opt. Express},
  KEYWORDS = {Computational imaging; Computed tomography; Light fields; Light propagation; Neural networks; Propagation methods},
  NUMBER = {15},
  PAGES = {23682--23700},
  PUBLISHER = {OSA},
  TITLE = {Adaptive weight matrix and phantom intensity learning for computed tomography of chemiluminescence},
  VOLUME = {29},
  MONTH = {Jul},
  YEAR = {2021},
  URL = {http://www.opticsexpress.org/abstract.cfm?URI=oe-29-15-23682},
  DOI = {10.1364/OE.427459},
  ABSTRACT = {Classic algebraic reconstruction technique (ART) for computed tomography requires pre-determined weights of the voxels for the projected pixel values to build the equations. However, such weights cannot be accurately obtained in the application of chemiluminescence measurements due to the high physical complexity and computation resources required. Moreover, streaks arise in the results from ART method especially with imperfect projections. In this study, we propose a semi-case-wise learning-based method named Weight Encode Reconstruction Network (WERNet) to co-learn the target phantom intensities and the adaptive weight matrix of the case without labeling the target voxel set and thus offers a more applicable solution for computed tomography problems. Both numerical and experimental validations were conducted to evaluate the algorithm. In the numerical test, with the help of gradient normalization, the WERNet reconstructed voxel set with a high accuracy and showed a higher capability of denoising compared to the classic ART methods. In the experimental test, WERNet produces comparable results to the ART method while having a better performance in avoiding the streaks. Furthermore, with the adaptive weight matrix, WERNet is not sensitive to the ensemble intensity of the projection which shows much better robustness than ART method.}
 }

@article{yu2021uorf,
  AUTHOR = {Hong-Xing Yu and Leonidas J. Guibas and Jiajun Wu},
  TITLE = {Unsupervised Discovery of Object Radiance Fields},
  EPRINT = {2107.07905v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We study the problem of inferring an object-centric scene representation froma single image, aiming to derive a representation that explains the imageformation process, captures the scene's 3D nature, and is learned withoutsupervision. Most existing methods on scene decomposition lack one or more ofthese characteristics, due to the fundamental challenge in integrating thecomplex 3D-to-2D image formation process into powerful inference schemes likedeep networks. In this paper, we propose unsupervised discovery of ObjectRadiance Fields (uORF), integrating recent progresses in neural 3D scenerepresentations and rendering with deep inference networks for unsupervised 3Dscene decomposition. Trained on multi-view RGB images without annotations, uORFlearns to decompose complex scenes with diverse, textured background from asingle image. We show that uORF performs well on unsupervised 3D scenesegmentation, novel view synthesis, and scene editing on three datasets.},
  YEAR = {2021},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2107.07905v1},
  FILE = {2107.07905v1.pdf}
  }

@article{zobeidi2021a,
  AUTHOR = {Ehsan Zobeidi and Nikolay Atanasov},
  TITLE = {A Deep Signed Directional Distance Function for Object ShapeRepresentation},
  EPRINT = {2107.11024v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural networks that map 3D coordinates to signed distance function (SDF) oroccupancy values have enabled high-fidelity implicit representations of objectshape. This paper develops a new shape model that allows synthesizing noveldistance views by optimizing a continuous signed directional distance function(SDDF). Similar to deep SDF models, our SDDF formulation can represent wholecategories of shapes and complete or interpolate across shapes from partialinput data. Unlike an SDF, which measures distance to the nearest surface inany direction, an SDDF measures distance in a given direction. This allowstraining an SDDF model without 3D shape supervision, using only distancemeasurements, readily available from depth camera or Lidar sensors. Our modelalso removes post-processing steps like surface extraction or rendering bydirectly predicting distance at arbitrary locations and viewing directions.Unlike deep view-synthesis techniques, such as Neural Radiance Fields, whichtrain high-capacity black-box models, our model encodes by construction theproperty that SDDF values decrease linearly along the viewing direction. Thisstructure constraint not only results in dimensionality reduction but alsoprovides analytical confidence about the accuracy of SDDF predictions,regardless of the distance to the object surface.},
  YEAR = {2021},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2107.11024v1},
  FILE = {2107.11024v1.pdf}
  }

@article{ramon2021h3dnet,
  AUTHOR = {Eduard Ramon and Gil Triginer and Janna Escur and Albert Pumarola and Jaime Garcia and Xavier Giro-i-Nieto and Francesc Moreno-Noguer},
  TITLE = {H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction},
  EPRINT = {2107.12512v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent learning approaches that implicitly represent surface geometry usingcoordinate-based neural representations have shown impressive results in theproblem of multi-view 3D reconstruction. The effectiveness of these techniquesis, however, subject to the availability of a large number (several tens) ofinput views of the scene, and computationally demanding optimizations. In thispaper, we tackle these limitations for the specific problem of few-shot full 3Dhead reconstruction, by endowing coordinate-based representations with aprobabilistic shape prior that enables faster convergence and bettergeneralization when using few input images (down to three). First, we learn ashape model of 3D heads from thousands of incomplete raw scans using implicitrepresentations. At test time, we jointly overfit two coordinate-based neuralnetworks to the scene, one modeling the geometry and another estimating thesurface radiance, using implicit differentiable rendering. We devise atwo-stage optimization strategy in which the learned prior is used toinitialize and constrain the geometry during an initial optimization phase.Then, the prior is unfrozen and fine-tuned to the scene. By doing this, weachieve high-fidelity head reconstructions, including hair and shoulders, andwith a high level of detail that consistently outperforms both state-of-the-art3D Morphable Models methods in the few-shot scenario, and non-parametricmethods when large sets of views are available.},
  YEAR = {2021},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2107.12512v1},
  FILE = {2107.12512v1.pdf}
  }

@article{nam2021neural,
  AUTHOR = {Seonghyeon Nam and Marcus A. Brubaker and Michael S. Brown},
  TITLE = {Neural Image Representations for Multi-Image Fusion and Layer Separation},
  EPRINT = {2108.01199v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose a framework for aligning and fusing multiple images into a singlecoordinate-based neural representations. Our framework targets burst imagesthat have misalignment due to camera ego motion and small changes in the scene.We describe different strategies for alignment depending on the assumption ofthe scene motion, namely, perspective planar (i.e., homography), optical flowwith minimal scene change, and optical flow with notable occlusion anddisocclusion. Our framework effectively combines the multiple inputs into asingle neural implicit function without the need for selecting one of theimages as a reference frame. We demonstrate how to use this multi-frame fusionframework for various layer separation tasks.},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.01199v2},
  FILE = {2108.01199v2.pdf}
  }

@inproceedings{khademi2021cylindrical,
 author = {Khademi, Wesley and Ventura, Jonathan},
 title = {View Synthesis In Casually Captured Scenes Using a Cylindrical Neural Radiance Field With Exposure Compensation},
 year = {2021},
 isbn = {9781450383714},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/3450618.3469147},
 doi = {10.1145/3450618.3469147},
 abstract = { We extend Neural Radiance Fields (NeRF) with a cylindrical parameterization that
 enables rendering photorealistic novel views of 360 outward facing scenes. We further
 introduce a learned exposure compensation parameter to account for the varying exposure
 in training images that may occur from casually capturing a scene. We evaluate our
 method on a variety of 360 casually captured scenes.},
 booktitle = {ACM SIGGRAPH 2021 Posters},
 articleno = {28},
 numpages = {2},
 location = {Virtual Event, USA},
 series = {SIGGRAPH '21}
 }

@article{athar2021flameinnerf,
  AUTHOR = {ShahRukh Athar and Zhixin Shu and Dimitris Samaras},
  TITLE = {FLAME-in-NeRF : Neural control of Radiance Fields for Free View FaceAnimation},
  EPRINT = {2108.04913v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {This paper presents a neural rendering method for controllable portrait videosynthesis. Recent advances in volumetric neural rendering, such as neuralradiance fields (NeRF), has enabled the photorealistic novel view synthesis ofstatic scenes with impressive results. However, modeling dynamic andcontrollable objects as part of a scene with such scene representations isstill challenging. In this work, we design a system that enables both novelview synthesis for portrait video, including the human subject and the scenebackground, and explicit control of the facial expressions through alow-dimensional expression representation. We leverage the expression space ofa 3D morphable face model (3DMM) to represent the distribution of human facialexpressions, and use it to condition the NeRF volumetric function. Furthermore,we impose a spatial prior brought by 3DMM fitting to guide the network to learndisentangled control for scene appearance and facial actions. We demonstratethe effectiveness of our method on free view synthesis of portrait videos withexpression controls. To train a scene, our method only requires a short videoof a subject captured by a mobile device.},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.04913v1},
  FILE = {2108.04913v1.pdf}
  }

@article{chatziagapi2021sider,
  AUTHOR = {Aggelina Chatziagapi and ShahRukh Athar and Francesc Moreno-Noguer and Dimitris Samaras},
  TITLE = {SIDER: Single-Image Neural Optimization for Facial Geometric DetailRecovery},
  EPRINT = {2108.05465v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present SIDER(Single-Image neural optimization for facial geometric DEtailRecovery), a novel photometric optimization method that recovers detailedfacial geometry from a single image in an unsupervised manner. Inspired byclassical techniques of coarse-to-fine optimization and recent advances inimplicit neural representations of 3D shape, SIDER combines a geometry priorbased on statistical models and Signed Distance Functions (SDFs) to recoverfacial details from single images. First, it estimates a coarse geometry usinga morphable model represented as an SDF. Next, it reconstructs facial geometrydetails by optimizing a photometric loss with respect to the ground truthimage. In contrast to prior work, SIDER does not rely on any dataset priors anddoes not require additional supervision from multiple views, lighting changesor ground truth 3D shape. Extensive qualitative and quantitative evaluationdemonstrates that our method achieves state-of-the-art on facial geometricdetail recovery, using only a single in-the-wild image.},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.05465v1},
  FILE = {2108.05465v1.pdf}
  }

@article{yan2021continual,
  AUTHOR = {Zike Yan and Yuxin Tian and Xuesong Shi and Ping Guo and Peng Wang and Hongbin Zha},
  TITLE = {Continual Neural Mapping: Learning An Implicit Scene Representation fromSequential Observations},
  EPRINT = {2108.05851v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent advances have enabled a single neural network to serve as an implicitscene representation, establishing the mapping function between spatialcoordinates and scene properties. In this paper, we make a further step towardscontinual learning of the implicit scene representation directly fromsequential observations, namely Continual Neural Mapping. The proposed problemsetting bridges the gap between batch-trained implicit neural representationsand commonly used streaming data in robotics and vision communities. Weintroduce an experience replay approach to tackle an exemplary task ofcontinual neural mapping: approximating a continuous signed distance function(SDF) from sequential depth images as a scene geometry representation. We showfor the first time that a single network can represent scene geometry over timecontinually without catastrophic forgetting, while achieving promisingtrade-offs between accuracy and efficiency.},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.05851v1},
  FILE = {2108.05851v1.pdf}
  }

@article{tiwari2021neuralgif,
  AUTHOR = {Garvita Tiwari and Nikolaos Sarafianos and Tony Tung and Gerard Pons-Moll},
  TITLE = {Neural-GIF: Neural Generalized Implicit Functions for Animating Peoplein Clothing},
  EPRINT = {2108.08807v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present Neural Generalized Implicit Functions(Neural-GIF), to animatepeople in clothing as a function of the body pose. Given a sequence of scans ofa subject in various poses, we learn to animate the character for new poses.Existing methods have relied on template-based representations of the humanbody (or clothing). However such models usually have fixed and limitedresolutions, require difficult data pre-processing steps and cannot be usedwith complex clothing. We draw inspiration from template-based methods, whichfactorize motion into articulation and non-rigid deformation, but generalizethis concept for implicit shape learning to obtain a more flexible model. Welearn to map every point in the space to a canonical space, where a learneddeformation field is applied to model non-rigid effects, before evaluating thesigned distance field. Our formulation allows the learning of complex andnon-rigid deformations of clothing and soft tissue, without computing atemplate registration as it is common with current approaches. Neural-GIF canbe trained on raw 3D scans and reconstructs detailed complex surface geometryand deformations. Moreover, the model can generalize to new poses. We evaluateour method on a variety of characters from different public datasets in diverseclothing styles and show significant improvements over baseline methods,quantitatively and qualitatively. We also extend our model to multiple shapesetting. To stimulate further research, we will make the model, code and datapublicly available at: https://virtualhumans.mpi-inf.mpg.de/neuralgif/},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.08807v2},
  FILE = {2108.08807v2.pdf}
  }

@article{atzmon2021augmenting,
  AUTHOR = {Matan Atzmon and David Novotny and Andrea Vedaldi and Yaron Lipman},
  TITLE = {Augmenting Implicit Neural Shape Representations with ExplicitDeformation Fields},
  EPRINT = {2108.08931v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicit neural representation is a recent approach to learn shapecollections as zero level-sets of neural networks, where each shape isrepresented by a latent code. So far, the focus has been shape reconstruction,while shape generalization was mostly left to generic encoder-decoder orauto-decoder regularization.In this paper we advocate deformation-aware regularization for implicitneural representations, aiming at producing plausible deformations as latentcode changes. The challenge is that implicit representations do not capturecorrespondences between different shapes, which makes it difficult to representand regularize their deformations. Thus, we propose to pair the implicitrepresentation of the shapes with an explicit, piecewise linear deformationfield, learned as an auxiliary function. We demonstrate that, by regularizingthese deformation fields, we can encourage the implicit neural representationto induce natural deformations in the learned shape space, such asas-rigid-as-possible deformations.},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.08931v1},
  FILE = {2108.08931v1.pdf}
  }

@article{wulearning,
  ABSTRACT = {Different from popular neural networks using quasiconvex activations, non-monotonic networks activated by periodic nonlinearities have emerged as a more competitive paradigm, offering revolutionary benefits: 1) compactly characterizing highfrequency patterns; 2) precisely representing highorder derivatives. Nevertheless, they are also wellknown for being hard to train, due to easily overfitting dissonant noise and only allowing for tiny architectures (shallower than 5 layers). The fundamental bottleneck is that the},
  AUTHOR = {Wu, Zheng-Fan and Xue, Hui and Bai, Weimin},
  PUB_YEAR = {NA},
  TITLE = {Learning Deeper Non-Monotonic Networks by Softly Transferring Solution Space},
  VENUE = {NA}
 }

@article{reed2021implicit,
  ABSTRACT = {Synthetic aperture sonar (SAS) image resolution is constrained by waveform bandwidth and array geometry. Specifically, the waveform bandwidth determines a point spread function (PSF) that blurs the locations of point scatterers in the scene. In theory, deconvolving the reconstructed SAS image with the scene PSF restores the original distribution of scatterers and yields sharper reconstructions. However, deconvolution is an ill-posed operation that is highly sensitive to noise. In this work, we leverage implicit neural representations (INRs)},
  AUTHOR = {Reed, Albert and Blanford, Thomas and Brown, Daniel C and Jayasuriya, Suren},
  PUB_YEAR = {NA},
  TITLE = {Implicit Neural Representations for Deconvolving SAS Images},
  VENUE = {NA}
 }

@article{zhang2021learning,
  AUTHOR = {Jingyang Zhang and Yao Yao and Long Quan},
  TITLE = {Learning Signed Distance Field for Multi-view Surface Reconstruction},
  EPRINT = {2108.09964v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Recent works on implicit neural representations have shown promising resultsfor multi-view surface reconstruction. However, most approaches are limited torelatively simple geometries and usually require clean object masks forreconstructing complex and concave objects. In this work, we introduce a novelneural surface reconstruction framework that leverages the knowledge of stereomatching and feature consistency to optimize the implicit surfacerepresentation. More specifically, we apply a signed distance field (SDF) and asurface light field to represent the scene geometry and appearancerespectively. The SDF is directly supervised by geometry from stereo matching,and is refined by optimizing the multi-view feature consistency and thefidelity of rendered images. Our method is able to improve the robustness ofgeometry estimation and support reconstruction of complex scene topologies.Extensive experiments have been conducted on DTU, EPFL and Tanks and Templesdatasets. Compared to previous state-of-the-art methods, our method achievesbetter mesh reconstruction in wide open scenes without masks as input.},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.09964v1},
  FILE = {2108.09964v1.pdf}
  }

@article{shen2021nerp,
  AUTHOR = {Liyue Shen and John Pauly and Lei Xing},
  TITLE = {NeRP: Implicit Neural Representation Learning with Prior Embedding for Sparsely Sampled Image Reconstruction},
  EPRINT = {2108.10991v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {eess.IV},
  ABSTRACT = {Image reconstruction is an inverse problem that solves for a computationalimage based on sampled sensor measurement. Sparsely sampled imagereconstruction poses addition challenges due to limited measurements. In thiswork, we propose an implicit Neural Representation learning methodology withPrior embedding (NeRP) to reconstruct a computational image from sparselysampled measurements. The method differs fundamentally from previous deeplearning-based image reconstruction approaches in that NeRP exploits theinternal information in an image prior, and the physics of the sparsely sampledmeasurements to produce a representation of the unknown subject. No large-scaledata is required to train the NeRP except for a prior image and sparselysampled measurements. In addition, we demonstrate that NeRP is a generalmethodology that generalizes to different imaging modalities such as CT andMRI. We also show that NeRP can robustly capture the subtle yet significantimage changes required for assessing tumor progression.},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.10991v1},
  FILE = {2108.10991v1.pdf}
  }

@article{jeong2021self,
  ABSTRACT = {In this work, we propose a camera self-calibration algorithm for generic cameras with arbitrary non-linear distortions. We jointly learn the geometry of the scene and the accurate camera parameters without any calibration objects. Our camera model consists of a pinhole model, a fourth order radial distortion, and a generic noise model that can learn arbitrary non-linear camera distortions. While traditional self-calibration algorithms mostly rely on geometric constraints, we additionally incorporate photometric consistency. This requires},
  AUTHOR = {Jeong, Yoonwoo and Ahn, Seokjun and Choy, Christopher and Anandkumar, Animashree and Cho, Minsu and Park, Jaesik},
  JOURNAL = {arXiv preprint arXiv:2108.13826},
  PUB_YEAR = {2021},
  TITLE = {Self-Calibrating Neural Radiance Fields},
  VENUE = {arXiv preprint arXiv }
 }

@article{zheng2020neural,
  ABSTRACT = {Modern 3D printers are capable of printing large-size light-field displays at high-resolutions. However, optimizing such displays in full 3D volume for a given light-field imagery is still a challenging task. Existing light field displays optimize over relatively small resolutions using},
  AUTHOR = {Zheng, Quan and Babaei, Vahid and Wetzstein, Gordon and Seidel, Hans-Peter and Zwicker, Matthias and Singh, Gurprit},
  JOURNAL = {ACM Transactions on Graphics (TOG)},
  NUMBER = {6},
  PAGES = {1--12},
  PUB_YEAR = {2020},
  PUBLISHER = {ACM New York, NY, USA},
  TITLE = {Neural light field 3D printing},
  VENUE = {ACM Transactions on ...},
  VOLUME = {39}
 }

@article{alldieck2021imghum,
  AUTHOR = {Thiemo Alldieck and Hongyi Xu and Cristian Sminchisescu},
  TITLE = {imGHUM: Implicit Generative Models of 3D Human Shape and ArticulatedPose},
  EPRINT = {2108.10842v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present imGHUM, the first holistic generative model of 3D human shape andarticulated pose, represented as a signed distance function. In contrast toprior work, we model the full human body implicitly as a functionzero-level-set and without the use of an explicit template mesh. We propose anovel network architecture and a learning paradigm, which make it possible tolearn a detailed implicit generative model of human pose, shape, and semantics,on par with state-of-the-art mesh-based models. Our model features desireddetail for human models, such as articulated pose including hand motion andfacial expressions, a broad spectrum of shape variations, and can be queried atarbitrary resolutions and spatial locations. Additionally, our model hasattached spatial semantics making it straightforward to establishcorrespondences between different shape instances, thus enabling applicationsthat are difficult to tackle using classical implicit representations. Inextensive experiments, we demonstrate the model accuracy and its applicabilityto current research problems.},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.10842v1},
  FILE = {2108.10842v1.pdf}
 }

@article{karunratanakul2020grasping field,
  AUTHOR = {Korrawe Karunratanakul and Jinlong Yang and Yan Zhang and Michael Black and Krikamol Muandet and Siyu Tang},
  TITLE = {Grasping Field: Learning Implicit Representations for Human Grasps},
  EPRINT = {2008.04451v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Robotic grasping of house-hold objects has made remarkable progress in recentyears. Yet, human grasps are still difficult to synthesize realistically. Thereare several key reasons: (1) the human hand has many degrees of freedom (morethan robotic manipulators); (2) the synthesized hand should conform to thesurface of the object; and (3) it should interact with the object in asemantically and physically plausible manner. To make progress in thisdirection, we draw inspiration from the recent progress on learning-basedimplicit representations for 3D object reconstruction. Specifically, we proposean expressive representation for human grasp modelling that is efficient andeasy to integrate with deep neural networks. Our insight is that every point ina three-dimensional space can be characterized by the signed distances to thesurface of the hand and the object, respectively. Consequently, the hand, theobject, and the contact area can be represented by implicit surfaces in acommon space, in which the proximity between the hand and the object can bemodelled explicitly. We name this 3D to 2D mapping as Grasping Field,parameterize it with a deep neural network, and learn it from data. Wedemonstrate that the proposed grasping field is an effective and expressiverepresentation for human grasp generation. Specifically, our generative modelis able to synthesize high-quality human grasps, given only on a 3D objectpoint cloud. The extensive experiments demonstrate that our generative modelcompares favorably with a strong baseline and approaches the level of naturalhuman grasps. Our method improves the physical plausibility of the hand-objectcontact reconstruction and achieves comparable performance for 3D handreconstruction compared to state-of-the-art methods.},
  YEAR = {2020},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2008.04451v3},
  FILE = {2008.04451v3.pdf}
 }

@article{corona2021smplicit,
  AUTHOR = {Enric Corona and Albert Pumarola and Guillem Alenya and Gerard Pons-Moll and Francesc Moreno-Noguer},
  TITLE = {SMPLicit: Topology-aware Generative Model for Clothed People},
  EPRINT = {2103.06871v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper we introduce SMPLicit, a novel generative model to jointlyrepresent body pose, shape and clothing geometry. In contrast to existinglearning-based approaches that require training specific models for each typeof garment, SMPLicit can represent in a unified manner different garmenttopologies (e.g. from sleeveless tops to hoodies and to open jackets), whilecontrolling other properties like the garment size or tightness/looseness. Weshow our model to be applicable to a large variety of garments includingT-shirts, hoodies, jackets, shorts, pants, skirts, shoes and even hair. Therepresentation flexibility of SMPLicit builds upon an implicit modelconditioned with the SMPL human body parameters and a learnable latent spacewhich is semantically interpretable and aligned with the clothing attributes.The proposed model is fully differentiable, allowing for its use into largerend-to-end trainable systems. In the experimental section, we demonstrateSMPLicit can be readily used for fitting 3D scans and for 3D reconstruction inimages of dressed people. In both cases we are able to go beyond state of theart, by retrieving complex garment geometries, handling situations withmultiple clothing layers and providing a tool for easy outfit editing. Tostimulate further research in this direction, we will make our code and modelpublicly available at http://www.iri.upc.edu/people/ecorona/smplicit/.},
  YEAR = {2021},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2103.06871v2},
  FILE = {2103.06871v2.pdf}
 }

@article{yang2021s3,
  AUTHOR = {Ze Yang and Shenlong Wang and Sivabalan Manivasagam and Zeng Huang and Wei-Chiu Ma and Xinchen Yan and Ersin Yumer and Raquel Urtasun},
  TITLE = {S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling},
  EPRINT = {2101.06571v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Constructing and animating humans is an important component for buildingvirtual worlds in a wide variety of applications such as virtual reality orrobotics testing in simulation. As there are exponentially many variations ofhumans with different shape, pose and clothing, it is critical to developmethods that can automatically reconstruct and animate humans at scale fromreal world data. Towards this goal, we represent the pedestrian's shape, poseand skinning weights as neural implicit functions that are directly learnedfrom data. This representation enables us to handle a wide variety of differentpedestrian shapes and poses without explicitly fitting a human parametric bodymodel, allowing us to handle a wider range of human geometries and topologies.We demonstrate the effectiveness of our approach on various datasets and showthat our reconstructions outperform existing state-of-the-art methods.Furthermore, our re-animation experiments show that we can generate 3D humananimations at scale from a single RGB image (and/or an optional LiDAR sweep) asinput.},
  YEAR = {2021},
  MONTH = {Jan},
  URL = {http://arxiv.org/abs/2101.06571v1},
  FILE = {2101.06571v1.pdf}
 }

@article{mihajlovic2021leap,
  AUTHOR = {Marko Mihajlovic and Yan Zhang and Michael J. Black and Siyu Tang},
  TITLE = {LEAP: Learning Articulated Occupancy of People},
  EPRINT = {2104.06849v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Substantial progress has been made on modeling rigid 3D objects using deepimplicit representations. Yet, extending these methods to learn neural modelsof human shape is still in its infancy. Human bodies are complex and the keychallenge is to learn a representation that generalizes such that it canexpress body shape deformations for unseen subjects in unseen,highly-articulated, poses. To address this challenge, we introduce LEAP(LEarning Articulated occupancy of People), a novel neural occupancyrepresentation of the human body. Given a set of bone transformations (i.e.joint locations and rotations) and a query point in space, LEAP first maps thequery point to a canonical space via learned linear blend skinning (LBS)functions and then efficiently queries the occupancy value via an occupancynetwork that models accurate identity- and pose-dependent deformations in thecanonical space. Experiments show that our canonicalized occupancy estimationwith the learned LBS functions greatly improves the generalization capabilityof the learned occupancy representation across various human shapes and poses,outperforming existing solutions in all settings.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.06849v1},
  FILE = {2104.06849v1.pdf}
 }

@article{deng2019nasa,
  AUTHOR = {Boyang Deng and JP Lewis and Timothy Jeruzalski and Gerard Pons-Moll and Geoffrey Hinton and Mohammad Norouzi and Andrea Tagliasacchi},
  TITLE = {NASA: Neural Articulated Shape Approximation},
  EPRINT = {1912.03207v4},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Efficient representation of articulated objects such as human bodies is animportant problem in computer vision and graphics. To efficiently simulatedeformation, existing approaches represent 3D objects using polygonal meshesand deform them using skinning techniques. This paper introduces neuralarticulated shape approximation (NASA), an alternative framework that enablesefficient representation of articulated deformable objects using neuralindicator functions that are conditioned on pose. Occupancy testing using NASAis straightforward, circumventing the complexity of meshes and the issue ofwater-tightness. We demonstrate the effectiveness of NASA for 3D trackingapplications, and discuss other potential extensions.},
  YEAR = {2019},
  MONTH = {Dec},
  URL = {http://arxiv.org/abs/1912.03207v4},
  FILE = {1912.03207v4.pdf}
 }

@article{wang2021locally,
  AUTHOR = {Shaofei Wang and Andreas Geiger and Siyu Tang},
  TITLE = {Locally Aware Piecewise Transformation Fields for 3D Human MeshRegistration},
  EPRINT = {2104.08160v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Registering point clouds of dressed humans to parametric human models is achallenging task in computer vision. Traditional approaches often rely onheavily engineered pipelines that require accurate manual initialization ofhuman poses and tedious post-processing. More recently, learning-based methodsare proposed in hope to automate this process. We observe that poseinitialization is key to accurate registration but existing methods often failto provide accurate pose initialization. One major obstacle is that, regressingjoint rotations from point clouds or images of humans is still verychallenging. To this end, we propose novel piecewise transformation fields(PTF), a set of functions that learn 3D translation vectors to map any querypoint in posed space to its correspond position in rest-pose space. We combinePTF with multi-class occupancy networks, obtaining a novel learning-basedframework that learns to simultaneously predict shape and per-pointcorrespondences between the posed space and the canonical space for clothedhuman. Our key insight is that the translation vector for each query point canbe effectively estimated using the point-aligned local features; consequently,rigid per bone transformations and joint rotations can be obtained efficientlyvia a least-square fitting given the estimated point correspondences,circumventing the challenging task of directly regressing joint rotations fromneural networks. Furthermore, the proposed PTF facilitate canonicalizedoccupancy estimation, which greatly improves generalization capability andresults in more accurate surface reconstruction with only half of theparameters compared with the state-of-the-art. Both qualitative andquantitative studies show that fitting parametric models with poses initializedby our network results in much better registration quality, especially forextreme poses.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.08160v1},
  FILE = {2104.08160v1.pdf}
 }

@article{bhatnagar2020ipnet,
  AUTHOR = {Bharat Lal Bhatnagar and Cristian Sminchisescu and Christian Theobalt and Gerard Pons-Moll},
  TITLE = {Combining Implicit Function Learning and Parametric Models for 3D HumanReconstruction},
  EPRINT = {2007.11432v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicit functions represented as deep learning approximations are powerfulfor reconstructing 3D surfaces. However, they can only produce static surfacesthat are not controllable, which provides limited ability to modify theresulting model by editing its pose or shape parameters. Nevertheless, suchfeatures are essential in building flexible models for both computer graphicsand computer vision. In this work, we present methodology that combinesdetail-rich implicit functions and parametric representations in order toreconstruct 3D models of people that remain controllable and accurate even inthe presence of clothing. Given sparse 3D point clouds sampled on the surfaceof a dressed person, we use an Implicit Part Network (IP-Net)to jointly predictthe outer 3D surface of the dressed person, the and inner body surface, and thesemantic correspondences to a parametric body model. We subsequently usecorrespondences to fit the body model to our inner surface and then non-rigidlydeform it (under a parametric body + displacement model) to the outer surfacein order to capture garment, face and hair detail. In quantitative andqualitative experiments with both full body data and hand scans we show thatthe proposed methodology generalizes, and is effective even given incompletepoint clouds collected from single-view depth images. Our models and code canbe downloaded from http://virtualhumans.mpi-inf.mpg.de/ipnet.},
  YEAR = {2020},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2007.11432v1},
  FILE = {2007.11432v1.pdf}
 }

@article{bhatnagar2020loopreg,
  AUTHOR = {Bharat Lal Bhatnagar and Cristian Sminchisescu and Christian Theobalt and Gerard Pons-Moll},
  TITLE = {LoopReg: Self-supervised Learning of Implicit Surface Correspondences,Pose and Shape for 3D Human Mesh Registration},
  EPRINT = {2010.12447v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We address the problem of fitting 3D human models to 3D scans of dressedhumans. Classical methods optimize both the data-to-model correspondences andthe human model parameters (pose and shape), but are reliable only wheninitialized close to the solution. Some methods initialize the optimizationbased on fully supervised correspondence predictors, which is notdifferentiable end-to-end, and can only process a single scan at a time. Ourmain contribution is LoopReg, an end-to-end learning framework to register acorpus of scans to a common 3D human model. The key idea is to create aself-supervised loop. A backward map, parameterized by a Neural Network,predicts the correspondence from every scan point to the surface of the humanmodel. A forward map, parameterized by a human model, transforms thecorresponding points back to the scan based on the model parameters (pose andshape), thus closing the loop. Formulating this closed loop is notstraightforward because it is not trivial to force the output of the NN to beon the surface of the human model - outside this surface the human model is noteven defined. To this end, we propose two key innovations. First, we define thecanonical surface implicitly as the zero level set of a distance field in R3,which in contrast to morecommon UV parameterizations, does not require cuttingthe surface, does not have discontinuities, and does not induce distortion.Second, we diffuse the human model to the 3D domain R3. This allows to map theNN predictions forward,even when they slightly deviate from the zero level set.Results demonstrate that we can train LoopRegmainly self-supervised - followinga supervised warm-start, the model becomes increasingly more accurate asadditional unlabelled raw scans are processed. Our code and pre-trained modelscan be downloaded for research.},
  YEAR = {2020},
  MONTH = {Oct},
  NOTE = {NeurIPS 2020},
  URL = {http://arxiv.org/abs/2010.12447v1},
  FILE = {2010.12447v1.pdf}
 }

@article{li2020pifusion,
  AUTHOR = {Zhe Li and Tao Yu and Chuanyu Pan and Zerong Zheng and Yebin Liu},
  TITLE = {Robust 3D Self-portraits in Seconds},
  EPRINT = {2004.02460v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper, we propose an efficient method for robust 3D self-portraitsusing a single RGBD camera. Benefiting from the proposed PIFusion andlightweight bundle adjustment algorithm, our method can generate detailed 3Dself-portraits in seconds and shows the ability to handle subjects wearingextremely loose clothes. To achieve highly efficient and robust reconstruction,we propose PIFusion, which combines learning-based 3D recovery with volumetricnon-rigid fusion to generate accurate sparse partial scans of the subject.Moreover, a non-rigid volumetric deformation method is proposed to continuouslyrefine the learned shape prior. Finally, a lightweight bundle adjustmentalgorithm is proposed to guarantee that all the partial scans can not only"loop" with each other but also remain consistent with the selected live keyobservations. The results and experiments show that the proposed methodachieves more robust and efficient 3D self-portraits compared withstate-of-the-art methods.},
  YEAR = {2020},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2004.02460v1},
  FILE = {2004.02460v1.pdf}
 }

@article{hong2021stereopifu,
  AUTHOR = {Yang Hong and Juyong Zhang and Boyi Jiang and Yudong Guo and Ligang Liu and Hujun Bao},
  TITLE = {StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision},
  EPRINT = {2104.05289v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this paper, we propose StereoPIFu, which integrates the geometricconstraints of stereo vision with implicit function representation of PIFu, torecover the 3D shape of the clothed human from a pair of low-cost rectifiedimages. First, we introduce the effective voxel-aligned features from a stereovision-based network to enable depth-aware reconstruction. Moreover, the novelrelative z-offset is employed to associate predicted high-fidelity human depthand occupancy inference, which helps restore fine-level surface details.Second, a network structure that fully utilizes the geometry information fromthe stereo images is designed to improve the human body reconstruction quality.Consequently, our StereoPIFu can naturally infer the human body's spatiallocation in camera space and maintain the correct relative position ofdifferent parts of the human body, which enables our method to capture humanperformance. Compared with previous works, our StereoPIFu significantlyimproves the robustness, completeness, and accuracy of the clothed humanreconstruction, which is demonstrated by extensive experimental results.},
  YEAR = {2021},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2104.05289v2},
  FILE = {2104.05289v2.pdf}
 }

@article{he2020geopifu,
  AUTHOR = {Tong He and John Collomosse and Hailin Jin and Stefano Soatto},
  TITLE = {Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-viewHuman Reconstruction},
  EPRINT = {2006.08072v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We propose Geo-PIFu, a method to recover a 3D mesh from a monocular colorimage of a clothed person. Our method is based on a deep implicitfunction-based representation to learn latent voxel features using astructure-aware 3D U-Net, to constrain the model in two ways: first, to resolvefeature ambiguities in query point encoding, second, to serve as a coarse humanshape proxy to regularize the high-resolution mesh and encourage global shaperegularity. We show that, by both encoding query points and constraining globalshape using latent voxel features, the reconstruction we obtain for clothedhuman meshes exhibits less shape distortion and improved surface detailscompared to competing methods. We evaluate Geo-PIFu on a recent human meshpublic dataset that is $10 \times$ larger than the private commercial datasetused in PIFu and previous derivative work. On average, we exceed the state ofthe art by $42.7\%$ reduction in Chamfer and Point-to-Surface Distances, and$19.4\%$ reduction in normal estimation errors.},
  YEAR = {2020},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2006.08072v2},
  FILE = {2006.08072v2.pdf}
 }

@article{li2020monoport,
  AUTHOR = {Ruilong Li and Yuliang Xiu and Shunsuke Saito and Zeng Huang and Kyle Olszewski and Hao Li},
  TITLE = {Monocular Real-Time Volumetric Performance Capture},
  EPRINT = {2007.13988v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present the first approach to volumetric performance capture andnovel-view rendering at real-time speed from monocular video, eliminating theneed for expensive multi-view systems or cumbersome pre-acquisition of apersonalized template model. Our system reconstructs a fully textured 3D humanfrom each frame by leveraging Pixel-Aligned Implicit Function (PIFu). WhilePIFu achieves high-resolution reconstruction in a memory-efficient manner, itscomputationally expensive inference prevents us from deploying such a systemfor real-time applications. To this end, we propose a novel hierarchicalsurface localization algorithm and a direct rendering method without explicitlyextracting surface meshes. By culling unnecessary regions for evaluation in acoarse-to-fine manner, we successfully accelerate the reconstruction by twoorders of magnitude from the baseline without compromising the quality.Furthermore, we introduce an Online Hard Example Mining (OHEM) technique thateffectively suppresses failure modes due to the rare occurrence of challengingexamples. We adaptively update the sampling probability of the training databased on the current reconstruction accuracy, which effectively alleviatesreconstruction artifacts. Our experiments and evaluations demonstrate therobustness of our system to various challenging angles, illuminations, poses,and clothing styles. We also show that our approach compares favorably with thestate-of-the-art monocular performance capture. Our proposed approach removesthe need for multi-view studio settings and enables a consumer-accessiblesolution for volumetric capture.},
  YEAR = {2020},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2007.13988v1},
  FILE = {2007.13988v1.pdf}
 }

@article{he2021arch++,
  AUTHOR = {Tong He and Yuanlu Xu and Shunsuke Saito and Stefano Soatto and Tony Tung},
  TITLE = {ARCH++: Animation-Ready Clothed Human Reconstruction Revisited},
  EPRINT = {2108.07845v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present ARCH++, an image-based method to reconstruct 3D avatars witharbitrary clothing styles. Our reconstructed avatars are animation-ready andhighly realistic, in both the visible regions from input views and the unseenregions. While prior work shows great promise of reconstructing animatableclothed humans with various topologies, we observe that there exist fundamentallimitations resulting in sub-optimal reconstruction quality. In this paper, werevisit the major steps of image-based avatar reconstruction and address thelimitations with ARCH++. First, we introduce an end-to-end point based geometryencoder to better describe the semantics of the underlying 3D human body, inreplacement of previous hand-crafted features. Second, in order to address theoccupancy ambiguity caused by topological changes of clothed humans in thecanonical pose, we propose a co-supervising framework with cross-spaceconsistency to jointly estimate the occupancy in both the posed and canonicalspaces. Last, we use image-to-image translation networks to further refinedetailed geometry and texture on the reconstructed surface, which improves thefidelity and consistency across arbitrary viewpoints. In the experiments, wedemonstrate improvements over the state of the art on both public benchmarksand user studies in reconstruction quality and realism.},
  YEAR = {2021},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2108.07845v1},
  FILE = {2108.07845v1.pdf}
 }

@article{huang2020arch,
  AUTHOR = {Zeng Huang and Yuanlu Xu and Christoph Lassner and Hao Li and Tony Tung},
  TITLE = {ARCH: Animatable Reconstruction of Clothed Humans},
  EPRINT = {2004.04572v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {In this paper, we propose ARCH (Animatable Reconstruction of Clothed Humans),a novel end-to-end framework for accurate reconstruction of animation-ready 3Dclothed humans from a monocular image. Existing approaches to digitize 3Dhumans struggle to handle pose variations and recover details. Also, they donot produce models that are animation ready. In contrast, ARCH is a learnedpose-aware model that produces detailed 3D rigged full-body human avatars froma single unconstrained RGB image. A Semantic Space and a Semantic DeformationField are created using a parametric 3D body estimator. They allow thetransformation of 2D/3D clothed humans into a canonical space, reducingambiguities in geometry caused by pose variations and occlusions in trainingdata. Detailed surface geometry and appearance are learned using an implicitfunction representation with spatial local features. Furthermore, we proposeadditional per-pixel supervision on the 3D reconstruction using opacity-awaredifferentiable rendering. Our experiments indicate that ARCH increases thefidelity of the reconstructed humans. We obtain more than 50% lowerreconstruction errors for standard metrics compared to state-of-the-art methodson public datasets. We also show numerous qualitative examples of animated,high-quality reconstructed avatars unseen in the literature so far.},
  YEAR = {2020},
  MONTH = {Apr},
  URL = {http://arxiv.org/abs/2004.04572v2},
  FILE = {2004.04572v2.pdf}
 }

@article{shen2021snerf,
  AUTHOR = {Jianxiong Shen and Adria Ruiz and Antonio Agudo and Francesc Moreno},
  TITLE = {Stochastic Neural Radiance Fields:Quantifying Uncertainty in Implicit 3DRepresentations},
  EPRINT = {2109.02123v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Neural Radiance Fields (NeRF) has become a popular framework for learningimplicit 3D representations and addressing different tasks such as novel-viewsynthesis or depth-map estimation. However, in downstream applications wheredecisions need to be made based on automatic predictions, it is critical toleverage the confidence associated with the model estimations. Whereasuncertainty quantification is a long-standing problem in Machine Learning, ithas been largely overlooked in the recent NeRF literature. In this context, wepropose Stochastic Neural Radiance Fields (S-NeRF), a generalization ofstandard NeRF that learns a probability distribution over all the possibleradiance fields modeling the scene. This distribution allows to quantify theuncertainty associated with the scene information provided by the model. S-NeRFoptimization is posed as a Bayesian learning problem which is efficientlyaddressed using the Variational Inference framework. Exhaustive experimentsover benchmark datasets demonstrate that S-NeRF is able to provide morereliable predictions and confidence values than generic approaches previouslyproposed for uncertainty estimation in other domains.},
  YEAR = {2021},
  MONTH = {Sep},
  URL = {http://arxiv.org/abs/2109.02123v1},
  FILE = {2109.02123v1.pdf}
 }

@article{chitta2021neat,
  AUTHOR = {Kashyap Chitta and Aditya Prakash and Andreas Geiger},
  TITLE = {NEAT: Neural Attention Fields for End-to-End Autonomous Driving},
  EPRINT = {2109.04456v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Efficient reasoning about the semantic, spatial, and temporal structure of ascene is a crucial prerequisite for autonomous driving. We present NEuralATtention fields (NEAT), a novel representation that enables such reasoning forend-to-end imitation learning models. NEAT is a continuous function which mapslocations in Bird's Eye View (BEV) scene coordinates to waypoints andsemantics, using intermediate attention maps to iteratively compresshigh-dimensional 2D image features into a compact representation. This allowsour model to selectively attend to relevant regions in the input while ignoringinformation irrelevant to the driving task, effectively associating the imageswith the BEV representation. In a new evaluation setting involving adverseenvironmental conditions and challenging scenarios, NEAT outperforms severalstrong baselines and achieves driving scores on par with the privileged CARLAexpert used to generate its training data. Furthermore, visualizing theattention maps for models with NEAT intermediate representations providesimproved interpretability.},
  YEAR = {2021},
  MONTH = {Sep},
  URL = {http://arxiv.org/abs/2109.04456v1},
  FILE = {2109.04456v1.pdf}
 }

@article{sun2021nelf,
  AUTHOR = {Tiancheng Sun and Kai-En Lin and Sai Bi and Zexiang Xu and Ravi Ramamoorthi},
  TITLE = {NeLF: Neural Light-transport Field for Portrait View Synthesis andRelighting},
  EPRINT = {2107.12351v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Human portraits exhibit various appearances when observed from differentviews under different lighting conditions. We can easily imagine how the facewill look like in another setup, but computer algorithms still fail on thisproblem given limited observations. To this end, we present a system forportrait view synthesis and relighting: given multiple portraits, we use aneural network to predict the light-transport field in 3D space, and from thepredicted Neural Light-transport Field (NeLF) produce a portrait from a newcamera view under a new environmental lighting. Our system is trained on alarge number of synthetic models, and can generalize to different synthetic andreal portraits under various lighting conditions. Our method achievessimultaneous view synthesis and relighting given multi-view portraits as theinput, and achieves state-of-the-art results.},
  YEAR = {2021},
  MONTH = {Jul},
  URL = {http://arxiv.org/abs/2107.12351v1},
  FILE = {2107.12351v1.pdf}
 }

@article{yang2021learning,
  AUTHOR = {Bangbang Yang and Yinda Zhang and Yinghao Xu and Yijin Li and Han Zhou and Hujun Bao and Guofeng Zhang and Zhaopeng Cui},
  TITLE = {Learning Object-Compositional Neural Radiance Field for Editable SceneRendering},
  EPRINT = {2109.01847v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicit neural rendering techniques have shown promising results for novelview synthesis. However, existing methods usually encode the entire scene as awhole, which is generally not aware of the object identity and limits theability to the high-level editing tasks such as moving or adding furniture. Inthis paper, we present a novel neural scene rendering system, which learns anobject-compositional neural radiance field and produces realistic renderingwith editing capability for a clustered and real-world scene. Specifically, wedesign a novel two-pathway architecture, in which the scene branch encodes thescene geometry and appearance, and the object branch encodes each standaloneobject conditioned on learnable object activation codes. To survive thetraining in heavily cluttered scenes, we propose a scene-guided trainingstrategy to solve the 3D space ambiguity in the occluded regions and learnsharp boundaries for each object. Extensive experiments demonstrate that oursystem not only achieves competitive performance for static scene novel-viewsynthesis, but also produces realistic rendering for object-level editing.},
  YEAR = {2021},
  MONTH = {Sep},
  URL = {http://arxiv.org/abs/2109.01847v1},
  FILE = {2109.01847v1.pdf}
 }

@article{raissi2019physics,
  ABSTRACT = {We introduce physics-informed neural networks-neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and},
  AUTHOR = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},
  JOURNAL = {Journal of Computational Physics},
  PAGES = {686--707},
  PUB_YEAR = {2019},
  PUBLISHER = {Elsevier},
  TITLE = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  VENUE = {Journal of Computational Physics},
  VOLUME = {378}
 }

@article{jang2021codenerf,
  AUTHOR = {Wonbong Jang and Lourdes Agapito},
  TITLE = {CodeNeRF: Disentangled Neural Radiance Fields for Object Categories},
  EPRINT = {2109.01750v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.GR},
  ABSTRACT = {CodeNeRF is an implicit 3D neural representation that learns the variation ofobject shapes and textures across a category and can be trained, from a set ofposed images, to synthesize novel views of unseen objects. Unlike the originalNeRF, which is scene specific, CodeNeRF learns to disentangle shape and textureby learning separate embeddings. At test time, given a single unposed image ofan unseen object, CodeNeRF jointly estimates camera viewpoint, and shape andappearance codes via optimization. Unseen objects can be reconstructed from asingle image, and then rendered from new viewpoints or their shape and textureedited by varying the latent codes. We conduct experiments on the SRNbenchmark, which show that CodeNeRF generalises well to unseen objects andachieves on-par performance with methods that require known camera pose at testtime. Our results on real-world images demonstrate that CodeNeRF can bridge thesim-to-real gap. Project page: \url{https://github.com/wayne1123/code-nerf}},
  YEAR = {2021},
  MONTH = {Sep},
  URL = {http://arxiv.org/abs/2109.01750v1},
  FILE = {2109.01750v1.pdf}
 }

@article{chen2021mdif,
  AUTHOR = {Zhang Chen and Yinda Zhang and Kyle Genova and Sean Fanello and Sofien Bouaziz and Christian Haene and Ruofei Du and Cem Keskin and Thomas Funkhouser and Danhang Tang},
  TITLE = {Multiresolution Deep Implicit Functions for 3D Shape Representation},
  EPRINT = {2109.05591v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We introduce Multiresolution Deep Implicit Functions (MDIF), a hierarchicalrepresentation that can recover fine geometry detail, while being able toperform global operations such as shape completion. Our model represents acomplex 3D shape with a hierarchy of latent grids, which can be decoded intodifferent levels of detail and also achieve better accuracy. For shapecompletion, we propose latent grid dropout to simulate partial data in thelatent space and therefore defer the completing functionality to the decoderside. This along with our multires design significantly improves the shapecompletion quality under decoder-only latent optimization. To the best of ourknowledge, MDIF is the first deep implicit function model that can at the sametime (1) represent different levels of detail and allow progressive decoding;(2) support both encoder-decoder inference and decoder-only latentoptimization, and fulfill multiple applications; (3) perform detaileddecoder-only shape completion. Experiments demonstrate its superior performanceagainst prior art in various 3D reconstruction tasks.},
  YEAR = {2021},
  MONTH = {Sep},
  URL = {http://arxiv.org/abs/2109.05591v2},
  FILE = {2109.05591v2.pdf}
 }

@article{albahar2021pose with style,
  AUTHOR = {Badour AlBahar and Jingwan Lu and Jimei Yang and Zhixin Shu and Eli Shechtman and Jia-Bin Huang},
  TITLE = {Pose with Style: Detail-Preserving Pose-Guided Image Synthesis withConditional StyleGAN},
  EPRINT = {2109.06166v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We present an algorithm for re-rendering a person from a single image underarbitrary poses. Existing methods often have difficulties in hallucinatingoccluded contents photo-realistically while preserving the identity and finedetails in the source image. We first learn to inpaint the correspondence fieldbetween the body surface texture and the source image with a human bodysymmetry prior. The inpainted correspondence field allows us to transfer/warplocal features extracted from the source to the target view even under largepose changes. Directly mapping the warped local features to an RGB image usinga simple CNN decoder often leads to visible artifacts. Thus, we extend theStyleGAN generator so that it takes pose as input (for controlling poses) andintroduces a spatially varying modulation for the latent space using the warpedlocal features (for controlling appearances). We show that our method comparesfavorably against the state-of-the-art algorithms in both quantitativeevaluation and visual comparison.},
  YEAR = {2021},
  MONTH = {Sep},
  URL = {http://arxiv.org/abs/2109.06166v1},
  FILE = {2109.06166v1.pdf}
 }

@article{mandlneural,
  ABSTRACT = {Coherent rendering is important for generating plausible Mixed Reality presentations of virtual objects within a user's real-world environment. Besides photo-realistic rendering and correct lighting, visual coherence requires simulating the imaging system that is used to capture the real environment. While existing approaches either focus on a specific camera or a specific component of the imaging system, we introduce Neural Cameras, the first approach that jointly simulates all major components of an arbitrary modern camera using},
  AUTHOR = {Mandl, David and Mohr, Peter and Langlotz, Tobias and Ebner, Christoph and Mori, Shohei and Zollmann, Stefanie and Roth, Peter M and Kalkofen, Denis},
  PUB_YEAR = {NA},
  TITLE = {Neural Cameras: Learning Camera Characteristics for Coherent Mixed Reality Rendering},
  VENUE = {NA}
 }

@article{wei2021nerfingmvs,
  AUTHOR = {Yi Wei and Shaohui Liu and Yongming Rao and Wang Zhao and Jiwen Lu and Jie Zhou},
  TITLE = {NerfingMVS: Guided Optimization of Neural Radiance Fields for IndoorMulti-view Stereo},
  EPRINT = {2109.01129v2},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {In this work, we present a new multi-view depth estimation method thatutilizes both conventional SfM reconstruction and learning-based priors overthe recently proposed neural radiance fields (NeRF). Unlike existing neuralnetwork based optimization method that relies on estimated correspondences, ourmethod directly optimizes over implicit volumes, eliminating the challengingstep of matching pixels in indoor scenes. The key to our approach is to utilizethe learning-based priors to guide the optimization process of NeRF. Our systemfirstly adapts a monocular depth network over the target scene by finetuning onits sparse SfM reconstruction. Then, we show that the shape-radiance ambiguityof NeRF still exists in indoor environments and propose to address the issue byemploying the adapted depth priors to monitor the sampling process of volumerendering. Finally, a per-pixel confidence map acquired by error computation onthe rendered image can be used to further improve the depth quality.Experiments show that our proposed framework significantly outperformsstate-of-the-art methods on indoor scenes, with surprising findings presentedon the effectiveness of correspondence-based optimization and NeRF-basedoptimization over the adapted depth priors. In addition, we show that theguided optimization scheme does not sacrifice the original synthesis capabilityof neural radiance fields, improving the rendering quality on both seen andnovel views. Code is available at https://github.com/weiyithu/NerfingMVS.},
  YEAR = {2021},
  MONTH = {Sep},
  URL = {http://arxiv.org/abs/2109.01129v2},
  FILE = {2109.01129v2.pdf}
 }

@article{smith2020eikonet,
  AUTHOR = {Jonathan D. Smith and Kamyar Azizzadenesheli and Zachary E. Ross},
  TITLE = {EikoNet: Solving the Eikonal equation with Deep Neural Networks},
  EPRINT = {2004.00361v3},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {physics.comp-ph},
  ABSTRACT = {The recent deep learning revolution has created an enormous opportunity foraccelerating compute capabilities in the context of physics-based simulations.Here, we propose EikoNet, a deep learning approach to solving the Eikonalequation, which characterizes the first-arrival-time field in heterogeneous 3Dvelocity structures. Our grid-free approach allows for rapid determination ofthe travel time between any two points within a continuous 3D domain. Thesetravel time solutions are allowed to violate the differential equation - whichcasts the problem as one of optimization - with the goal of finding networkparameters that minimize the degree to which the equation is violated. In doingso, the method exploits the differentiability of neural networks to calculatethe spatial gradients analytically, meaning the network can be trained on itsown without ever needing solutions from a finite difference algorithm. EikoNetis rigorously tested on several velocity models and sampling methods todemonstrate robustness and versatility. Training and inference are highlyparallelized, making the approach well-suited for GPUs. EikoNet has low memoryoverhead, and further avoids the need for travel-time lookup tables. Thedeveloped approach has important applications to earthquake hypocenterinversion, ray multi-pathing, and tomographic modeling, as well as to otherfields beyond seismology where ray tracing is essential.},
  YEAR = {2020},
  MONTH = {Mar},
  URL = {http://arxiv.org/abs/2004.00361v3},
  FILE = {2004.00361v3.pdf}
 }

@inproceedings{huang2021modified,
  ABSTRACT = {Recently developed physics-informed neural network (PINN) for solving for the scattered wavefield in the Helmholtz equation showed large potential in seismic modeling because of its flexibility, low memory requirement, and no limitations on the shape of the solution space. However, the predicted solutions were somewhat smooth and the convergence of the training was slow. Thus, we propose a modified PINN using sinusoidal activation functions and positional encoding, aiming to accelerate the convergence and fit better. We transform},
  AUTHOR = {Huang, Xinquan and Alkhalifah, Tariq and Song, Chao},
  BOOKTITLE = {First International Meeting for Applied Geoscience \& Energy},
  ORGANIZATION = {Society of Exploration Geophysicists},
  PAGES = {2480--2484},
  PUB_YEAR = {2021},
  TITLE = {A modified physics-informed neural network with positional encoding},
  VENUE = {First International Meeting for ...}
 }

@article{benbarka2021seeing,
  AUTHOR = {Nuri Benbarka and Timon Hofer and Hamd ul-moqeet Riaz and Andreas Zell},
  TITLE = {Seeing Implicit Neural Representations as Fourier Series},
  EPRINT = {2109.00249v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Implicit Neural Representations (INR) use multilayer perceptrons to representhigh-frequency functions in low-dimensional problem domains. Recently theserepresentations achieved state-of-the-art results on tasks related to complex3D objects and scenes. A core problem is the representation of highly detailedsignals, which is tackled using networks with periodic activation functions(SIRENs) or applying Fourier mappings to the input. This work analyzes theconnection between the two methods and shows that a Fourier mapped perceptronis structurally like one hidden layer SIREN. Furthermore, we identify therelationship between the previously proposed Fourier mapping and the generald-dimensional Fourier series, leading to an integer lattice mapping. Moreover,we modify a progressive training strategy to work on arbitrary Fourier mappingsand show that it improves the generalization of the interpolation task. Lastly,we compare the different mappings on the image regression and novel viewsynthesis tasks. We confirm the previous finding that the main contributor tothe mapping performance is the size of the embedding and standard deviation ofits elements.},
  YEAR = {2021},
  MONTH = {Sep},
  URL = {http://arxiv.org/abs/2109.00249v1},
  FILE = {2109.00249v1.pdf}
 }

@article{zang2021intratomo,
  ABSTRACT = {We propose IntraTomo, a powerful framework that combines the benefits of learning-based and model-based approaches for solving highly ill-posed inverse problems, in the Computed Tomography (CT) context. IntraTomo is composed of two core modules: a novel sinogram prediction module and a geometry refinement module, which are applied iteratively. In the first module, the unknown density field is represented as a continuous and differentiable function, parameterized by a deep neural network. This network is learned, in},
  AUTHOR = {Zang, Guangming and Idoughi, Ramzi and Li, Rui and Wonka, Peter and Heidrich, Wolfgang},
  PUB_YEAR = {2021},
  PUBLISHER = {IEEE},
  TITLE = {IntraTomo: Self-supervised Learning-based Tomography via Sinogram Synthesis and Prediction},
  VENUE = {NA}
 }

@article{reizenstein2021co3d,
  AUTHOR = {Jeremy Reizenstein and Roman Shapovalov and Philipp Henzler and Luca Sbordone and Patrick Labatut and David Novotny},
  TITLE = {Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life3D Category Reconstruction},
  EPRINT = {2109.00512v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Traditional approaches for learning 3D object categories have beenpredominantly trained and evaluated on synthetic datasets due to theunavailability of real 3D-annotated category-centric data. Our main goal is tofacilitate advances in this field by collecting real-world data in a magnitudesimilar to the existing synthetic counterparts. The principal contribution ofthis work is thus a large-scale dataset, called Common Objects in 3D, with realmulti-view images of object categories annotated with camera poses and groundtruth 3D point clouds. The dataset contains a total of 1.5 million frames fromnearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such,it is significantly larger than alternatives both in terms of the number ofcategories and objects. We exploit this new dataset to conduct one of the firstlarge-scale "in-the-wild" evaluations of several new-view-synthesis andcategory-centric 3D reconstruction methods. Finally, we contribute NerFormer -a novel neural rendering method that leverages the powerful Transformer toreconstruct an object given a small number of its views. The CO3D dataset isavailable at https://github.com/facebookresearch/co3d .},
  YEAR = {2021},
  MONTH = {Sep},
  NOTE = {International Conference on Computer Vision, 2021},
  URL = {http://arxiv.org/abs/2109.00512v1},
  FILE = {2109.00512v1.pdf}
 }

@article{lei2020pix2surf,
  AUTHOR = {Jiahui Lei and Srinath Sridhar and Paul Guerrero and Minhyuk Sung and Niloy Mitra and Leonidas J. Guibas},
  TITLE = {Pix2Surf: Learning Parametric 3D Surface Models of Objects from Images},
  EPRINT = {2008.07760v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {We investigate the problem of learning to generate 3D parametric surfacerepresentations for novel object instances, as seen from one or more views.Previous work on learning shape reconstruction from multiple views usesdiscrete representations such as point clouds or voxels, while continuoussurface generation approaches lack multi-view consistency. We address theseissues by designing neural networks capable of generating high-qualityparametric 3D surfaces which are also consistent between views. Furthermore,the generated 3D surfaces preserve accurate image pixel to 3D surface pointcorrespondences, allowing us to lift texture information to reconstruct shapeswith rich geometry and appearance. Our method is supervised and trained on apublic dataset of shapes from common object categories. Quantitative resultsindicate that our method significantly outperforms previous work, whilequalitative results demonstrate the high quality of our reconstructions.},
  YEAR = {2020},
  MONTH = {Aug},
  URL = {http://arxiv.org/abs/2008.07760v1},
  FILE = {2008.07760v1.pdf}
 }

@article{liunsupervised,
  ABSTRACT = {Many computer vision problems face difficulties when imaging through turbulent refractive media (eg, air and water) due to the refraction and scattering of light. These effects cause geometric distortion that requires either handcrafted physical priors or supervised learning methods to remove. In this paper, we present a novel unsupervised network to recover the latent distortion-free image. The key idea is to model non-rigid distortions as deformable grids. Our network consists of a grid deformer that estimates the distortion field and an image},
  AUTHOR = {Li, Nianyi and Thapa, Simron and Whyte, Cameron and Reed, Albert and Jayasuriya, Suren and Ye, Jinwei},
  PUB_YEAR = {NA},
  TITLE = {Unsupervised Non-Rigid Image Distortion Removal via Grid Deformation},
  VENUE = {NA}
 }

@article{murphy2021implicitpdf,
  AUTHOR = {Kieran Murphy and Carlos Esteves and Varun Jampani and Srikumar Ramalingam and Ameesh Makadia},
  TITLE = {Implicit-PDF: Non-Parametric Representation of Probability Distributionson the Rotation Manifold},
  EPRINT = {2106.05965v1},
  ARCHIVEPREFIX = {arXiv},
  PRIMARYCLASS = {cs.CV},
  ABSTRACT = {Single image pose estimation is a fundamental problem in many vision androbotics tasks, and existing deep learning approaches suffer by not completelymodeling and handling: i) uncertainty about the predictions, and ii) symmetricobjects with multiple (sometimes infinite) correct poses. To this end, weintroduce a method to estimate arbitrary, non-parametric distributions onSO(3). Our key idea is to represent the distributions implicitly, with a neuralnetwork that estimates the probability given the input image and a candidatepose. Grid sampling or gradient ascent can be used to find the most likelypose, but it is also possible to evaluate the probability at any pose, enablingreasoning about symmetries and uncertainty. This is the most general way ofrepresenting distributions on manifolds, and to showcase the rich expressivepower, we introduce a dataset of challenging symmetric and nearly-symmetricobjects. We require no supervision on pose uncertainty -- the model trains onlywith a single pose per example. Nonetheless, our implicit model is highlyexpressive to handle complex distributions over 3D poses, while still obtainingaccurate pose estimation on standard non-ambiguous environments, achievingstate-of-the-art performance on Pascal3D+ and ModelNet10-SO(3) benchmarks.},
  YEAR = {2021},
  MONTH = {Jun},
  URL = {http://arxiv.org/abs/2106.05965v1},
  FILE = {2106.05965v1.pdf}
 }

